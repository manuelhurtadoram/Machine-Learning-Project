{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "\n",
    "In this capstone project, we will be training and testing an MLP on the UCI Bank Marketing Data Set in order to predict whether calls will end in a sale based off of the data for each client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bank-full.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of labels\n",
    "\n",
    "#### Client Data\n",
    "1. age (numeric)\n",
    "2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5. default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6. balance: balance in account at the time of last call (int)\n",
    "7. housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "8. loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "#### Related with the last contact of the current campaign:\n",
    "9. contact: contact communication type (categorical: 'cellular','telephone')\n",
    "10. day: day of the month in which contact was made (int)\n",
    "11. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "12. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "#### Other attributes:\n",
    "13. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "14. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "15. previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "16. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "\n",
    "#### Target variable:\n",
    "17. y: has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the labels, the first one that we can immediately rule out is 'duration' because it refers to the duration of the last call (the call that yielded the target variable). Therefore, for our purpose of creating a realistic predictive model, this label is pretty useless (call duration cannot be known beforehand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'duration' column from dataset\n",
    "df = df.drop('duration', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We will now explore the data a bit, in order to better understand what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 16 columns):\n",
      "age          45211 non-null int64\n",
      "job          45211 non-null object\n",
      "marital      45211 non-null object\n",
      "education    45211 non-null object\n",
      "default      45211 non-null object\n",
      "balance      45211 non-null int64\n",
      "housing      45211 non-null object\n",
      "loan         45211 non-null object\n",
      "contact      45211 non-null object\n",
      "day          45211 non-null int64\n",
      "month        45211 non-null object\n",
      "campaign     45211 non-null int64\n",
      "pdays        45211 non-null int64\n",
      "previous     45211 non-null int64\n",
      "poutcome     45211 non-null object\n",
      "y            45211 non-null object\n",
      "dtypes: int64(6), object(10)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# basic data info to check for missing values and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may         1     -1         0  unknown  no  \n",
       "1  unknown    5   may         1     -1         0  unknown  no  \n",
       "2  unknown    5   may         1     -1         0  unknown  no  \n",
       "3  unknown    5   may         1     -1         0  unknown  no  \n",
       "4  unknown    5   may         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick view of the data \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.936210</td>\n",
       "      <td>1362.272058</td>\n",
       "      <td>15.806419</td>\n",
       "      <td>2.763841</td>\n",
       "      <td>40.197828</td>\n",
       "      <td>0.580323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.618762</td>\n",
       "      <td>3044.765829</td>\n",
       "      <td>8.322476</td>\n",
       "      <td>3.098021</td>\n",
       "      <td>100.128746</td>\n",
       "      <td>2.303441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>-8019.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>95.000000</td>\n",
       "      <td>102127.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>275.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        balance           day      campaign         pdays  \\\n",
       "count  45211.000000   45211.000000  45211.000000  45211.000000  45211.000000   \n",
       "mean      40.936210    1362.272058     15.806419      2.763841     40.197828   \n",
       "std       10.618762    3044.765829      8.322476      3.098021    100.128746   \n",
       "min       18.000000   -8019.000000      1.000000      1.000000     -1.000000   \n",
       "25%       33.000000      72.000000      8.000000      1.000000     -1.000000   \n",
       "50%       39.000000     448.000000     16.000000      2.000000     -1.000000   \n",
       "75%       48.000000    1428.000000     21.000000      3.000000     -1.000000   \n",
       "max       95.000000  102127.000000     31.000000     63.000000    871.000000   \n",
       "\n",
       "           previous  \n",
       "count  45211.000000  \n",
       "mean       0.580323  \n",
       "std        2.303441  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max      275.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple statistics of the data's numeric features\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     39922\n",
       "yes     5289\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check target variable value counts\n",
    "df.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is evidently skewed in favor of negative outcomes. Let's check the proportion of this imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of \"yes\" to \"no\" is: 1:7.5\n"
     ]
    }
   ],
   "source": [
    "print('Ratio of \"yes\" to \"no\" is:', '1:{:.1f}'.format(df.y.value_counts()[0]/df.y.value_counts()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Features: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference purposes, let's separate the numerical columns from the categorical ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numerical and categorical labels\n",
    "numerical = ['age', 'balance', 'campaign', 'pdays', 'previous']\n",
    "categorical = ['job', 'day', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming 'Day' Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two arrays above one can infer that the feature 'day'has not been included in the numeical array but rather the categorical one. This is because, although it is a variable with data in numerical form (int64), it really isn't a feature from which one can extract meaning by measuring a range or something like it. In fact, its distribution does not resemble a normal distribution, like that of other variables, because calls could happen in any day of the month. Therefore, rather than a numerical variable, this feature resembles a categorical variable. \n",
    "\n",
    "Since the 1st, 3rd, 7th, 15th, etc of each month could fall on any day of the week, we cannot infer a lot from the categorical values of this feature. Another complication that this variable presents is that it represents 31 different categories (one for each possible day of the month). If we attempted to combine the month in which the call was made with the day, we could maybe extract more information from this feature, but this would result in 12 x 31 different possible categories! This just makes it worse!\n",
    "\n",
    "How, then, could we attempt to extract more meaning from such a seemingly meaningless feature? A pragmatical answer is to break down the days of the month into groups that convey the most information in the context of this problem. We are dealing with a bank dataset, and so the underlying mechanics of the services offered and accepted/rejected in each call revolve around money. Could it be that maybe people feel more inclined towards accepting the services offered by the bank when they have just been paid? In most of the world, salaries are paid on a semi-monthly basis (on the 1st and 15th day of each month). Therefore, if we maybe replaced the day the call was made with which week of the month (1, 2, 3, or 4) it was made in, we theoretically could draw a more latent pattern between weeks 1, 3 and 'yes' (salaries paid during these weeks, so people feel 'wealthier'), and weeks 2, 4 and 'no' (people have not yet been paid).\n",
    "\n",
    "Let's modify the 'day' column by applying a function that maps each numeric day to its corresponding week (NOTE: Week 4 is numerically longer, but for the purposes outlined above (salary paid on the 1st and 15th), this is not an issue):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mapping function\n",
    "def day2week_mapper(day):\n",
    "    if 1 <= day and day <= 7:\n",
    "        return 1\n",
    "    elif 8 <= day and day <= 14:\n",
    "        return 2\n",
    "    elif 15 <= day and day <= 21:\n",
    "        return 3\n",
    "    elif 22 <= day and day <= 31:\n",
    "        return 4\n",
    "    else: return 0 #unnecessary but it closes the if-statement\n",
    "\n",
    "# apply mapping   \n",
    "df['day'] = df['day'].apply(day2week_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    13900\n",
       "4    11071\n",
       "2    10442\n",
       "1     9798\n",
       "Name: day, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check results\n",
    "df.day.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change categorical features to categorical type\n",
    "\n",
    "Categorical types are more useful than 'object' (or generalized strings) for the purposes of this project, because they occupy less space in memory and allow the features to be used by other python libraries for analysis (ex. keras.utils.to_categorical):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply func\n",
    "df[categorical] = df[categorical].apply(lambda x: x.astype('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 16 columns):\n",
      "age          45211 non-null int64\n",
      "job          45211 non-null category\n",
      "marital      45211 non-null category\n",
      "education    45211 non-null category\n",
      "default      45211 non-null category\n",
      "balance      45211 non-null int64\n",
      "housing      45211 non-null category\n",
      "loan         45211 non-null category\n",
      "contact      45211 non-null category\n",
      "day          45211 non-null category\n",
      "month        45211 non-null category\n",
      "campaign     45211 non-null int64\n",
      "pdays        45211 non-null int64\n",
      "previous     45211 non-null int64\n",
      "poutcome     45211 non-null category\n",
      "y            45211 non-null category\n",
      "dtypes: category(11), int64(5)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is relevant to note that with these columns as type 'object', the df occupied 5.3+ MB in memory, while now its size has been reduced by nearly 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examination of Numerical Features: Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by examining how the numerical features' values are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple pipeline method to facilitate analysis\n",
    "def interpret(feature, kind, minmax):\n",
    "    print('\\n\\n', feature)\n",
    "    df[feature].plot(kind=kind)\n",
    "    plt.show()\n",
    "    if minmax:\n",
    "        print('Min:', df[feature].min())\n",
    "        print('Max:', df[feature].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " age\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 18\n",
      "Max: 95\n",
      "\n",
      "\n",
      " balance\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -8019\n",
      "Max: 102127\n",
      "\n",
      "\n",
      " campaign\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1\n",
      "Max: 63\n",
      "\n",
      "\n",
      " pdays\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -1\n",
      "Max: 871\n",
      "\n",
      "\n",
      " previous\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Max: 275\n"
     ]
    }
   ],
   "source": [
    "# plot histograms for all of the numerical variables to see distribution\n",
    "for feature in numerical:\n",
    "    interpret(feature, 'hist', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, it is evident that many of the variables in the dataset are heavily skewed towards smaller values but present vastly larger values, too. Therefore, it is important to normalize these features, since the algorithms can be very sensitive to these distributions.\n",
    "\n",
    "- Age: Needs normalization to improve performance. \n",
    "- Balance: Needs normalization to improve performance. \n",
    "- Campaign: Needs normalization to improve performance.\n",
    "- Pdays: Needs normalization to improve performance.\n",
    "- Previous: Needs normalization to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an examination of the dataset description online (included at the beginning of the notebook), we can draw some conclusions about the numerical features:\n",
    "\n",
    "- Age: Presents no abnormalities. \n",
    "- Balance: Presents no abnormalities. \n",
    "- Campaign: This refers to the number of calls during the current campaign (including the last call). Because the last call is the call that yielded the target variable (y/n), for realistic prediction purposes, we must subtract 1 from all the values to remove this last call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract 1 to all the values in the 'campaign' column\n",
    "df['campaign'] = df['campaign'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pdays: This refers to the number of days that have elapsed since the last call was made to contact the client. Clients that were not previously contacted contain a value of -1. In the context of this problem, clients that were not previously contacted resemble clients that have been contacted again after a very long time. Therefore, it could make more sense to replace these -1 values with the greatest value of this feature (871), or even 999. This, however, would only result in a sort of bimodal distribution, and may even make all of these previously uncontacted clients outliers in the data. Therefore, it is more useful to first remove the outliers and then make this -1 value equal to, say, 2 times the max value.\n",
    "- Previous: Presents no abnormalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examination of numerical features: Identifying Outliers\n",
    "\n",
    "To examine the presence of outliers, we can plot box plots and density plots for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " age\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEN5JREFUeJzt3W+MXfV95/H3xx7AxA1hTKYWhVAjBZpENNB2FBGS/gsJm22bwm4iRFUhq0K2LKVOsgkirvdBtlJlgYSSrVS1limtrKgNIRBKEkvZIJNUYqOSmIRQwGlJCbQgwNPYTtNJ7TD42wdzzDqsJ/fOnzvX87vvl3R17zn3nLmfBzOf+el3zj0nVYUkaeVbNewAkqSlYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGjG2nB/22te+tjZs2LCcHylJK95DDz30r1U10Wu7ZS30DRs2sG/fvuX8SEla8ZI83c92TrlIUiMsdElqhIUuSY2w0CWpERa6JDXCQtdI27p1K2vWrCEJa9asYevWrcOOJC2Yha6RtXXrVnbu3MmOHTuYnp5mx44d7Ny501LXipXlvAXd5ORkeR66ThVr1qxhx44dfPjDH3553cc//nG2b9/OkSNHhphM+nFJHqqqyV7bOULXyDp69Cjj4+NccsklrF69mksuuYTx8XGOHj067GjSgizrN0WlU8nY2Bgf+chHuPvuu3n729/OAw88wHvf+17Gxvyz0Mrkb65G1llnncXBgwe56qqrmJmZYWxsjJmZGdatWzfsaNKCOOWikXXw4EGScPw4UlWRhIMHDw45mbQwFrpGVhK2bNnCzMwMVcXMzAxbtmwhybCjSQviWS4aWUlYu3YtP/rRj3jxxRc57bTTOP3005menmY5/y6kXjzLReph9erVTE9Pvzxnvm7dOqanp1m9evWQk0kLY6FrZFUVq1at4qabbmJ6epqbbrqJVatWOTrXimWha2QdO3aMTZs2sX37dtauXcv27dvZtGkTx44dG3Y0aUH6KvQkH0zyaJLHknyoW7cuyX1JnuiexwcbVVpaZ5xxBhdffDFHjhyhqjhy5AgXX3wxZ5xxxrCjSQvSs9CTXAJsAt4CXAr8VpLXA9uAvVV1EbC3W5ZWjE2bNnHjjTcyNjZGEsbGxrjxxhvZtGnTsKNJC9LPCP2NwINV9cOqmgH+FvjvwNXA7m6b3cA1g4koSepHP4X+KPDLSc5J8irgN4DXAeur6rlum+eB9QPKKA3Ebbfdxq233vpj56Hfeuut3HbbbcOOJi1Iz0Kvqv3ALcCXgC8CDwMvvWKbAk56akCSzUn2Jdk3NTW1+MTSEjl69Ch79uxh1apVJGHVqlXs2bPHi3NpxerroGhV3V5Vv1RVvwIcAv4ReCHJuQDd84E59t1VVZNVNTkxMbFUuaVFS8L999/Pli1bOHz4MFu2bOH+++/3m6Jasfr6pmiSn66qA0kuYHakfjnwP4HvVdXNSbYB66rqpp/0c/ymqE4lx4v7+EW5jj8DnouuU0q/3xTt92qLdyc5B3gReH9VHU5yM3BnkhuAp4FrFx5XGp4TL84lrWT9Trn8clW9qaouraq93brvVdWVVXVRVb2zqrxEnVacK6644scOil5xxRXDjiQtmNdD10j76le/6py5muFX/zWy5ipyC14rlYWukTXXnLlz6VqpLHRJaoSFrpG2evVqqurlh9dC10rmQVGNtJdeesk5czXDEbokNcJCl6RGWOiS1AgLXSPt+D1Ejz9WrfJPQiuXB0U10o4dO+ZBUTXD4YgkNcJC18gbHx/nkUceYXzc+5xrZXPKRSPv0KFDvPnNbx52DGnRHKFLwB133DHsCNKiWegScN111w07grRoFrokNaKvQk/yP5I8luTRJJ9KsibJhUkeTPKdJJ9Ocvqgw0qDcOJ56NJK1rPQk5wHfACYrKpLgNXAdcAtwCeq6vXAIeCGQQaVBiXJyw9pJet3ymUMODPJGPAq4DngHcBd3fu7gWuWPp4kqV89C72qngVuBf6Z2SL/PvAQcLiqZrrNngHOG1RISVJv/Uy5jANXAxcCPwOsBd7d7wck2ZxkX5J9U1NTCw4qSfrJ+plyeSfw3aqaqqoXgc8CbwPO7qZgAM4Hnj3ZzlW1q6omq2pyYmJiSUJLS8mDompFP4X+z8DlSV6V2aNGVwKPA18G3tdtsxG4dzARpcHyoKha0c8c+oPMHvz8BvD33T67gI8CH07yHeAc4PYB5pQk9dDXtVyq6mPAx16x+kngLUueSJK0IH5TVJIa4dUWNfJOPBjqPLpWMgtdI88SVyuccpGkRljoEnD99dcPO4K0aBa6BHzyk58cdgRp0Sx0CdiyZcuwI0iLZqFLwM6dO4cdQVo0C12SGuFpixp5noeuVljoGnmWuFrhlIskNcJCl6RGWOiS1Ajn0DXyPCiqVljoGnmWuFrhlIskNaJnoSf5uSQPn/D4tyQfSrIuyX1Jnuiex5cjsDQI73nPe4YdQVq0fu4p+g9VdVlVXQb8EvBD4B5gG7C3qi4C9nbL0or0+c9/ftgRpEWb75TLlcA/VdXTwNXA7m79buCapQwmLZeqevkhrWTzPSh6HfCp7vX6qnque/08sP5kOyTZDGwGuOCCCxaSUZq3+RzoPNm2/e7vPwGdSvoeoSc5Hfht4DOvfK9mf6tP+ptdVbuqarKqJicmJhYcVJqPE0fdP+mxmH0tc51q5jPl8l+Bb1TVC93yC0nOBeieDyx1OGnQjhfzz370C5a0Vrz5FPrv8P+mWwA+B2zsXm8E7l2qUJKk+eur0JOsBd4FfPaE1TcD70ryBPDOblmSNCR9HRStqmngnFes+x6zZ71Ikk4BflNUkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktSIfu9YdHaSu5J8O8n+JG9Nsi7JfUme6J7HBx1WkjS3fkfofwx8sareAFwK7Ae2AXur6iJgb7csSRqSnoWe5DXArwC3A1TVj6rqMHA1sLvbbDdwzaBCSpJ662eEfiEwBfxlkm8m+fPuptHrq+q5bpvngfWDCilJ6q2fQh8DfhH4s6r6BWCaV0yvVFUBdbKdk2xOsi/JvqmpqcXmlSTNoZ9CfwZ4pqoe7JbvYrbgX0hyLkD3fOBkO1fVrqqarKrJiYmJpcgsSTqJnoVeVc8D/5Lk57pVVwKPA58DNnbrNgL3DiShJKkvY31utxX4qySnA08Cv8fsP4M7k9wAPA1cO5iIkqR+9FXoVfUwMHmSt65c2jiSpIXym6KS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUiL7uWJTkKeAHwEvATFVNJlkHfBrYADwFXFtVhwYTU5LUy3xG6L9eVZdV1fFb0W0D9lbVRcDeblmSNCSLmXK5Gtjdvd4NXLP4OJKkheq30Av4UpKHkmzu1q2vque6188D65c8nSSpb33NoQNvr6pnk/w0cF+Sb5/4ZlVVkjrZjt0/gM0AF1xwwaLCSpLm1tcIvaqe7Z4PAPcAbwFeSHIuQPd8YI59d1XVZFVNTkxMLE1qSdL/p2ehJ1mb5NXHXwNXAY8CnwM2dpttBO4dVEhJUm/9TLmsB+5Jcnz7v66qLyb5OnBnkhuAp4FrBxdTktRLz0KvqieBS0+y/nvAlYMIJUmav34PikpDc+kffonv/8eLA/+cDdv2DPTnv+bM0/jWx64a6GdotFnoOuV9/z9e5Kmbf3PYMRZt0P8wJK/lIkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiP6LvQkq5N8M8kXuuULkzyY5DtJPp3k9MHFlCT1Mp8R+geB/Scs3wJ8oqpeDxwCbljKYJKk+emr0JOcD/wm8OfdcoB3AHd1m+wGrhlEQElSf/odof9v4CbgWLd8DnC4qma65WeA8062Y5LNSfYl2Tc1NbWosJKkufUs9CS/BRyoqocW8gFVtauqJqtqcmJiYiE/QpLUh35uEv024LeT/AawBjgL+GPg7CRj3Sj9fODZwcWUJPXSs9Cr6g+APwBI8mvAjVX1u0k+A7wPuAPYCNw7wJwaYa9+4zZ+fve2YcdYtFe/EWYPRUmD0c8IfS4fBe5I8kfAN4HblyaS9ON+sP9mnrp55Rfhhm17hh1BjZtXoVfVV4CvdK+fBN6y9JEkSQvhN0UlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1IjFXG1RWjYtXKnwNWeeNuwIapyFrlPeclw6d8O2PU1colejzSkXSWqEhS5JjejnJtFrknwtybeSPJbkD7v1FyZ5MMl3knw6yemDjytJmks/I/SjwDuq6lLgMuDdSS4HbgE+UVWvBw4BNwwupiSpl56FXrP+vVs8rXsU8A7grm79buCagSSUJPWlrzn0JKuTPAwcAO4D/gk4XFUz3SbPAOcNJqIkqR99FXpVvVRVlwHnM3tj6Df0+wFJNifZl2Tf1NTUAmNKknqZ11kuVXUY+DLwVuDsJMfPYz8feHaOfXZV1WRVTU5MTCwqrCRpbv2c5TKR5Ozu9ZnAu4D9zBb7+7rNNgL3DiqkJKm3fr4pei6wO8lqZv8B3FlVX0jyOHBHkj8CvgncPsCckqQeehZ6VT0C/MJJ1j/J7Hy6JOkU4DdFJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqRH93FP0dUm+nOTxJI8l+WC3fl2S+5I80T2PDz6uJGku/YzQZ4CPVNWbgMuB9yd5E7AN2FtVFwF7u2VJ0pD0LPSqeq6qvtG9/gGwHzgPuBrY3W22G7hmUCElSb3Naw49yQZmbxj9ILC+qp7r3noeWL+kySRJ89J3oSf5KeBu4ENV9W8nvldVBdQc+21Osi/JvqmpqUWFlSTNra9CT3Ias2X+V1X12W71C0nO7d4/Fzhwsn2raldVTVbV5MTExFJkliSdRD9nuQS4HdhfVR8/4a3PARu71xuBe5c+niSpX2N9bPM24Hrg75M83K3bDtwM3JnkBuBp4NrBRJQk9aNnoVfVA0DmePvKpY0jSVqofkbo0oozO1M4z31umf/nzJ4PIJ0aLHQ1yaLVKPJaLpLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGZDm/gJFkitnrvkinmtcC/zrsENIcfraqel6udlkLXTpVJdlXVZPDziEthlMuktQIC12SGmGhS7N2DTuAtFjOoUtSIxyhS1IjLHRJaoSFLkmNsNA1MpL8TZKHkjyWZHO37oYk/5jka0luS/In3fqJJHcn+Xr3eNtw00u9eVBUIyPJuqo6mORM4OvAfwH+L/CLwA+A+4FvVdXvJ/lr4E+r6oEkFwD/p6reOLTwUh+8p6hGyQeS/Lfu9euA64G/raqDAEk+A1zcvf9O4E0n3Gz6rCQ/VVX/vpyBpfmw0DUSkvwasyX91qr6YZKvAN8G5hp1rwIur6ojy5NQWjzn0DUqXgMc6sr8DcDlwFrgV5OMJxkD3nvC9l8Cth5fSHLZsqaVFsBC16j4IjCWZD9wM/B3wLPADuBrzM6lPwV8v9v+A8BkkkeSPA5sWfbE0jx5UFQj7fi8eDdCvwf4i6q6Z9i5pIVwhK5R97+SPAw8CnwX+Jsh55EWzBG6JDXCEbokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqxH8CIYPa9aOah4oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 18\n",
      "Max: 95\n",
      "\n",
      "\n",
      " balance\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGKJJREFUeJzt3X9wXfWZ3/H3gyxLxpAAiYdJbajphNnKUSAElbDFxWizJaa7g/1HJoPpFDd4cMkPlW0ywy/NlP5YOYSs2V1Eg4fGSmCLRRiaIaYbShms0nEZEuQlwT9UFpMEsCGOwQlJhGUZ++kfOnJkx1iHe319dZX3a0Zzz33O99z73BlLH59zvufcyEwkSSrjpHo3IElqHIaGJKk0Q0OSVJqhIUkqzdCQJJVmaEiSSjM0JEmlGRqSpNIMDUlSaTPq3cDx9sEPfjDnz59f7zYkqaFs2rTpjcycM9m4aRca8+fPZ3BwsN5tSFJDiYiXy4zz8JQkqTRDQ5JUmqEhSSrN0JAklWZoSJJKMzSkGuvv76e9vZ2mpiba29vp7++vd0tSxabdlFtpKunv76e7u5u1a9eycOFCNm7cyIoVKwBYtmxZnbuT3ruYbl/32tHRkV6noamivb2d3t5eOjs7D9UGBgbo6upiy5YtdexMOlxEbMrMjsnGTXp4KiL6IuLnEbFlQu2MiHgiIl4sHk8v6hERd0XE9oh4PiI+PmGb5cX4FyNi+YT6hRGxudjmroiIY72H1EiGhoZYuHDhYbWFCxcyNDRUp46k6pQ5p/EtYPERtZuBJzPzXODJ4jnAFcC5xc9K4B4YCwDgNuATwEXAbRNC4B7gugnbLZ7kPaSG0dbWxsaNGw+rbdy4kba2tjp1JFVn0tDIzP8D7DmivAS4r1i+D1g6oX5/jnkGOC0iPgR8CngiM/dk5i+AJ4DFxbr3ZeYzOXac7P4jXuto7yE1jO7ublasWMHAwAD79+9nYGCAFStW0N3dXe/WpIpUeiL8zMx8vVj+GXBmsTwXeHXCuB1F7Vj1HUepH+s9pIYxfrK7q6uLoaEh2tra6Onp8SS4GlbVU26LPYSank2f7D0iYmVEDEbE4O7du2vZiiT9Xqs0NHYVh5YoHn9e1HcCZ00YN6+oHas+7yj1Y73H78jMezOzIzM75syZ9M6+0gkzPuW2t7eXkZERent76e7u9loNNaxKQ2M9MD4Dajnw3Qn1a4pZVBcDbxWHmB4HLo+I04sT4JcDjxfrfhURFxezpq454rWO9h5Sw+jp6WHt2rV0dnbS3NxMZ2cna9eupaenp96tSRWZ9DqNiOgHLgM+COxibBbUI8BDwNnAy8BnMnNP8Yf/bsZmQL0NfDYzB4vXuRa4tXjZnsz8ZlHvYGyG1izgMaArMzMiPnC095jsA3mdhqaSpqYmRkZGaG5uPlTbv38/ra2tHDhwoI6dSYcre53GpCfCM/Pdzth98ihjE/jCu7xOH9B3lPog0H6U+ptHew+pkYxPuZ14cZ9TbtXIvPeUVENOudV0472npBpyyq2mG+89JUk6fveekiRpnKEhSSrN0JAklWZoSJJKMzQkSaUZGpKk0gwNSVJphoYkqTRDQ5JUmqEhSSrN0JAklWZoSJJKMzQkSaUZGpKk0gwNSVJphoYkqTRDQ5JUmqEhSSrN0JAklWZoSJJKMzQkSaUZGpKk0gwNSVJphoZUY11dXbS2thIRtLa20tXVVe+WpIpVFRoR8e8iYmtEbImI/ohojYhzIuL7EbE9Ir4dETOLsS3F8+3F+vkTXueWov5CRHxqQn1xUdseETdX06tUD11dXaxZs4ZVq1YxPDzMqlWrWLNmjcGhxpWZFf0Ac4GfALOK5w8B/7p4vKqorQE+Vyx/HlhTLF8FfLtYXgD8CGgBzgFeApqKn5eAfwTMLMYsmKyvCy+8MKWpoqWlJVevXn1YbfXq1dnS0lKnjqSjAwazxN/+ag9PzQBmRcQM4GTgdeCPgIeL9fcBS4vlJcVzivWfjIgo6g9m5r7M/AmwHbio+NmemT/OzFHgwWKs1DD27dvH9ddff1jt+uuvZ9++fXXqSKpOxaGRmTuBvwBeYSws3gI2Ab/MzHeKYTsY2yOheHy12PadYvwHJtaP2Obd6lLDaGlpYc2aNYfV1qxZQ0tLS506kqpTcWhExOmM/c//HOAfALOBxcepr/fay8qIGIyIwd27d9ejBemorrvuOm666SbuvPNO3n77be68805uuukmrrvuunq3JlVkRhXb/jHwk8zcDRAR3wEuAU6LiBnF3sQ8YGcxfidwFrCjOJz1fuDNCfVxE7d5t/phMvNe4F6Ajo6OrOIzScdVb28vALfeeitf/vKXaWlp4frrrz9UlxpNNec0XgEujoiTi3MTnwS2AQPAp4sxy4HvFsvri+cU6zcUJ1/WA1cVs6vOAc4FfgA8C5xbzMaaydjJ8/VV9CvVRW9vLyMjI2QmIyMjBoYaWsV7Gpn5/Yh4GPg74B3gOcb+t/+3wIMR8edFbW2xyVrgbyJiO7CHsRAgM7dGxEOMBc47wBcy8wBARHwReJyxmVR9mbm10n4lSdWLsf/sTx8dHR05ODhY7zYkqaFExKbM7JhsnFeES5JKMzQkSaUZGpKk0gwNSVJphoYkqTRDQ5JUmqEhSSrN0JAklWZoSDXW399Pe3s7TU1NtLe309/fX++WpIpVc8NCSZPo7++nu7ubtWvXsnDhQjZu3MiKFSsAWLZsWZ27k947byMi1VB7ezu9vb10dnYeqg0MDNDV1cWWLVvq2Jl0uLK3ETE0pBpqampiZGSE5ubmQ7X9+/fT2trKgQMH6tiZdDjvPSVNAW1tbWzcuPGw2saNG2lra6tTR1J1DA2phrq7u1mxYgUDAwPs37+fgYEBVqxYQXd3d71bkyriiXCphsZPdnd1dTE0NERbWxs9PT2eBFfD8pyGJMlzGtJU0dXVRWtrKxFBa2srXV1d9W5JqpihIdVQV1cXa9asYdWqVQwPD7Nq1SrWrFljcKhheXhKqqHW1lZWrVrFl770pUO1O++8k1tvvZWRkZE6diYdzus0pCkgIhgeHubkk08+VHv77beZPXs20+13T42tbGg4e0qqoZaWFlauXMkPf/jDQ7OnPvaxj9HS0lLv1qSKeE5DqqFFixbxwAMPcOmll7Jnzx4uvfRSHnjgARYtWlTv1qSKGBpSDe3cuZOlS5fS19fHaaedRl9fH0uXLmXnzp31bk2qiIenpBoaGhriueeeO+q9p6RG5J6GVEPee0rTjaEh1ZD3ntJ04+EpqYa895Smm6r2NCLitIh4OCL+X0QMRcQfRsQZEfFERLxYPJ5ejI2IuCsitkfE8xHx8Qmvs7wY/2JELJ9QvzAiNhfb3BURUU2/Uj0sW7aMLVu2cODAAbZs2WJgqKFVe3jqr4H/mZn/GDgfGAJuBp7MzHOBJ4vnAFcA5xY/K4F7ACLiDOA24BPARcBt40FTjLluwnaLq+xXOuH8jnBNJxWHRkS8H7gUWAuQmaOZ+UtgCXBfMew+YGmxvAS4P8c8A5wWER8CPgU8kZl7MvMXwBPA4mLd+zLzmRy7dPb+Ca8lNYTx7wjv7e1lZGSE3t5euru7DQ41rGr2NM4BdgPfjIjnIuIbETEbODMzXy/G/Aw4s1ieC7w6YfsdRe1Y9R1HqUsNo6enh/PPP58rrriCmTNncsUVV3D++efT09NT79akilQTGjOAjwP3ZOYFwDC/PRQFQLGHUPMb7ETEyogYjIjB3bt31/rtpNK2bdvGo48+ethdbh999FG2bdtW79akilQTGjuAHZn5/eL5w4yFyK7i0BLF48+L9TuBsyZsP6+oHas+7yj135GZ92ZmR2Z2zJkzp4qPJB1/l112GX19fZx66qn09fVx2WWX1bslqWIVh0Zm/gx4NSL+oCh9EtgGrAfGZ0AtB75bLK8HrilmUV0MvFUcxnocuDwiTi9OgF8OPF6s+1VEXFzMmrpmwmtJDSEzeeqpp7j22mv59a9/zbXXXstTTz3lHW7VsKq6NXpEfAz4BjAT+DHwWcaC6CHgbOBl4DOZuaf4w383YzOg3gY+m5mDxetcC9xavGxPZn6zqHcA3wJmAY8BXTlJw94aXVPJSSedxIIFC9i+fTv79u2jpaWFD3/4w2zbto2DBw/Wuz3pEL9PQ5oCxi8t+tznPsdXvvIVbrnlFu655x4A9zY0pfgd4dIU0NLSwiWXXHLYXW4vueQSv09DDcvQkGpodHSU1157jccee4zR0VEee+wxXnvtNUZHR+vdmlQR7z0l1dCCBQtYunTpYfeeuvrqq3nkkUfq3ZpUEfc0pBrq7u5m3bp1h10Rvm7dOu9yq4blnoZUQ97lVtONs6ekGjvvvPPYvHnzoecf/ehHef755+vYkfS7nD0lTQHjgXHllVeye/durrzySjZv3sx5551X79akihgaUg1t3ryZCy64gJdeeokzzzyTl156iQsuuOCwPQ+pkRgaUo29+eabh50If/PNN+vdklQxQ0OqsXnz5tHZ2UlzczOdnZ3Mmzdv8o2kKcrQkGrs6aefZsmSJbzxxhssWbKEp59+ut4tSRVzyq1UQx/5yEd4++23Wb9+PeO37T/nnHM4+eST69yZVBn3NKQaGr+Ib8OGDYyOjrJhw4bD6lKjcU9DqiEv7tN048V9kiQv7pMkHX+GhiSpNENDqrH+/n7a29tpamqivb2d/v7+erckVczQkGqov7+fG264geHhYTKT4eFhbrjhBoNDDcvQkGroxhtvPPQtfePfFz46OsqNN95Yz7akihkaUg3t2LGD0dFRdu7cycGDB9m5cyejo6Ps2LGj3q1JFTE0pBrbt28ft99+O8PDw9x+++3s27ev3i1JFTM0pBqbOXMmvb29nHrqqfT29jJz5sx6tyRVzNCQamxkZIS9e/eSmezdu5eRkZF6tyRVzNCQauykk05i165dZCa7du3ipJP8tVPj8l+vVGMHDx48NHMqIjh48GCdO5IqZ2hINdbU1MT4Pd4yk6ampjp3JFWu6tCIiKaIeC4i/kfx/JyI+H5EbI+Ib0fEzKLeUjzfXqyfP+E1binqL0TEpybUFxe17RFxc7W9SvWyevVqhoeHWb16db1bkapyPPY0bgCGJjz/KvCXmflh4BfAiqK+AvhFUf/LYhwRsQC4CvgIsBj4ehFETcB/Aa4AFgDLirFSQ1m0aBF9fX2ceuqp9PX1sWjRonq3JFWsqtCIiHnAnwDfKJ4H8EfAw8WQ+4ClxfKS4jnF+k8W45cAD2bmvsz8CbAduKj42Z6ZP87MUeDBYqzUUDZs2MALL7zAwYMHeeGFFw59EZPUiKrd0/gr4EZg/MzeB4BfZuY7xfMdwNxieS7wKkCx/q1i/KH6Edu8W11qGLNnzwY4dPJ7/HG8LjWaikMjIv4U+HlmbjqO/VTay8qIGIyIwd27d9e7HemQffv20dLScujkd1NTEy0tLV4VroZVzZ7GJcCVEfFTxg4d/RHw18BpETH+NbLzgJ3F8k7gLIBi/fuBNyfWj9jm3eq/IzPvzcyOzOyYM2dOFR9JOr7eeWdsp3v//v2HPY7XpUZTcWhk5i2ZOS8z5zN2IntDZv5LYAD4dDFsOfDdYnl98Zxi/YYcm4e4HriqmF11DnAu8APgWeDcYjbWzOI91lfar1QvR+5VuJehRjZj8iHv2U3AgxHx58BzwNqivhb4m4jYDuxhLATIzK0R8RCwDXgH+EJmHgCIiC8CjwNNQF9mbq1Bv5KkkmL8oqPpoqOjIwcHB+vdhgT89js0jma6/e6psUXEpszsmGycV4RLJ8DEE+FSIzM0pBPgyCm3UqMyNKQTYOK9p6RGZmhIkkozNCRJpRkakqTSDA1JUmmGhiSpNENDOgHGvxfc7wdXo/NfsHQCfO1rX2N4eJivfe1r9W5Fqoq3EZFqyNuIqFF4GxFJ0nFnaEiSSjM0JEmlGRqSpNIMDUlSaYaGJKk0Q0OSVJqhIUkqzdCQJJVmaEiSSjM0JEmlGRqSpNIMDUlSaYaGJKk0Q0OSVJqhIUkqreLQiIizImIgIrZFxNaIuKGonxERT0TEi8Xj6UU9IuKuiNgeEc9HxMcnvNbyYvyLEbF8Qv3CiNhcbHNXHOsbbSRJNVfNnsY7wJczcwFwMfCFiFgA3Aw8mZnnAk8WzwGuAM4tflYC98BYyAC3AZ8ALgJuGw+aYsx1E7ZbXEW/kqQqVRwamfl6Zv5dsfxrYAiYCywB7iuG3QcsLZaXAPfnmGeA0yLiQ8CngCcyc09m/gJ4AlhcrHtfZj6TY9+Lef+E15Ik1cFxOacREfOBC4DvA2dm5uvFqp8BZxbLc4FXJ2y2o6gdq77jKHVJUp1UHRoRcQrw34E/y8xfTVxX7CFkte9RooeVETEYEYO7d++u9dtJ0u+tqkIjIpoZC4wHMvM7RXlXcWiJ4vHnRX0ncNaEzecVtWPV5x2l/jsy897M7MjMjjlz5lTzkSRJx1DN7KkA1gJDmXnnhFXrgfEZUMuB706oX1PMoroYeKs4jPU4cHlEnF6cAL8ceLxY96uIuLh4r2smvJYkqQ5mVLHtJcC/AjZHxA+L2q3A7cBDEbECeBn4TLHue8C/ALYDbwOfBcjMPRHxn4Fni3H/KTP3FMufB74FzAIeK34kSXUSY6cdpo+Ojo4cHBysdxsSAMe6tGi6/e6psUXEpszsmGycV4RLkkozNCRJpRkakqTSDA1JUmmGhiSpNENDklSaoSFJKs3QkCSVZmhIkkozNCRJpRkakqTSDA1JUmmGhiSpNENDklSaoSFJKs3QkCSVZmhIkkozNCRJpRkakqTSDA1JUmmGhiSpNENDklSaoSFJKs3QkCSVNqPeDUiNKiJOyPaZWdX7SMeToSFVqMwf82MFg2GgRuThKUlSaVM+NCJicUS8EBHbI+LmevcjvRfvtjfhXoYaVUzlf7wR0QT8PfDPgR3As8CyzNz2btt0dHTk4ODgCepQ08X5//F/8dbe/fVuo2rvn9XMj267vN5tqAFFxKbM7Jhs3FQ/p3ERsD0zfwwQEQ8CS4B3DQ2pEm/t3c9Pb/+TerdRtfk3/229W9A0N9VDYy7w6oTnO4BP1KkXTWOntt3MR+9r/KOfp7YBNH74aeqa6qFRSkSsBFYCnH322XXuRo1o8/LNNXvtU045heHh4UPPZ8+ezW9+85uavZ9US1P9RPhO4KwJz+cVtcNk5r2Z2ZGZHXPmzDlhzUmTOTIwAIaHhznllFPq1JFUnakeGs8C50bEORExE7gKWF/nnqTSjgyMyerSVDelD09l5jsR8UXgcaAJ6MvMrXVuSwK8Ily/n6Z0aABk5veA79W7D+lI7+WK8Pnz5/PKK69w9tln89Of/rT09tJUM9UPT0nTwt69e9m6dSt79+6tdytSVab8noY0HezatYu2trZ6tyFVzT0NSVJphoZUQ/PmzaO5ufmwWnNzM/PmzatTR1J1DA2phu644w5mzZp1KDiam5uZNWsWd9xxR507kypjaEg11tLSwty5cznppJOYO3cuLS0t9W5JqpihIdVQT08PK1euZPbs2cDYLURWrlxJT09PnTuTKuPsKamGtm3bxvDwMH19fSxcuJCNGzdy7bXX8vLLL9e7Naki7mlINTRz5ky6urro7OykubmZzs5Ourq6mDlzZr1bkypiaEg1NDo6yt13383AwAD79+9nYGCAu+++m9HR0Xq3JlXEw1NSDS1YsIClS5fS1dXF0NAQbW1tXH311TzyyCP1bk2qiHsaUg11d3ezbt06ent7GRkZobe3l3Xr1tHd3V3v1qSKuKch1dCyZcsADtvT6OnpOVSXGk1MtzttdnR05ODgYL3bkKSGEhGbMrNjsnEenpIklWZoSJJKMzQkSaUZGpKk0gwNSVJp0272VETsBryxj6aiDwJv1LsJ6V38w8ycM9mgaRca0lQVEYNlpjRKU5mHpyRJpRkakqTSDA3pxLm33g1I1fKchiSpNPc0JEmlGRrSMUTE/IjY8h7GfysiPl3LnqR6MjQkSaUZGtLkZkTEAxExFBEPR8TJEfHvI+LZiNgSEfdGRBy50buNiYj/HRFfjYgfRMTfR8Q/K+pNEfEXxfjnI6KrqF8YEU9FxKaIeDwiPnRiP770W4aGNLk/AL6emW3Ar4DPA3dn5j/JzHZgFvCnR9nuWGNmZOZFwJ8BtxW1lcB84GOZeR7wQEQ0A73ApzPzQqAP6Dnun1AqyW/ukyb3amb+32L5vwH/FvhJRNwInAycAWwFHj1iu85jjPlO8biJsaAA+GNgTWa+A5CZeyKiHWgHnih2VJqA14/rp5PeA0NDmtyR89IT+DrQkZmvRsR/AFonDoiI1knG7CseD3Ds38MAtmbmH1bevnT8eHhKmtzZETH+R/tqYGOx/EZEnAIcbbZUa4kxR3oC+DcRMQMgIs4AXgDmjL9/RDRHxEcq/BxS1QwNaXIvAF+IiCHgdOAe4L8CW4DHgWeP3CAzfznZmKP4BvAK8HxE/Ai4OjNHGQucrxa1HwL/tOpPJFXIK8IlSaW5pyFJKs3QkCSVZmhIkkozNCRJpRkakqTSDA1JUmmGhiSpNENDklTa/wfwstElHx+l1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -8019\n",
      "Max: 102127\n",
      "\n",
      "\n",
      " campaign\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Max: 62\n",
      "\n",
      "\n",
      " pdays\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -1\n",
      "Max: 871\n",
      "\n",
      "\n",
      " previous\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEAtJREFUeJzt3X2MZXV9x/H3h9lFCpgKMhK6PIyhVAfHiDoxttLqalPRNIBpRbatEjPNmpbdYqIh6vyhNSGRpGrUVFoMEzC1g7Q+kYZqLZ3WrokPs0hhYaVudDfsgrAiVZQunWW//WPPLheZZZ65Oz/fr2Ryz/3dc+/9jpH3npw5dyZVhSSpXcf0ewBJ0soy9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY1b0+8BAE455ZQaGhrq9xiStKps3br1R1U1ONd+R0Xoh4aGmJ6e7vcYkrSqJNk1n/08dSNJjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP00hFMTk4yMjLCwMAAIyMjTE5O9nskaVGOissrpaPN5OQk4+PjXHfddZx//vls2bKFsbExADZs2NDn6aSFydHwpwRHR0fL6+h1NBkZGeETn/gE69evP7w2NTXF5s2b2bZtWx8nk56QZGtVjc65n6GXnmpgYIB9+/axdu3aw2szMzMcd9xxPP74432cTHrCfEPvOXppFsPDw2zZsuVJa1u2bGF4eLhPE0mLZ+ilWYyPjzM2NsbU1BQzMzNMTU0xNjbG+Ph4v0eTFswfxkqzOPQD182bN7N9+3aGh4e56qqr/EGsViXP0UvSKuU5ekkSYOglqXmGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXFzhj7JGUmmktyd5K4kV3TrH0iyJ8nt3dcbe57z3iQ7ktyT5PUr+Q1Ikp7efH6p2X7gXVV1W5JnA1uTfLV77KNV9Ve9Oyc5F7gUeBHwa8C/JvmNqvKXeEtSH8x5RF9V91fVbd32I8B2YN3TPOUi4MaqeqyqfgDsAF6xHMNKkhZuQefokwwBLwW+2S1tSnJHkokkJ3Vr64B7e562m1n+YUiyMcl0kum9e/cueHBJ0vzMO/RJTgQ+B7yzqn4KXAOcDZwH3A98eCFvXFXXVtVoVY0ODg4u5KmSpAWYV+iTrOVg5D9TVZ8HqKoHqurxqjoAfIonTs/sAc7oefrp3ZokqQ/mc9VNgOuA7VX1kZ7103p2exOwrdu+Gbg0ybOSPB84B/jW8o0sSVqI+Vx18yrgrcCdSW7v1t4HbEhyHlDATuAdAFV1V5KbgLs5eMXO5V5xI0n9M2foq2oLkFkeuuVpnnMVcNUS5pIkLRM/GStJjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktS4OUOf5IwkU0nuTnJXkiu69ZOTfDXJ97rbk7r1JPl4kh1J7kjyspX+JiRJRzafI/r9wLuq6lzglcDlSc4F3gPcWlXnALd29wHeAJzTfW0Erln2qSVJ8zZn6Kvq/qq6rdt+BNgOrAMuAm7odrsBuLjbvgj4dB30DeA5SU5b9sklSfOyoHP0SYaAlwLfBE6tqvu7h34InNptrwPu7Xna7m5NktQH8w59khOBzwHvrKqf9j5WVQXUQt44ycYk00mm9+7du5CnSpIWYF6hT7KWg5H/TFV9vlt+4NApme72wW59D3BGz9NP79aepKqurarRqhodHBxc7PySpDnM56qbANcB26vqIz0P3Qxc1m1fBnypZ/1t3dU3rwR+0nOKR5L0DFszj31eBbwVuDPJ7d3a+4APATclGQN2AZd0j90CvBHYATwKvH1ZJ5YkLcicoa+qLUCO8PDrZtm/gMuXOJckaZn4yVhJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJatycoU8ykeTBJNt61j6QZE+S27uvN/Y89t4kO5Lck+T1KzW4JGl+5nNEfz1wwSzrH62q87qvWwCSnAtcCryoe84nkwws17CSpIWbM/RV9TXgx/N8vYuAG6vqsar6AbADeMUS5pMkLdFSztFvSnJHd2rnpG5tHXBvzz67uzVJUp8sNvTXAGcD5wH3Ax9e6Ask2ZhkOsn03r17FzmGJGkuiwp9VT1QVY9X1QHgUzxxemYPcEbPrqd3a7O9xrVVNVpVo4ODg4sZQ5I0D4sKfZLTeu6+CTh0Rc7NwKVJnpXk+cA5wLeWNqIkaSnWzLVDkkngNcApSXYD7wdek+Q8oICdwDsAququJDcBdwP7gcur6vGVGV2SNB+pqn7PwOjoaE1PT/d7DElaVZJsrarRufbzk7GS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1Lg5Q59kIsmDSbb1rJ2c5KtJvtfdntStJ8nHk+xIckeSl63k8JKkuc3niP564IJfWHsPcGtVnQPc2t0HeANwTve1EbhmecaUJC3WnKGvqq8BP/6F5YuAG7rtG4CLe9Y/XQd9A3hOktOWa1hJ0sIt9hz9qVV1f7f9Q+DUbnsdcG/Pfru7NUlSnyz5h7FVVUAt9HlJNiaZTjK9d+/epY4hSTqCxYb+gUOnZLrbB7v1PcAZPfud3q09RVVdW1WjVTU6ODi4yDEkSXNZbOhvBi7rti8DvtSz/rbu6ptXAj/pOcUjSeqDNXPtkGQSeA1wSpLdwPuBDwE3JRkDdgGXdLvfArwR2AE8Crx9BWaWJC3AnKGvqg1HeOh1s+xbwOVLHUqStHz8ZKwkNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL10BJOTk4yMjDAwMMDIyAiTk5P9HklalDX9HkA6Gk1OTjI+Ps51113H+eefz5YtWxgbGwNgw4YNfZ5OWphUVb9nYHR0tKanp/s9hnTYyMgIF198MV/84hfZvn07w8PDh+9v27at3+NJACTZWlWjc+3nEb00i7vvvptHH330KUf0O3fu7Pdo0oJ5jl6axbHHHsumTZtYv349a9euZf369WzatIljjz2236NJC7akUzdJdgKPAI8D+6tqNMnJwGeBIWAncElVPfx0r+OpGx1tjjnmGJ773Ody4oknsmvXLs466yx+9rOf8dBDD3HgwIF+jycB8z91sxxH9Our6ryeN3sPcGtVnQPc2t2XVpV169YxMzMDQBIAZmZmWLduXT/HkhZlJU7dXATc0G3fAFy8Au8hrbjjjz+eiYkJ9u3bx8TEBMcff3y/R5IWZamhL+BfkmxNsrFbO7Wq7u+2fwicusT3kJ5x9913H1dffTWbN2/muOOOY/PmzVx99dXcd999/R5NWrClXnVzflXtSfI84KtJvtv7YFVVkll/CND9w7AR4Mwzz1ziGNLyGh4e5vTTT3/SpZRTU1MMDw/3cSppcZZ0RF9Ve7rbB4EvAK8AHkhyGkB3++ARnnttVY1W1ejg4OBSxpCW3fj4OGNjY0xNTTEzM8PU1BRjY2OMj4/3ezRpwRZ9RJ/kBOCYqnqk2/494IPAzcBlwIe62y8tx6DSM+nQp183b958+ANTV111lZ+K1aq0lFM3pwJf6K5IWAP8fVV9Ocm3gZuSjAG7gEuWPqYkabEWHfqq+j7wklnWHwJet5ShpH6bnJzkiiuu4IQTTqCq+PnPf84VV1wB+LtutPr4yVhpFldeeSUDAwNMTEzw2GOPMTExwcDAAFdeeWW/R5MWzN91I81i9+7dDA0N8drXvvbw2tDQkL/rRquSR/TSEezcuZMLL7yQvXv3cuGFFxp5rVoe0UtHsGbNGu644w6e97zncdZZZ7FmzRr279/f77GkBfOIXjqCAwcOsG/fPpKwb98+f5mZVi1DLx3B4OAgDz/8MAcOHODhhx/GD/ZptfLUjXQEDzzwwOHtxx577En3pdXEI3ppFod+NfF816WjmaGXZnHoD/IcCvuh26PhbyxLC2XopadxKOwGXquZoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxq1Y6JNckOSeJDuSvGel3keS9PRWJPRJBoC/Bt4AnAtsSHLuSryXJOnprdQR/SuAHVX1/ar6P+BG4KIVei9J0tNYqdCvA+7tub+7W5MkPcPW9OuNk2wENgKceeaZ/RpDq9iLb3jxir32yPUjz/j73nnZnSvyutJKhX4PcEbP/dO7tcOq6lrgWoDR0dFaoTnUsJUMY5IjPlbl/121uqzUqZtvA+ckeX6SY4FLgZtX6L2kZXekmBt5rUYrckRfVfuTbAK+AgwAE1V110q8l7RSjLpasWLn6KvqFuCWlXp9SdL8+MlYSWqcoZekxhl6SWqcoZekxhl6SWpcjoZLyJLsBXb1ew7pCE4BftTvIaRZnFVVg3PtdFSEXjqaJZmuqtF+zyEtlqduJKlxhl6SGmfopbld2+8BpKXwHL0kNc4jeklqnKGX5iHJB5P8br/nkBbDUzf6pZNkoKoe7/cc0jPFI3o1JclQku8m+UyS7Un+McnxSXYmuTrJbcCbk5yd5MtJtib5zyQvTPKrSXYlOaZ7rROS3JtkbZLrk/xht/66JN9JcmeSiSTP6tZ3Jjml2x5N8u/d9quT3N59fSfJs/vzv45+WRl6tegFwCerahj4KfDn3fpDVfWyqrqRg1fSbK6qlwPv7vb/CXA78Opu/98HvlJVM4deOMlxwPXAW6rqxRz8mw5/Nsc87wYur6rzgN8G/ncZvkdp3gy9WnRvVX292/474Pxu+7MASU4Efgv4hyS3A38LnNazz1u67UsPPafHC4AfVNV/d/dvAH5njnm+DnwkyV8Az6mq/Qv/lqTFM/Rq0S/+4OnQ/Z93t8cA/1NV5/V8DXeP3QxckORk4OXAvy3gfffzxH9Txx1+86oPAX8K/Arw9SQvXMBrSktm6NWiM5P8Zrf9R8CW3ger6qfAD5K8GSAHvaR77Gcc/OP2HwP+aZYf2t4DDCX59e7+W4H/6LZ3cvAfB4A/OPSEJGdX1Z1VdXX32oZezyhDrxbdA1yeZDtwEnDNLPv8MTCW5L+Au4CLeh77LPAnPPW0DVW1D3g7B0/73AkcAP6me/gvgY8lmQZ6/4F4Z5JtSe4AZoB/Xso3Jy2Ul1eqKUmGOHgkPtLnUaSjhkf0ktQ4j+glqXEe0UtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXu/wHzIE2RclZT9gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Max: 275\n"
     ]
    }
   ],
   "source": [
    "# Box plots\n",
    "for feature in numerical:\n",
    "    interpret(feature, 'box', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " age\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " balance\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " campaign\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " pdays\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " previous\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Density plots\n",
    "for feature in numerical:\n",
    "    interpret(feature, 'density', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From both series of plots above, it becomes very evident that this dataset presents many outliers in its numerical features. This needs to be removed, since outliers can greatly affect the performance of our prediction models.\n",
    "\n",
    "With respect to the variable 'pdays', we can see that there is a great concentration of clients around the value 0. This represents all the clients that were only contacted yesterday (pdays = 0) and the clients that had never been contacted before (pdays = -1). It doesn't seem logical to huddle both of these very different types of clients so closely together in the dataset. As discussed before, simply replacing the -1 values with 999 or the max value would only distort the data further. Granted, the statistics would change, and perhaps the outlier threshold would be incremented, too. However, this would only create a data 'abyss' between very small values (huddled close to 0) and very large values (all the 871s or 999s). Although this huge range can be scaled, it still distorts the data. Therefore, I see it fit to remove most of the outliers in the data, and then simply assign a value of 1.5 * the max value after the outliers have been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining Numerical Features: Removing Outliers\n",
    "\n",
    "To identify outliers, I will use the widely-accepted statistical definition of an outlier, proposed by John Tukey:\n",
    "\n",
    "According to Tukey's definition, outliers are points that lie outside the range:\n",
    "\n",
    "[Q1 - k(Q3 - Q1), Q3 + k(Q3 - Q1)]\n",
    "\n",
    "Where *Q3 - Q1* is the Interquartile Range (IQR) and *k* is a nonnegative number. For Tukey, a value *k = 1.5* identifies points that are 'outliers', while *k = 3* identifies points that are 'far out'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to count outliers in a series \n",
    "def get_outliers(feature, k):\n",
    "    feature_outliers = pd.DataFrame()\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    iqr = Q3 - Q1\n",
    "    for i in range(df.index.size):\n",
    "        client = df.loc[i]\n",
    "        if client[feature] < Q1 - k * iqr or client[feature] > Q3 + k * iqr:\n",
    "            feature_outliers = feature_outliers.append(client)\n",
    "    return feature_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's iterate over the numerical features in the dataframe and append the outliers in those features to another dataframe. After we've done this, let's drop the duplicates so that we are left with unique outlier samples (samples that are outliers in at least one feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14804 outliers in the data\n"
     ]
    }
   ],
   "source": [
    "# Select all outliers in the dataframe and drop duplicate rows. \n",
    "outliers = pd.DataFrame()\n",
    "\n",
    "for feature in numerical:\n",
    "    outliers = outliers.append(get_outliers(feature, 1.5))\n",
    "    \n",
    "outliers = outliers.drop_duplicates()\n",
    "\n",
    "# Number of unique outliers in the data\n",
    "print(outliers.shape[0], 'outliers in the data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to drop the outliers from the dataset, it is important to consider that the dataset is imbalanced towards negative outcomes ('no'), with a proportion of 'yes'-to-'no' outcomes of roughly 1:7.5 (imbalance calculated towards the beginning of the notebook).\n",
    "\n",
    "What would happen to this proportion if we did actually drop the outliers from the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 2684, 'no': 27723}\n",
      "New ratio of \"yes\" to \"no\" is: 1:10.3\n"
     ]
    }
   ],
   "source": [
    "# Calculate and report new value counts of target variable after having dropped outliers\n",
    "val_counts_dropped = {'yes': df.y.value_counts()[1] - outliers.y.value_counts()[1], \n",
    "                      'no': df.y.value_counts()[0] - outliers.y.value_counts()[0]}\n",
    "\n",
    "print(val_counts_dropped)\n",
    "print('New ratio of \"yes\" to \"no\" is:', '1:{:.1f}'.format(val_counts_dropped['no'] / val_counts_dropped['yes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evidently very harmful for the dataset to increase this imbalance further. To alleviate this issue, I see two possible solutions:\n",
    "\n",
    "1. Calculate the exact number of outliers with target variable 'yes' needed so that the proportion of 1:7.5 remains untouched.\n",
    "2. Identify the 'outliers' within the outliers with target variable 'yes' (in order to shave off less clients with target variable 'yes') using *k = 1* instead of *k = 1.5* so that Tukey's range is slightly less lenient.\n",
    "3. Identify the outliers with target variable 'yes' of the whole dataset again, this time using a slightly greater value of k (such as *k = 2*) so that the number of 'yes' outliers is slightly smaller.\n",
    "\n",
    "The advantage of the first alternative is clearly that the proportion can remain untouched. However, by randomly selecting elements from the outlier list, I would be risking the appearance of a very large value in the dataset that would distort the model's predictions. This option can therefore be ruled out in favor of a more systematic solution. \n",
    "\n",
    "The second alternative is more systematic than the first, but still presents an important issue: the Q3, Q1, and IQR calculated will be calculated using the outlier data. This will make the final, resulting list of outliers not dropped from the dataset to be clustered around the IQR of the initial outliers. In the end, all I would be doing is eliminating the very large values and very small values of outlier dataset, but end up with outliers nonetheless in my original bank dataset. This, therefore, is not an option either.\n",
    "\n",
    "The last option will be the one I use, then. This way, I will be making Tukey's range slightly more lenient to exclude less values as outliers, but I will still be avoiding the main complication of option 2. As a result, I will get slightly more data (including outliers, according to Tukey's *k=1.5* threshold, but not the *k = 2*) that is clustered around the data I *actually* want to use for prediction - non-outliers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     12199\n",
       "yes     2605\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reference purposes, let's keep a count of the 'yes' and 'no' values in outliers\n",
    "outliers.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate outliers with 'yes' and 'no' target variables\n",
    "outliers_y = outliers.loc[outliers.y == 'yes']\n",
    "outliers_n = outliers.drop(outliers_y.index.values)\n",
    "assert outliers_y.index.size == outliers.y.value_counts()[1] and outliers_n.index.size == outliers.y.value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Data Frame with only the rows that contain the target variable 'yes' and check for equality\n",
    "df_y = df.loc[df.y == 'yes']\n",
    "assert df_y.index.size == df.y.value_counts()[1]\n",
    "\n",
    "# Adaptation of method to count outliers in a series (this time using df_y instead of df)\n",
    "def get_outliers_y(feature, k):\n",
    "    feature_outliers = pd.DataFrame()\n",
    "    Q3 = df_y[feature].quantile(0.75)\n",
    "    Q1 = df_y[feature].quantile(0.25)\n",
    "    iqr = Q3 - Q1\n",
    "    for i in df_y.index.values:\n",
    "        client = df_y.loc[i]\n",
    "        if client[feature] < Q1 - k * iqr or client[feature] > Q3 + k * iqr:\n",
    "            feature_outliers = feature_outliers.append(client)\n",
    "    return feature_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235 \"yes\" outliers in the data\n"
     ]
    }
   ],
   "source": [
    "# Select all 'outliers' in df_y and drop duplicate rows. \n",
    "outliers_y = pd.DataFrame()\n",
    "\n",
    "for feature in numerical:\n",
    "    outliers_y = outliers_y.append(get_outliers_y(feature, 2))\n",
    "    \n",
    "outliers_y = outliers_y.drop_duplicates()\n",
    "\n",
    "# Number of unique outliers in the data\n",
    "print(outliers_y.shape[0], '\"yes\" outliers in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 4054, 'no': 27723}\n",
      "New ratio of \"yes\" to \"no\" is: 1:6.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate new proportion of 'yes'-to-'no' targets remaining in the data after removing outliers_n and outliers_y\n",
    "new_val_counts_dropped = {'yes': df.y.value_counts()[1] - outliers_y.index.size, \n",
    "                          'no': df.y.value_counts()[0] - outliers_n.index.size}\n",
    "\n",
    "print(new_val_counts_dropped)\n",
    "print('New ratio of \"yes\" to \"no\" is:', '1:{:.1f}'.format(new_val_counts_dropped['no'] / new_val_counts_dropped['yes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very good. By removing the current outliers contained in outliers_n (ruled out using *k=1.5*) and outliers_y (using *k=2*) the proportion of 'yes' to 'no' targets drops from 1:7.5 to 1:6.8. \n",
    "\n",
    "Now we can proceed to drop these outliers from the actual data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'no' outliers\n",
    "df = df.drop(outliers_n.index.values)\n",
    "\n",
    "# drop 'yes' outliers\n",
    "df = df.drop(outliers_y.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     27723\n",
       "yes     4054\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for equality between 'new_val_counts_dropped' (above) and df.y.value_counts() after dropping outliers\n",
    "df.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the numbers coincide. After dropping outliers_y (*k=2*) and outliers_n (*k=1.5*) from the dataset, we end up with the same number of 'yes' and 'no' target features calculated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Data Frame index for better presentation from now onwards\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining Numerical Features: Post-Outlier Removal\n",
    "\n",
    "Now that we have successfully removed the outliers from the dataset, we can proceed to do two things:\n",
    "\n",
    "1. Replace 'pdays' -1 values with 1.5 * pdays.max()\n",
    "2. Examine the histogram, box, and density plots of all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -1 with new value in df.pdays\n",
    "df.pdays = df.pdays.replace(to_replace=-1, value=1.5*df.pdays.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " age\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 18\n",
      "Max: 87\n",
      "\n",
      "\n",
      " balance\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -3058\n",
      "Max: 6046\n",
      "\n",
      "\n",
      " campaign\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAD8CAYAAAC/1zkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEq9JREFUeJzt3XuwXXd53vHvgwTFJoDtWiiu5UZOqiF1aTBGMW7phcSNke0EOZmWwiSxSj0oM5gGWmYaQTMxAyXjTBJInCZuHFAtJwTiGIjVImKEy4TpTI0tE8dXqDRgsIQvCnJswAyO4e0f+3fsjXSOzpb822edbX0/M2v2Wu9el3dLOnrOuu5UFZIk9fCsoRuQJD1zGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndrBy6gaV28skn19q1a4duQ5Jmyq233vrXVbVqsfmOuVBZu3Ytu3btGroNSZopSb48yXwe/pIkdWOoSJK6MVQkSd0YKpKkbgwVSVI3hookqRtDRZLUjaEiSerGUJEkdXPM3VH/dKzd8vFBtnvv5RcOsl1JOlLuqUiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHUztVBJclqSTye5O8ldSd7S6icl2Zlkd3s9sdWT5Ioke5LcnuSssXVtavPvTrJprP7yJHe0Za5Ikml9HknS4qa5p/IE8LaqOgM4B7g0yRnAFuDGqloH3NimAc4H1rVhM3AljEIIuAx4BXA2cNlcELV53ji23IYpfh5J0iKmFipVdX9Vfa6Nfx24BzgV2Ahsa7NtAy5q4xuBa2rkJuCEJKcArwZ2VtWBqnoY2AlsaO+9oKpuqqoCrhlblyRpAEtyTiXJWuBlwGeB1VV1f3vrAWB1Gz8VuG9ssb2tdrj63nnq821/c5JdSXbt37//aX0WSdLCph4qSb4P+Ajw1qp6dPy9todR0+6hqq6qqvVVtX7VqlXT3pwkHbOmGipJns0oUD5YVR9t5QfboSva60Otvg84bWzxNa12uPqaeeqSpIFM8+qvAB8A7qmq9469tR2Yu4JrE3D9WP3idhXYOcAj7TDZDcB5SU5sJ+jPA25o7z2a5Jy2rYvH1iVJGsDKKa77lcDPA3ckua3V3gFcDlyb5BLgy8Br23s7gAuAPcBjwBsAqupAkncDt7T53lVVB9r4m4CrgeOAT7RBkjSQqYVKVf0fYKH7Rs6dZ/4CLl1gXVuBrfPUdwEveRptSpI68o56SVI3hookqRtDRZLUjaEiSerGUJEkdWOoSJK6MVQkSd0YKpKkbgwVSVI3hookqRtDRZLUjaEiSerGUJEkdWOoSJK6MVQkSd0YKpKkbgwVSVI3hookqRtDRZLUjaEiSerGUJEkdWOoSJK6MVQkSd0YKpKkbgwVSVI3hookqRtDRZLUjaEiSerGUJEkdWOoSJK6MVQkSd0YKpKkbgwVSVI3hookqRtDRZLUjaEiSerGUJEkdTO1UEmyNclDSe4cq70zyb4kt7XhgrH33p5kT5IvJHn1WH1Dq+1JsmWsfnqSz7b6nyR5zrQ+iyRpMtPcU7ka2DBP/X1VdWYbdgAkOQN4HfCP2jK/l2RFkhXA7wLnA2cAr2/zAvxaW9c/AB4GLpniZ5EkTWBqoVJVnwEOTDj7RuDDVfXtqvoSsAc4uw17quqLVfU48GFgY5IAPw5c15bfBlzU9QNIko7YEOdU3pzk9nZ47MRWOxW4b2yeva22UP3vAn9TVU8cVJckDWipQ+VK4IeAM4H7gd9cio0m2ZxkV5Jd+/fvX4pNStIxaUlDpaoerKrvVNV3gT9gdHgLYB9w2tisa1ptofrXgBOSrDyovtB2r6qq9VW1ftWqVX0+jCTpEEsaKklOGZv8aWDuyrDtwOuS/J0kpwPrgJuBW4B17Uqv5zA6mb+9qgr4NPCv2/KbgOuX4jNIkha2cvFZjk6SDwGvAk5Oshe4DHhVkjOBAu4FfgGgqu5Kci1wN/AEcGlVfaet583ADcAKYGtV3dU28UvAh5P8V+AvgQ9M67NIkiYztVCpqtfPU17wP/6qeg/wnnnqO4Ad89S/yFOHzyRJy4B31EuSujFUJEndTBQqSf7xtBuRJM2+SfdUfi/JzUnelOSFU+1IkjSzJgqVqvrnwM8yumfk1iR/nOQnptqZJGnmTHxOpap2A7/M6FLefwlckeTzSX5mWs1JkmbLpOdUfiTJ+4B7GD3I8aeq6h+28fdNsT9J0gyZ9D6V3wHeD7yjqr41V6yqryb55al0JkmaOZOGyoXAt8bucn8W8Nyqeqyq/nBq3UmSZsqk51Q+BRw3Nn18q0mS9KRJQ+W5VfWNuYk2fvx0WpIkzapJQ+WbSc6am0jycuBbh5lfknQMmvScyluBP03yVSDA9wP/dmpdSZJm0kShUlW3JPlh4MWt9IWq+tvptSVJmkVH8uj7HwXWtmXOSkJVXTOVriRJM2miUEnyh4y+W/424DutXIChIkl60qR7KuuBM9rX+EqSNK9Jr/66k9HJeUmSFjTpnsrJwN1Jbga+PVesqtdMpStJ0kyaNFTeOc0mJEnPDJNeUvwXSX4AWFdVn0pyPLBiuq1JkmbNpI++fyNwHfD7rXQq8GfTakqSNJsmPVF/KfBK4FF48gu7XjStpiRJs2nSUPl2VT0+N5FkJaP7VCRJetKkofIXSd4BHNe+m/5Pgf85vbYkSbNo0lDZAuwH7gB+AdjB6PvqJUl60qRXf30X+IM2SJI0r0mf/fUl5jmHUlU/2L0jSdLMOpJnf815LvBvgJP6tyNJmmUTnVOpqq+NDfuq6reAC6fcmyRpxkx6+OussclnMdpzOZLvYpEkHQMmDYbfHBt/ArgXeG33biRJM23Sq79+bNqNSJJm36SHv/7T4d6vqvf2aUeSNMuO5OqvHwW2t+mfAm4Gdk+jKUnSbJo0VNYAZ1XV1wGSvBP4eFX93LQakyTNnkkf07IaeHxs+vFWkyTpSZPuqVwD3JzkY236ImDbdFqSJM2qSW9+fA/wBuDhNryhqn71cMsk2ZrkoSR3jtVOSrIzye72emKrJ8kVSfYkuX38vpgkm9r8u5NsGqu/PMkdbZkrkuTIProkqbdJD38BHA88WlW/DexNcvoi818NbDiotgW4sarWATe2aYDzgXVt2AxcCaMQAi4DXgGcDVw2F0RtnjeOLXfwtiRJS2zSrxO+DPgl4O2t9Gzgjw63TFV9BjhwUHkjTx0228boMNpc/ZoauQk4IckpwKuBnVV1oKoeBnYCG9p7L6iqm6qqGB2euwhJ0qAm3VP5aeA1wDcBquqrwPOPYnurq+r+Nv4AT53sPxW4b2y+va12uPreeeqSpAFNGiqPtz2CAkjyvKe74fH1TVuSzUl2Jdm1f//+pdikJB2TJg2Va5P8PqPDUm8EPsXRfWHXg+3QFe31oVbfB5w2Nt+aVjtcfc089XlV1VVVtb6q1q9ateoo2pYkTWLSq79+A7gO+AjwYuBXqup3jmJ724G5K7g2AdeP1S9uV4GdAzzSDpPdAJyX5MR2gv484Ib23qNJzmlXfV08ti5J0kAWvU8lyQrgU+2hkjsnXXGSDwGvAk5OspfRVVyXM9rruQT4Mk896XgHcAGwB3iM0eXLVNWBJO8Gbmnzvauq5k7+v4nRFWbHAZ9ogyRpQIuGSlV9J8l3k7ywqh6ZdMVV9foF3jp3nnkLuHSB9WwFts5T3wW8ZNJ+JEnTN+kd9d8A7kiyk3YFGEBV/eJUupIkzaRJQ+WjbZAkaUGHDZUkf7+qvlJVPudLkrSoxa7++rO5kSQfmXIvkqQZt1iojD+k8Qen2YgkafYtFiq1wLgkSYdY7ET9S5M8ymiP5bg2TpuuqnrBVLuTJM2Uw4ZKVa1YqkYkSbPvSL5PRZKkwzJUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1M0goZLk3iR3JLktya5WOynJziS72+uJrZ4kVyTZk+T2JGeNrWdTm393kk1DfBZJ0lOG3FP5sao6s6rWt+ktwI1VtQ64sU0DnA+sa8Nm4EoYhRBwGfAK4GzgsrkgkiQNYzkd/toIbGvj24CLxurX1MhNwAlJTgFeDeysqgNV9TCwE9iw1E1Lkp4yVKgU8MkktybZ3Gqrq+r+Nv4AsLqNnwrcN7bs3lZbqH6IJJuT7Eqya//+/b0+gyTpICsH2u4/q6p9SV4E7Ezy+fE3q6qSVK+NVdVVwFUA69ev77ZeSdL3GmRPpar2tdeHgI8xOifyYDusRXt9qM2+DzhtbPE1rbZQXZI0kCUPlSTPS/L8uXHgPOBOYDswdwXXJuD6Nr4duLhdBXYO8Eg7THYDcF6SE9sJ+vNaTZI0kCEOf60GPpZkbvt/XFV/nuQW4NoklwBfBl7b5t8BXADsAR4D3gBQVQeSvBu4pc33rqo6sHQfQ5J0sCUPlar6IvDSeepfA86dp17ApQusayuwtXePkqSjs5wuKZYkzbihrv7SEVi75eNDt7Dk7r38wqFbkHQU3FORJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSN4aKJKkbQ0WS1I2hIknqxlCRJHVjqEiSujFUJEndGCqSpG4MFUlSNyuHbkCaz9otHx9s2/defuFg25ZmnXsqkqRuDBVJUjeGiiSpG0NFktSNoSJJ6mbmQyXJhiRfSLInyZah+5GkY9lMh0qSFcDvAucDZwCvT3LGsF1J0rFrpkMFOBvYU1VfrKrHgQ8DGwfuSZKOWbN+8+OpwH1j03uBVwzUi54hhrrx0psu9Uww66EykSSbgc1t8htJvnCUqzoZ+Os+XU3Ncu9xufcHA/WYX5t4Vv8Mn77l3h8svx5/YJKZZj1U9gGnjU2vabXvUVVXAVc93Y0l2VVV65/ueqZpufe43PuD5d/jcu8Pln+Py70/mI0e5zPr51RuAdYlOT3Jc4DXAdsH7kmSjlkzvadSVU8keTNwA7AC2FpVdw3cliQds2Y6VACqagewY4k297QPoS2B5d7jcu8Pln+Py70/WP49Lvf+YDZ6PESqaugeJEnPELN+TkWStIwYKhNY7o+CSXJakk8nuTvJXUneMnRP80myIslfJvlfQ/cynyQnJLkuyeeT3JPknwzd08GS/Mf2d3xnkg8lee4y6GlrkoeS3DlWOynJziS72+uJy6y/X29/z7cn+ViSE4bqb6Eex957W5JKcvIQvR0pQ2URM/IomCeAt1XVGcA5wKXLsEeAtwD3DN3EYfw28OdV9cPAS1lmvSY5FfhFYH1VvYTRxSmvG7YrAK4GNhxU2wLcWFXrgBvb9FCu5tD+dgIvqaofAf4f8PalbuogV3NojyQ5DTgP+MpSN3S0DJXFLftHwVTV/VX1uTb+dUb/GZ46bFffK8ka4ELg/UP3Mp8kLwT+BfABgKp6vKr+Ztiu5rUSOC7JSuB44KsD90NVfQY4cFB5I7CtjW8DLlrSpsbM119VfbKqnmiTNzG6x20wC/wZArwP+M/AzJz8NlQWN9+jYJbVf9jjkqwFXgZ8dthODvFbjH44vjt0Iws4HdgP/I92iO79SZ43dFPjqmof8BuMfmu9H3ikqj45bFcLWl1V97fxB4DVQzaziH8PfGLoJg6WZCOwr6r+auhejoSh8gyS5PuAjwBvrapHh+5nTpKfBB6qqluH7uUwVgJnAVdW1cuAbzLsIZtDtPMSGxkF4N8Dnpfk54btanE1usR0Wf6mneS/MDp8/MGhexmX5HjgHcCvDN3LkTJUFjfRo2CGluTZjALlg1X10aH7OcgrgdckuZfR4cMfT/JHw7Z0iL3A3qqa28O7jlHILCf/CvhSVe2vqr8FPgr804F7WsiDSU4BaK8PDdzPIZL8O+AngZ+t5XdvxQ8x+uXhr9rPzRrgc0m+f9CuJmCoLG7ZPwomSRidC7inqt47dD8Hq6q3V9WaqlrL6M/vf1fVsvoNu6oeAO5L8uJWOhe4e8CW5vMV4Jwkx7e/83NZZhcTjNkObGrjm4DrB+zlEEk2MDoc+5qqemzofg5WVXdU1Yuqam37udkLnNX+nS5rhsoi2sm8uUfB3ANcuwwfBfNK4OcZ7QHc1oYLhm5qBv0H4INJbgfOBH514H6+R9uLug74HHAHo5/fwe+6TvIh4P8CL06yN8klwOXATyTZzWgP6/Jl1t9/A54P7Gw/L/99qP4O0+NM8o56SVI37qlIkroxVCRJ3RgqkqRuDBVJUjeGiiSpG0NFktSNoSJJ6sZQkSR18/8B5K0i+UxYjUwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Max: 15\n",
      "\n",
      "\n",
      " pdays\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAD8CAYAAAC/1zkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFZdJREFUeJzt3X+wHeV93/H3x+Kn49j8UlQqqRGO1bhyGgusgjJOZhwYg4DUwjPEhbZB42qspBZTe+qZRrid4Nhmxp5JTEJr08hBtXBcyxjbQbXlUoGZePIHoIshgEQp1xgXyTJSED9M7EKEv/3jPBcfpCvdA+y5h6v7fs3s3N3vPnv22R3QZ3b3OXtSVUiS1IXXjLoDkqQjh6EiSeqMoSJJ6oyhIknqjKEiSeqMoSJJ6oyhIknqjKEiSeqMoSJJ6sxRo+7AdDvllFNq0aJFo+6GJM0od911199W1dyp2s26UFm0aBFjY2Oj7oYkzShJvj9IO29/SZI6Y6hIkjpjqEiSOmOoSJI6Y6hIkjpjqEiSOmOoSJI6Y6hIkjpjqEiSOjPrvlEvSaO0aN03RrLfRz5x4bTsxysVSVJnhhYqSY5LcmeSv0myPckftvppSe5IMp7kS0mOafVj2/J4W7+o77OuaPUHk5zXV1/RauNJ1g3rWCRJgxnmlcqzwNlV9VZgKbAiyXLgk8DVVfUm4AlgdWu/Gnii1a9u7UiyBLgEeAuwAvhMkjlJ5gCfBs4HlgCXtraSpBEZWqhUzzNt8eg2FXA2cGOrbwQuavMr2zJt/TlJ0uqbqurZqvoeMA6c2abxqnq4qp4DNrW2kqQRGeozlXZFcQ+wB9gKfBd4sqr2tyY7gfltfj7wKEBb/xRwcn/9gG0OVZ+sH2uSjCUZ27t3bxeHJkmaxFBDpaqer6qlwAJ6VxZvHub+DtOP9VW1rKqWzZ075W/MSJJepmkZ/VVVTwK3Ab8GnJBkYijzAmBXm98FLARo698APN5fP2CbQ9UlSSMyzNFfc5Oc0OaPB94JPEAvXC5uzVYBN7X5zW2Ztv5bVVWtfkkbHXYasBi4E9gGLG6jyY6h9zB/87COR5I0tWF++fFUYGMbpfUa4Iaq+nqSHcCmJB8H7gaua+2vAz6fZBzYRy8kqKrtSW4AdgD7gbVV9TxAksuBm4E5wIaq2j7E45EkTWFooVJV9wKnT1J/mN7zlQPr/w/47UN81lXAVZPUtwBbXnFnJUmd8Bv1kqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzgwtVJIsTHJbkh1Jtif5QKt/JMmuJPe06YK+ba5IMp7kwSTn9dVXtNp4knV99dOS3NHqX0pyzLCOR5I0tWFeqewHPlRVS4DlwNokS9q6q6tqaZu2ALR1lwBvAVYAn0kyJ8kc4NPA+cAS4NK+z/lk+6w3AU8Aq4d4PJKkKQwtVKpqd1V9p83/CHgAmH+YTVYCm6rq2ar6HjAOnNmm8ap6uKqeAzYBK5MEOBu4sW2/EbhoOEcjSRrEtDxTSbIIOB24o5UuT3Jvkg1JTmy1+cCjfZvtbLVD1U8Gnqyq/QfUJUkjMvRQSfI64CvAB6vqaeBa4JeApcBu4I+noQ9rkowlGdu7d++wdydJs9ZQQyXJ0fQC5QtV9VWAqnqsqp6vqp8Cn6V3ewtgF7Cwb/MFrXao+uPACUmOOqB+kKpaX1XLqmrZ3Llzuzk4SdJBhjn6K8B1wANV9am++ql9zd4N3N/mNwOXJDk2yWnAYuBOYBuwuI30Oobew/zNVVXAbcDFbftVwE3DOh5J0tSOmrrJy/Z24HeA+5Lc02ofpjd6aylQwCPA7wJU1fYkNwA76I0cW1tVzwMkuRy4GZgDbKiq7e3zfh/YlOTjwN30QkySNCJDC5Wq+msgk6zacphtrgKumqS+ZbLtquphfnb7TJI0Yn6jXpLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUmaGFSpKFSW5LsiPJ9iQfaPWTkmxN8lD7e2KrJ8k1ScaT3JvkjL7PWtXaP5RkVV/9bUnua9tckyTDOh5J0tSGeaWyH/hQVS0BlgNrkywB1gG3VtVi4Na2DHA+sLhNa4BroRdCwJXAWcCZwJUTQdTavK9vuxVDPB5J0hSGFipVtbuqvtPmfwQ8AMwHVgIbW7ONwEVtfiVwffXcDpyQ5FTgPGBrVe2rqieArcCKtu71VXV7VRVwfd9nSZJGYFqeqSRZBJwO3AHMq6rdbdUPgXltfj7waN9mO1vtcPWdk9Qn2/+aJGNJxvbu3fuKjkWSdGhDD5UkrwO+Anywqp7uX9euMGrYfaiq9VW1rKqWzZ07d9i7k6RZa6BQSfJPX86HJzmaXqB8oaq+2sqPtVtXtL97Wn0XsLBv8wWtdrj6gknqkqQRGfRK5TNJ7kzy/iRvGGSDNhLrOuCBqvpU36rNwMQIrlXATX31y9oosOXAU+022c3AuUlObA/ozwVubuueTrK87euyvs+SJI3AUYM0qqrfSLIY+DfAXUnuBP5bVW09zGZvB34HuC/JPa32YeATwA1JVgPfB97T1m0BLgDGgR8D72373pfkY8C21u6jVbWvzb8f+BxwPPDNNkmSRmSgUAGoqoeS/CdgDLgGOL1dIXy479ZWf/u/Bg71vZFzJmlfwNpD7HsDsGGS+hjwK4MegyRpuAZ9pvKrSa6mNyz4bOCfV9U/afNXD7F/kqQZZNArlf8M/Dm9q5KfTBSr6gft6kWSpIFD5ULgJ1X1PECS1wDHVdWPq+rzQ+udJGlGGXT01y30HoZPeG2rSZL0gkFD5biqemZioc2/djhdkiTNVIOGyt8d8NbgtwE/OUx7SdIsNOgzlQ8CX07yA3rDhP8B8C+G1itJ0ow06JcftyV5M/DLrfRgVf398LolSZqJBv7yI/DPgEVtmzOSUFXXD6VXkqQZaaBQSfJ54JeAe4DnW3niN0wkSQIGv1JZBixpr1KRJGlSg47+up/ew3lJkg5p0CuVU4Ad7e3Ez04Uq+pdQ+mVJGlGGjRUPjLMTkiSjgyDDin+qyS/CCyuqluSvBaYM9yuSZJmmkFfff8+4Ebgz1ppPvCXw+qUJGlmGvRB/Vp6v+T4NPR+sAv4hWF1SpI0Mw0aKs9W1XMTC0mOovc9FUmSXjBoqPxVkg8Dxyd5J/Bl4H8Mr1uSpJlo0FBZB+wF7gN+F9gC+IuPkqQXGXT010+Bz7ZJkqRJDfrur+8xyTOUqnpj5z2SJM1YL+XdXxOOA34bOKn77kiSZrKBnqlU1eN9066q+hPgwiH3TZI0wwz65ccz+qZlSX6PKa5ykmxIsifJ/X21jyTZleSeNl3Qt+6KJONJHkxyXl99RauNJ1nXVz8tyR2t/qUkx7ykI5ckdW7Q219/3De/H3gEeM8U23wO+C8c/JsrV1fVH/UXkiwBLgHeAvxD4JYk/7it/jTwTmAnsC3J5qraAXyyfdamJP8VWA1cO+DxSJKGYNDRX7/5Uj+4qr6dZNGAzVcCm6rqWeB7ScaBM9u68ap6GCDJJmBlkgeAs4F/2dpspPfSS0NFkkZo0NFf//5w66vqUy9hn5cnuQwYAz5UVU/Qe5fY7X1tdrYawKMH1M8CTgaerKr9k7SXJI3IoF9+XAb8W3r/cM8Hfg84A/j5Ng3qWno/S7wU2M2Lb6sNTZI1ScaSjO3du3c6dilJs9Kgz1QWAGdU1Y+g98Ad+EZV/euXsrOqemxiPslnga+3xV3AwgP2t6vNT1Z/HDghyVHtaqW//WT7XQ+sB1i2bJnvLJOkIRn0SmUe8Fzf8nOt9pIkObVv8d30fqYYYDNwSZJjk5wGLAbuBLYBi9tIr2PoPczfXFUF3AZc3LZfBdz0UvsjSerWoFcq1wN3JvlaW76I3sPxQ0ryReAdwClJdgJXAu9IspTet/MfofceMapqe5IbgB30Rpetrarn2+dcDtxM70fBNlTV9raL3wc2Jfk4cDdw3YDHIkkakkFHf12V5JvAb7TSe6vq7im2uXSS8iH/4a+qq4CrJqlvofcCywPrD/OzEWKSpFeBQW9/AbwWeLqq/hTY2W5TSZL0gkG/UX8lvdtNV7TS0cBfDKtTkqSZadArlXcD7wL+DqCqfsBLG0osSZoFBg2V59qIqwJI8nPD65IkaaYaNFRuSPJn9L4b8j7gFvzBLknSAQYd/fVH7bfpnwZ+GfiDqto61J5JkmacKUMlyRzglvZSSYNEknRIU97+al9C/GmSN0xDfyRJM9ig36h/BrgvyVbaCDCAqvp3Q+mVJGlGGjRUvtomSZIOaaqfBP5HVfV/q+qw7/mSJAmmfqbylxMzSb4y5L5Ikma4qUIlffNvHGZHJEkz31ShUoeYlyTpIFM9qH9rkqfpXbEc3+Zpy1VVrx9q7yRJM8phQ6Wq5kxXRyRJM99L+T0VSZIOy1CRJHXGUJEkdcZQkSR1xlCRJHXGUJEkdcZQkSR1xlCRJHXGUJEkdWZooZJkQ5I9Se7vq52UZGuSh9rfE1s9Sa5JMp7k3iRn9G2zqrV/KMmqvvrbktzXtrkmSZAkjdQwr1Q+B6w4oLYOuLWqFgO3tmWA84HFbVoDXAu9EAKuBM4CzgSunAii1uZ9fdsduC9J0jQbWqhU1beBfQeUVwITP/i1Ebior3599dwOnJDkVOA8YGtV7auqJ4CtwIq27vVVdXtVFXB932dJkkZkup+pzKuq3W3+h8C8Nj8feLSv3c5WO1x95yR1SdIIjexBfbvCmJbfaEmyJslYkrG9e/dOxy4laVaa7lB5rN26ov3d0+q7gIV97Ra02uHqCyapT6qq1lfVsqpaNnfu3Fd8EJKkyU13qGwGJkZwrQJu6qtf1kaBLQeearfJbgbOTXJie0B/LnBzW/d0kuVt1NdlfZ8lSRqRqX758WVL8kXgHcApSXbSG8X1CeCGJKuB7wPvac23ABcA48CPgfcCVNW+JB8DtrV2H62qiYf/76c3wux44JttkiSN0NBCpaouPcSqcyZpW8DaQ3zOBmDDJPUx4FdeSR8lSd3yG/WSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM4YKpKkzhgqkqTOGCqSpM6MJFSSPJLkviT3JBlrtZOSbE3yUPt7YqsnyTVJxpPcm+SMvs9Z1do/lGTVKI5FkvQzo7xS+c2qWlpVy9ryOuDWqloM3NqWAc4HFrdpDXAt9EIIuBI4CzgTuHIiiCRJo/Fquv21EtjY5jcCF/XVr6+e24ETkpwKnAdsrap9VfUEsBVYMd2dliT9zKhCpYD/leSuJGtabV5V7W7zPwTmtfn5wKN92+5stUPVD5JkTZKxJGN79+7t6hgkSQc4akT7/fWq2pXkF4CtSf53/8qqqiTV1c6qaj2wHmDZsmWdfa4k6cVGcqVSVbva3z3A1+g9E3ms3dai/d3Tmu8CFvZtvqDVDlWXJI3ItIdKkp9L8vMT88C5wP3AZmBiBNcq4KY2vxm4rI0CWw481W6T3Qycm+TE9oD+3FaTJI3IKG5/zQO+lmRi//+9qv5nkm3ADUlWA98H3tPabwEuAMaBHwPvBaiqfUk+Bmxr7T5aVfum7zAkSQea9lCpqoeBt05Sfxw4Z5J6AWsP8VkbgA1d91GS9PK8moYUS5JmOENFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1JkZHypJViR5MMl4knWj7o8kzWYzOlSSzAE+DZwPLAEuTbJktL2SpNnrqFF34BU6ExivqocBkmwCVgI7RtqrI8iidd8YyX4f+cSFI9kvzM5jlroy00NlPvBo3/JO4KwR9WVoRvWP3CjNxmMeJYNUXZnpoTKQJGuANW3xmSQPvoyPOQX42+56dUTwnBzsZZ+TfLLjnrw6HPZ8HKHHPJWR/H/Twbn+xUEazfRQ2QUs7Fte0GovUlXrgfWvZEdJxqpq2Sv5jCON5+RgnpMX83wc7Eg/JzP6QT2wDVic5LQkxwCXAJtH3CdJmrVm9JVKVe1PcjlwMzAH2FBV20fcLUmatWZ0qABU1RZgyzTs6hXdPjtCeU4O5jl5Mc/HwY7oc5KqGnUfJElHiJn+TEWS9CpiqAxgtr4KJsmGJHuS3N9XOynJ1iQPtb8ntnqSXNPO0b1Jzhhdz4cjycIktyXZkWR7kg+0+mw+J8cluTPJ37Rz8oetflqSO9qxf6kNpCHJsW15vK1fNMr+D0uSOUnuTvL1tjxrzoehMoVZ/iqYzwErDqitA26tqsXArW0ZeudncZvWANdOUx+n037gQ1W1BFgOrG3/Lczmc/IscHZVvRVYCqxIshz4JHB1Vb0JeAJY3dqvBp5o9atbuyPRB4AH+pZnzfkwVKb2wqtgquo5YOJVMEe8qvo2sO+A8kpgY5vfCFzUV7++em4HTkhy6vT0dHpU1e6q+k6b/xG9fzTmM7vPSVXVM23x6DYVcDZwY6sfeE4mztWNwDlJMk3dnRZJFgAXAn/elsMsOh+GytQmexXM/BH15dVgXlXtbvM/BOa1+Vl1ntptitOBO5jl56Td6rkH2ANsBb4LPFlV+1uT/uN+4Zy09U8BJ09vj4fuT4D/APy0LZ/MLDofhopetuoNHZx1wweTvA74CvDBqnq6f91sPCdV9XxVLaX3RoszgTePuEsjk+S3gD1Vddeo+zIqhsrUBnoVzCzy2MQtnPZ3T6vPivOU5Gh6gfKFqvpqK8/qczKhqp4EbgN+jd6tvonvwfUf9wvnpK1/A/D4NHd1mN4OvCvJI/RulZ8N/Cmz6HwYKlPzVTAvthlY1eZXATf11S9rI56WA0/13RI6IrR73dcBD1TVp/pWzeZzMjfJCW3+eOCd9J413QZc3JodeE4mztXFwLfqCPqyXFVdUVULqmoRvX8rvlVV/4rZdD6qymmKCbgA+D/07hX/x1H3ZxqP+4vAbuDv6d0HXk3vfu+twEPALcBJrW3ojZL7LnAfsGzU/R/C+fh1ere27gXuadMFs/yc/Cpwdzsn9wN/0OpvBO4ExoEvA8e2+nFtebytf+Ooj2GI5+YdwNdn2/nwG/WSpM54+0uS1BlDRZLUGUNFktQZQ0WS1BlDRZLUGUNFktQZQ0WS1BlDRZLUmf8PCECnlCDXb8sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1\n",
      "Max: 444\n",
      "\n",
      "\n",
      " previous\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Max: 3\n"
     ]
    }
   ],
   "source": [
    "# Histogram plots\n",
    "for feature in numerical:\n",
    "    interpret(feature, 'hist', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " age\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 18\n",
      "Max: 87\n",
      "\n",
      "\n",
      " balance\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -3058\n",
      "Max: 6046\n",
      "\n",
      "\n",
      " campaign\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Max: 15\n",
      "\n",
      "\n",
      " pdays\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1\n",
      "Max: 444\n",
      "\n",
      "\n",
      " previous\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADalJREFUeJzt3V2MHeV9x/HvD9uBtCBo45WCjM1GAeWV8rai0LQNKo0ECMUXAcW0JQQRWaLQBCm5iHJBFK7KDVFSUqhVECZFCS2JIjcyRUjQBpBCWTsGYxwqKwHZCImNSezQkBen/17s4G7XZzlnd8/6sA/fjzTyvDyeeYzs745m57CpKiRJbTlm1BOQJA2fcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWrQylFdePXq1TU+Pj6qy0vSsrRt27afVNVYv3Eji/v4+DiTk5OjurwkLUtJXhhknI9lJKlBxl2SGmTcJalBxl2SGmTcJalBfeOe5Lgk/5nkqSS7knypx5hjk9yXZE+SJ5KML8VkpaWW5IhFWo4GuXP/FfBnVXUmcBZwcZLzZ425FvhpVZ0GfBm4ZbjTlJbeXCE38FqO+sa9pr3aba7qltk/m289sLlbvx+4KP6L0DJVVYcXabka6Jl7khVJdgAvAw9V1ROzhqwB9gJU1SHgAPCOHufZmGQyyeTU1NTiZi5JmtNAca+q31bVWcApwHlJPriQi1XVpqqaqKqJsbG+n56VJC3QvN6WqaqfAY8AF8869CKwFiDJSuBEYP8wJigdbX4zVS0Y5G2ZsSQndetvBz4C/HDWsC3A1d365cDD5QNLLTNz/ZX1r7KWo0H+x2EnA5uTrGD6i8E/V9V3k9wMTFbVFuBO4OtJ9gCvABuWbMbSEjLkakXfuFfV08DZPfbfNGP9l8AVw52aJGmh/ISqJDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg/rGPcnaJI8keTbJriSf6THmwiQHkuzolpuWZrqSpEGsHGDMIeCzVbU9yQnAtiQPVdWzs8Y9WlWXDX+KkqT56nvnXlUvVdX2bv3nwG5gzVJPTJK0cPN65p5kHDgbeKLH4QuSPJXkgSQfmOP3b0wymWRyampq3pOVJA1m4LgnOR74FnBjVR2cdXg7cGpVnQn8HfCdXueoqk1VNVFVE2NjYwudsySpj4HinmQV02G/t6q+Pft4VR2sqle79a3AqiSrhzpTSdLABnlbJsCdwO6qunWOMe/sxpHkvO68+4c5UUnS4AZ5W+ZDwFXAziQ7un1fANYBVNUdwOXAdUkOAa8BG6qqlmC+kqQB9I17VT0GpM+Y24DbhjUpSdLi+AlVSWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWpQ37gnWZvkkSTPJtmV5DM9xiTJV5PsSfJ0knOWZrqSpEGsHGDMIeCzVbU9yQnAtiQPVdWzM8ZcApzeLX8I3N79Ki0rSY7YV1UjmIm0OH3v3Kvqpara3q3/HNgNrJk1bD1wT037PnBSkpOHPltpCfUK+xvtl97M5vXMPck4cDbwxKxDa4C9M7b3ceQXAGlZqKrDi7RcDRz3JMcD3wJurKqDC7lYko1JJpNMTk1NLeQUkqQBDBT3JKuYDvu9VfXtHkNeBNbO2D6l2/f/VNWmqpqoqomxsbGFzFeSNIBB3pYJcCewu6punWPYFuAT3Vsz5wMHquqlIc5TOmqSHF6k5WqQt2U+BFwF7Eyyo9v3BWAdQFXdAWwFLgX2AL8Arhn+VKWlVVW+LaNm9I17VT0GvOEtTE3/7b9+WJOSRsWQqxV+QlWSGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBfeOe5K4kLyd5Zo7jFyY5kGRHt9w0/GlKkuZj5QBj7gZuA+55gzGPVtVlQ5mRJGnR+t65V9X3gFeOwlwkSUMyrGfuFyR5KskDST4wpHNKkhZokMcy/WwHTq2qV5NcCnwHOL3XwCQbgY0A69atG8KlJUm9LPrOvaoOVtWr3fpWYFWS1XOM3VRVE1U1MTY2tthLS5LmsOi4J3lnknTr53Xn3L/Y80qSFq7vY5kk3wAuBFYn2Qd8EVgFUFV3AJcD1yU5BLwGbKiqWrIZS5L66hv3qrqyz/HbmH5VUpL0JuEnVCWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhrUN+5J7krycpJn5jieJF9NsifJ00nOGf40JUnzMcid+93AxW9w/BLg9G7ZCNy++GlJo5HkiEVajvrGvaq+B7zyBkPWA/fUtO8DJyU5eVgTlI6WuUJu4LUcDeOZ+xpg74ztfd0+aVmqqsOLtFwd1W+oJtmYZDLJ5NTU1NG8tCS9pQwj7i8Ca2dsn9LtO0JVbaqqiaqaGBsbG8KlJUm9DCPuW4BPdG/NnA8cqKqXhnBeaST8ZqpasLLfgCTfAC4EVifZB3wRWAVQVXcAW4FLgT3AL4Brlmqy0lKqqp5B99m7lqO+ca+qK/scL+D6oc1IGiFDrlb4CVVJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGDRT3JBcneS7JniSf73H8k0mmkuzolk8Nf6qSpEGt7DcgyQrga8BHgH3Ak0m2VNWzs4beV1U3LMEcJUnzNMid+3nAnqr6UVX9GvgmsH5ppyVJWoxB4r4G2Dtje1+3b7aPJXk6yf1J1g5ldpKkBRnWN1T/FRivqj8AHgI29xqUZGOSySSTU1NTQ7q0JGm2QeL+IjDzTvyUbt9hVbW/qn7Vbf4jcG6vE1XVpqqaqKqJsbGxhcxXkjSAQeL+JHB6kncleRuwAdgyc0CSk2dsfhTYPbwpSpLmq+/bMlV1KMkNwIPACuCuqtqV5GZgsqq2AJ9O8lHgEPAK8MklnLMkqY9U1UguPDExUZOTkyO5tiQtV0m2VdVEv3F+QlWSGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBA8U9ycVJnkuyJ8nnexw/Nsl93fEnkowPe6KSpMH1jXuSFcDXgEuA9wNXJnn/rGHXAj+tqtOALwO3DHuikqTBDXLnfh6wp6p+VFW/Br4JrJ81Zj2wuVu/H7goSYY3TUnSfAwS9zXA3hnb+7p9PcdU1SHgAPCOYUxQkjR/K4/mxZJsBDYCrFu37mheWo04Y/MZo57CUO28eueop6BGDRL3F4G1M7ZP6fb1GrMvyUrgRGD/7BNV1SZgE8DExEQtZMJ6azOG0mAGeSzzJHB6kncleRuwAdgya8wW4Opu/XLg4aoy3pI0In3v3KvqUJIbgAeBFcBdVbUryc3AZFVtAe4Evp5kD/AK018AJEkjMtAz96raCmydte+mGeu/BK4Y7tQkSQvlJ1QlqUHGXZIaZNwlqUHGXZIaZNwlqUEZ1evoSaaAF0Zycam/1cBPRj0JqYdTq2qs36CRxV16M0syWVUTo56HtFA+lpGkBhl3SWqQcZd62zTqCUiL4TN3SWqQd+6S1CDjLs0hyc1J/nzU85AWwscyektIsqKqfjvqeUhHi3fuWvaSjCf5YZJ7k+xOcn+S30nyfJJbkmwHrkjy7iT/lmRbkkeTvDfJiUleSHJMd67fTbI3yaokdye5vNt/UZIfJNmZ5K4kx3b7n0+yulufSPLv3fqHk+zolh8kOWE0/3X0VmXc1Yr3AH9fVe8DDgJ/3e3fX1XnVNU3mX4D5m+q6lzgc934A8AO4MPd+MuAB6vqN6+fOMlxwN3Ax6vqDKZ/DsJ1febzOeD6qjoL+BPgtSH8GaWBGXe1Ym9VPd6t/xPwx936fQBJjgf+CPiXJDuAfwBOnjHm4936htd/zwzvAX5cVf/VbW8G/rTPfB4Hbk3yaeCkqjo0/z+StHDGXa2Y/c2j17f/u/v1GOBnVXXWjOV93bEtwMVJfh84F3h4Htc9xP/9Ozru8MWr/hb4FPB24PEk753HOaVFM+5qxbokF3TrfwE8NvNgVR0EfpzkCoBMO7M79irTPwj+K8B3e3zj9TlgPMlp3fZVwH90688z/QUB4GOv/4Yk766qnVV1S3du466jyrirFc8B1yfZDfwecHuPMX8JXJvkKWAXsH7GsfuAv+LIRzKv/4zga5h+pLMT+B/gju7wl4CvJJkEZn5RuDHJM0meBn4DPLCYP5w0X74KqWUvyTjTd9wfHPFUpDcN79wlqUHeuUtSg7xzl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJatD/AmpIdLYKJeq9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Max: 3\n"
     ]
    }
   ],
   "source": [
    "# Box plots\n",
    "for feature in numerical:\n",
    "    interpret(feature, 'box', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " age\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt0nXWd7/H3Nzv3+7Vtbm1a2lLaImDDTfA2gHJRqkdYgqjgMMczSxlHPeMZ1Dku9MyZo+NZ4zgjOqI4MowOKF6mCsIRcBxQKW25SG9AeiNpkja35n7P7/yxn9RNSLt3kv3s50n257VWVvZ+9pOdb5/V5JPf87uZcw4REZHTyQi6ABERCT+FhYiIxKWwEBGRuBQWIiISl8JCRETiUliIiEhcCgsREYlLYSEiInEpLEREJK7MoAtIlsrKStfQ0BB0GSIii8quXbs6nXNV8c5bMmHR0NDAzp07gy5DRGRRMbMjiZyn21AiIhKXwkJEROJSWIiISFwKCxERiUthISIicSksREQkLoWFiIjEpbCQRcM5x0+fPcoPdzYzNaXtgEVSaclMypOl757fHuaOn+0FYE9rH3dcuyngikTSh1oWsihMTE5x138e5ILV5dx88Sq++9vD7DjcHXRZImlDYSGLwo7DPbT2jnDzxQ3cftVZVBbm8A+PvRx0WSJpQ2Ehi8LvDnaRYfDG9ZXkZUf4kzeu5omXO3m++UTQpYmkBYWFLArbD3axubaE4twsAG66cCXFuZnc9cTBgCsTSQ8KCwk95xx72/o4p6705LGi3Cyub6znkd3tHO8fCbA6kfSgsJDQa+0doX9kgjNXFL3q+E0XrmRiynH/080BVSaSPhQWEnovtvcBsGFGWKypKuTStZX829OvMDE5FURpImlDYSGhd+D4IADrlhW95rX3X7SK1t4RfvViR6rLEkkrCgsJveaeIYpzMynJz3rNa5eftYzKwmx+/ExLAJWJpA+FhYRec/cQ9eX5s76WGcngnefU8Ni+4/QOjae4MpH0obCQ0GvuGaa+bPawAHj3ebWMTU7x0O62FFYlkl58DQszu9LMXjSzJjO7fZbXc8zsfu/17WbWMOP1lWY2YGZ/4WedEl7OOVp6hqgryzvlOWfXlrCmsoAHf6+wEPGLb2FhZhHgTuAqYCNwo5ltnHHarUCPc24t8BXgSzNe/zvgF37VKOHXMTDKyPjUKW9DAZgZl29czvZDXfSP6FaUiB/8bFlcADQ55w4658aA+4CtM87ZCtzjPX4AuMzMDMDM3gUcAvb4WKOEXHP3MAD15aduWQBctmEZ45OOJ1/uTEVZImnHz7CoBWJnS7V4x2Y9xzk3AfQCFWZWCPwl8Hkf65NFoK03Gha1paduWQBsWVVGcW4mv35JQ2hF/BDWDu47gK845wZOd5KZfdjMdprZzo4O/ZJYiroGxgCoKMw+7XmZkQwuWF3BUwe7UlGWSNrxMyyOAvUxz+u8Y7OeY2aZQAnQBVwI/K2ZHQY+DnzGzG6b+Q2cc3c55xqdc41VVVXJ/xdI4LoGRjGDsvzThwXARWvKOdw1dLI1IiLJ42dY7ADWmdlqM8sGbgC2zThnG3Cz9/g64HEX9UbnXINzrgH4e+BvnHNf87FWCamuwTHK87OJZFjccy9aUwHA9oPaFEkk2XwLC68P4jbgEWAf8APn3B4z+4KZXeuddjfRPoom4JPAa4bXSnrrGhijvCB+qwLgrOpiCrIjPPNKj89ViaQfX/fgds49BDw049jnYh6PANfHeY87fClOFoXuwbG4/RXTIhnG5toSnm/p9bkqkfQT1g5uEQA6B0epKMhJ+Pxz60vZ19rH2IRWoRVJJoWFhFrXQOItC4DX1ZUyNjnFfm9ZcxFJDoWFhNb45BS9w+MJ91kAvK6uBEC3okSSTGEhodUzOD3HIvHbUHVleVQUZPP75hN+lSWSlhQWElpd02Exh5aFWbSTe3erbkOJJJPCQkLr5OztOYQFwObaYl4+1s/I+KQfZYmkJYWFhFbX4CgQf6mPmTbXlDAx5XjpWL8fZYmkJYWFhNYfWhaJ91kAbK6NdnLvPqpbUSLJorCQ0OoeHCOSYZTkvXbv7dOpK8ujODeT3a0aESWSLAoLCa2uwVHK8rPJSGBdqFhmxqaaEvaok1skaRQWElqdA2NUzrG/Ytrm2mL2tfUxPqmZ3CLJoLCQ0OoeTHwRwZk215YwNjHFgY7TbokiIglSWEhodQ2MzmlCXqxNNerkFkkmhYWEVtfg2JznWExbXVlAfnaE3UfVyS2SDAoLCaXRiUn6RybmHRaRDOOs6mL2qpNbJCkUFhJKPYPjAJTPs4MbYHNNMXtae5macskqSyRtKSwklDoHvNnbc5yQF2tTbQmDY5Mc7hpMVlkiaUthIaHUfXLF2YW0LLxObt2KElkwhYWE0sl1oebZZwGwbnkh2ZEM9qiTW2TBFBYSSvNdFypWViSDDdVFmsktkgQKCwmlrsExMjOM4rzMBb3Ppppidrf24pw6uUUWQmEhoRSdkJeN2dzWhZppU00JJ4bGOXpiOEmViaQnhYWEUnSpj/nfgpqm5cpFkkNhIaG0kEUEY21YUUQkw9ij5cpFFkRhIaG0kEUEY+VmRVi3rFDLfogskMJCQqlrYHRBI6Fibawp5oWjferkFlkAhYWEzsj4JINjkwuakBfr/IZyOgdGOdChmdwi86WwkNDpmp69nYTbUACXrq0E4DdNnUl5P5F0pLCQ0On2JuQlo88CoL48n/ryPJ5UWIjMm8JCQqdzeqmPeW58NJtL11by1IEuJrTNqsi8KCwkdP6w1EdyWhYAl6ytpH90gmebTyTtPUXSicJCQqf7ZMsieWHxpvVVZEWMR/ceS9p7iqQThYWETtfAGNmRDApzFrYuVKzi3CwuWlPBLxUWIvOisJDQ6RocS8q6UDNdsXE5BzsHaTo+kNT3FUkHCgsJnelFBJPt8rOWA6h1ITIPCgsJnWQtIjhTTWkem2uL+eXe9qS/t8hSp7CQ0OkcGKMyiSOhYr1t4wqebT7Bsb4RX95fZKnyNSzM7Eoze9HMmszs9llezzGz+73Xt5tZg3f8AjN7zvt43sze7WedEi7JWkRwNlefvQLn4OHdal2IzIVvYWFmEeBO4CpgI3CjmW2ccdqtQI9zbi3wFeBL3vHdQKNz7lzgSuCbZpa8oTESWkNjEwyPTyZ1Ql6stcuKWL+8kAdfaPPl/UWWKj9bFhcATc65g865MeA+YOuMc7YC93iPHwAuMzNzzg055ya847mAlgtNE35MyJvp6rOr2XG4m+O6FSWSMD/DohZojnne4h2b9RwvHHqBCgAzu9DM9gAvAH8aEx4nmdmHzWynme3s6Ojw4Z8gqXZyEUEfRkNNu+bs6uitqD26FSWSqNB2cDvntjvnNgHnA582s9xZzrnLOdfonGusqqpKfZGSdF0D0dnbfvVZAKxbXsS6ZYU8+HvdihJJlJ9hcRSoj3le5x2b9RyvT6IE6Io9wTm3DxgANvtWqYRGpxcWlT71WUy7+uxqnj7czfF+3YoSSYSfYbEDWGdmq80sG7gB2DbjnG3Azd7j64DHnXPO+5pMADNbBWwADvtYq4REp9dnUVXkb1hc87rorahHNCpKJCG+hYXXx3Ab8AiwD/iBc26PmX3BzK71TrsbqDCzJuCTwPTw2kuB583sOeAnwEecc9qMIA109I9SmJNJblbE1++zfnkRa5dpVJRIonwdjuqcewh4aMaxz8U8HgGun+Xr7gXu9bM2CafOgVEqfezcjnX12dV87fGX6egf9b0lI7LYhbaDW9JTNCxS84v7mrOrmdKoKJGEKCwkVDoHxlIWFuuXF7J2WSHbnps57kJEZlJYSKh0DoxSWZSa21BmxrvPq2XH4R6OdA2m5HuKLFYKCwmNsYkpTgyNp6xlAfBfXl+LGfzoGbUuRE5HYSGh0TWYmjkWsapL8rjkjEp+/EwLU1NaVUbkVBQWEhqd/dE5FqkMC4D3bKmlpWeY7Ye6U/p9RRYThYWExvTs7aoU9VlMu3JTNUW5mfzrU0dS+n1FFhOFhYRGR4qW+pgpLzvCTReu4he723ilayil31tksVBYSGikal2o2dzyhgYiGcZdTxxI+fcWWQwUFhIax/uiS30U5KR+n6sVJbm89/x6vr/9Ffa09qb8+4uEncJCQqO9d4QVJa9ZiT5lPvW2DZQXZPNn33+W1hPDgdUhEkbaqlRCo71vhBXFwYVFSX4WX79pCx/656d5y5f/g9WVBRTnZXJOXSkfeetaX/fYEAk7tSwkNIJuWQBcsLqcX/z5m/jQJQ2sqsjHML7728O8687fnOxTEUlHallIKExOOToGRgNtWUxbWZHPp68+6+TzXUd6eN+3nuKzP3mBb36gMcDKRIKjloWEQufAKJNTLvCWxWy2rCrjY5et45E9x3jmlZ6gyxEJREJhYWY/NrNrzEzhIr5o641ubxqGlsVsbnlDA6X5WXz9V01BlyISiER/+X8deB/wspl90czO9LEmSUPt02ERwpYFQEFOJh+8aBWP7T9OS48m7kn6SSgsnHOPOuduAl5PdC/sR83st2b2ITPL8rNASQ/tvdGhqmENC4DrG+sB+NEurVAr6Sfh20pmVgHcAvwJ8CzwVaLh8UtfKpO00t43SnYkg/L88A5PrS/P55IzKvnhrmatUCtpJ9E+i58ATwD5wDudc9c65+53zv0ZUOhngZIejvWNsKw4h4wMC7qU07q+sY6WnmGeOtgVdCkiKZVoy+JbzrmNzrn/45xrAzCzHADnnMYSyoK1nhimOsS3oKa9fdMKinIyeWBXS9CliKRUomHx17Mc+10yC5H01tIzTH1ZftBlxJWbFeEd59Tw0O42+kfGgy5HJGVOGxZmtsLMtgB5Znaemb3e+3gL0VtSIgs2PjlFW+8wdWV5QZeSkOsb6xgZn+KhF9qCLkUkZeLN4H470U7tOuDvYo73A5/xqSZJM+29I0w5qFsELQuA8+pLWVNVwAO7Wnjv+SuDLkckJU4bFs65e4B7zOw9zrkfpagmSTPN3dF5C3Xli6NlYWZcv6WeLz28n8OdgzRUFgRdkojv4t2Ger/3sMHMPjnzIwX1SRpo6YnOsVgMfRbT3n1eLRkG9+9sDroUkZSI18E9/SdTIVA0y4fIgrX0DJFh4Z6QN9OKklzetnEF//rUEXV0S1qIdxvqm97nz6emHElHzT3DVJfkkRVZXEuPfeStZ/Dwnna+t/0V/vTNZwRdjoivEp2U97dmVmxmWWb2mJl1xNyiElmQlp6hRTMSKtbr6kp547pK/unXBzgxNBZ0OSK+SvRPubc55/qAdxBdG2ot8Cm/ipL00tw9vGhGQs30mavPom94nK/88qWgSxHxVaJhMX276hrgh8457WgvSTE6Mcmx/pFF2bIAOKu6mPdftIp7nzrCjsPdQZcj4ptEw+LnZrYf2AI8ZmZVwIh/ZUm6aDsxgnPRRfoWq/9x5QbqyvL5+H3P0Tuszm5ZmhJdovx24A1Ao3NuHBgEtvpZmKSHZm9viMXasgAozMnkqzecS3vfCJ+8/zkmtSKtLEFzGX6yAXivmX0QuA54mz8lSTpp7vbmWCzilgXAeSvLuOOdG3ls/3H+94P7gi5HJOniLfcBgJndC5wBPAdMeocd8C8+1SVporlniKyIhXY71bn4wMUNHOwc5Du/OcTqqgI+cNGqoEsSSZqEwgJoBDY659S+lqRq7h6ipjSPSMj3sUjUX12zkSNdQ3x+2x7WVhVy8RkVQZckkhSJ3obaDayY65ub2ZVm9qKZNZnZ7bO8nmNm93uvbzezBu/4FWa2y8xe8D7/0Vy/tywOzT2LZ7XZREQyjK/ecC4NlQV85Hu7aPO2ixVZ7BINi0pgr5k9Ymbbpj9O9wVmFgHuBK4CNgI3mtnGGafdCvQ459YCXwG+5B3vJLoj39nAzcC9CdYpi8zRnqFFtSZUIopys7jrA1sYHp/ksz/ZjRrkshQkehvqjnm89wVAk3PuIICZ3Ud0BNXemHO2xrz3A8DXzMycc8/GnLOH6H4aOc650XnUISE1NDZB58DYou/cns2aqkI+9fYN/K+f7+Vnv2/j2nNqgi5JZEESHTr7a6Izt7O8xzuAZ+J8WS0QuyRni3ds1nOccxNALzDzJu97gGcUFEvP9GqzS+k2VKxb3tDAWdXFfPmR/YxNTAVdjsiCJLo21H8l+pf/N71DtcBP/Soq5vtuInpr6r+d4vUPm9lOM9vZ0dHhdzmSZNP7WCzFlgVE+y/+8sozae4e5vvbjwRdjsiCJNpn8VHgEqAPwDn3MrAsztccBepjntd5x2Y9x8wygRKgy3teB/wE+KBz7sBs38A5d5dzrtE511hVVZXgP0XC4mRYLLE+i1hvXl/FRWvK+cfHmxgcnQi6HJF5SzQsRp1zJ5fV9H6xx+u12wGsM7PVZpYN3ADM7BTfRrQDG6IT/R53zjkzKwUeBG53zv0mwRplkWnuGSYvK0JlYXbQpfjGzPjU2zfQNTjGPb87HHQ5IvOWaFj82sw+Q7Sj+Qrgh8DPTvcFXh/EbcAjwD7gB865PWb2BTO71jvtbqDCzJqATwLTw2tvI7qy7efM7DnvI15LRhaZ5u7o0uRmS2OOxalsWVXGW8+s4pu/PkifNkqSRcoSGdZnZhlEh7m+DTCiAfDtME3Sa2xsdDt37gy6DJmDq776BNUluXznlvODLsV3L7T08s6vPcnHL1/Hxy9fH3Q5IieZ2S7nXGO88xIdDTVFtEP7I86565xz3wpTUMji45yjpXuI+iU6Emqms+tKePum5dz9xCFtlCSL0mnDwqLuMLNO4EXgRW+XvM+lpjxZqvqGJ+gfnViyI6Fm84kr1jM4NsEXf7E/6FJE5ixey+ITREdBne+cK3fOlQMXApeY2Sd8r06WrKWwNPlcbVhRzIffdAb37Wjm8f3Hgi5HZE7ihcUHgBudc4emD3gzst8PfNDPwmRpmx42u1i3U52vT1yxjo3VxfzZ95/VznqyqMQLiyznXOfMg865DiDLn5IkHRw9sbRnb59KTmaE737ofJYV53LjXU/x7ScOMqXNkmQRiBcWp+uJUy+dzFtb7wh5WRFK8tLvb45lxbn89KOX8EcblvHXD+7jhm89xStdQ0GXJXJa8cLiHDPrm+WjHzg7FQXK0tTeO0J1ae6Sn2NxKiV5WXzzA1v48nWvY19rH+/82pM833wi6LJETum0YeGcizjnimf5KHLOpd+fhJI0rb3DVJcs/t3xFsLMuL6xngc/9kaK8zL50Hd30HpC+19IOM1lD26RpGk7MUJ1SXr1V5zKyop8/vmWCxibmOIvfvi89r+QUFJYSMpNTE5xvH+EmjRvWcRau6yQv7xqA7890MXDu9uDLkfkNRQWknLH+0eZcrBCLYtXed8FK1m3rJB/fLxJrQsJHYWFpNz0vtTVpWpZxIpkGH986Wr2tvXx9CHNwZBwUVhIyrWeGAGgRi2L13jXubUU5WTywK6WoEsReRWFhaRce280LNSyeK287Ahv3bCMR/cdY2JSW7FKeCgsJOVae4cpyI5QlJMZdCmh9PZNK+gZGmfnkZ6gSxE5SWEhKdd2YoTq0qW/6dF8vfnMKiIZxpMvv2alHZHAKCwk5dr6RtJ+Qt7pFOZksrG6WAsNSqgoLCTl2k5o9nY8W1aV8XzLCcbVbyEhobCQlBqbmKJjYFSzt+NobChjZHyKPa19QZciAigsJMWO94/gHNRoJNRpnbeyDIAXjvYGXIlIlMJCUqrNGzar2dunV1OSS1FuJi+2q2Uh4aCwkJSaXlVV60Kdnplx5vIiXmofCLoUEUBhISn2hwl5alnEs35FEfvb+7ROlISCwkJSqq13hKKcTAo1IS+uDSuK6BuZ4FjfaNCliCgsJLVaTwxrmY8ErV9eBMBLx/oDrkREYSEp1t6nTY8StbqyAIAj3dqfW4KnsJCUaj0xomGzCVpWlENuVgZHOgeDLkVEYSGpMzoxSefAKCuK1bJIhJmxqryAw11qWUjwFBaSMsd6ox216rNI3KqKfF7pVstCgqewkJRp7Z2eY6GWRaJWVeRzpGuIqSkNn5VgKSwkZbTp0dytqihgdGKKY/0jQZciaU5hISkz3bLQirOJqy/PB6ClZzjgSiTdKSwkZdpOjFCSl0V+tibkJarWa4VNL5MiEhSFhaRMW6/2sZir6TkpRxUWEjCFhaRMdI6FOrfnoiAnk9L8LNpOqM9CgqWwkJRp13aq81JTkqfbUBI4hYWkxMj4JN2DYwqLeagpzdNtKAmcr2FhZlea2Ytm1mRmt8/yeo6Z3e+9vt3MGrzjFWb2KzMbMLOv+VmjpMb0pkdaF2ruaktz1bKQwPkWFmYWAe4ErgI2Ajea2cYZp90K9Djn1gJfAb7kHR8B/ifwF37VJ6nV5v2y0xyLuaspzaNvZIL+kfGgS5E05mfL4gKgyTl30Dk3BtwHbJ1xzlbgHu/xA8BlZmbOuUHn3JNEQ0OWgFavZaHZ23M3PShgunUmEgQ/w6IWaI553uIdm/Uc59wE0AtUJPoNzOzDZrbTzHZ2dHQssFzxU7s3IW+F+izmbDos1G8hQVrUHdzOubucc43Oucaqqqqgy5HTaO0dobwgm9ysSNClLDq1Xlio30KC5GdYHAXqY57XecdmPcfMMoESoMvHmiQgLT3DJ3/pydxUFeWQmWEKCwmUn2GxA1hnZqvNLBu4Adg245xtwM3e4+uAx512p1+SWrqHqC9XWMxHJMNYUZLLUa0PJQHybZEe59yEmd0GPAJEgO845/aY2ReAnc65bcDdwL1m1gR0Ew0UAMzsMFAMZJvZu4C3Oef2+lWv+GdqytHSM8wVm5YHXcqiVVOad3KQgEgQfF3RzTn3EPDQjGOfi3k8Alx/iq9t8LM2SZ1j/SOMTU5RX5YfdCmLVm1pHjsOdwddhqSxRd3BLYtDc3f09sn0ctsydzWlubT3jjCpTZAkIAoL8V1zd3QP6ZUKi3mrKc1jYsrR0T8adCmSphQW4rtXuocwi/51LPOjuRYSNIWF+K65Z4gVxbnkZGqOxXxproUETWEhvmvpHlbn9gJNr9arsJCgKCzEd4e7BllZobBYiKLcLIpzMxUWEhiFhfiqb2Sc4/2jnFFVGHQpi150XwvNtZBgKCzEVweODwCwdpnCYqFqS7VjngRHYSG+alJYJI12zJMgKSzEV00dA2RHMqgv07pQC1VTmkfv8DgDoxNBlyJpSGEhvjpwfJCGynwyI/qvtlDT81Ta1LqQAOgnWHx1oGNAt6CSpFYT8yRACgvxzejEJK90D2kkVJLUnJyYpxFRknoKC/HNy8cGmJxyrF9eFHQpS8KyohwiGUZLz1DQpUgaUliIb3Yf7QXg7NqSgCtZGjIjGdSV5XGkS2EhqaewEN/sbu2lKDeTVZq9nTSrKws41DkYdBmShhQW4pvdR/vYVFOMmQVdypLRUFHA4a5BtPuwpJrCQnwxNjHFvrY+NtfoFlQyra4sYGhsUvtaSMopLMQXu1t7GZ2YYsuqsqBLWVIaKgsAdCtKUk5hIb7Y6e0X3dhQHnAlS8vqCoWFBENhIb54+lAPqysLqCrKCbqUJaW2LI+8rAgvHusPuhRJMwoLSbrJKceOw92c36BbUMkWyTA2VBext7Uv6FIkzSgsJOmea+6hd3icN62vCrqUJWljdTF72/o0IkpSSmEhSfcfL3YQyTDeuFZh4YeNNcX0j0zQ0qM1oiR1FBaSdI/uO87rV5ZSkp8VdClL0sbqYuAPM+RFUkFhIUnVdLyffW19XLW5OuhSlqyNNcXkZUV46mBX0KVIGlFYSFL99NlWMgzecY7Cwi85mREuWF3Obw4oLCR1FBaSNFNTjn9//iiXrK1kWVFu0OUsaZeuraTp+ADtvVquXFJDYSFJ8+uXOmjuHua6LXVBl7LkvfnM6OCBh3e3BVyJpAuFhSTNt588yIriXK4+W7eg/LZ+eRGba4u5b0ezhtBKSigsJCleaOnlN01d3HJJA1nabzslPvSG1exv7+fBF9S6EP/pp1oWzDnHXz+4l/KCbN534cqgy0kb7zqvls21xfzVT3drGK34TmEhC/bw7na2H+rmE1espzhXcytSJZJh3Pm+11OQncl7vvFbPv+zPbzQ0qvbUuILWyr/sRobG93OnTuDLiPtHO8b4cqvPsGK4ly23XYJmboFlXLH+0b44i/2s+35ViamHJWF2Vy4poI3nFHBxWsqWF1ZoA2o5JTMbJdzrjHueQoLma/RiUk+ePfTPNd8ggc/dilrlxUFXVJaOzE0xqP7jvObpk5+d6CL9r7osNrlxTlcvKaCi8+o4Nz6MlZV5JObFQm4WgmLRMMiMxXFyNIzNjHFJ+9/nu2Huvn7956roAiB0vxsrttSx3Vb6nDOcbhriN8eiAbHk02d/PS5VgDMoKYkj1UV+d5HAWeuKGJTdTFVRTlqhcisfA0LM7sS+CoQAb7tnPvijNdzgH8BtgBdwHudc4e91z4N3ApMAh9zzj3iZ62SuN7hcT76vWd4sqmTv7rmLN51Xm3QJckMZsbqygJWVxZw04WrcM7RdHyAvW19HOwY5HDXIEe6hnhkzzG6B8dOfl1FQTZnVRdzVnWR97mYtcsKNcJN/AsLM4sAdwJXAC3ADjPb5pzbG3ParUCPc26tmd0AfAl4r5ltBG4ANgE1wKNmtt45N+lXvRKfc45fvXicz/5kNx39o3z5utdxfWN90GVJAsyMdcuLWLf8tS3A3qFx9rf3sa+tj71tfexr6+ee3x1hbGIKgKyIcUZVIWX52eRlR8jLilCYk0lpQRZl+dmUF2RTVZRDVWEOVUU5lBdkK1yWID9bFhcATc65gwBmdh+wFYgNi63AHd7jB4CvWbQNvBW4zzk3Chwysybv/X7nY70yi7GJKV7pHuJ3Bzp5YFcLz7f0snZZId94/xbOrS8NujxJgpL8LC5cU8GFaypOHpuYnOJQ5+DJ8HjpWD/9I+Mc7x9naGyS/pEJTgyNMT45e59neUE2ZflZFOZkUpibSUF29HNRTiYF3rHCnOhHQU70eH5OJjmZGWRnZsR8jkQfRzLIyNDtsSD5GRa1QHPM8xbgwlOd45ybMLNeoMJxwlxPAAAGtElEQVQ7/tSMr/XlXsf+9j5u+/6zeDWcPP6qHwE368NTnu9edb6b/fgpxhUs6D1PcT6znJ/ov6N/ZJwp7+CaygL+5t1n854tteRkqoN0KcuMZJxsiWw9d/ZznHMMjU3SPTjG8f5ROvpH6Rz4w+eeoTEGRicZGBmns3+MgdEJBkYnGBydYGJq7gNrsiJGdiSDnKwI2ZFomGRnZqAIgbecWcVnr9no6/dY1B3cZvZh4MMAK1fObzJYbmaEM2Ob5jbrw1d1+r36+NzOf/X7x5xzyvdJ4PxTfIO5vOepzi3Jz6a+LI8tq8o0BFNexcwo8FoG9eX5CX+dc47RialoeIz8IUAGxyYYm5hi1PsYe9XnyVmfj01O+fgvXDyWF/u/cKefYXEUiL2hXecdm+2cFjPLBEqIdnQn8rU45+4C7oLo0Nn5FNlQWcCdN71+Pl8qIvNgZuRmRcjNilBZmBN0OZIgP3uhdgDrzGy1mWUT7bDeNuOcbcDN3uPrgMdd9J7INuAGM8sxs9XAOuBpH2sVEZHT8K1l4fVB3AY8QnTo7Hecc3vM7AvATufcNuBu4F6vA7ubaKDgnfcDop3hE8BHNRJKRCQ4msEtIpLGEp3BrcHQIiISl8JCRETiUliIiEhcCgsREYlLYSEiInEtmdFQZtYBHDnNKZVAZ4rKSQbV6y/V6y/V679k1bzKOVcV76QlExbxmNnORIaHhYXq9Zfq9Zfq9V+qa9ZtKBERiUthISIicaVTWNwVdAFzpHr9pXr9pXr9l9Ka06bPQkRE5i+dWhYiIjJPSzoszOzLZrbfzH5vZj8xs9KY1z5tZk1m9qKZvT3IOmOZ2ZVeTU1mdnvQ9cxkZvVm9isz22tme8zsz73j5Wb2SzN72ftcFnStscwsYmbPmtnPveerzWy7d53v95bRDw0zKzWzB7z/v/vM7OIwX2Mz+4T3/2G3mf2bmeWG6Rqb2XfM7LiZ7Y45Nuv1tKh/8Or+vZmlfMObU9Qb6O+zJR0WwC+Bzc651wEvAZ8GMLONRJdD3wRcCXzdzALfJ9Sr4U7gKmAjcKNXa5hMAP/dObcRuAj4qFfj7cBjzrl1wGPe8zD5c2BfzPMvAV9xzq0FeoBbA6nq1L4KPOyc2wCcQ7T2UF5jM6sFPgY0Ouc2E92S4AbCdY2/S/RnPdaprudVRPfQWUd0J85vpKjGWN/ltfUG+vtsSYeFc+7/OecmvKdPEd1xD2ArcJ9zbtQ5dwhoAi4IosYZLgCanHMHnXNjwH1Eaw0N51ybc+4Z73E/0V9itUTrvMc77R7gXcFU+FpmVgdcA3zbe27AHwEPeKeErd4S4E1E93vBOTfmnDtBiK8x0b1x8rwdL/OBNkJ0jZ1z/0l0z5xYp7qeW4F/cVFPAaVmVp2aSqNmqzfo32dLOixm+GPgF97jWqA55rUW71jQwlrXrMysATgP2A4sd861eS+1A8sDKms2fw/8D2B6w+YK4ETMD17YrvNqoAP4Z+/W2bfNrICQXmPn3FHg/wKvEA2JXmAX4b7GcOrruRh+DlP++2zRh4WZPerdJ535sTXmnM8SvX3yveAqXVrMrBD4EfBx51xf7Gve1rihGGZnZu8AjjvndgVdyxxkAq8HvuGcOw8YZMYtp5Bd4zKif92uBmqAAl57CyXUwnQ94wnq95lv26qminPu8tO9bma3AO8ALnN/GCd8FKiPOa3OOxa0sNb1KmaWRTQovuec+7F3+JiZVTvn2rwm+/HgKnyVS4BrzexqIBcoJtofUGpmmd5fvmG7zi1Ai3Nuu/f8AaJhEdZrfDlwyDnXAWBmPyZ63cN8jeHU1zO0P4dB/j5b9C2L0zGzK4nefrjWOTcU89I24AYzyzGz1UQ7sp4OosYZdgDrvFEk2UQ7rbYFXNOrePf77wb2Oef+LualbcDN3uObgX9PdW2zcc592jlX55xrIHo9H3fO3QT8CrjOOy009QI459qBZjM70zt0GdH96EN5jYnefrrIzPK9/x/T9Yb2GntOdT23AR/0RkVdBPTG3K4KTOC/z5xzS/aDaEdPM/Cc9/FPMa99FjgAvAhcFXStMXVdTXSkwwHgs0HXM0t9lxJtrv8+5rpeTbQf4DHgZeBRoDzoWmep/S3Az73Ha7wfqCbgh0BO0PXNqPVcYKd3nX8KlIX5GgOfB/YDu4F7gZwwXWPg34j2p4wTbbndeqrrCRjRUYkHgBeIjvIKQ72B/j7TDG4REYlrSd+GEhGR5FBYiIhIXAoLERGJS2EhIiJxKSxERCQuhYWIiMSlsBARkbgUFiIiEtf/B/msyhnDfUpyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " balance\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " campaign\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " pdays\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuQXOdd5vHv091z0c2SJY9N0MWSkWyQQwjJxMkSCClMgk0gYgunYm9YzK5Bm9q4ioVlWYewLuNiqzBbFS9bmIsXZ8sYKBscErTErDeJU9na3cSxnIQkciIylpOVjBPrZtm6zExffvvHOd3T0+qZbjs6c15pnk/VlE6f83bP22dG/cx7Oe9RRGBmZraYStkVMDOz9DkszMxsIIeFmZkN5LAwM7OBHBZmZjaQw8LMzAZyWJiZ2UAOCzMzG8hhYWZmA9XKrsC5cskll8TWrVvLroaZ2XnlySefPBIRE4PKXTBhsXXrVvbu3Vt2NczMziuSvjlMOXdDmZnZQIWGhaTrJO2XNCXptj7HxyQ9lB9/XNLWrmOvkfQZSfskfVnSeJF1NTOzhRUWFpKqwD3A9cBO4CZJO3uK3QIcj4jtwN3AXflza8CfAe+NiKuBtwL1oupqZmaLK7JlcQ0wFREHImIWeBDY1VNmF3B/vv0wcK0kAW8HvhQRfw8QEUcjollgXc3MbBFFhsVG4GDX40P5vr5lIqIBnAA2AFcCIelRSZ+X9Ov9voGk3ZL2Stp7+PDhc/4GzMwsk+oAdw34YeA9+b//VNK1vYUi4t6ImIyIyYmJgTO/zMzsFSoyLJ4FNnc93pTv61smH6dYCxwla4X8r4g4EhGngUeA1xVYVzMzW0SRYfEEsEPSNkmjwI3Anp4ye4Cb8+0bgMciu8/ro8D3S1qZh8iPAk8VWFczS8THvvQc335xuuxqWI/CLsqLiIakW8k++KvAhyJin6Q7gb0RsQe4D3hA0hRwjCxQiIjjkj5IFjgBPBIRHyuqrmaWhlMzDd73F5/nktVj7P3NHy+7Otal0Cu4I+IRsi6k7n23d21PA+9a4Ll/RjZ91syWiedOnAHgyMmZkmtivVId4DazZegfX3D3U6ocFmaWjBNnfO1tqhwWZpaMM/W5a29nGr4ONyUOCzNLxnRXWLiVkRaHhZkl4/TsXFhMz7ZKrIn1cliYWTLOdIVFd5eUlc9hYWbJ6O6GclikxWFhZsnoDojuVoaVz2FhZsmY3w3VKLEm1sthYWbJOD2vZeEB7pQ4LMwsGdOzTdaMZ6sQecwiLQ4LM0vGmXqT9atGO9uWDoeFmSXjTL3J2hUjANQb7oZKicPCzJJxZrYrLJoOi5Q4LMwsGWfqTS4az8Ji1i2LpDgszCwZZ2abrB6rIcGsWxZJcViYWTKm603GRyqMVCsOi8Q4LMwsGY1WMFKtMFatuBsqMQ4LM0tGvdlipFZhtOawSI3DwsySEBHUm8FIRYxUK54NlRiHhZklodkKAGpVtyxS5LAwsyQ0OmEhRqqi3oySa2TdHBZmloT27KfRaoXRWpUZtyyS4rAwsyQ08pZEraKsG8pjFkkpNCwkXSdpv6QpSbf1OT4m6aH8+OOStub7t0o6I+mL+dcfFVlPMytfIw+HWrXCaFVeGyoxtaJeWFIVuAd4G3AIeELSnoh4qqvYLcDxiNgu6UbgLuDd+bGnI+K1RdXPzNJSz8csRvMB7um6wyIlRbYsrgGmIuJARMwCDwK7esrsAu7Ptx8GrpWkAutkZolqtyRqVTHqqbPJKTIsNgIHux4fyvf1LRMRDeAEsCE/tk3SFyR9WtKPFFhPM0tAozXXDTXiK7iTU1g31HfoOWBLRByV9Hrgo5KujogXuwtJ2g3sBtiyZUsJ1TSzc6U9VXakPcDtsEhKkS2LZ4HNXY835fv6lpFUA9YCRyNiJiKOAkTEk8DTwJW93yAi7o2IyYiYnJiYKOAtmNlSaXc7jVQrjHohweQUGRZPADskbZM0CtwI7Okpswe4Od++AXgsIkLSRD5AjqQrgB3AgQLramYla7csalW3LFJUWDdURDQk3Qo8ClSBD0XEPkl3AnsjYg9wH/CApCngGFmgALwFuFNSHWgB742IY0XV1czK1+huWdQ8wJ2aQscsIuIR4JGefbd3bU8D7+rzvA8DHy6ybmaWls5yH/lCgm5ZpMVXcJtZEtpjFJ0lyt2ySIrDwsyS0OjMhqrkS5QHrZYXE0yFw8LMkjC33IcYrWbX5jYcFslwWJhZEtrLfYxUszELwIPcCXFYmFkS2st9jFQr1PKwaPieFslwWJhZEuYv95F1Q9VbblmkwmFhZknoXu7D3VDpcViYWRK672dRq+QD3O6GSobDwsyS0GlZ5Mt9ZPvcskiFw8LMktAenxipVqhV2mHhlkUqHBZmloTue3DX2gPcblkkw2FhZkloj1lUK9md8sBhkRKHhZklYbYZjFYrSHMtC1/BnQ6HhZklodFsdUJibszCLYtUOCzMLAmNVnSmzI7W2mMWblmkwmFhZkmoN1udKbPtlkXDLYtkOCzMLAn1ZqsTEnNXcLtlkQqHhZklodGMzpjFiKfOJsdhYWZJqLei06LorDrrhQST4bAwsyTUG61Oi6LTsmi4GyoVDgszS0Kj1WfMwi2LZDgszCwJ9WZ0WhRedTY9DgszS0Kj1eqMVYx41dnkOCzMLAn1xlzLYsSrzibHYWFmSai3Wp2xinZo+KK8dBQaFpKuk7Rf0pSk2/ocH5P0UH78cUlbe45vkXRS0q8VWU8zK1+jObfcR7Xi6yxSU1hYSKoC9wDXAzuBmyTt7Cl2C3A8IrYDdwN39Rz/IPB3RdXRzNJRb86NWUhipCrqXnU2GUW2LK4BpiLiQETMAg8Cu3rK7ALuz7cfBq6VJABJPwM8A+wrsI5mloh6s9W5jwVk02frDbcsUlFkWGwEDnY9PpTv61smIhrACWCDpNXAvwd+q8D6mVlCGq255T4gmz7r+1mkI9UB7juAuyPi5GKFJO2WtFfS3sOHDy9NzcysENmYxdxH0mit4jGLhNQKfO1ngc1djzfl+/qVOSSpBqwFjgJvBG6Q9LvAOqAlaToifr/7yRFxL3AvwOTkpP8EMTuP1Ztzy31Atky5wyIdRYbFE8AOSdvIQuFG4J/1lNkD3Ax8BrgBeCwiAviRdgFJdwAne4PCzC4sWVjMtSxqVfkK7oQUFhYR0ZB0K/AoUAU+FBH7JN0J7I2IPcB9wAOSpoBjZIFiZstQ9xLlAKPVimdDJaTIlgUR8QjwSM++27u2p4F3DXiNOwqpnJklpfuiPMhaFp4NlY5UB7jNbJlpdC0kCNmYhe9nkQ6HhZmVLiKyqbNds6FGahVmPWaRDIeFmZWuvWBgd8tipCKvDZUQh4WZla7d3VTruYLbs6HS4bAws9LNtSzmD3DPumWRDIeFmZWuffHdvG6oqge4U+KwMLPStbub5g1w+6K8pDgszKx07ZbFvIUEqxV3QyXEYWFmpWuHxbwlyituWaTEYWFmpWsvRV7rGbPwQoLpcFiYWek63VCV7tlQlc4sKSufw8LMStfoc1HeaFWeDZUQh4WZlW5u6mxPy8ILCSZjqLCQ9NeS3iHJ4WJm51y7u2n+bCh5ifKEDPvh/wdkNy76uqTfkXRVgXUys2Wm3d3U3bIYrVa8NlRChgqLiPhERLwHeB3wDeATkv6vpH8haaTICprZhW/uorz5S5S3AppuXSRh6G4lSRuAXwB+EfgC8Htk4fHxQmpmZsvGbN8xiyw4PH02DUPdKU/SR4CrgAeAn46I5/JDD0naW1TlzGx5aPRZSLB9gV692WJ8pFpKvWzOsLdV/a/5LVI7JI1FxExETBZQLzNbRuaWKJ8/wA34Ku5EDNsN9dt99n3mXFbEzJavzhLllfn3s4Ds3txWvkVbFpK+C9gIrJD0g0A79i8CVhZcNzNbJjrXWdS6l/toj1m4ZZGCQd1QP0E2qL0J+GDX/peA3yioTma2zDT6LfeRb3v6bBoWDYuIuB+4X9LPRsSHl6hOZrbM9L0Hd60y75iVa1A31M9FxJ8BWyX9au/xiPhgn6eZmb0s/S7KG6l46mxKBnVDrcr/XV10Rcxs+eq/3Ee7G8otixQM6ob64/zf33olLy7pOrKL96rAn0TE7/QcHwP+FHg9cBR4d0R8Q9I1wL3tYsAdEfGRV1IHM0tfZ4C757aqgO+Wl4hhFxL8XUkXSRqR9ElJhyX93IDnVIF7gOuBncBNknb2FLsFOB4R24G7gbvy/V8BJiPitcB1wB9LGvaaEDM7zzSaQUVQqcy/+VF2zGGRgmGvs3h7RLwI/BTZ2lDbgX834DnXAFMRcSAiZoEHgV09ZXYB9+fbDwPXSlJEnI6IRr5/HHA71OwCVm+15o1XQFdYeG2oJAwbFu2/6t8B/FVEnBjiORuBg12PD+X7+pbJw+EEsAFA0hsl7QO+DLy3Kzw6JO2WtFfS3sOHDw/5VswsNfVGnBUWNXdDJWXYsPhbSV8jG1v4pKQJYLq4akFEPB4RVwNvAN4vabxPmXsjYjIiJicmJoqsjpkVqNFqzRvchrnxCw9wp2HYJcpvA36IbByhDpzi7C6lXs8Cm7seb8r39S2Tj0msJRvo7v7eXwVOAq8epq5mdv6pN2PeBXkwdzW3xyzS8HIGjb+X7HqL7uf86SLlnwB2SNpGFgo3kt1Aqdse4GaydaZuAB6LiMifczAiGpIuz7/3N15GXc3sPFJvthjtaVm0w8PdUGkYdonyB4DvAb4INPPdwSJhkX/Q3wo8SjZ19kMRsU/SncDeiNgD3Ac8IGkKOEYWKAA/DNwmqQ60gH8dEUde9rszs/NCo9nqXFfRNuJVZ5MybMtiEtgZES/rp5Yva/5Iz77bu7angXf1ed4DZPfOMLNloN6Ks8csuu5nYeUbdoD7K8B3FVkRM1u+Gs3WvAvyoOtOeZ46m4RhWxaXAE9J+hww094ZEe8spFZmtqzUmzFveXKYu1OeB7jTMGxY3FFkJcxseas3W2fNhqq5GyopQ4VFRHw6n5W0IyI+IWkl2aC1mdl3rNGMecuTA9QqvvlRSoZdG+qXyJbj+ON810bgo0VVysyWl0br7JbFiFedTcqwA9zvA94MvAgQEV8HLi2qUma2vMw2g9Ha/I+jakVU5G6oVAwbFjP5YoBA52prx72ZnRP1RuusbijIxi3qLYdFCoYNi09L+g1ghaS3AX8F/PfiqmVmy0m9efaqs5DNiKo3/HdpCoYNi9uAw2QrwP4rsgvtfrOoSpnZ8tJonb3qLGTXWjTcskjCsLOhWpI+Cnw0IrwWuJmdU7ON/i2LkWrFs6ESsWjLQpk7JB0B9gP787vk3b7Y88zMXo56s8Vo7ewxi5GKPMCdiEHdUL9CNgvqDRGxPiLWA28E3izpVwqvnZktC/0uyoNsgNtXcKdhUFj8c+CmiHimvSMiDgA/B/x8kRUzs+UjuyivXzeU3A2ViEFhMdJvafB83GKkmCqZ2XIz22ydtTYUtMcs3LJIwaCwmH2Fx8zMhpbd/Gih2VBuWaRg0GyoH5D0Yp/9As66J7aZ2cvVbAWtYJHZUG5ZpGDRsIgILxZoZoVqh0HfsKg4LFIx7EV5ZmaFmAuLPmMWNXkhwUQ4LMysVO3ZTn2v4HbLIhkOCzMr1aLdUJ46mwyHhZmVaraxSDeUB7iT4bAws1K1w6D3fhaQX8HtqbNJcFiYWanaYdBvuQ+vDZUOh4WZlcrdUOeHQsNC0nWS9kuaknRbn+Njkh7Kjz8uaWu+/22SnpT05fzfHyuynmZWns4Ad99uKE+dTUVhYSGpCtwDXA/sBG6StLOn2C3A8YjYDtwN3JXvPwL8dER8P3Az8EBR9TSzcrVnO/Vb7sMti3QU2bK4BpiKiAP5/bsfBHb1lNkF3J9vPwxcK0kR8YWI+Md8/z6y27mOFVhXMytJw1NnzwtFhsVG4GDX40P5vr5lIqIBnAA29JT5WeDzETFTUD3NrESzeVjU+oxZZLOh3LJIwVC3VS2LpKvJuqbevsDx3cBugC1btixhzczsXBncDRVEBNLZYWJLp8iWxbPA5q7Hm/J9fctIqgFrgaP5403AR4Cfj4in+32DiLg3IiYjYnJiYuIcV9/MlsLiCwlmAeFrLcpXZFg8AeyQtE3SKHAjsKenzB6yAWyAG4DHIiIkrQM+BtwWEf+nwDqaWckWW0iwlgeIZ0SVr7CwyMcgbgUeBb4K/GVE7JN0p6R35sXuAzZImgJ+FWhPr70V2A7cLumL+delRdXVzMqz2EKC7QCZ9Yyo0hU6ZhERjwCP9Oy7vWt7GnhXn+f9NvDbRdbNzNKw+EKC7ZaFw6JsvoLbzEq16P0s2mHhMYvSOSzMrFSd5T4WuIK7u4yVx2FhZqVqtxr6T531bKhUOCzMrFT1xuAxCy/5UT6HhZmVqt5sIUG10mfqbMVhkQqHhZmVarYZfVsV0NUN5essSuewMLNS1ZutvuMV4G6olDgszKxUjWar77RZmJsN5ZVny+ewMLNSLdYNNdq5zsIti7I5LMysVPVma8GwqLkbKhkOCzMrVX2xbqiKu6FS4bAws1It1rIYrbllkQqHhZmVarbRYmxkgW6oiqfOpsJhYWalmq63GKtV+x7z1Nl0OCzMrFQzjSZjfRYRhO6wcMuibA4LMyvVTKO1YFjUOgsJumVRNoeFmZVqZohuKC9RXj6HhZmVaqbRXHCAu93i8G1Vy+ewMLNSzTRajC/QsmhfwT1dd1iUzWFhZqWaWWTqbKUiRmsVZhrNJa6V9XJYmFmpZuoLz4YCGK9VmHHLonQOCzMrVTYbqn83FMD4SJXpulsWZXNYmFlpGs0WjVYs3rJwWCTBYWFmpWnPclpozAKyGVEe4C6fw8LMStMeixjUDeUB7vIVGhaSrpO0X9KUpNv6HB+T9FB+/HFJW/P9GyR9StJJSb9fZB3NrDzTeQgs3g3llkUKCgsLSVXgHuB6YCdwk6SdPcVuAY5HxHbgbuCufP808B+AXyuqfmZWvk7LYpFuqPGRaidUrDxFtiyuAaYi4kBEzAIPArt6yuwC7s+3HwaulaSIOBUR/5ssNMzsAjXTGNwNNVarumWRgCLDYiNwsOvxoXxf3zIR0QBOABuG/QaSdkvaK2nv4cOHv8PqmtlSmxmiG2pspMKMZ0OV7rwe4I6IeyNiMiImJyYmyq6Omb1Mw7QsxmvVTjkrT5Fh8Sywuevxpnxf3zKSasBa4GiBdTKzhAw3ZlHxdRYJKDIsngB2SNomaRS4EdjTU2YPcHO+fQPwWET4Lidmy8Tp2QYAK0Z8BXfqakW9cEQ0JN0KPApUgQ9FxD5JdwJ7I2IPcB/wgKQp4BhZoAAg6RvARcCopJ8B3h4RTxVVXzNbemfyEFg1tvBH0fhIhWl3Q5WusLAAiIhHgEd69t3etT0NvGuB524tsm5mVr5TM1lYrBxdfDZUsxXUm63OzZBs6fnMm1lp2t1Qi4XF+Ej7nhbuiiqTw8LMSjPXsli4k6N97PSsw6JMDgszK83p2QbjIxWqFS1YZs14FhYnZxpLVS3rw2FhZqU5PdtctFUBsCo/fsphUSqHhZmV5tRsY9HxCpibKeWWRbkcFmZWmtMzzU7LYSGr22Ex7bAok8PCzEpzarbByrFBLYtqp6yVx2FhZqXJxiwWD4tOy2LGs6HK5LAws9IMM8C9etwD3ClwWJhZaU7PNlg1oGWxYqRKRQ6LsjkszKw0L003Oi2HhUhi1WjNs6FK5rAws1K0WsELp2e5eOXowLKrxmpuWZTMYWFmpXhpukErYO2KkYFlV4/XeMlTZ0vlsDCzUhw/PQswVMti/crRTnkrh8PCzErRCYtVg1sW61eNcvSkw6JMDgszK8ULZ+oArF0xRMti9SjHTjksyuSwMLNSvNDphhrcstiwKuuGarZ81+WyOCzMrBTHT2Uti2HGLDasGqUVcwFjS89hYWalOHpqhorgoiFmQ61fPQbgrqgSOSzMrBTPHj/Dq9auWPTGR20bVmWtjyMe5C6Nw8LMSnHw+Bk2r18xVNnvXpeVO3T8dJFVskU4LMysFIeOn2bTxSuHKrvp4hXUKuKZI6cKrpUtxGFhZktuut7k2y/OsHnIsBipVtiyfqXDokQOCzNbcgcOZx/6Wy8ZLiwAtl2yymFRokLDQtJ1kvZLmpJ0W5/jY5Ieyo8/Lmlr17H35/v3S/qJIutpZkvriwdfAOA1m9YN/ZxXb1zLP3z7JU+fLcniawN/ByRVgXuAtwGHgCck7YmIp7qK3QIcj4jtkm4E7gLeLWkncCNwNfDdwCckXRkRvlXWMtVqBU899yKfe+YYs80Wmy5ewTVb13PpReNL8r2/9q2X+PtDL3B6tsnl61fymk1rl+R7X6ge+9rzvGrtOFs3DN+y+NGrJvi9T36dT//DYXa9dmOBtbN+CgsL4BpgKiIOAEh6ENgFdIfFLuCOfPth4PclKd//YETMAM9Imspf7zMF1tdK1Gi2qDeD2UaL2Wb29ezxM3zl2RN87pljfPaZo7xwun7W866YWMWbrtjA5OUXc/mGlVyyeoyxWpXRWoVaVVQkBEgghPJZmt2Ps+OiIphptHhxus6LZ+pMPX+Szx44xqP7vsVzJ6bP+t6Xb1jJ6y+/mDdsXc+Vl61hYvUYl6wZZcVIFWnwdNBhtVpBM6Jz9XKtIqoVndPvsZSe+MYxPrX/eX7xR7a9rPfwA5vWsXHdCv7gU0/z1isvZe0QV37buVNkWGwEDnY9PgS8caEyEdGQdALYkO//bM9zC/lT4mvfepFb/+ILnccRc8sJzFtYIPpuLvqc6CkYXUe7j/WWO6evvchzGOI5w9Yhel58mHMXkX0IzjZaLLaKw8Z1K3jb913GD23fwJuu2MBF4yN8/fmTfO6Zo3z2wDH2fPEf+YvH/9/CL/AdGKtV+NErJ/jVt13Jm67YwKqxGs8cOcnnv/kCe795jE/vP8xff/7Zvs8brVUYq2V3eQva5ytoRfbegywI8t3Z4wgioBnRCYmFfj9GqllojFSyYKxWKozkAVmp0AnKivJQlDo/p85Lxtx2u04w97MN5r5/v593LFR23utEZ7sVwfHTdTZdvIL3vuV7Fj/5PaoVceeuq9n9wJNM/sePc+maccZqFSpDXKdRtqJr+NarJvjAO3YW+j2KDIvCSdoN7AbYsmXLK3qN8VqVqy5b0/PCfTfn/RXU+8PXEM8563nzntP12j0vPv/1+j/nrGMLfKNz8trz9i/832Ch1+veX62I0Wr2wTqS/ztaFaO1ChNrxrj6u9dy6Zqxs77Pazev47Wb17H7Ld9Do9niG0dPcfD4GY6dnGW22WKm3qTRis4HVPtDbf6HX+T75j7IxkYqrBkfYc1YjW2XrOKq71rD+Mj8236uX7We11++nl/iCiKCZ46c4pvHTnPkpRmOnJzlTL3JTKPJTL3FTKNFROTvOWu9tFs1Fc2dP4l5raBKRVQlahV1ttsfis1W0Gi2aLSCRiuoN1s0W0G9me1v5m+qFXkw0Q6hyH4O7dZV189wbnv+/k65znO6WmfznjP3PuZ+VF37uspuWb+Sd79hy1D3seh17fddxt+878187MvP8fyLM8w0mov+sZWCOOvPr3PvsiXoEi0yLJ4FNnc93pTv61fmkKQasBY4OuRziYh7gXsBJicnX9FPZOslq7jnPa97JU+1RNSqFbZfuobtl64ZXPgck8QVE6u5YmL1kn/v5erVG9fy6o1ry67GslPkbKgngB2StkkaJRuw3tNTZg9wc759A/BYZO3bPcCN+WypbcAO4HMF1tXMzBZRWMsiH4O4FXgUqAIfioh9ku4E9kbEHuA+4IF8APsYWaCQl/tLssHwBvA+z4QyMyuPegcmz1eTk5Oxd+/esqthZnZekfRkREwOKucruM3MbCCHhZmZDeSwMDOzgRwWZmY2kMPCzMwGumBmQ0k6DHyzoJe/BDhS0Gufj3w+zuZzcjafk/lSPR+XR8TEoEIXTFgUSdLeYaaWLRc+H2fzOTmbz8l85/v5cDeUmZkN5LAwM7OBHBbDubfsCiTG5+NsPidn8zmZ77w+Hx6zMDOzgdyyMDOzgRwWXST9J0lfk/QlSR+RtK7r2PslTUnaL+knuvZfl++bknRbOTVfOsvt/QJI2izpU5KekrRP0i/n+9dL+rikr+f/Xpzvl6T/kp+jL0m6YG+YIqkq6QuS/jZ/vE3S4/l7fyi/PQH57QYeyvc/LmlrmfUuiqR1kh7OP0e+KumfXCi/Jw6L+T4OvDoiXgP8A/B+AEk7yZZPvxq4DviD/D9JFbgHuB7YCdyUl70gLbf326UB/NuI2Am8CXhf/r5vAz4ZETuAT+aPITs/O/Kv3cAfLn2Vl8wvA1/tenwXcHdEbAeOA7fk+28Bjuf7787LXYh+D/gfEfG9wA+QnZsL4vfEYdElIv5nRDTyh58lu0MfwC7gwYiYiYhngCngmvxrKiIORMQs8GBe9kK13N4vABHxXER8Pt9+iewDYCPZe78/L3Y/8DP59i7gTyPzWWCdpFctcbULJ2kT8A7gT/LHAn4MeDgv0ntO2ufqYeBaLXZP3vOQpLXAW8ju00NEzEbEC1wgvycOi4X9S+Dv8u2NwMGuY4fyfQvtv1Att/d7lrz75AeBx4HLIuK5/NC3gMvy7eVynv4z8OtAK3+8AXih6w+u7vfdOSf58RN5+QvJNuAw8N/yrrk/kbSKC+T3ZNmFhaRPSPpKn69dXWU+QNb18Ofl1dRSI2k18GHg30TEi93H8tsBL5uphZJ+Cng+Ip4suy4JqQGvA/4wIn4QOMVclxNwfv+eFHZb1VRFxI8vdlzSLwA/BVwbc/OKnwU2dxXblO9jkf0XosXOwwVN0ghZUPx5RPx1vvvbkl4VEc/l3QfP5/uXw3l6M/BOST8JjAMXkfXXr5NUy1sP3e+7fU4OSaoBa4GjS1/tQh0CDkXE4/njh8nC4oL4PVl2LYvFSLqOrFn9zog43XVoD3BjPqNjG9mA1OeAJ4Ad+QyQUbJB8D1LXe8ltNzeL9Dpi78P+GpEfLDr0B7g5nz7ZuBvuvb/fD7b5U3Aia5uiAtCRLw/IjZFxFay34PHIuI9wKeAG/Jiveekfa5uyMufl39hLyRC88AvAAAAx0lEQVQivgUclHRVvuta4CkulN+TiPBX/kU2cH0Q+GL+9Uddxz4APA3sB67v2v+TZDOnngY+UPZ7WIJztKzeb/6ef5is6+BLXb8bP0nW5/5J4OvAJ4D1eXmRzRp7GvgyMFn2eyj4/LwV+Nt8+wqyP6SmgL8CxvL94/njqfz4FWXXu6Bz8Vpgb/678lHg4gvl98RXcJuZ2UDuhjIzs4EcFmZmNpDDwszMBnJYmJnZQA4LMzMbyGFhZmYDOSzMzGwgh4WZmQ30/wHP3iWtkaxvmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " previous\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Density plots\n",
    "for feature in numerical:\n",
    "    interpret(feature, 'density', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we can see that the data is a lot more centralized after having removed the outliers from the dataset. There is still a very pronounced skew in features such as 'pdays' and 'previous', but this is due to the nature of the data: approximately 30,000 people had never received a call before the call registered in the dataset, and that's whiy there's a spike; a similiar thing occurs with the 'previous' feature. \n",
    "\n",
    "Despite these spikes, the data is, in fact, more centralized, and we can proceed to transform it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Features: Transformations\n",
    "\n",
    "Having cleaned the data for use, we can now proceed to transform the features to optimize the model's performance. \n",
    "\n",
    "The three main transformations that I will be performing are as follows:\n",
    "\n",
    "1. Transforming (numerical) skewed continuous features\n",
    "2. Normalizing the (numerical) features using a scaler\n",
    "3. One-hot encoding the categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Skewed Continuous Features\n",
    "\n",
    "In the 'Finding Donors' project I was introduced to this concept. In that project, we used the transformation:\n",
    "    \n",
    "    lambda x: np.log(x + 1)\n",
    "    \n",
    "However, because I am dealing with negative values in features like 'balance', I cannot use this log approach for all features. \n",
    "\n",
    "Researching online, I found that a convenient way of attempting to transform skewed data is using a Box-Cox transformation. Essentially, what is done is that the data is transformed by elevating each feature to a specific value, lambda (more info on https://www.isixsigma.com/tools-templates/normality/tips-recognizing-and-transforming-non-normal-data/).\n",
    "\n",
    "To determine the best value of lambda one should elevate the data to, one must look at the standard deviation produced by each transformation. Logically, the transformation that minimizes the standard deviation is the optimal one. \n",
    "\n",
    "I will therefore use a simple for-loop to test the transformation on 'dummy' feature series, and identify the one that produces the smallest standard deviation for each.\n",
    "\n",
    "(Note: Because 0 cannot be elevated to a negative exponent nor can it be used as the argument of a logarithm, I added a small value (0.01) to each element.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features onto a dummy data frame\n",
    "import numpy as np\n",
    "df_test = df[numerical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/numpy/core/_methods.py:32: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': '-2.0', 'balance': '-0.5', 'campaign': '0.5', 'pdays': '-2.0', 'previous': '0.5'}\n"
     ]
    }
   ],
   "source": [
    "# Initiate a dictionary and possible values of lambda\n",
    "std = {}\n",
    "lmbda = [-2.0, -0.5, 0.0, 0.5, 1.0, 2.0]\n",
    "\n",
    "# Iterate over numerical features and each value of lambda for each feature\n",
    "for feature in numerical:\n",
    "    feature_std = {}\n",
    "    for i in lmbda:\n",
    "        dummy = df_test[feature]\n",
    "        if i == 0.0:\n",
    "            feature_std[str(i)] = dummy.apply(lambda x: np.log(x + 0.01)).std()\n",
    "        else:\n",
    "            feature_std[str(i)] = dummy.apply(lambda x: (x + 0.01) ** i).std()\n",
    "    std[str(feature)] = feature_std\n",
    "\n",
    "# Select best value of lambda for each feature\n",
    "best_lmbda = dict((feature, min(std[feature], key=(lambda k: std[feature][k]))) for feature in numerical)\n",
    "print(best_lmbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having identified the best value of lambda for each numerical feature, I can now transform all features with a simple for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform skewed features with best value of lambda:\n",
    "for feature in numerical:\n",
    "    df[feature] = df[feature].apply(lambda x: (x + 0.01) ** float(best_lmbda[feature])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing Numerical Features\n",
    "\n",
    "In addition to the transformations applied on the skewed features, I will normalize them by using a scaler. This is a good practice that will further enhance the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/utils/validation.py:433: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  array = np.array(array, dtype=dtype, order=order, copy=copy)\n"
     ]
    }
   ],
   "source": [
    "# Import sklearn.preprocessing.StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "df[numerical] = scaler.fit_transform(df[numerical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding categorical features\n",
    "\n",
    "The last pre-processing step to be taken is simply to one-hot encode the many categorical features included in the dataset. Because the algorithms expect numerical features instead of categorical ones, we simply one-hot encode to create binary 'vectors' for each category.\n",
    "\n",
    "The only categorical variable that does not need to be one-hot encoded is 'y', the target variable. For this one, we can simply replace 'yes' with 1 and 'no' with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 total features after one-hot encoding.\n"
     ]
    }
   ],
   "source": [
    "# Replace values in target variable\n",
    "target = df.y.replace(to_replace=['yes', 'no'], value=[1, 0])\n",
    "\n",
    "# Drop target variable from data set before one-hot encoding\n",
    "features = df.drop('y', axis=1)\n",
    "\n",
    "# One-hot encode features\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded = list(features.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31777 entries, 0 to 31776\n",
      "Columns: 53 entries, age to poutcome_unknown\n",
      "dtypes: float64(5), uint8(48)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Check gist of features \n",
    "features.info(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27723\n",
       "1     4054\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check gist of targets\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having finished the data pre-processing, we can now break it down into training and testing sets, apply PCA on it to reduce dimensionality, and then design and train both the benchmark model and the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Shuffle & Split\n",
    "\n",
    "This is a simple step that precedes the training of both the benchmark and MLP models.\n",
    "\n",
    "80% of the data will be used for training and the remaining 20% will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 25421 samples.\n",
      "Testing set has 6356 samples.\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the 'features' and 'target' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Decomposition\n",
    "\n",
    "Initially, I had believed it useful to first apply feature selection on the data and afterwards reduce the dimensionality of those features. In order to get the feature importances I would need to either train a Linear Regressor on the current dataset or use an SVC with a linear kernel, or any other model that allows one to calculate the importances of the different features in a dataset. \n",
    "\n",
    "Because this extra training step is computationally complex, I have decided to skip the feature selection and delve into PCA directly. This way, the dimensions found by PCA will be able to capture as many patterns in the data as possible. If I did apply feature selection beforehand, I would be hampering PCA's ability to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create PCA object that seeks to retain 95% of the variance and fit it to the training data\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Transform X_train and X_test data using PCA\n",
    "X_train_red = pca.transform(X_train)\n",
    "X_test_red = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by PCA: 95.0%\n",
      "Principal Components shape: (27, 53)\n",
      "Training set dimensions: \n",
      "Before PCA: (25421, 53) \n",
      "After PCA: (25421, 27)\n"
     ]
    }
   ],
   "source": [
    "# PCA overview\n",
    "print('Variance captured by PCA: {:.1f}%'.format(pca.n_components*100))\n",
    "print('Principal Components shape:', pca.components_.shape)\n",
    "\n",
    "# Compare training set dimensions before and after PCA\n",
    "print('Training set dimensions: \\nBefore PCA:', X_train.shape, '\\nAfter PCA:', X_train_red.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, PCA has effectively reduced the number of variables/features in the dataset from 53 to 27 (a reduction of nearly 50%). This is noteworthy because through PCA we have been able to capture 95% of the variance in the dataset with only half as many features. Computationally, this is very beneficial to the training and testing times of our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model Design & Training\n",
    "\n",
    "For this project I will be using a Support Vector Machine Classifier as the benchmark model to which I will compare the MLP later.\n",
    "\n",
    "For this SVC, I have decided to use an RBF kernel because it is more specific ('contours' of data clusters are more flexible) and less generic than other kernels such as 'poly' and 'linear'. In order to fine-tune it, I have decided to iterate over different combinations of hyperparameters C and gamma. In addition, I have kept the number of folds at 3, which is the default, in order to avoid elongating the training times further.\n",
    "\n",
    "For comparison purposes, I have decided to train the benchmark model on both the full training set (X_train) and the PCA-reduced training set (X_train_red). This is unnecessary from a practical standpoint, but it does help to better understand how the data reduction step affects the training process.  \n",
    "\n",
    "My hypothesis is that the PCA-reduced training set will take considerably less time to train, but due to the fact it is essentially the same data as the full training set, the model's performance after training and testing will not differ vastly from those of the model trained on the full training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] C=0.001, gamma=0.001 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.001, gamma=0.001 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.3s remaining:    0.0s\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.001, gamma=0.001 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   50.5s remaining:    0.0s\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.001, gamma=0.01 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.3min remaining:    0.0s\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.0s\n",
      "[CV] C=0.001, gamma=0.01 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.001, gamma=0.01 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.0s\n",
      "[CV] C=0.001, gamma=0.1 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.1, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.3s\n",
      "[CV] C=0.001, gamma=0.1 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.1, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.3s\n",
      "[CV] C=0.001, gamma=0.1 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.1, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.4s\n",
      "[CV] C=0.001, gamma=1 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.3s\n",
      "[CV] C=0.001, gamma=1 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.3s\n",
      "[CV] C=0.001, gamma=1 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.3s\n",
      "[CV] C=0.01, gamma=0.001 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.01, gamma=0.001 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.01, gamma=0.001 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.0s\n",
      "[CV] C=0.01, gamma=0.01 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.01, gamma=0.01 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.2s\n",
      "[CV] C=0.01, gamma=0.01 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.01, gamma=0.1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.1, F2-Score=0.0, Cohen's Kappa=0.0, total=  16.8s\n",
      "[CV] C=0.01, gamma=0.1 ...............................................\n",
      "[CV]  C=0.01, gamma=0.1, F2-Score=0.002303086135421465, Cohen's Kappa=0.003210201742215313, total=  16.8s\n",
      "[CV] C=0.01, gamma=0.1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.1, F2-Score=0.0, Cohen's Kappa=0.0, total=  16.6s\n",
      "[CV] C=0.01, gamma=1 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=0.01, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  30.2s\n",
      "[CV] C=0.01, gamma=1 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=0.01, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  30.3s\n",
      "[CV] C=0.01, gamma=1 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=0.01, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  30.3s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.2s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=  13.1s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV]  C=0.1, gamma=0.01, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  15.8s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV]  C=0.1, gamma=0.01, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  15.6s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV]  C=0.1, gamma=0.01, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=  15.7s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV]  C=0.1, gamma=0.1, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  16.6s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV]  C=0.1, gamma=0.1, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  16.3s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV]  C=0.1, gamma=0.1, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=  16.5s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=0.1, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  55.2s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=0.1, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  44.7s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=0.1, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  56.4s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV]  C=1, gamma=0.001, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  13.0s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV]  C=1, gamma=0.001, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  13.1s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV]  C=1, gamma=0.001, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=  12.9s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV]  C=1, gamma=0.01, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  15.6s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV]  C=1, gamma=0.01, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  15.5s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV]  C=1, gamma=0.01, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=  15.7s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV]  C=1, gamma=0.1, F2-Score=0.3073770491803279, Cohen's Kappa=0.37863980584929446, total=  18.2s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV]  C=1, gamma=0.1, F2-Score=0.30478643485726553, Cohen's Kappa=0.3677736878357015, total=  17.7s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV]  C=1, gamma=0.1, F2-Score=0.317732932589094, Cohen's Kappa=0.3863302707313264, total=  18.2s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV]  C=1, gamma=1, F2-Score=0.18804347826086956, Cohen's Kappa=0.2185665903281816, total= 1.1min\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV]  C=1, gamma=1, F2-Score=0.17732811140121846, Cohen's Kappa=0.20419880841375293, total=  56.1s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV]  C=1, gamma=1, F2-Score=0.19569471624266144, Cohen's Kappa=0.22984695098193364, total=  56.4s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV]  C=10, gamma=0.001, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  13.6s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV]  C=10, gamma=0.001, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  13.3s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV]  C=10, gamma=0.001, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=  13.4s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV]  C=10, gamma=0.01, F2-Score=0.32669237360754066, Cohen's Kappa=0.39578930992908723, total=  23.1s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV]  C=10, gamma=0.01, F2-Score=0.3047210300429185, Cohen's Kappa=0.3673733473545633, total=  23.2s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV]  C=10, gamma=0.01, F2-Score=0.31551835157759184, Cohen's Kappa=0.38289421986811323, total=  23.9s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV]  C=10, gamma=0.1, F2-Score=0.4076367389060887, Cohen's Kappa=0.45231022289080214, total=  22.3s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV]  C=10, gamma=0.1, F2-Score=0.39881074431002667, Cohen's Kappa=0.43145320599214376, total=  21.3s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV]  C=10, gamma=0.1, F2-Score=0.3990464344941957, Cohen's Kappa=0.4470756233105303, total=  21.5s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV]  C=10, gamma=1, F2-Score=0.2526205450733753, Cohen's Kappa=0.26470939545619865, total= 1.3min\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV]  C=10, gamma=1, F2-Score=0.2411912751677852, Cohen's Kappa=0.2496787444622206, total= 1.3min\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV]  C=10, gamma=1, F2-Score=0.2548238255033557, Cohen's Kappa=0.26819845009065346, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 37.9min finished\n"
     ]
    }
   ],
   "source": [
    "# Import SVC classifier, etc.\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, fbeta_score, cohen_kappa_score\n",
    "\n",
    "# Create classifier, scorer, and parameter grid\n",
    "classifier = SVC(kernel = 'rbf', random_state=42)\n",
    "f2_score = make_scorer(fbeta_score, beta=2)\n",
    "cohen_score = make_scorer(cohen_kappa_score)\n",
    "scores = {'F2-Score': f2_score, \n",
    "          \"Cohen's Kappa\": cohen_score}\n",
    "params = {'C':[0.001, 0.01, 0.1, 1, 10], \n",
    "          'gamma':[0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "# Create GSCV object\n",
    "grid = GridSearchCV(classifier, param_grid=params, scoring=scores, refit='F2-Score', verbose=4)\n",
    "\n",
    "# Fit grid to FULL data\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance: F-2 Score of 0.40 with parameters: {'C': 10, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Observe best performance\n",
    "print('Best performance: F-2 Score of {:.2f} with parameters: {}'.format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having our model in place, we can proceed to predict the training and testing set targets with it to view its performance (on the full training and testing data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-2 & Cohen's Kappa:\n",
      "\n",
      "Training set F2-Score: 0.46 \n",
      "Training set Cohen's Kappa: 0.52 \n",
      "\n",
      "Testing set F2-Score: 0.41 \n",
      "Testing set Cohen's Kappa: 0.45 \n",
      "\n",
      "\n",
      "Accuracy Score:\n",
      "\n",
      "Training set Accuracy: 0.92\n",
      "Testing set Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Create 'semi' benchmark model (because it was trained on the full dataset, it is not the actual benchmark model)\n",
    "semi_benchmark = grid_result.best_estimator_\n",
    "\n",
    "# Predict training and testing sets to observe performance\n",
    "semi_benchmark_train_predict = semi_benchmark.predict(X_train)\n",
    "semi_benchmark_test_predict = semi_benchmark.predict(X_test)\n",
    "\n",
    "# Print out metrics used during training\n",
    "print(\"F-2 & Cohen's Kappa:\\n\")\n",
    "print(\"Training set F2-Score: {:.2f} \\nTraining set Cohen's Kappa: {:.2f} \\n\".format(fbeta_score(y_train, semi_benchmark_train_predict, beta=2),\n",
    "                                                                                   cohen_kappa_score(y_train, semi_benchmark_train_predict)))\n",
    "print(\"Testing set F2-Score: {:.2f} \\nTesting set Cohen's Kappa: {:.2f} \\n\\n\".format(fbeta_score(y_test, semi_benchmark_test_predict, beta=2),\n",
    "                                                                                   cohen_kappa_score(y_test, semi_benchmark_test_predict)))\n",
    "\n",
    "# Print out accuracy (accuracy isn't a good metric to use during training for the purposes of this problem,\n",
    "# but we can use it here post-mortem to view performance)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy Score:\\n\")\n",
    "print(\"Training set Accuracy: {:.2f}\".format(accuracy_score(y_train, semi_benchmark_train_predict)))\n",
    "print(\"Testing set Accuracy: {:.2f}\".format(accuracy_score(y_test, semi_benchmark_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it would be expected, the model performs better on the training set than it does on the testing set, according to the metrics used during training. This it does, however, by a very tight margin. \n",
    "\n",
    "Otherwise, it is very positive to note that the accuracy of the model on the training and testing sets is nearly identical. This means that the model has neither underfit (low training and testing accuracies) nor overfit (high training and low testing accuracies). \n",
    "\n",
    "It is worth noting that this predictive accuracy far surpasses the bibliographical figure of cold-calling's 13% average success rate. According to this benchmark model, one could predict with 91% accuracy whether a call will be successful or unsuccessful, making the success rate effectively 91%. \n",
    "\n",
    "Will this performance be maintained by a model trained on the reduced data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] C=0.001, gamma=0.001 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.0s\n",
      "[CV] C=0.001, gamma=0.001 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   17.0s remaining:    0.0s\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.0s\n",
      "[CV] C=0.001, gamma=0.001 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   34.0s remaining:    0.0s\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.0s\n",
      "[CV] C=0.001, gamma=0.01 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   50.9s remaining:    0.0s\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.0s\n",
      "[CV] C=0.001, gamma=0.01 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=   8.9s\n",
      "[CV] C=0.001, gamma=0.01 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.0s\n",
      "[CV] C=0.001, gamma=0.1 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.1, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.3s\n",
      "[CV] C=0.001, gamma=0.1 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.1, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.3s\n",
      "[CV] C=0.001, gamma=0.1 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=0.1, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.3s\n",
      "[CV] C=0.001, gamma=1 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.1s\n",
      "[CV] C=0.001, gamma=1 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.0s\n",
      "[CV] C=0.001, gamma=1 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.001, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.0s\n",
      "[CV] C=0.01, gamma=0.001 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=   8.9s\n",
      "[CV] C=0.01, gamma=0.001 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.0s\n",
      "[CV] C=0.01, gamma=0.001 .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.0s\n",
      "[CV] C=0.01, gamma=0.01 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.9s\n",
      "[CV] C=0.01, gamma=0.01 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=  10.0s\n",
      "[CV] C=0.01, gamma=0.01 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, gamma=0.01, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.7s\n",
      "[CV] C=0.01, gamma=0.1 ...............................................\n",
      "[CV]  C=0.01, gamma=0.1, F2-Score=0.018365472910927456, Cohen's Kappa=0.02543777112245793, total=  10.5s\n",
      "[CV] C=0.01, gamma=0.1 ...............................................\n",
      "[CV]  C=0.01, gamma=0.1, F2-Score=0.02635801054320422, Cohen's Kappa=0.036394018298194775, total=  10.5s\n",
      "[CV] C=0.01, gamma=0.1 ...............................................\n",
      "[CV]  C=0.01, gamma=0.1, F2-Score=0.02065167508031207, Cohen's Kappa=0.028578247037954085, total=  10.5s\n",
      "[CV] C=0.01, gamma=1 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=0.01, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  16.5s\n",
      "[CV] C=0.01, gamma=1 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=0.01, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  16.9s\n",
      "[CV] C=0.01, gamma=1 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=0.01, gamma=1, F2-Score=0.0, Cohen's Kappa=0.0, total=  17.2s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.8s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.8s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, gamma=0.001, F2-Score=0.0, Cohen's Kappa=0.0, total=   9.8s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV]  C=0.1, gamma=0.01, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  10.1s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV]  C=0.1, gamma=0.01, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  10.1s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV]  C=0.1, gamma=0.01, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=  10.1s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV]  C=0.1, gamma=0.1, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  10.2s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV]  C=0.1, gamma=0.1, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  10.0s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV]  C=0.1, gamma=0.1, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=  10.1s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV]  C=0.1, gamma=1, F2-Score=0.0217939894471209, Cohen's Kappa=0.030146017861586882, total=  23.3s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV]  C=0.1, gamma=1, F2-Score=0.01607717041800643, Cohen's Kappa=0.022288281684787803, total=  24.4s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV]  C=0.1, gamma=1, F2-Score=0.01378676470588235, Cohen's Kappa=0.01912989984400426, total=  24.4s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV]  C=1, gamma=0.001, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  10.1s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV]  C=1, gamma=0.001, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  10.1s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV]  C=1, gamma=0.001, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=  10.1s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV]  C=1, gamma=0.01, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=   9.6s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV]  C=1, gamma=0.01, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=   9.5s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV]  C=1, gamma=0.01, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=   9.5s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV]  C=1, gamma=0.1, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  12.2s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV]  C=1, gamma=0.1, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  11.8s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV]  C=1, gamma=0.1, F2-Score=0.27747551686615884, Cohen's Kappa=0.34886259579444223, total=  12.1s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV]  C=1, gamma=1, F2-Score=0.30938758211485484, Cohen's Kappa=0.3562294524645646, total=  34.9s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV]  C=1, gamma=1, F2-Score=0.2621442328268778, Cohen's Kappa=0.30368294319522293, total=  35.1s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV]  C=1, gamma=1, F2-Score=0.29187009127573765, Cohen's Kappa=0.334305211983262, total=  29.4s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV]  C=10, gamma=0.001, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=   9.6s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV]  C=10, gamma=0.001, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=   9.6s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV]  C=10, gamma=0.001, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=   9.7s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV]  C=10, gamma=0.01, F2-Score=0.2897764271760365, Cohen's Kappa=0.36274339096505825, total=  11.8s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV]  C=10, gamma=0.01, F2-Score=0.27027027027027023, Cohen's Kappa=0.3406815706208829, total=  11.4s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV]  C=10, gamma=0.01, F2-Score=0.2764475402699173, Cohen's Kappa=0.3476974184297871, total=  11.5s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV]  C=10, gamma=0.1, F2-Score=0.3451196273554944, Cohen's Kappa=0.4043458013724508, total=  15.4s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV]  C=10, gamma=0.1, F2-Score=0.3402148725510849, Cohen's Kappa=0.39039174940038346, total=  15.0s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV]  C=10, gamma=0.1, F2-Score=0.3358129649309245, Cohen's Kappa=0.3969867232768972, total=  15.0s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV]  C=10, gamma=1, F2-Score=0.3613963039014374, Cohen's Kappa=0.3841594331577507, total=  42.8s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV]  C=10, gamma=1, F2-Score=0.30952380952380953, Cohen's Kappa=0.32598473531028027, total=  43.4s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV]  C=10, gamma=1, F2-Score=0.34602784602784603, Cohen's Kappa=0.3602641950740597, total=  43.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 22.9min finished\n"
     ]
    }
   ],
   "source": [
    "# Create new GSCV object (identical to the one used for training the model on the full dataset)\n",
    "grid_red = GridSearchCV(classifier, param_grid=params, scoring=scores, refit='F2-Score', verbose=4)\n",
    "\n",
    "# Fit grid to PCA-REDUCED data\n",
    "grid_red_result = grid_red.fit(X_train_red, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance: F-2 Score of 0.34 with parameters: {'C': 10, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Observe best performance\n",
    "print('Best performance: F-2 Score of {:.2f} with parameters: {}'.format(grid_red_result.best_score_, grid_red_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, both the model trained on the full dataset and the model trained on the reduced dataset converged on the same ideal combination of hyperparameters. In addition, the model trained on the reduced dataset took significantly less time (nearly half as long, which is to be expected since the reduced data contained nearly half as many features). \n",
    "\n",
    "Let's now observe the perfomance of this model, and compare it to the last model's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-2 & Cohen's Kappa:\n",
      "\n",
      "Training set F2-Score: 0.37 \n",
      "Training set Cohen's Kappa: 0.44 \n",
      "\n",
      "Testing set F2-Score: 0.34 \n",
      "Testing set Cohen's Kappa: 0.40 \n",
      "\n",
      "\n",
      "Accuracy Score:\n",
      "\n",
      "Training set Accuracy: 0.91\n",
      "Testing set Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Create benchmark model (actually trained on the reduced dataset)\n",
    "benchmark = grid_red_result.best_estimator_\n",
    "\n",
    "# Predict training and testing sets to observe performance\n",
    "benchmark_train_predict = benchmark.predict(X_train_red)\n",
    "benchmark_test_predict = benchmark.predict(X_test_red)\n",
    "\n",
    "# Print out metrics used during training\n",
    "print(\"F-2 & Cohen's Kappa:\\n\")\n",
    "print(\"Training set F2-Score: {:.2f} \\nTraining set Cohen's Kappa: {:.2f} \\n\".format(fbeta_score(y_train, benchmark_train_predict, beta=2),\n",
    "                                                                                   cohen_kappa_score(y_train, benchmark_train_predict)))\n",
    "print(\"Testing set F2-Score: {:.2f} \\nTesting set Cohen's Kappa: {:.2f} \\n\\n\".format(fbeta_score(y_test, benchmark_test_predict, beta=2),\n",
    "                                                                                   cohen_kappa_score(y_test, benchmark_test_predict)))\n",
    "\n",
    "# Print out accuracy (accuracy isn't a good metric to use during training for the purposes of this problem,\n",
    "# but we can use it here post-mortem to view performance)\n",
    "print(\"Accuracy Score:\\n\")\n",
    "print(\"Training set Accuracy: {:.2f}\".format(accuracy_score(y_train, benchmark_train_predict)))\n",
    "print(\"Testing set Accuracy: {:.2f}\".format(accuracy_score(y_test, benchmark_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My hypothesis was correct: the training time for the model trained on the reduced data (X_train_red) is considerably shorter than the other model's training time. In addition, the difference in results is almost negligible: a mere 11% increase in both F-2 Score and Cohen's Kappa, compared to a 50% reduction in training time. What is more, in terms of accuracy, both models performed nearly identically on both the training and testing sets, differing by only 1% in results.\n",
    "\n",
    "From this analysis it follows that it is unnecessary from a performance standpoint to train the MLP on the full training set, since the computational cost saved by training it on the PCA-reduced data will yield very close results in F-2 Score and Cohen's Kappa, and nearly identical ones in accuracy.\n",
    "\n",
    "Therefore, we can conclude that we have arrived at a good benchmark model to compare the ensuing MLP to, using the reduced data for a more computationally-effective training and testing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model Design & Training\n",
    "\n",
    "Having completed the design, training, and testing of the benchmark model, we can now proceed to design, train, and test the MLP to outperform the benchmark. \n",
    "\n",
    "Designing an MLP from scratch is something of an art in and of itself, and there is evidently no single correct answer to how an MLP could be built. The abundance of hyperparameters that can be tuned, as well as the number of epochs or even batch size make it all the more complicated. Grid Search Cross Validation is a method that facilitates this process somewhat, as it did with the training of the SVC above. \n",
    "\n",
    "In the realm of MLPs, however, it is not as simple as fine-tuning two different hyperparameters. Therefore, I will be using Grid Search CV to fine-tune:\n",
    " \n",
    "    1. Nodes: The number of nodes to be included in the hidden layers of the network. \n",
    "    2. Dropout: The proportion of nodes to be dropped in the dropout layer.\n",
    "    3. Penalty: The penalty used by the L2 regularizer\n",
    "    4. Batch_Size: The size of the sample group on which the model is trained to perform one update to the weights.\n",
    "\n",
    "Although one can fine-tune the above parameters, this does not ensure that the number of hidden layers included in the network is the ideal one. Therefore, I carried out an exploration of the possible different architectures in another notebook (Architecture_Exploration). In this notebook I designed, trained, and tested 35 possible different combinations, and selected the two best from my observations of their performance. Having these in place, I will now run them through the same Grid Search CV procedure in order to find the ideal combination of hyperparameters corresponding to each specific architecture. Once I have the best possible performance for both, I can choose the architecture with the combination of hyperparameters that maximizes performance. \n",
    "\n",
    "The two architectures that I will use are each defined below in a constructor method passed to a wrapper from keras.wrappers.scikit_learn. Both architectures include three densely-connected hidden layers, but differ as follows: \n",
    "\n",
    "1. The first architecture includes only two hidden dropout layers, and uses RMSprop as optimizer.\n",
    "\n",
    "2. The second architecture includes three hidden dropout layers, and uses Adamax as optimizer. \n",
    "\n",
    "I am aware that the exploratory process carried out in the Architecture_Exploration notebook is only a minute search through the infinite possibilities that abound in the realm of ML. However, it is computationally expensive and practically unfeasible to attempt to devise tens or hundreds of architectures and fine-tune them all. \n",
    "\n",
    "Lastly, it must be noted that all the models I have devised include a kernel regularizer in their hidden layers as well as a ReLU activation function. The final activation function is a sigmoid in both model architectures because this is essentially a logistic regression (classification) problem, and so a logistic activation function will work well. Softmax could also work in its place (by calculating different probabilities of each call being successful or unsuccessful), but since Sigmoid is a simpler mathematical function (logistic), it is the natural choice in this case. \n",
    "\n",
    "I will include a ModelChekpoint callback to save the best weights during training after I have found the best-performing model. This is because the Keras Wrapper for GSCV in Scikit learn only helps find the best combination of hyperparameters, but does not permit one to 'extract' the best model like so:\n",
    "    \n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "Therefore, I will use it later when I have found the best model architecture with the best combination of hyperparameters, to train it and save the best weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and set random seeds for reproducible results\n",
    "from numpy.random import seed\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import wrapper\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Import necessary keras elements\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to use the F-2 Score on Keras, it is necessary to create a custom metric function. This is because Keras works with tensors and expects tensor inputs and outputs in its metrics. Therefore, Scikit Learn's F2 metric used previously would not work. \n",
    "\n",
    "For this reason, I kindly borrowed a block of code written by Arseny Kravchenko on GitHub entitled 'F-beta score for Keras', which can be found through: https://www.kaggle.com/arsenyinfo/f-beta-score-for-keras\n",
    "\n",
    "I took the liberty of modifying the name of the method slightly for the sake of clarity, changing it from 'fbeta' to 'f_2'.\n",
    "\n",
    "Kravchenko's code can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FULL CREDIT TO ARSENY KRAVCHENKO FOR THE CODE BELOW\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def f_2(y_true, y_pred, threshold_shift=0):\n",
    "    beta = 2\n",
    "\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 architecture\n",
    "def build_model_1(nodes, penalty, dropout):\n",
    "    # Achitecture\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(units=X_train_red.shape[1], input_dim=X_train_red.shape[1], activation='relu'))\n",
    "    model.add(Dense(nodes, activation='relu', kernel_regularizer=regularizers.l2(penalty)))\n",
    "    model.add(Dropout(dropout)) \n",
    "    model.add(Dense(nodes + 5, activation='relu', kernel_regularizer=regularizers.l2(penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(nodes + 10, activation='relu', kernel_regularizer=regularizers.l2(penalty)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=[f_2, 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 architecture\n",
    "def build_model_2(nodes, penalty, dropout):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=X_train_red.shape[1], input_dim=X_train_red.shape[1], activation='relu'))\n",
    "    model.add(Dense(nodes, activation='relu', kernel_regularizer=regularizers.l2(penalty)))\n",
    "    model.add(Dropout(dropout)) \n",
    "    model.add(Dense(nodes + 5, activation='relu', kernel_regularizer=regularizers.l2(penalty)))\n",
    "    model.add(Dropout(dropout)) \n",
    "    model.add(Dense(nodes + 10, activation='relu', kernel_regularizer=regularizers.l2(penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adamax', \n",
    "                  metrics=[f_2, 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined both different architectures, we can now proceed to train, fine-tune, test, and compare them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preambular definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "parameters = {'nodes':[10, 15, 20], \n",
    "              'penalty':[0.01, 0.001],\n",
    "              'dropout':[0.2, 0.3],\n",
    "              'batch_size':[40, 80],}\n",
    "\n",
    "# Scores used for the GSCV objects will be the same as those used for the SVC fine-tuning, and\n",
    "# so do not need to be defined again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Grid Search Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 57s - loss: 0.5879 - f_2: 0.0182 - acc: 0.8588 - val_loss: 0.4085 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3821 - f_2: 3.1119e-08 - acc: 0.8753 - val_loss: 0.3660 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3566 - f_2: 3.0450e-08 - acc: 0.8753 - val_loss: 0.3497 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3454 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3410 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3402 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3373 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3357 - f_2: 0.0284 - acc: 0.8784 - val_loss: 0.3315 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3313 - f_2: 0.1425 - acc: 0.8902 - val_loss: 0.3291 - val_f_2: 0.0310 - val_acc: 0.8702\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.1708 - acc: 0.8933 - val_loss: 0.3318 - val_f_2: 0.0095 - val_acc: 0.8678\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.1837 - acc: 0.8936 - val_loss: 0.3261 - val_f_2: 0.2518 - val_acc: 0.8979\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.1794 - acc: 0.8945 - val_loss: 0.3238 - val_f_2: 0.2558 - val_acc: 0.8985\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3245 - f_2: 0.1869 - acc: 0.8942 - val_loss: 0.3224 - val_f_2: 0.2518 - val_acc: 0.8979\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.1960 - acc: 0.8951 - val_loss: 0.3239 - val_f_2: 0.2885 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.1890 - acc: 0.8936 - val_loss: 0.3204 - val_f_2: 0.2717 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.2127 - acc: 0.8944 - val_loss: 0.3186 - val_f_2: 0.2939 - val_acc: 0.9009\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.1994 - acc: 0.8940 - val_loss: 0.3194 - val_f_2: 0.2718 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.1914 - acc: 0.8927 - val_loss: 0.3216 - val_f_2: 0.2755 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.2135 - acc: 0.8949 - val_loss: 0.3178 - val_f_2: 0.2957 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.2216 - acc: 0.8971 - val_loss: 0.3152 - val_f_2: 0.3125 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.2416 - acc: 0.8974 - val_loss: 0.3166 - val_f_2: 0.2892 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.2412 - acc: 0.8982 - val_loss: 0.3145 - val_f_2: 0.3379 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2627 - acc: 0.8989 - val_loss: 0.3139 - val_f_2: 0.3139 - val_acc: 0.9015\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.2666 - acc: 0.8995 - val_loss: 0.3151 - val_f_2: 0.3091 - val_acc: 0.9012\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.2647 - acc: 0.8984 - val_loss: 0.3146 - val_f_2: 0.3070 - val_acc: 0.9018\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.2605 - acc: 0.8989 - val_loss: 0.3128 - val_f_2: 0.3282 - val_acc: 0.9021\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.2743 - acc: 0.9000 - val_loss: 0.3169 - val_f_2: 0.3211 - val_acc: 0.9018\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.2825 - acc: 0.9008 - val_loss: 0.3113 - val_f_2: 0.4049 - val_acc: 0.9024\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.2766 - acc: 0.9000 - val_loss: 0.3110 - val_f_2: 0.3402 - val_acc: 0.9018\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.2814 - acc: 0.9013 - val_loss: 0.3110 - val_f_2: 0.3479 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.2655 - acc: 0.8984 - val_loss: 0.3169 - val_f_2: 0.3144 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3086 - f_2: 0.3022 - acc: 0.9015 - val_loss: 0.3124 - val_f_2: 0.3599 - val_acc: 0.9012\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.2851 - acc: 0.9006 - val_loss: 0.3093 - val_f_2: 0.3473 - val_acc: 0.9009\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3025 - acc: 0.9017 - val_loss: 0.3106 - val_f_2: 0.3078 - val_acc: 0.9009\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.2846 - acc: 0.9010 - val_loss: 0.3133 - val_f_2: 0.3268 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3089 - f_2: 0.2913 - acc: 0.8999 - val_loss: 0.3100 - val_f_2: 0.3541 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3027 - acc: 0.9018 - val_loss: 0.3098 - val_f_2: 0.3184 - val_acc: 0.9018\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.2945 - acc: 0.9004 - val_loss: 0.3089 - val_f_2: 0.3452 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3019 - acc: 0.9017 - val_loss: 0.3072 - val_f_2: 0.3859 - val_acc: 0.9021\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3056 - f_2: 0.2997 - acc: 0.9000 - val_loss: 0.3107 - val_f_2: 0.3231 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3162 - acc: 0.9031 - val_loss: 0.3108 - val_f_2: 0.3225 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3056 - f_2: 0.3106 - acc: 0.9017 - val_loss: 0.3082 - val_f_2: 0.3676 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3102 - acc: 0.9013 - val_loss: 0.3100 - val_f_2: 0.3434 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.3217 - acc: 0.9028 - val_loss: 0.3097 - val_f_2: 0.3522 - val_acc: 0.9003\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.2977 - acc: 0.8996 - val_loss: 0.3101 - val_f_2: 0.3513 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3155 - acc: 0.9017 - val_loss: 0.3128 - val_f_2: 0.3552 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.3246 - acc: 0.9031 - val_loss: 0.3095 - val_f_2: 0.3430 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3038 - f_2: 0.3026 - acc: 0.9010 - val_loss: 0.3082 - val_f_2: 0.3535 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3007 - acc: 0.9009 - val_loss: 0.3084 - val_f_2: 0.3246 - val_acc: 0.9015\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3020 - f_2: 0.3014 - acc: 0.9015 - val_loss: 0.3073 - val_f_2: 0.3565 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3037 - f_2: 0.2965 - acc: 0.9001 - val_loss: 0.3067 - val_f_2: 0.3797 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3020 - f_2: 0.3036 - acc: 0.9015 - val_loss: 0.3179 - val_f_2: 0.3146 - val_acc: 0.9015\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.01, total= 2.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min remaining:    0.0s\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.5911 - f_2: 0.0115 - acc: 0.8572 - val_loss: 0.4104 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3835 - f_2: 2.9101e-08 - acc: 0.8722 - val_loss: 0.3636 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3566 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3484 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3454 - f_2: 3.0078e-08 - acc: 0.8722 - val_loss: 0.3407 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3390 - f_2: 0.0125 - acc: 0.8731 - val_loss: 0.3352 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3364 - f_2: 0.1375 - acc: 0.8867 - val_loss: 0.3322 - val_f_2: 0.0503 - val_acc: 0.8729\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3330 - f_2: 0.1749 - acc: 0.8899 - val_loss: 0.3301 - val_f_2: 0.1442 - val_acc: 0.8844\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3298 - f_2: 0.1684 - acc: 0.8904 - val_loss: 0.3272 - val_f_2: 0.2167 - val_acc: 0.8932\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3294 - f_2: 0.1823 - acc: 0.8912 - val_loss: 0.3259 - val_f_2: 0.2717 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.1845 - acc: 0.8920 - val_loss: 0.3242 - val_f_2: 0.2618 - val_acc: 0.8991\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.2114 - acc: 0.8933 - val_loss: 0.3240 - val_f_2: 0.2568 - val_acc: 0.8982\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3239 - f_2: 0.1995 - acc: 0.8934 - val_loss: 0.3211 - val_f_2: 0.2902 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.2098 - acc: 0.8927 - val_loss: 0.3220 - val_f_2: 0.2717 - val_acc: 0.8997\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.2243 - acc: 0.8942 - val_loss: 0.3248 - val_f_2: 0.2749 - val_acc: 0.8991\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.2102 - acc: 0.8919 - val_loss: 0.3185 - val_f_2: 0.3048 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2416 - acc: 0.8945 - val_loss: 0.3226 - val_f_2: 0.2771 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.2455 - acc: 0.8951 - val_loss: 0.3172 - val_f_2: 0.3063 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.2451 - acc: 0.8950 - val_loss: 0.3157 - val_f_2: 0.3659 - val_acc: 0.9032\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.2507 - acc: 0.8958 - val_loss: 0.3152 - val_f_2: 0.3342 - val_acc: 0.9006\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.2703 - acc: 0.8978 - val_loss: 0.3151 - val_f_2: 0.3480 - val_acc: 0.9024\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.2608 - acc: 0.8978 - val_loss: 0.3141 - val_f_2: 0.3109 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3136 - f_2: 0.2754 - acc: 0.8981 - val_loss: 0.3147 - val_f_2: 0.2807 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.2694 - acc: 0.8978 - val_loss: 0.3115 - val_f_2: 0.3361 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.2964 - acc: 0.9002 - val_loss: 0.3120 - val_f_2: 0.3357 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.2790 - acc: 0.8973 - val_loss: 0.3116 - val_f_2: 0.3316 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.2781 - acc: 0.8981 - val_loss: 0.3135 - val_f_2: 0.3269 - val_acc: 0.9003\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.2851 - acc: 0.8978 - val_loss: 0.3107 - val_f_2: 0.3195 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.2826 - acc: 0.8978 - val_loss: 0.3135 - val_f_2: 0.3126 - val_acc: 0.9012\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3074 - acc: 0.9001 - val_loss: 0.3104 - val_f_2: 0.3294 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.2932 - acc: 0.8986 - val_loss: 0.3089 - val_f_2: 0.3512 - val_acc: 0.9015\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.2951 - acc: 0.8989 - val_loss: 0.3101 - val_f_2: 0.3156 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.2923 - acc: 0.8987 - val_loss: 0.3145 - val_f_2: 0.3173 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.2973 - acc: 0.8989 - val_loss: 0.3148 - val_f_2: 0.3824 - val_acc: 0.9018\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.2937 - acc: 0.8973 - val_loss: 0.3087 - val_f_2: 0.3453 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3159 - acc: 0.8978 - val_loss: 0.3186 - val_f_2: 0.3149 - val_acc: 0.9021\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3221 - acc: 0.9016 - val_loss: 0.3081 - val_f_2: 0.3546 - val_acc: 0.9003\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3024 - acc: 0.8989 - val_loss: 0.3078 - val_f_2: 0.3396 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3062 - f_2: 0.3258 - acc: 0.9003 - val_loss: 0.3061 - val_f_2: 0.3656 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3125 - acc: 0.8989 - val_loss: 0.3073 - val_f_2: 0.3354 - val_acc: 0.8997\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3146 - acc: 0.9002 - val_loss: 0.3071 - val_f_2: 0.3330 - val_acc: 0.9009\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3230 - acc: 0.9010 - val_loss: 0.3074 - val_f_2: 0.3247 - val_acc: 0.8997\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.3087 - acc: 0.8987 - val_loss: 0.3083 - val_f_2: 0.3300 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3014 - acc: 0.8984 - val_loss: 0.3060 - val_f_2: 0.3940 - val_acc: 0.9027\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3084 - acc: 0.8981 - val_loss: 0.3073 - val_f_2: 0.3635 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3185 - acc: 0.8994 - val_loss: 0.3106 - val_f_2: 0.3160 - val_acc: 0.9021\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3075 - acc: 0.8988 - val_loss: 0.3066 - val_f_2: 0.3578 - val_acc: 0.9012\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3327 - acc: 0.9006 - val_loss: 0.3066 - val_f_2: 0.3390 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3186 - acc: 0.9001 - val_loss: 0.3036 - val_f_2: 0.3813 - val_acc: 0.9018\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3225 - acc: 0.9012 - val_loss: 0.3055 - val_f_2: 0.3642 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2999 - f_2: 0.3236 - acc: 0.9003 - val_loss: 0.3068 - val_f_2: 0.3800 - val_acc: 0.9021\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.5944 - f_2: 0.0165 - acc: 0.8523 - val_loss: 0.3927 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3906 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3463 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3659 - f_2: 2.9867e-08 - acc: 0.8691 - val_loss: 0.3326 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3532 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3240 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3451 - f_2: 2.9998e-08 - acc: 0.8691 - val_loss: 0.3208 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3413 - f_2: 0.1119 - acc: 0.8816 - val_loss: 0.3176 - val_f_2: 0.0066 - val_acc: 0.8802\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3403 - f_2: 0.1605 - acc: 0.8866 - val_loss: 0.3137 - val_f_2: 0.1170 - val_acc: 0.8906\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3372 - f_2: 0.1791 - acc: 0.8883 - val_loss: 0.3121 - val_f_2: 0.2399 - val_acc: 0.9044\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3345 - f_2: 0.1680 - acc: 0.8877 - val_loss: 0.3092 - val_f_2: 0.2401 - val_acc: 0.9044\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3340 - f_2: 0.1875 - acc: 0.8894 - val_loss: 0.3110 - val_f_2: 0.2952 - val_acc: 0.9109\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3336 - f_2: 0.2013 - acc: 0.8894 - val_loss: 0.3093 - val_f_2: 0.2952 - val_acc: 0.9109\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3308 - f_2: 0.2173 - acc: 0.8903 - val_loss: 0.3057 - val_f_2: 0.2861 - val_acc: 0.9100\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3281 - f_2: 0.2010 - acc: 0.8895 - val_loss: 0.3057 - val_f_2: 0.3023 - val_acc: 0.9109\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.1917 - acc: 0.8884 - val_loss: 0.3049 - val_f_2: 0.3095 - val_acc: 0.9118\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.2101 - acc: 0.8894 - val_loss: 0.3022 - val_f_2: 0.3152 - val_acc: 0.9094\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.2202 - acc: 0.8908 - val_loss: 0.3032 - val_f_2: 0.3221 - val_acc: 0.9094\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2129 - acc: 0.8903 - val_loss: 0.2998 - val_f_2: 0.3133 - val_acc: 0.9094\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3245 - f_2: 0.2374 - acc: 0.8906 - val_loss: 0.2996 - val_f_2: 0.3096 - val_acc: 0.9118\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2553 - acc: 0.8935 - val_loss: 0.3006 - val_f_2: 0.3096 - val_acc: 0.9118\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2548 - acc: 0.8933 - val_loss: 0.2990 - val_f_2: 0.3190 - val_acc: 0.9100\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2288 - acc: 0.8911 - val_loss: 0.2985 - val_f_2: 0.3221 - val_acc: 0.9109\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.2349 - acc: 0.8922 - val_loss: 0.2966 - val_f_2: 0.3243 - val_acc: 0.9086\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.2646 - acc: 0.8925 - val_loss: 0.2967 - val_f_2: 0.3217 - val_acc: 0.9100\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2729 - acc: 0.8939 - val_loss: 0.2960 - val_f_2: 0.3220 - val_acc: 0.9103\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.2679 - acc: 0.8943 - val_loss: 0.2990 - val_f_2: 0.3402 - val_acc: 0.9071\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.2602 - acc: 0.8919 - val_loss: 0.2979 - val_f_2: 0.3608 - val_acc: 0.9065\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.2814 - acc: 0.8947 - val_loss: 0.2943 - val_f_2: 0.3206 - val_acc: 0.9103\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3180 - f_2: 0.2748 - acc: 0.8936 - val_loss: 0.2942 - val_f_2: 0.3257 - val_acc: 0.9100\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.2779 - acc: 0.8940 - val_loss: 0.2995 - val_f_2: 0.3728 - val_acc: 0.9074\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2848 - acc: 0.8955 - val_loss: 0.2966 - val_f_2: 0.3517 - val_acc: 0.9077\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.2850 - acc: 0.8938 - val_loss: 0.2934 - val_f_2: 0.3486 - val_acc: 0.9077\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3015 - acc: 0.8965 - val_loss: 0.2924 - val_f_2: 0.3355 - val_acc: 0.9091\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.2891 - acc: 0.8953 - val_loss: 0.2909 - val_f_2: 0.3456 - val_acc: 0.9091\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.2867 - acc: 0.8940 - val_loss: 0.2935 - val_f_2: 0.3273 - val_acc: 0.9118\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.2918 - acc: 0.8937 - val_loss: 0.2917 - val_f_2: 0.3501 - val_acc: 0.9080\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3108 - acc: 0.8966 - val_loss: 0.2964 - val_f_2: 0.3220 - val_acc: 0.9106\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.2965 - acc: 0.8937 - val_loss: 0.2912 - val_f_2: 0.3230 - val_acc: 0.9112\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.3023 - acc: 0.8955 - val_loss: 0.2913 - val_f_2: 0.3644 - val_acc: 0.9074\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3098 - acc: 0.8965 - val_loss: 0.2957 - val_f_2: 0.3879 - val_acc: 0.9080\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3126 - f_2: 0.3113 - acc: 0.8950 - val_loss: 0.2914 - val_f_2: 0.3256 - val_acc: 0.9109\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3043 - acc: 0.8968 - val_loss: 0.2912 - val_f_2: 0.3564 - val_acc: 0.9088\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3113 - f_2: 0.3021 - acc: 0.8942 - val_loss: 0.2923 - val_f_2: 0.3562 - val_acc: 0.9088\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3038 - acc: 0.8949 - val_loss: 0.2918 - val_f_2: 0.3737 - val_acc: 0.9088\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3042 - acc: 0.8940 - val_loss: 0.2898 - val_f_2: 0.3515 - val_acc: 0.9094\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3169 - acc: 0.8956 - val_loss: 0.2911 - val_f_2: 0.3721 - val_acc: 0.9086\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3103 - f_2: 0.3142 - acc: 0.8961 - val_loss: 0.2893 - val_f_2: 0.3614 - val_acc: 0.9091\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.3114 - acc: 0.8947 - val_loss: 0.2885 - val_f_2: 0.3563 - val_acc: 0.9094\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3129 - acc: 0.8953 - val_loss: 0.2904 - val_f_2: 0.3214 - val_acc: 0.9112\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3084 - f_2: 0.2989 - acc: 0.8950 - val_loss: 0.2897 - val_f_2: 0.3498 - val_acc: 0.9091\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3084 - f_2: 0.3123 - acc: 0.8943 - val_loss: 0.2892 - val_f_2: 0.3549 - val_acc: 0.9103\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4218 - f_2: 0.0113 - acc: 0.8598 - val_loss: 0.3477 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3417 - f_2: 0.1692 - acc: 0.8922 - val_loss: 0.3195 - val_f_2: 0.2859 - val_acc: 0.9012\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3224 - f_2: 0.2489 - acc: 0.8973 - val_loss: 0.3074 - val_f_2: 0.3307 - val_acc: 0.9012\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.2941 - acc: 0.9009 - val_loss: 0.3030 - val_f_2: 0.3351 - val_acc: 0.8988\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3232 - acc: 0.9030 - val_loss: 0.3012 - val_f_2: 0.3623 - val_acc: 0.9003\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3259 - acc: 0.9048 - val_loss: 0.2996 - val_f_2: 0.3848 - val_acc: 0.9003\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3456 - acc: 0.9060 - val_loss: 0.2973 - val_f_2: 0.3476 - val_acc: 0.8994\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.2999 - f_2: 0.3397 - acc: 0.9046 - val_loss: 0.2984 - val_f_2: 0.3777 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.3514 - acc: 0.9068 - val_loss: 0.2965 - val_f_2: 0.3770 - val_acc: 0.9018\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2944 - f_2: 0.3606 - acc: 0.9074 - val_loss: 0.2969 - val_f_2: 0.3479 - val_acc: 0.9012\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2919 - f_2: 0.3652 - acc: 0.9088 - val_loss: 0.2969 - val_f_2: 0.3897 - val_acc: 0.9021\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3765 - acc: 0.9090 - val_loss: 0.2971 - val_f_2: 0.3703 - val_acc: 0.9032\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2901 - f_2: 0.3813 - acc: 0.9095 - val_loss: 0.2950 - val_f_2: 0.3859 - val_acc: 0.9021\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.3855 - acc: 0.9096 - val_loss: 0.2958 - val_f_2: 0.3658 - val_acc: 0.9009\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.3908 - acc: 0.9091 - val_loss: 0.2942 - val_f_2: 0.3936 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2857 - f_2: 0.3966 - acc: 0.9085 - val_loss: 0.2957 - val_f_2: 0.3847 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.3877 - acc: 0.9095 - val_loss: 0.2965 - val_f_2: 0.3679 - val_acc: 0.9006\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.3892 - acc: 0.9095 - val_loss: 0.2943 - val_f_2: 0.3723 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.3926 - acc: 0.9104 - val_loss: 0.2946 - val_f_2: 0.3766 - val_acc: 0.8985\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2821 - f_2: 0.4021 - acc: 0.9105 - val_loss: 0.2985 - val_f_2: 0.3712 - val_acc: 0.9003\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2802 - f_2: 0.4077 - acc: 0.9101 - val_loss: 0.3013 - val_f_2: 0.3698 - val_acc: 0.8997\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4020 - acc: 0.9099 - val_loss: 0.2957 - val_f_2: 0.4070 - val_acc: 0.8997\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2790 - f_2: 0.4109 - acc: 0.9107 - val_loss: 0.2960 - val_f_2: 0.4192 - val_acc: 0.8979\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2792 - f_2: 0.4187 - acc: 0.9115 - val_loss: 0.2966 - val_f_2: 0.3977 - val_acc: 0.8994\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2763 - f_2: 0.4062 - acc: 0.9112 - val_loss: 0.3020 - val_f_2: 0.3549 - val_acc: 0.9018\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2779 - f_2: 0.4163 - acc: 0.9116 - val_loss: 0.2962 - val_f_2: 0.4172 - val_acc: 0.8994\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4122 - acc: 0.9118 - val_loss: 0.3003 - val_f_2: 0.4413 - val_acc: 0.8950\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2769 - f_2: 0.4358 - acc: 0.9129 - val_loss: 0.2975 - val_f_2: 0.3999 - val_acc: 0.8976\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2768 - f_2: 0.4200 - acc: 0.9123 - val_loss: 0.3001 - val_f_2: 0.3640 - val_acc: 0.8994\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2756 - f_2: 0.4279 - acc: 0.9124 - val_loss: 0.2964 - val_f_2: 0.4181 - val_acc: 0.8985\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2757 - f_2: 0.4344 - acc: 0.9121 - val_loss: 0.2956 - val_f_2: 0.4064 - val_acc: 0.8979\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2737 - f_2: 0.4328 - acc: 0.9124 - val_loss: 0.2959 - val_f_2: 0.4012 - val_acc: 0.8973\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2736 - f_2: 0.4325 - acc: 0.9130 - val_loss: 0.2983 - val_f_2: 0.4148 - val_acc: 0.8991\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2757 - f_2: 0.4561 - acc: 0.9134 - val_loss: 0.2993 - val_f_2: 0.3930 - val_acc: 0.8991\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2743 - f_2: 0.4344 - acc: 0.9133 - val_loss: 0.2977 - val_f_2: 0.4075 - val_acc: 0.8985\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2726 - f_2: 0.4365 - acc: 0.9136 - val_loss: 0.2983 - val_f_2: 0.4261 - val_acc: 0.8973\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2758 - f_2: 0.4443 - acc: 0.9127 - val_loss: 0.2980 - val_f_2: 0.4032 - val_acc: 0.8971\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2727 - f_2: 0.4359 - acc: 0.9137 - val_loss: 0.2979 - val_f_2: 0.3889 - val_acc: 0.8971\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2722 - f_2: 0.4383 - acc: 0.9148 - val_loss: 0.3011 - val_f_2: 0.4493 - val_acc: 0.8979\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2712 - f_2: 0.4402 - acc: 0.9136 - val_loss: 0.2968 - val_f_2: 0.4164 - val_acc: 0.8988\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2727 - f_2: 0.4395 - acc: 0.9131 - val_loss: 0.2976 - val_f_2: 0.4191 - val_acc: 0.8979\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2711 - f_2: 0.4587 - acc: 0.9141 - val_loss: 0.3014 - val_f_2: 0.4070 - val_acc: 0.8971\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2704 - f_2: 0.4389 - acc: 0.9151 - val_loss: 0.2996 - val_f_2: 0.4407 - val_acc: 0.8950\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2717 - f_2: 0.4460 - acc: 0.9143 - val_loss: 0.2959 - val_f_2: 0.3998 - val_acc: 0.8982\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2697 - f_2: 0.4517 - acc: 0.9144 - val_loss: 0.3002 - val_f_2: 0.3879 - val_acc: 0.8991\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2718 - f_2: 0.4471 - acc: 0.9144 - val_loss: 0.2986 - val_f_2: 0.4049 - val_acc: 0.8982\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2695 - f_2: 0.4564 - acc: 0.9148 - val_loss: 0.2988 - val_f_2: 0.4181 - val_acc: 0.8968\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2708 - f_2: 0.4563 - acc: 0.9147 - val_loss: 0.2985 - val_f_2: 0.4067 - val_acc: 0.8985\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2699 - f_2: 0.4527 - acc: 0.9159 - val_loss: 0.2993 - val_f_2: 0.4129 - val_acc: 0.8962\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2711 - f_2: 0.4363 - acc: 0.9136 - val_loss: 0.2980 - val_f_2: 0.4017 - val_acc: 0.8965\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4210 - f_2: 0.0205 - acc: 0.8582 - val_loss: 0.3476 - val_f_2: 0.0336 - val_acc: 0.8705\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3391 - f_2: 0.2119 - acc: 0.8942 - val_loss: 0.3179 - val_f_2: 0.3059 - val_acc: 0.9027\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2690 - acc: 0.8972 - val_loss: 0.3072 - val_f_2: 0.3603 - val_acc: 0.8997\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3362 - acc: 0.9017 - val_loss: 0.3036 - val_f_2: 0.3946 - val_acc: 0.9021\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3417 - acc: 0.9031 - val_loss: 0.3019 - val_f_2: 0.3360 - val_acc: 0.9021\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3598 - acc: 0.9054 - val_loss: 0.2987 - val_f_2: 0.3745 - val_acc: 0.9018\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3732 - acc: 0.9044 - val_loss: 0.3018 - val_f_2: 0.3255 - val_acc: 0.8994\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.3674 - acc: 0.9061 - val_loss: 0.2983 - val_f_2: 0.3788 - val_acc: 0.8997\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3686 - acc: 0.9059 - val_loss: 0.2964 - val_f_2: 0.3742 - val_acc: 0.9018\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2935 - f_2: 0.3768 - acc: 0.9053 - val_loss: 0.2968 - val_f_2: 0.3560 - val_acc: 0.9003\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2921 - f_2: 0.3691 - acc: 0.9056 - val_loss: 0.2965 - val_f_2: 0.3604 - val_acc: 0.9009\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3614 - acc: 0.9062 - val_loss: 0.2963 - val_f_2: 0.4217 - val_acc: 0.9029\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.3803 - acc: 0.9062 - val_loss: 0.2962 - val_f_2: 0.4273 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2878 - f_2: 0.3984 - acc: 0.9065 - val_loss: 0.2947 - val_f_2: 0.4214 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.3901 - acc: 0.9065 - val_loss: 0.2968 - val_f_2: 0.4276 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2846 - f_2: 0.4155 - acc: 0.9087 - val_loss: 0.3002 - val_f_2: 0.3361 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2829 - f_2: 0.4112 - acc: 0.9089 - val_loss: 0.2965 - val_f_2: 0.3720 - val_acc: 0.8997\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.4108 - acc: 0.9075 - val_loss: 0.2939 - val_f_2: 0.4064 - val_acc: 0.9027\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2824 - f_2: 0.4247 - acc: 0.9077 - val_loss: 0.2949 - val_f_2: 0.3847 - val_acc: 0.9006\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2822 - f_2: 0.4069 - acc: 0.9084 - val_loss: 0.2980 - val_f_2: 0.3627 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4303 - acc: 0.9084 - val_loss: 0.2966 - val_f_2: 0.4059 - val_acc: 0.9029\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4404 - acc: 0.9088 - val_loss: 0.2975 - val_f_2: 0.4345 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4258 - acc: 0.9085 - val_loss: 0.2954 - val_f_2: 0.4158 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2799 - f_2: 0.4282 - acc: 0.9097 - val_loss: 0.2973 - val_f_2: 0.4242 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4484 - acc: 0.9093 - val_loss: 0.2973 - val_f_2: 0.4312 - val_acc: 0.9032\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2783 - f_2: 0.4338 - acc: 0.9102 - val_loss: 0.2979 - val_f_2: 0.4240 - val_acc: 0.8976\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2753 - f_2: 0.4562 - acc: 0.9106 - val_loss: 0.2979 - val_f_2: 0.4358 - val_acc: 0.8991\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2763 - f_2: 0.4516 - acc: 0.9109 - val_loss: 0.2983 - val_f_2: 0.4381 - val_acc: 0.8991\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2748 - f_2: 0.4447 - acc: 0.9115 - val_loss: 0.3000 - val_f_2: 0.4154 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4444 - acc: 0.9098 - val_loss: 0.2987 - val_f_2: 0.4240 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2743 - f_2: 0.4611 - acc: 0.9113 - val_loss: 0.3023 - val_f_2: 0.4079 - val_acc: 0.9018\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2746 - f_2: 0.4480 - acc: 0.9109 - val_loss: 0.2991 - val_f_2: 0.4276 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2745 - f_2: 0.4583 - acc: 0.9119 - val_loss: 0.2994 - val_f_2: 0.3977 - val_acc: 0.9021\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2747 - f_2: 0.4594 - acc: 0.9116 - val_loss: 0.2975 - val_f_2: 0.4293 - val_acc: 0.9003\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2732 - f_2: 0.4674 - acc: 0.9124 - val_loss: 0.2974 - val_f_2: 0.4322 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2715 - f_2: 0.4490 - acc: 0.9127 - val_loss: 0.2994 - val_f_2: 0.4302 - val_acc: 0.9021\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2721 - f_2: 0.4678 - acc: 0.9130 - val_loss: 0.2973 - val_f_2: 0.4159 - val_acc: 0.9012\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2717 - f_2: 0.4690 - acc: 0.9132 - val_loss: 0.3015 - val_f_2: 0.4223 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2724 - f_2: 0.4613 - acc: 0.9135 - val_loss: 0.3025 - val_f_2: 0.4379 - val_acc: 0.8968\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2713 - f_2: 0.4631 - acc: 0.9117 - val_loss: 0.3008 - val_f_2: 0.4374 - val_acc: 0.8962\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2714 - f_2: 0.4661 - acc: 0.9128 - val_loss: 0.3019 - val_f_2: 0.4489 - val_acc: 0.8962\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2720 - f_2: 0.4578 - acc: 0.9108 - val_loss: 0.3020 - val_f_2: 0.4396 - val_acc: 0.8953\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2708 - f_2: 0.4871 - acc: 0.9139 - val_loss: 0.3011 - val_f_2: 0.4308 - val_acc: 0.8982\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2706 - f_2: 0.4876 - acc: 0.9139 - val_loss: 0.3018 - val_f_2: 0.4194 - val_acc: 0.8968\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2707 - f_2: 0.4709 - acc: 0.9135 - val_loss: 0.3010 - val_f_2: 0.4328 - val_acc: 0.8968\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2704 - f_2: 0.4660 - acc: 0.9139 - val_loss: 0.3068 - val_f_2: 0.4524 - val_acc: 0.8938\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2684 - f_2: 0.4656 - acc: 0.9133 - val_loss: 0.3069 - val_f_2: 0.4417 - val_acc: 0.8956\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2696 - f_2: 0.4838 - acc: 0.9138 - val_loss: 0.3012 - val_f_2: 0.4405 - val_acc: 0.8962\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2686 - f_2: 0.4722 - acc: 0.9128 - val_loss: 0.3042 - val_f_2: 0.4224 - val_acc: 0.8994\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2687 - f_2: 0.4892 - acc: 0.9146 - val_loss: 0.3024 - val_f_2: 0.4304 - val_acc: 0.8950\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4268 - f_2: 0.0120 - acc: 0.8566 - val_loss: 0.3313 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3500 - f_2: 2.9370e-08 - acc: 0.8691 - val_loss: 0.3179 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3396 - f_2: 0.0762 - acc: 0.8773 - val_loss: 0.3110 - val_f_2: 0.2701 - val_acc: 0.9080\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3341 - f_2: 0.2188 - acc: 0.8920 - val_loss: 0.3063 - val_f_2: 0.2893 - val_acc: 0.9100\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.2307 - acc: 0.8928 - val_loss: 0.3028 - val_f_2: 0.2917 - val_acc: 0.9106\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3238 - f_2: 0.2549 - acc: 0.8945 - val_loss: 0.3018 - val_f_2: 0.3101 - val_acc: 0.9106\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2810 - acc: 0.8962 - val_loss: 0.2981 - val_f_2: 0.3215 - val_acc: 0.9100\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.2957 - acc: 0.8969 - val_loss: 0.2970 - val_f_2: 0.3206 - val_acc: 0.9100\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.3096 - acc: 0.8970 - val_loss: 0.2968 - val_f_2: 0.3588 - val_acc: 0.9074\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.3164 - acc: 0.8978 - val_loss: 0.2936 - val_f_2: 0.3376 - val_acc: 0.9071\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3185 - acc: 0.8987 - val_loss: 0.2929 - val_f_2: 0.3487 - val_acc: 0.9071\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3343 - acc: 0.8987 - val_loss: 0.2929 - val_f_2: 0.3859 - val_acc: 0.9094\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3081 - f_2: 0.3376 - acc: 0.8978 - val_loss: 0.2947 - val_f_2: 0.4253 - val_acc: 0.9100\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3512 - acc: 0.8969 - val_loss: 0.2937 - val_f_2: 0.3947 - val_acc: 0.9103\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3530 - acc: 0.8995 - val_loss: 0.2893 - val_f_2: 0.3940 - val_acc: 0.9088\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3562 - acc: 0.8994 - val_loss: 0.2897 - val_f_2: 0.3863 - val_acc: 0.9086\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3543 - acc: 0.9003 - val_loss: 0.2879 - val_f_2: 0.4033 - val_acc: 0.9077\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3708 - acc: 0.8979 - val_loss: 0.2867 - val_f_2: 0.3806 - val_acc: 0.9077\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.3730 - acc: 0.9004 - val_loss: 0.2882 - val_f_2: 0.4022 - val_acc: 0.9094\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3547 - acc: 0.8984 - val_loss: 0.2865 - val_f_2: 0.4122 - val_acc: 0.9077\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2999 - f_2: 0.3946 - acc: 0.9014 - val_loss: 0.2864 - val_f_2: 0.4042 - val_acc: 0.9068\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3583 - acc: 0.8999 - val_loss: 0.2884 - val_f_2: 0.4308 - val_acc: 0.9068\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3934 - acc: 0.9003 - val_loss: 0.2854 - val_f_2: 0.4113 - val_acc: 0.9074\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2938 - f_2: 0.3877 - acc: 0.8995 - val_loss: 0.2875 - val_f_2: 0.4174 - val_acc: 0.9065\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.3854 - acc: 0.8992 - val_loss: 0.2869 - val_f_2: 0.4323 - val_acc: 0.9080\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.3803 - acc: 0.9006 - val_loss: 0.2879 - val_f_2: 0.4358 - val_acc: 0.9050\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3922 - acc: 0.9012 - val_loss: 0.2893 - val_f_2: 0.4239 - val_acc: 0.9038\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.3857 - acc: 0.9009 - val_loss: 0.2861 - val_f_2: 0.4102 - val_acc: 0.9047\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2922 - f_2: 0.4007 - acc: 0.9014 - val_loss: 0.2847 - val_f_2: 0.4075 - val_acc: 0.9083\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2908 - f_2: 0.4007 - acc: 0.9022 - val_loss: 0.2881 - val_f_2: 0.4402 - val_acc: 0.9018\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2896 - f_2: 0.3904 - acc: 0.9037 - val_loss: 0.2854 - val_f_2: 0.4334 - val_acc: 0.9050\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.4058 - acc: 0.9024 - val_loss: 0.2845 - val_f_2: 0.3986 - val_acc: 0.9059\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3909 - acc: 0.9012 - val_loss: 0.2837 - val_f_2: 0.4283 - val_acc: 0.9044\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2893 - f_2: 0.3984 - acc: 0.9021 - val_loss: 0.2850 - val_f_2: 0.4136 - val_acc: 0.9047\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2885 - f_2: 0.4312 - acc: 0.9045 - val_loss: 0.2852 - val_f_2: 0.4144 - val_acc: 0.9032\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2874 - f_2: 0.4119 - acc: 0.9019 - val_loss: 0.2856 - val_f_2: 0.4090 - val_acc: 0.9029\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2887 - f_2: 0.3956 - acc: 0.9015 - val_loss: 0.2866 - val_f_2: 0.4248 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2883 - f_2: 0.4135 - acc: 0.9027 - val_loss: 0.2837 - val_f_2: 0.3937 - val_acc: 0.9050\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2885 - f_2: 0.3913 - acc: 0.9012 - val_loss: 0.2856 - val_f_2: 0.4091 - val_acc: 0.9027\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2841 - f_2: 0.4050 - acc: 0.9027 - val_loss: 0.2835 - val_f_2: 0.4369 - val_acc: 0.9047\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2865 - f_2: 0.4091 - acc: 0.9021 - val_loss: 0.2859 - val_f_2: 0.4366 - val_acc: 0.9024\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2872 - f_2: 0.4098 - acc: 0.9017 - val_loss: 0.2832 - val_f_2: 0.4129 - val_acc: 0.9035\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2838 - f_2: 0.4205 - acc: 0.9046 - val_loss: 0.2865 - val_f_2: 0.4064 - val_acc: 0.9047\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.4044 - acc: 0.9027 - val_loss: 0.2828 - val_f_2: 0.4128 - val_acc: 0.9029\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.4232 - acc: 0.9035 - val_loss: 0.2824 - val_f_2: 0.4056 - val_acc: 0.9056\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2855 - f_2: 0.4127 - acc: 0.9021 - val_loss: 0.2822 - val_f_2: 0.3916 - val_acc: 0.9091\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2822 - f_2: 0.4108 - acc: 0.9036 - val_loss: 0.2827 - val_f_2: 0.3973 - val_acc: 0.9056\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.3988 - acc: 0.9035 - val_loss: 0.2844 - val_f_2: 0.4047 - val_acc: 0.9015\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2820 - f_2: 0.4261 - acc: 0.9036 - val_loss: 0.2897 - val_f_2: 0.4111 - val_acc: 0.8985\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.4126 - acc: 0.9031 - val_loss: 0.2857 - val_f_2: 0.4144 - val_acc: 0.9015\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6016 - f_2: 0.0074 - acc: 0.8638 - val_loss: 0.3976 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3764 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3627 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3584 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3537 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3448 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3427 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3384 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3351 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3331 - f_2: 0.0700 - acc: 0.8823 - val_loss: 0.3302 - val_f_2: 0.2531 - val_acc: 0.8982\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.2005 - acc: 0.8961 - val_loss: 0.3292 - val_f_2: 0.2696 - val_acc: 0.8991\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.2246 - acc: 0.8978 - val_loss: 0.3265 - val_f_2: 0.2734 - val_acc: 0.8991\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2339 - acc: 0.9001 - val_loss: 0.3241 - val_f_2: 0.2735 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3258 - f_2: 0.2423 - acc: 0.9000 - val_loss: 0.3222 - val_f_2: 0.2786 - val_acc: 0.8997\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2658 - acc: 0.9013 - val_loss: 0.3208 - val_f_2: 0.2920 - val_acc: 0.8997\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2716 - acc: 0.9026 - val_loss: 0.3218 - val_f_2: 0.2737 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2835 - acc: 0.9041 - val_loss: 0.3213 - val_f_2: 0.2638 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2703 - acc: 0.9028 - val_loss: 0.3176 - val_f_2: 0.3048 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.2866 - acc: 0.9033 - val_loss: 0.3178 - val_f_2: 0.2768 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2811 - acc: 0.9029 - val_loss: 0.3167 - val_f_2: 0.3123 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.2802 - acc: 0.9037 - val_loss: 0.3210 - val_f_2: 0.2892 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2957 - acc: 0.9045 - val_loss: 0.3224 - val_f_2: 0.2736 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.3042 - acc: 0.9043 - val_loss: 0.3203 - val_f_2: 0.2909 - val_acc: 0.9006\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.2977 - acc: 0.9034 - val_loss: 0.3135 - val_f_2: 0.3090 - val_acc: 0.9012\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3098 - acc: 0.9045 - val_loss: 0.3182 - val_f_2: 0.2892 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.3024 - acc: 0.9051 - val_loss: 0.3120 - val_f_2: 0.3278 - val_acc: 0.9018\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3114 - f_2: 0.3032 - acc: 0.9040 - val_loss: 0.3128 - val_f_2: 0.3197 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.3104 - acc: 0.9037 - val_loss: 0.3120 - val_f_2: 0.3203 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3161 - acc: 0.9046 - val_loss: 0.3114 - val_f_2: 0.3050 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3117 - acc: 0.9044 - val_loss: 0.3108 - val_f_2: 0.3424 - val_acc: 0.9003\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3334 - acc: 0.9062 - val_loss: 0.3111 - val_f_2: 0.3150 - val_acc: 0.8997\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3078 - f_2: 0.3260 - acc: 0.9055 - val_loss: 0.3101 - val_f_2: 0.3016 - val_acc: 0.9012\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3215 - acc: 0.9039 - val_loss: 0.3103 - val_f_2: 0.3034 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3116 - acc: 0.9048 - val_loss: 0.3134 - val_f_2: 0.3145 - val_acc: 0.9021\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3131 - acc: 0.9033 - val_loss: 0.3089 - val_f_2: 0.3312 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.3142 - acc: 0.9046 - val_loss: 0.3089 - val_f_2: 0.3670 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3081 - f_2: 0.3405 - acc: 0.9064 - val_loss: 0.3091 - val_f_2: 0.3207 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3231 - acc: 0.9048 - val_loss: 0.3082 - val_f_2: 0.3660 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3075 - f_2: 0.3213 - acc: 0.9046 - val_loss: 0.3151 - val_f_2: 0.3007 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3492 - acc: 0.9060 - val_loss: 0.3098 - val_f_2: 0.3664 - val_acc: 0.9021\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3314 - acc: 0.9057 - val_loss: 0.3079 - val_f_2: 0.3145 - val_acc: 0.8991\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3178 - acc: 0.9040 - val_loss: 0.3078 - val_f_2: 0.3365 - val_acc: 0.9006\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3038 - f_2: 0.3375 - acc: 0.9061 - val_loss: 0.3089 - val_f_2: 0.3132 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3142 - acc: 0.9043 - val_loss: 0.3071 - val_f_2: 0.3252 - val_acc: 0.8982\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3293 - acc: 0.9057 - val_loss: 0.3091 - val_f_2: 0.3246 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3351 - acc: 0.9041 - val_loss: 0.3073 - val_f_2: 0.3333 - val_acc: 0.8997\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3510 - acc: 0.9068 - val_loss: 0.3131 - val_f_2: 0.3269 - val_acc: 0.9003\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3007 - f_2: 0.3500 - acc: 0.9069 - val_loss: 0.3080 - val_f_2: 0.3407 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3216 - acc: 0.9046 - val_loss: 0.3103 - val_f_2: 0.3221 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3272 - acc: 0.9043 - val_loss: 0.3124 - val_f_2: 0.3336 - val_acc: 0.9018\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3505 - acc: 0.9060 - val_loss: 0.3088 - val_f_2: 0.3189 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.3477 - acc: 0.9060 - val_loss: 0.3072 - val_f_2: 0.3680 - val_acc: 0.9015\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3412 - acc: 0.9048 - val_loss: 0.3075 - val_f_2: 0.3594 - val_acc: 0.9006\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3497 - acc: 0.9063 - val_loss: 0.3099 - val_f_2: 0.3208 - val_acc: 0.9003\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6018 - f_2: 0.0114 - acc: 0.8630 - val_loss: 0.3962 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3762 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3610 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3550 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3484 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3433 - f_2: 2.9712e-08 - acc: 0.8722 - val_loss: 0.3380 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3394 - f_2: 0.0448 - acc: 0.8770 - val_loss: 0.3330 - val_f_2: 0.2193 - val_acc: 0.8935\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3332 - f_2: 0.2233 - acc: 0.8959 - val_loss: 0.3311 - val_f_2: 0.2278 - val_acc: 0.8947\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3289 - f_2: 0.2390 - acc: 0.8977 - val_loss: 0.3280 - val_f_2: 0.2581 - val_acc: 0.8985\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3271 - f_2: 0.2517 - acc: 0.8991 - val_loss: 0.3246 - val_f_2: 0.2694 - val_acc: 0.8985\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2636 - acc: 0.9001 - val_loss: 0.3235 - val_f_2: 0.2716 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2614 - acc: 0.9009 - val_loss: 0.3226 - val_f_2: 0.3142 - val_acc: 0.8997\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.2798 - acc: 0.9006 - val_loss: 0.3239 - val_f_2: 0.2751 - val_acc: 0.8994\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.2717 - acc: 0.8999 - val_loss: 0.3199 - val_f_2: 0.2874 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2896 - acc: 0.9009 - val_loss: 0.3197 - val_f_2: 0.2884 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2865 - acc: 0.9010 - val_loss: 0.3158 - val_f_2: 0.2751 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.2897 - acc: 0.9013 - val_loss: 0.3167 - val_f_2: 0.2979 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.2997 - acc: 0.9020 - val_loss: 0.3153 - val_f_2: 0.3378 - val_acc: 0.9009\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3118 - acc: 0.9029 - val_loss: 0.3135 - val_f_2: 0.2924 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3112 - acc: 0.9022 - val_loss: 0.3134 - val_f_2: 0.3253 - val_acc: 0.9015\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3166 - acc: 0.9040 - val_loss: 0.3120 - val_f_2: 0.3297 - val_acc: 0.9009\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.3132 - acc: 0.9026 - val_loss: 0.3123 - val_f_2: 0.3524 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3194 - acc: 0.9034 - val_loss: 0.3126 - val_f_2: 0.3568 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3159 - acc: 0.9033 - val_loss: 0.3098 - val_f_2: 0.3113 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3283 - acc: 0.9037 - val_loss: 0.3097 - val_f_2: 0.3328 - val_acc: 0.9021\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.3240 - acc: 0.9034 - val_loss: 0.3118 - val_f_2: 0.3323 - val_acc: 0.9018\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3090 - f_2: 0.3298 - acc: 0.9040 - val_loss: 0.3110 - val_f_2: 0.3114 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3552 - acc: 0.9046 - val_loss: 0.3079 - val_f_2: 0.3055 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3089 - f_2: 0.3242 - acc: 0.9039 - val_loss: 0.3085 - val_f_2: 0.3705 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3350 - acc: 0.9038 - val_loss: 0.3126 - val_f_2: 0.3037 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3348 - acc: 0.9037 - val_loss: 0.3115 - val_f_2: 0.3334 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3392 - acc: 0.9048 - val_loss: 0.3072 - val_f_2: 0.3253 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3342 - acc: 0.9043 - val_loss: 0.3066 - val_f_2: 0.3546 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3433 - acc: 0.9049 - val_loss: 0.3064 - val_f_2: 0.3035 - val_acc: 0.9015\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3330 - acc: 0.9035 - val_loss: 0.3061 - val_f_2: 0.3375 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3368 - acc: 0.9046 - val_loss: 0.3068 - val_f_2: 0.3636 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3438 - acc: 0.9037 - val_loss: 0.3056 - val_f_2: 0.3565 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3471 - acc: 0.9046 - val_loss: 0.3142 - val_f_2: 0.3074 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3500 - acc: 0.9048 - val_loss: 0.3116 - val_f_2: 0.3260 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3032 - f_2: 0.3443 - acc: 0.9039 - val_loss: 0.3056 - val_f_2: 0.3420 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3511 - acc: 0.9046 - val_loss: 0.3045 - val_f_2: 0.3263 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3019 - f_2: 0.3446 - acc: 0.9043 - val_loss: 0.3045 - val_f_2: 0.3130 - val_acc: 0.9018\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.3559 - acc: 0.9046 - val_loss: 0.3038 - val_f_2: 0.3666 - val_acc: 0.9024\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3527 - acc: 0.9046 - val_loss: 0.3042 - val_f_2: 0.3496 - val_acc: 0.9015\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3708 - acc: 0.9052 - val_loss: 0.3036 - val_f_2: 0.3616 - val_acc: 0.9018\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.3599 - acc: 0.9048 - val_loss: 0.3050 - val_f_2: 0.3773 - val_acc: 0.9021\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3019 - f_2: 0.3621 - acc: 0.9055 - val_loss: 0.3069 - val_f_2: 0.3491 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.3664 - acc: 0.9061 - val_loss: 0.3079 - val_f_2: 0.3747 - val_acc: 0.9027\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3571 - acc: 0.9060 - val_loss: 0.3039 - val_f_2: 0.3167 - val_acc: 0.9018\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3007 - f_2: 0.3654 - acc: 0.9054 - val_loss: 0.3065 - val_f_2: 0.3302 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3519 - acc: 0.9050 - val_loss: 0.3086 - val_f_2: 0.3878 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.3757 - acc: 0.9048 - val_loss: 0.3066 - val_f_2: 0.3953 - val_acc: 0.9018\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6089 - f_2: 0.0124 - acc: 0.8590 - val_loss: 0.3745 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3825 - f_2: 2.9561e-08 - acc: 0.8691 - val_loss: 0.3421 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3621 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3311 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3509 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3229 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3431 - f_2: 0.0165 - acc: 0.8706 - val_loss: 0.3191 - val_f_2: 0.1985 - val_acc: 0.8994\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3411 - f_2: 0.1414 - acc: 0.8847 - val_loss: 0.3147 - val_f_2: 0.2525 - val_acc: 0.9059\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3371 - f_2: 0.1671 - acc: 0.8872 - val_loss: 0.3127 - val_f_2: 0.2814 - val_acc: 0.9094\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3329 - f_2: 0.1953 - acc: 0.8897 - val_loss: 0.3112 - val_f_2: 0.3011 - val_acc: 0.9106\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3329 - f_2: 0.2115 - acc: 0.8917 - val_loss: 0.3085 - val_f_2: 0.3091 - val_acc: 0.9106\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3314 - f_2: 0.2182 - acc: 0.8912 - val_loss: 0.3066 - val_f_2: 0.3011 - val_acc: 0.9106\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.2356 - acc: 0.8936 - val_loss: 0.3090 - val_f_2: 0.2970 - val_acc: 0.9109\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3289 - f_2: 0.2311 - acc: 0.8928 - val_loss: 0.3025 - val_f_2: 0.3048 - val_acc: 0.9109\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3262 - f_2: 0.2309 - acc: 0.8919 - val_loss: 0.3020 - val_f_2: 0.3011 - val_acc: 0.9106\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.2411 - acc: 0.8933 - val_loss: 0.2999 - val_f_2: 0.3068 - val_acc: 0.9109\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2563 - acc: 0.8942 - val_loss: 0.2999 - val_f_2: 0.3103 - val_acc: 0.9097\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2817 - acc: 0.8967 - val_loss: 0.3008 - val_f_2: 0.3127 - val_acc: 0.9112\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.2679 - acc: 0.8954 - val_loss: 0.3012 - val_f_2: 0.3328 - val_acc: 0.9091\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.2911 - acc: 0.8970 - val_loss: 0.2970 - val_f_2: 0.2999 - val_acc: 0.9106\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.2930 - acc: 0.8970 - val_loss: 0.2953 - val_f_2: 0.2999 - val_acc: 0.9106\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.2865 - acc: 0.8968 - val_loss: 0.2950 - val_f_2: 0.3150 - val_acc: 0.9115\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2901 - acc: 0.8980 - val_loss: 0.2939 - val_f_2: 0.2999 - val_acc: 0.9106\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.3101 - acc: 0.8992 - val_loss: 0.2935 - val_f_2: 0.3129 - val_acc: 0.9109\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2887 - acc: 0.8971 - val_loss: 0.2935 - val_f_2: 0.3064 - val_acc: 0.9112\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3009 - acc: 0.8989 - val_loss: 0.2929 - val_f_2: 0.3228 - val_acc: 0.9106\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3172 - f_2: 0.3003 - acc: 0.8973 - val_loss: 0.2921 - val_f_2: 0.3150 - val_acc: 0.9109\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.3205 - acc: 0.8998 - val_loss: 0.2920 - val_f_2: 0.2975 - val_acc: 0.9103\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3045 - acc: 0.8976 - val_loss: 0.2974 - val_f_2: 0.3467 - val_acc: 0.9086\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3139 - acc: 0.8984 - val_loss: 0.2911 - val_f_2: 0.3258 - val_acc: 0.9091\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3126 - f_2: 0.3032 - acc: 0.8984 - val_loss: 0.2905 - val_f_2: 0.3314 - val_acc: 0.9077\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3184 - acc: 0.9001 - val_loss: 0.2892 - val_f_2: 0.3123 - val_acc: 0.9100\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.3222 - acc: 0.8992 - val_loss: 0.2889 - val_f_2: 0.3043 - val_acc: 0.9112\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3058 - acc: 0.8988 - val_loss: 0.2971 - val_f_2: 0.3128 - val_acc: 0.9106\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3314 - acc: 0.9001 - val_loss: 0.2879 - val_f_2: 0.3011 - val_acc: 0.9103\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3169 - acc: 0.8992 - val_loss: 0.2886 - val_f_2: 0.3027 - val_acc: 0.9109\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3226 - acc: 0.8990 - val_loss: 0.2875 - val_f_2: 0.3234 - val_acc: 0.9091\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3318 - acc: 0.8995 - val_loss: 0.2869 - val_f_2: 0.3086 - val_acc: 0.9109\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.3027 - acc: 0.8982 - val_loss: 0.2936 - val_f_2: 0.3998 - val_acc: 0.9080\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3214 - acc: 0.8990 - val_loss: 0.2861 - val_f_2: 0.2992 - val_acc: 0.9106\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3103 - f_2: 0.3080 - acc: 0.8973 - val_loss: 0.2872 - val_f_2: 0.3180 - val_acc: 0.9103\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3190 - acc: 0.8980 - val_loss: 0.2860 - val_f_2: 0.3175 - val_acc: 0.9109\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3189 - acc: 0.8992 - val_loss: 0.2870 - val_f_2: 0.3108 - val_acc: 0.9109\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3360 - acc: 0.9001 - val_loss: 0.2861 - val_f_2: 0.3308 - val_acc: 0.9103\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3075 - f_2: 0.3116 - acc: 0.8986 - val_loss: 0.2864 - val_f_2: 0.3314 - val_acc: 0.9118\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3406 - acc: 0.9009 - val_loss: 0.2872 - val_f_2: 0.3338 - val_acc: 0.9100\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3392 - acc: 0.8993 - val_loss: 0.2859 - val_f_2: 0.3471 - val_acc: 0.9083\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3203 - acc: 0.8988 - val_loss: 0.2854 - val_f_2: 0.3096 - val_acc: 0.9106\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3331 - acc: 0.8995 - val_loss: 0.2860 - val_f_2: 0.3328 - val_acc: 0.9100\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3306 - acc: 0.9001 - val_loss: 0.2847 - val_f_2: 0.3161 - val_acc: 0.9109\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3241 - acc: 0.8995 - val_loss: 0.2862 - val_f_2: 0.3680 - val_acc: 0.9088\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3315 - acc: 0.8998 - val_loss: 0.2884 - val_f_2: 0.3890 - val_acc: 0.9083\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4217 - f_2: 0.0185 - acc: 0.8674 - val_loss: 0.3576 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3439 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3348 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3314 - f_2: 0.0188 - acc: 0.8773 - val_loss: 0.3275 - val_f_2: 0.2022 - val_acc: 0.8914\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3239 - f_2: 0.2000 - acc: 0.8970 - val_loss: 0.3232 - val_f_2: 0.2735 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2358 - acc: 0.8972 - val_loss: 0.3241 - val_f_2: 0.3257 - val_acc: 0.8997\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.2350 - acc: 0.8969 - val_loss: 0.3173 - val_f_2: 0.3138 - val_acc: 0.9003\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.2853 - acc: 0.9026 - val_loss: 0.3151 - val_f_2: 0.3403 - val_acc: 0.9012\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3130 - acc: 0.9052 - val_loss: 0.3137 - val_f_2: 0.3309 - val_acc: 0.9009\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3357 - acc: 0.9054 - val_loss: 0.3124 - val_f_2: 0.3426 - val_acc: 0.9018\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3340 - acc: 0.9062 - val_loss: 0.3114 - val_f_2: 0.3319 - val_acc: 0.9015\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3348 - acc: 0.9060 - val_loss: 0.3102 - val_f_2: 0.3405 - val_acc: 0.9021\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3399 - acc: 0.9058 - val_loss: 0.3112 - val_f_2: 0.3582 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3426 - acc: 0.9060 - val_loss: 0.3095 - val_f_2: 0.3961 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3635 - acc: 0.9060 - val_loss: 0.3079 - val_f_2: 0.3631 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3613 - acc: 0.9079 - val_loss: 0.3115 - val_f_2: 0.4010 - val_acc: 0.9012\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.3621 - acc: 0.9079 - val_loss: 0.3133 - val_f_2: 0.4049 - val_acc: 0.9015\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2943 - f_2: 0.3455 - acc: 0.9071 - val_loss: 0.3067 - val_f_2: 0.4023 - val_acc: 0.9018\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.3558 - acc: 0.9072 - val_loss: 0.3074 - val_f_2: 0.3973 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3699 - acc: 0.9075 - val_loss: 0.3063 - val_f_2: 0.3888 - val_acc: 0.9018\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2911 - f_2: 0.3777 - acc: 0.9081 - val_loss: 0.3065 - val_f_2: 0.3964 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.3727 - acc: 0.9080 - val_loss: 0.3060 - val_f_2: 0.4003 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2891 - f_2: 0.3645 - acc: 0.9080 - val_loss: 0.3050 - val_f_2: 0.3924 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2873 - f_2: 0.3917 - acc: 0.9082 - val_loss: 0.3056 - val_f_2: 0.3636 - val_acc: 0.9015\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2866 - f_2: 0.3719 - acc: 0.9082 - val_loss: 0.3036 - val_f_2: 0.3978 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.3833 - acc: 0.9090 - val_loss: 0.3036 - val_f_2: 0.4111 - val_acc: 0.9024\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.3863 - acc: 0.9097 - val_loss: 0.3098 - val_f_2: 0.3825 - val_acc: 0.9015\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2851 - f_2: 0.3866 - acc: 0.9086 - val_loss: 0.3055 - val_f_2: 0.3594 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2863 - f_2: 0.3938 - acc: 0.9087 - val_loss: 0.3048 - val_f_2: 0.3736 - val_acc: 0.9003\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.3878 - acc: 0.9086 - val_loss: 0.3058 - val_f_2: 0.4167 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2838 - f_2: 0.3938 - acc: 0.9105 - val_loss: 0.3067 - val_f_2: 0.4166 - val_acc: 0.8994\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2838 - f_2: 0.3872 - acc: 0.9099 - val_loss: 0.3063 - val_f_2: 0.4213 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.3863 - acc: 0.9104 - val_loss: 0.3041 - val_f_2: 0.4214 - val_acc: 0.8997\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.3920 - acc: 0.9097 - val_loss: 0.3045 - val_f_2: 0.4288 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.4173 - acc: 0.9113 - val_loss: 0.3043 - val_f_2: 0.3947 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2825 - f_2: 0.3963 - acc: 0.9099 - val_loss: 0.3038 - val_f_2: 0.4103 - val_acc: 0.8994\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.3946 - acc: 0.9102 - val_loss: 0.3103 - val_f_2: 0.4062 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2812 - f_2: 0.4023 - acc: 0.9102 - val_loss: 0.3035 - val_f_2: 0.3852 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2792 - f_2: 0.4041 - acc: 0.9092 - val_loss: 0.3035 - val_f_2: 0.4106 - val_acc: 0.8994\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2799 - f_2: 0.3986 - acc: 0.9101 - val_loss: 0.3056 - val_f_2: 0.3904 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4193 - acc: 0.9102 - val_loss: 0.3089 - val_f_2: 0.4047 - val_acc: 0.8994\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2784 - f_2: 0.4090 - acc: 0.9104 - val_loss: 0.3073 - val_f_2: 0.4107 - val_acc: 0.8994\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2792 - f_2: 0.3942 - acc: 0.9108 - val_loss: 0.3046 - val_f_2: 0.4214 - val_acc: 0.8979\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2781 - f_2: 0.4222 - acc: 0.9109 - val_loss: 0.3085 - val_f_2: 0.4101 - val_acc: 0.8985\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2786 - f_2: 0.4173 - acc: 0.9109 - val_loss: 0.3027 - val_f_2: 0.4284 - val_acc: 0.9018\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2796 - f_2: 0.4028 - acc: 0.9106 - val_loss: 0.3049 - val_f_2: 0.4055 - val_acc: 0.8994\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2778 - f_2: 0.4092 - acc: 0.9102 - val_loss: 0.3028 - val_f_2: 0.4002 - val_acc: 0.8994\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2765 - f_2: 0.4028 - acc: 0.9131 - val_loss: 0.3047 - val_f_2: 0.4050 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2766 - f_2: 0.4043 - acc: 0.9112 - val_loss: 0.3056 - val_f_2: 0.4338 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2760 - f_2: 0.3994 - acc: 0.9110 - val_loss: 0.3055 - val_f_2: 0.4389 - val_acc: 0.8997\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4174 - acc: 0.9116 - val_loss: 0.3069 - val_f_2: 0.4264 - val_acc: 0.8991\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4226 - f_2: 0.0132 - acc: 0.8641 - val_loss: 0.3537 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3403 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3333 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.1419 - acc: 0.8860 - val_loss: 0.3268 - val_f_2: 0.2764 - val_acc: 0.8997\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3238 - f_2: 0.2527 - acc: 0.8999 - val_loss: 0.3229 - val_f_2: 0.3072 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.2901 - acc: 0.9011 - val_loss: 0.3220 - val_f_2: 0.2919 - val_acc: 0.9006\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3125 - acc: 0.9019 - val_loss: 0.3170 - val_f_2: 0.3128 - val_acc: 0.9003\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3094 - acc: 0.9024 - val_loss: 0.3143 - val_f_2: 0.3400 - val_acc: 0.8991\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3243 - acc: 0.9013 - val_loss: 0.3135 - val_f_2: 0.3322 - val_acc: 0.9015\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.3279 - acc: 0.9038 - val_loss: 0.3110 - val_f_2: 0.3503 - val_acc: 0.9012\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3291 - acc: 0.9024 - val_loss: 0.3093 - val_f_2: 0.3899 - val_acc: 0.9003\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3465 - acc: 0.9035 - val_loss: 0.3078 - val_f_2: 0.3839 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3513 - acc: 0.9039 - val_loss: 0.3074 - val_f_2: 0.3919 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.3601 - acc: 0.9021 - val_loss: 0.3057 - val_f_2: 0.3835 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.3650 - acc: 0.9046 - val_loss: 0.3056 - val_f_2: 0.3914 - val_acc: 0.9003\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3735 - acc: 0.9047 - val_loss: 0.3047 - val_f_2: 0.3934 - val_acc: 0.9006\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2935 - f_2: 0.3897 - acc: 0.9062 - val_loss: 0.3073 - val_f_2: 0.3759 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2939 - f_2: 0.3764 - acc: 0.9064 - val_loss: 0.3044 - val_f_2: 0.3910 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2910 - f_2: 0.3628 - acc: 0.9051 - val_loss: 0.3060 - val_f_2: 0.3903 - val_acc: 0.9003\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.3752 - acc: 0.9060 - val_loss: 0.3031 - val_f_2: 0.3954 - val_acc: 0.9009\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.3773 - acc: 0.9064 - val_loss: 0.3050 - val_f_2: 0.4352 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2893 - f_2: 0.3957 - acc: 0.9065 - val_loss: 0.3050 - val_f_2: 0.4343 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2880 - f_2: 0.3774 - acc: 0.9064 - val_loss: 0.3063 - val_f_2: 0.4303 - val_acc: 0.9015\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2865 - f_2: 0.3990 - acc: 0.9064 - val_loss: 0.3024 - val_f_2: 0.3920 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.4048 - acc: 0.9070 - val_loss: 0.3015 - val_f_2: 0.3975 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.3764 - acc: 0.9057 - val_loss: 0.3010 - val_f_2: 0.4177 - val_acc: 0.9021\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2853 - f_2: 0.4026 - acc: 0.9082 - val_loss: 0.3019 - val_f_2: 0.4146 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.4024 - acc: 0.9091 - val_loss: 0.3021 - val_f_2: 0.4189 - val_acc: 0.9038\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2829 - f_2: 0.3992 - acc: 0.9063 - val_loss: 0.3047 - val_f_2: 0.4255 - val_acc: 0.9021\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.4093 - acc: 0.9074 - val_loss: 0.3016 - val_f_2: 0.4215 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2820 - f_2: 0.4032 - acc: 0.9078 - val_loss: 0.3070 - val_f_2: 0.4536 - val_acc: 0.8991\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2816 - f_2: 0.3956 - acc: 0.9065 - val_loss: 0.3010 - val_f_2: 0.4199 - val_acc: 0.8988\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4226 - acc: 0.9088 - val_loss: 0.3007 - val_f_2: 0.4118 - val_acc: 0.9029\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2791 - f_2: 0.4145 - acc: 0.9094 - val_loss: 0.3038 - val_f_2: 0.4535 - val_acc: 0.9003\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4275 - acc: 0.9076 - val_loss: 0.3026 - val_f_2: 0.4266 - val_acc: 0.9003\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4080 - acc: 0.9079 - val_loss: 0.3048 - val_f_2: 0.4313 - val_acc: 0.8991\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2785 - f_2: 0.4262 - acc: 0.9091 - val_loss: 0.3038 - val_f_2: 0.4438 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2766 - f_2: 0.4419 - acc: 0.9106 - val_loss: 0.3024 - val_f_2: 0.4263 - val_acc: 0.9018\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2776 - f_2: 0.4115 - acc: 0.9097 - val_loss: 0.3018 - val_f_2: 0.4062 - val_acc: 0.8994\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2773 - f_2: 0.4174 - acc: 0.9092 - val_loss: 0.3033 - val_f_2: 0.4463 - val_acc: 0.8991\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2774 - f_2: 0.4311 - acc: 0.9102 - val_loss: 0.3034 - val_f_2: 0.4337 - val_acc: 0.9021\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2769 - f_2: 0.4311 - acc: 0.9097 - val_loss: 0.3029 - val_f_2: 0.4259 - val_acc: 0.9027\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2756 - f_2: 0.4460 - acc: 0.9111 - val_loss: 0.3057 - val_f_2: 0.4407 - val_acc: 0.8982\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2758 - f_2: 0.4220 - acc: 0.9099 - val_loss: 0.3036 - val_f_2: 0.4285 - val_acc: 0.8994\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2752 - f_2: 0.4185 - acc: 0.9099 - val_loss: 0.3016 - val_f_2: 0.4159 - val_acc: 0.9018\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2753 - f_2: 0.4309 - acc: 0.9088 - val_loss: 0.3062 - val_f_2: 0.4400 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2744 - f_2: 0.4413 - acc: 0.9102 - val_loss: 0.3018 - val_f_2: 0.4259 - val_acc: 0.8997\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2757 - f_2: 0.4452 - acc: 0.9100 - val_loss: 0.3053 - val_f_2: 0.4434 - val_acc: 0.8971\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2744 - f_2: 0.4289 - acc: 0.9086 - val_loss: 0.3011 - val_f_2: 0.4315 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2729 - f_2: 0.4440 - acc: 0.9119 - val_loss: 0.3040 - val_f_2: 0.4136 - val_acc: 0.8994\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2744 - f_2: 0.4355 - acc: 0.9091 - val_loss: 0.3033 - val_f_2: 0.4423 - val_acc: 0.9009\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4305 - f_2: 0.0130 - acc: 0.8602 - val_loss: 0.3336 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3468 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3132 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3362 - f_2: 0.0750 - acc: 0.8767 - val_loss: 0.3096 - val_f_2: 0.2739 - val_acc: 0.9080\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.2195 - acc: 0.8912 - val_loss: 0.3077 - val_f_2: 0.3097 - val_acc: 0.9097\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.2386 - acc: 0.8931 - val_loss: 0.3030 - val_f_2: 0.3162 - val_acc: 0.9106\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.2662 - acc: 0.8942 - val_loss: 0.3015 - val_f_2: 0.3300 - val_acc: 0.9112\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3272 - acc: 0.8995 - val_loss: 0.3029 - val_f_2: 0.3333 - val_acc: 0.9121\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3135 - f_2: 0.3336 - acc: 0.9007 - val_loss: 0.2958 - val_f_2: 0.3380 - val_acc: 0.9083\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3318 - acc: 0.9009 - val_loss: 0.2970 - val_f_2: 0.3264 - val_acc: 0.9109\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3420 - acc: 0.9001 - val_loss: 0.2937 - val_f_2: 0.3607 - val_acc: 0.9094\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3086 - f_2: 0.3573 - acc: 0.9005 - val_loss: 0.2967 - val_f_2: 0.3472 - val_acc: 0.9088\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3507 - acc: 0.9015 - val_loss: 0.2927 - val_f_2: 0.3573 - val_acc: 0.9100\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3693 - acc: 0.9018 - val_loss: 0.2925 - val_f_2: 0.3882 - val_acc: 0.9091\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3647 - acc: 0.9028 - val_loss: 0.2904 - val_f_2: 0.3629 - val_acc: 0.9100\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.3682 - acc: 0.9026 - val_loss: 0.2918 - val_f_2: 0.4056 - val_acc: 0.9077\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.3835 - acc: 0.9028 - val_loss: 0.2881 - val_f_2: 0.3830 - val_acc: 0.9088\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2968 - f_2: 0.3689 - acc: 0.9021 - val_loss: 0.2877 - val_f_2: 0.3829 - val_acc: 0.9109\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2962 - f_2: 0.3826 - acc: 0.9038 - val_loss: 0.2942 - val_f_2: 0.4165 - val_acc: 0.9083\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2962 - f_2: 0.3825 - acc: 0.9016 - val_loss: 0.2865 - val_f_2: 0.3914 - val_acc: 0.9100\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2951 - f_2: 0.3909 - acc: 0.9035 - val_loss: 0.2878 - val_f_2: 0.3954 - val_acc: 0.9086\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2931 - f_2: 0.3848 - acc: 0.9035 - val_loss: 0.2884 - val_f_2: 0.4009 - val_acc: 0.9065\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2935 - f_2: 0.3826 - acc: 0.9023 - val_loss: 0.2906 - val_f_2: 0.4456 - val_acc: 0.9062\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2923 - f_2: 0.4061 - acc: 0.9049 - val_loss: 0.2861 - val_f_2: 0.3967 - val_acc: 0.9103\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.3914 - acc: 0.9037 - val_loss: 0.2887 - val_f_2: 0.3779 - val_acc: 0.9097\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2882 - f_2: 0.4147 - acc: 0.9053 - val_loss: 0.2869 - val_f_2: 0.3999 - val_acc: 0.9083\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2884 - f_2: 0.4057 - acc: 0.9049 - val_loss: 0.2850 - val_f_2: 0.3736 - val_acc: 0.9097\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.3950 - acc: 0.9035 - val_loss: 0.2863 - val_f_2: 0.4073 - val_acc: 0.9074\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2882 - f_2: 0.4016 - acc: 0.9044 - val_loss: 0.2856 - val_f_2: 0.3951 - val_acc: 0.9094\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2869 - f_2: 0.4115 - acc: 0.9040 - val_loss: 0.2905 - val_f_2: 0.4097 - val_acc: 0.9035\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.4033 - acc: 0.9054 - val_loss: 0.2874 - val_f_2: 0.4124 - val_acc: 0.9077\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2847 - f_2: 0.4012 - acc: 0.9049 - val_loss: 0.2862 - val_f_2: 0.3875 - val_acc: 0.9088\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.4116 - acc: 0.9051 - val_loss: 0.2892 - val_f_2: 0.4256 - val_acc: 0.9044\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2862 - f_2: 0.4076 - acc: 0.9045 - val_loss: 0.2904 - val_f_2: 0.4094 - val_acc: 0.9044\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.4032 - acc: 0.9056 - val_loss: 0.2844 - val_f_2: 0.4031 - val_acc: 0.9086\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2863 - f_2: 0.4065 - acc: 0.9043 - val_loss: 0.2840 - val_f_2: 0.4177 - val_acc: 0.9080\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2829 - f_2: 0.4156 - acc: 0.9054 - val_loss: 0.2856 - val_f_2: 0.4163 - val_acc: 0.9077\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2827 - f_2: 0.4268 - acc: 0.9040 - val_loss: 0.2839 - val_f_2: 0.4016 - val_acc: 0.9056\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.4205 - acc: 0.9046 - val_loss: 0.2863 - val_f_2: 0.4236 - val_acc: 0.9041\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2829 - f_2: 0.4193 - acc: 0.9052 - val_loss: 0.2859 - val_f_2: 0.4134 - val_acc: 0.9044\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4374 - acc: 0.9072 - val_loss: 0.2831 - val_f_2: 0.3904 - val_acc: 0.9074\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4265 - acc: 0.9065 - val_loss: 0.2908 - val_f_2: 0.4152 - val_acc: 0.9062\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2822 - f_2: 0.4039 - acc: 0.9036 - val_loss: 0.2928 - val_f_2: 0.4407 - val_acc: 0.9065\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4196 - acc: 0.9055 - val_loss: 0.2938 - val_f_2: 0.4599 - val_acc: 0.9050\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4363 - acc: 0.9065 - val_loss: 0.2829 - val_f_2: 0.3928 - val_acc: 0.9097\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4093 - acc: 0.9046 - val_loss: 0.2838 - val_f_2: 0.4084 - val_acc: 0.9059\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2804 - f_2: 0.4158 - acc: 0.9054 - val_loss: 0.2862 - val_f_2: 0.4273 - val_acc: 0.9041\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2798 - f_2: 0.4302 - acc: 0.9053 - val_loss: 0.2853 - val_f_2: 0.4321 - val_acc: 0.9068\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2794 - f_2: 0.4170 - acc: 0.9052 - val_loss: 0.2964 - val_f_2: 0.4768 - val_acc: 0.9056\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2790 - f_2: 0.4365 - acc: 0.9053 - val_loss: 0.2835 - val_f_2: 0.4032 - val_acc: 0.9077\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2791 - f_2: 0.4374 - acc: 0.9061 - val_loss: 0.2833 - val_f_2: 0.3936 - val_acc: 0.9065\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6140 - f_2: 0.0127 - acc: 0.8688 - val_loss: 0.3946 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3713 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3567 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3488 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3430 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3373 - f_2: 0.0194 - acc: 0.8767 - val_loss: 0.3374 - val_f_2: 0.2035 - val_acc: 0.8914\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.1748 - acc: 0.8935 - val_loss: 0.3354 - val_f_2: 0.2375 - val_acc: 0.8959\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3268 - f_2: 0.2033 - acc: 0.8968 - val_loss: 0.3300 - val_f_2: 0.3076 - val_acc: 0.8997\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3242 - f_2: 0.2291 - acc: 0.8981 - val_loss: 0.3256 - val_f_2: 0.2751 - val_acc: 0.8994\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2399 - acc: 0.8996 - val_loss: 0.3234 - val_f_2: 0.2754 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.2596 - acc: 0.9008 - val_loss: 0.3302 - val_f_2: 0.2704 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2714 - acc: 0.9016 - val_loss: 0.3206 - val_f_2: 0.3229 - val_acc: 0.9009\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.2737 - acc: 0.9015 - val_loss: 0.3190 - val_f_2: 0.2832 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2876 - acc: 0.9046 - val_loss: 0.3184 - val_f_2: 0.2875 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.2821 - acc: 0.9027 - val_loss: 0.3192 - val_f_2: 0.3353 - val_acc: 0.9012\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3126 - f_2: 0.2916 - acc: 0.9029 - val_loss: 0.3154 - val_f_2: 0.2974 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.3030 - acc: 0.9037 - val_loss: 0.3142 - val_f_2: 0.2994 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3081 - f_2: 0.3061 - acc: 0.9042 - val_loss: 0.3135 - val_f_2: 0.3082 - val_acc: 0.8997\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3128 - acc: 0.9040 - val_loss: 0.3179 - val_f_2: 0.3878 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3079 - acc: 0.9045 - val_loss: 0.3124 - val_f_2: 0.3303 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3094 - acc: 0.9040 - val_loss: 0.3117 - val_f_2: 0.2961 - val_acc: 0.9009\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3037 - f_2: 0.2946 - acc: 0.9029 - val_loss: 0.3109 - val_f_2: 0.3654 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3187 - acc: 0.9043 - val_loss: 0.3098 - val_f_2: 0.3575 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3032 - f_2: 0.3213 - acc: 0.9054 - val_loss: 0.3116 - val_f_2: 0.3149 - val_acc: 0.8994\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.3162 - acc: 0.9036 - val_loss: 0.3085 - val_f_2: 0.3445 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3138 - acc: 0.9054 - val_loss: 0.3102 - val_f_2: 0.3803 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3277 - acc: 0.9049 - val_loss: 0.3119 - val_f_2: 0.2943 - val_acc: 0.9006\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2991 - f_2: 0.3201 - acc: 0.9046 - val_loss: 0.3079 - val_f_2: 0.2978 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2971 - f_2: 0.3403 - acc: 0.9054 - val_loss: 0.3077 - val_f_2: 0.3134 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3309 - acc: 0.9045 - val_loss: 0.3085 - val_f_2: 0.3320 - val_acc: 0.9003\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.3420 - acc: 0.9057 - val_loss: 0.3108 - val_f_2: 0.3915 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3363 - acc: 0.9053 - val_loss: 0.3065 - val_f_2: 0.3605 - val_acc: 0.9015\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.3321 - acc: 0.9049 - val_loss: 0.3131 - val_f_2: 0.4184 - val_acc: 0.9012\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.3355 - acc: 0.9060 - val_loss: 0.3052 - val_f_2: 0.3243 - val_acc: 0.8997\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2969 - f_2: 0.3369 - acc: 0.9057 - val_loss: 0.3056 - val_f_2: 0.3423 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2955 - f_2: 0.3459 - acc: 0.9068 - val_loss: 0.3081 - val_f_2: 0.4159 - val_acc: 0.9018\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2947 - f_2: 0.3363 - acc: 0.9061 - val_loss: 0.3090 - val_f_2: 0.4046 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.3496 - acc: 0.9065 - val_loss: 0.3061 - val_f_2: 0.3707 - val_acc: 0.9009\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2947 - f_2: 0.3387 - acc: 0.9047 - val_loss: 0.3063 - val_f_2: 0.3298 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2941 - f_2: 0.3505 - acc: 0.9049 - val_loss: 0.3046 - val_f_2: 0.3668 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2941 - f_2: 0.3474 - acc: 0.9055 - val_loss: 0.3065 - val_f_2: 0.3634 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2921 - f_2: 0.3643 - acc: 0.9063 - val_loss: 0.3046 - val_f_2: 0.3691 - val_acc: 0.9021\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2918 - f_2: 0.3408 - acc: 0.9054 - val_loss: 0.3048 - val_f_2: 0.3900 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.3541 - acc: 0.9060 - val_loss: 0.3052 - val_f_2: 0.3924 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2908 - f_2: 0.3496 - acc: 0.9079 - val_loss: 0.3063 - val_f_2: 0.3268 - val_acc: 0.8994\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.3385 - acc: 0.9060 - val_loss: 0.3106 - val_f_2: 0.4329 - val_acc: 0.9018\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3584 - acc: 0.9064 - val_loss: 0.3068 - val_f_2: 0.4001 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2909 - f_2: 0.3494 - acc: 0.9055 - val_loss: 0.3043 - val_f_2: 0.3889 - val_acc: 0.9015\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2921 - f_2: 0.3382 - acc: 0.9051 - val_loss: 0.3086 - val_f_2: 0.4134 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3525 - acc: 0.9060 - val_loss: 0.3048 - val_f_2: 0.3034 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.3550 - acc: 0.9083 - val_loss: 0.3058 - val_f_2: 0.3257 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.3539 - acc: 0.9057 - val_loss: 0.3121 - val_f_2: 0.2988 - val_acc: 0.9000\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6178 - f_2: 0.0113 - acc: 0.8642 - val_loss: 0.3940 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3726 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3567 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3488 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3437 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3372 - f_2: 0.0727 - acc: 0.8800 - val_loss: 0.3379 - val_f_2: 0.2698 - val_acc: 0.8997\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3334 - f_2: 0.1943 - acc: 0.8927 - val_loss: 0.3346 - val_f_2: 0.2250 - val_acc: 0.8944\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3287 - f_2: 0.2265 - acc: 0.8965 - val_loss: 0.3319 - val_f_2: 0.2698 - val_acc: 0.8997\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3258 - f_2: 0.2553 - acc: 0.8989 - val_loss: 0.3257 - val_f_2: 0.2777 - val_acc: 0.8997\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.2625 - acc: 0.8997 - val_loss: 0.3245 - val_f_2: 0.2960 - val_acc: 0.8997\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.2831 - acc: 0.9015 - val_loss: 0.3220 - val_f_2: 0.2718 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3182 - f_2: 0.2767 - acc: 0.9010 - val_loss: 0.3198 - val_f_2: 0.2737 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.2891 - acc: 0.9022 - val_loss: 0.3197 - val_f_2: 0.2999 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.3007 - acc: 0.9023 - val_loss: 0.3178 - val_f_2: 0.2895 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.2890 - acc: 0.9017 - val_loss: 0.3188 - val_f_2: 0.3348 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3263 - acc: 0.9039 - val_loss: 0.3139 - val_f_2: 0.2773 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.2977 - acc: 0.9012 - val_loss: 0.3168 - val_f_2: 0.2832 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.3160 - acc: 0.9035 - val_loss: 0.3127 - val_f_2: 0.2816 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3372 - acc: 0.9046 - val_loss: 0.3136 - val_f_2: 0.3021 - val_acc: 0.9006\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3078 - f_2: 0.3193 - acc: 0.9026 - val_loss: 0.3116 - val_f_2: 0.3160 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.3174 - acc: 0.9036 - val_loss: 0.3093 - val_f_2: 0.3148 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3072 - f_2: 0.3264 - acc: 0.9026 - val_loss: 0.3091 - val_f_2: 0.3189 - val_acc: 0.9012\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3034 - f_2: 0.3234 - acc: 0.9043 - val_loss: 0.3080 - val_f_2: 0.3486 - val_acc: 0.9015\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3161 - acc: 0.9029 - val_loss: 0.3088 - val_f_2: 0.3717 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3467 - acc: 0.9043 - val_loss: 0.3076 - val_f_2: 0.3057 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.3408 - acc: 0.9031 - val_loss: 0.3086 - val_f_2: 0.2887 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3368 - acc: 0.9045 - val_loss: 0.3110 - val_f_2: 0.4087 - val_acc: 0.9029\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3412 - acc: 0.9048 - val_loss: 0.3062 - val_f_2: 0.4009 - val_acc: 0.9024\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.3431 - acc: 0.9050 - val_loss: 0.3061 - val_f_2: 0.3265 - val_acc: 0.9015\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3249 - acc: 0.9029 - val_loss: 0.3054 - val_f_2: 0.3686 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3409 - acc: 0.9046 - val_loss: 0.3045 - val_f_2: 0.3185 - val_acc: 0.9009\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3400 - acc: 0.9026 - val_loss: 0.3057 - val_f_2: 0.4108 - val_acc: 0.9021\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3525 - acc: 0.9050 - val_loss: 0.3068 - val_f_2: 0.3161 - val_acc: 0.9018\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.3356 - acc: 0.9043 - val_loss: 0.3053 - val_f_2: 0.3534 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.3294 - acc: 0.9033 - val_loss: 0.3034 - val_f_2: 0.3264 - val_acc: 0.9027\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2960 - f_2: 0.3542 - acc: 0.9040 - val_loss: 0.3040 - val_f_2: 0.3968 - val_acc: 0.9012\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3484 - acc: 0.9043 - val_loss: 0.3085 - val_f_2: 0.3265 - val_acc: 0.9021\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2960 - f_2: 0.3682 - acc: 0.9057 - val_loss: 0.3046 - val_f_2: 0.3418 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3546 - acc: 0.9051 - val_loss: 0.3029 - val_f_2: 0.4005 - val_acc: 0.9027\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3631 - acc: 0.9054 - val_loss: 0.3020 - val_f_2: 0.3250 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2941 - f_2: 0.3652 - acc: 0.9070 - val_loss: 0.3025 - val_f_2: 0.3965 - val_acc: 0.9012\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2937 - f_2: 0.3649 - acc: 0.9054 - val_loss: 0.3024 - val_f_2: 0.3417 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2927 - f_2: 0.3502 - acc: 0.9043 - val_loss: 0.3017 - val_f_2: 0.3911 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2922 - f_2: 0.3646 - acc: 0.9054 - val_loss: 0.3120 - val_f_2: 0.3669 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2908 - f_2: 0.3742 - acc: 0.9054 - val_loss: 0.3000 - val_f_2: 0.3638 - val_acc: 0.9029\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.3579 - acc: 0.9060 - val_loss: 0.3007 - val_f_2: 0.3270 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.3590 - acc: 0.9057 - val_loss: 0.3012 - val_f_2: 0.3238 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.3755 - acc: 0.9065 - val_loss: 0.3003 - val_f_2: 0.4026 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2901 - f_2: 0.3643 - acc: 0.9048 - val_loss: 0.2997 - val_f_2: 0.4014 - val_acc: 0.9044\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.3647 - acc: 0.9061 - val_loss: 0.3068 - val_f_2: 0.3801 - val_acc: 0.9024\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2903 - f_2: 0.3556 - acc: 0.9050 - val_loss: 0.3012 - val_f_2: 0.4267 - val_acc: 0.9027\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3840 - acc: 0.9063 - val_loss: 0.2996 - val_f_2: 0.4031 - val_acc: 0.9021\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6244 - f_2: 0.0132 - acc: 0.8604 - val_loss: 0.3724 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3780 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3385 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3564 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3250 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3459 - f_2: 0.0738 - acc: 0.8767 - val_loss: 0.3185 - val_f_2: 0.0325 - val_acc: 0.8829\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3392 - f_2: 0.1852 - acc: 0.8894 - val_loss: 0.3157 - val_f_2: 0.2950 - val_acc: 0.9103\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3382 - f_2: 0.2092 - acc: 0.8916 - val_loss: 0.3112 - val_f_2: 0.2805 - val_acc: 0.9091\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3337 - f_2: 0.2335 - acc: 0.8947 - val_loss: 0.3098 - val_f_2: 0.2932 - val_acc: 0.9106\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.2460 - acc: 0.8952 - val_loss: 0.3062 - val_f_2: 0.3006 - val_acc: 0.9106\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3268 - f_2: 0.2750 - acc: 0.8965 - val_loss: 0.3059 - val_f_2: 0.3034 - val_acc: 0.9109\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3258 - f_2: 0.2736 - acc: 0.8982 - val_loss: 0.3063 - val_f_2: 0.3126 - val_acc: 0.9112\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.2842 - acc: 0.8982 - val_loss: 0.3048 - val_f_2: 0.3370 - val_acc: 0.9080\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2936 - acc: 0.8972 - val_loss: 0.3004 - val_f_2: 0.3128 - val_acc: 0.9109\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2978 - acc: 0.8982 - val_loss: 0.2989 - val_f_2: 0.3161 - val_acc: 0.9100\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2952 - acc: 0.8988 - val_loss: 0.2981 - val_f_2: 0.3145 - val_acc: 0.9097\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.2965 - acc: 0.8975 - val_loss: 0.2973 - val_f_2: 0.3217 - val_acc: 0.9088\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.2993 - acc: 0.8992 - val_loss: 0.2948 - val_f_2: 0.3180 - val_acc: 0.9097\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.3019 - acc: 0.8981 - val_loss: 0.2937 - val_f_2: 0.3163 - val_acc: 0.9112\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3142 - f_2: 0.3049 - acc: 0.8990 - val_loss: 0.2957 - val_f_2: 0.3497 - val_acc: 0.9077\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.2988 - acc: 0.8973 - val_loss: 0.2939 - val_f_2: 0.3232 - val_acc: 0.9094\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3113 - f_2: 0.3207 - acc: 0.8997 - val_loss: 0.2934 - val_f_2: 0.3286 - val_acc: 0.9086\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.3185 - acc: 0.9001 - val_loss: 0.2898 - val_f_2: 0.3084 - val_acc: 0.9112\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3221 - acc: 0.9002 - val_loss: 0.2903 - val_f_2: 0.3082 - val_acc: 0.9109\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3130 - acc: 0.8988 - val_loss: 0.2940 - val_f_2: 0.3373 - val_acc: 0.9080\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3087 - f_2: 0.3251 - acc: 0.8984 - val_loss: 0.2888 - val_f_2: 0.3086 - val_acc: 0.9115\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3090 - f_2: 0.3296 - acc: 0.8995 - val_loss: 0.2884 - val_f_2: 0.3213 - val_acc: 0.9083\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.3265 - acc: 0.9001 - val_loss: 0.2891 - val_f_2: 0.3561 - val_acc: 0.9088\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3250 - acc: 0.8984 - val_loss: 0.2888 - val_f_2: 0.3450 - val_acc: 0.9083\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3335 - acc: 0.9005 - val_loss: 0.2892 - val_f_2: 0.3786 - val_acc: 0.9091\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3056 - f_2: 0.3330 - acc: 0.8998 - val_loss: 0.2882 - val_f_2: 0.3229 - val_acc: 0.9094\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3233 - acc: 0.8987 - val_loss: 0.2881 - val_f_2: 0.3647 - val_acc: 0.9091\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3032 - f_2: 0.3380 - acc: 0.8999 - val_loss: 0.2880 - val_f_2: 0.3196 - val_acc: 0.9106\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3357 - acc: 0.8984 - val_loss: 0.2894 - val_f_2: 0.3933 - val_acc: 0.9091\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.3205 - acc: 0.8992 - val_loss: 0.2860 - val_f_2: 0.3246 - val_acc: 0.9112\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.3406 - acc: 0.9006 - val_loss: 0.2918 - val_f_2: 0.3452 - val_acc: 0.9083\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3457 - acc: 0.9021 - val_loss: 0.2857 - val_f_2: 0.3635 - val_acc: 0.9097\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.3452 - acc: 0.9009 - val_loss: 0.2900 - val_f_2: 0.4043 - val_acc: 0.9094\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3331 - acc: 0.8984 - val_loss: 0.2868 - val_f_2: 0.3383 - val_acc: 0.9118\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.3540 - acc: 0.9021 - val_loss: 0.2853 - val_f_2: 0.3874 - val_acc: 0.9097\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.3527 - acc: 0.9002 - val_loss: 0.2871 - val_f_2: 0.3717 - val_acc: 0.9094\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3455 - acc: 0.9008 - val_loss: 0.2853 - val_f_2: 0.3999 - val_acc: 0.9097\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3494 - acc: 0.9001 - val_loss: 0.2859 - val_f_2: 0.3184 - val_acc: 0.9097\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2989 - f_2: 0.3372 - acc: 0.8985 - val_loss: 0.2857 - val_f_2: 0.3386 - val_acc: 0.9106\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3527 - acc: 0.9013 - val_loss: 0.2861 - val_f_2: 0.3965 - val_acc: 0.9083\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3632 - acc: 0.8997 - val_loss: 0.2825 - val_f_2: 0.3311 - val_acc: 0.9109\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2962 - f_2: 0.3488 - acc: 0.9004 - val_loss: 0.2866 - val_f_2: 0.3275 - val_acc: 0.9115\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.3599 - acc: 0.9011 - val_loss: 0.2839 - val_f_2: 0.3114 - val_acc: 0.9106\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2960 - f_2: 0.3505 - acc: 0.9004 - val_loss: 0.2830 - val_f_2: 0.3425 - val_acc: 0.9112\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.3482 - acc: 0.9007 - val_loss: 0.2859 - val_f_2: 0.3569 - val_acc: 0.9109\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2951 - f_2: 0.3598 - acc: 0.9008 - val_loss: 0.2848 - val_f_2: 0.3824 - val_acc: 0.9083\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2954 - f_2: 0.3507 - acc: 0.9001 - val_loss: 0.2842 - val_f_2: 0.3177 - val_acc: 0.9106\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4190 - f_2: 0.0081 - acc: 0.8683 - val_loss: 0.3652 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3480 - f_2: 0.0387 - acc: 0.8793 - val_loss: 0.3386 - val_f_2: 0.2353 - val_acc: 0.8953\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3326 - f_2: 0.2405 - acc: 0.9004 - val_loss: 0.3268 - val_f_2: 0.2893 - val_acc: 0.8994\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.2869 - acc: 0.9040 - val_loss: 0.3212 - val_f_2: 0.2968 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.3159 - acc: 0.9050 - val_loss: 0.3178 - val_f_2: 0.3133 - val_acc: 0.9015\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3214 - acc: 0.9046 - val_loss: 0.3098 - val_f_2: 0.3710 - val_acc: 0.9018\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3673 - acc: 0.9061 - val_loss: 0.3066 - val_f_2: 0.3602 - val_acc: 0.9021\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.3636 - acc: 0.9068 - val_loss: 0.3055 - val_f_2: 0.3893 - val_acc: 0.9027\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.3734 - acc: 0.9068 - val_loss: 0.3018 - val_f_2: 0.3722 - val_acc: 0.9021\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2937 - f_2: 0.3606 - acc: 0.9065 - val_loss: 0.3030 - val_f_2: 0.3451 - val_acc: 0.9027\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.3861 - acc: 0.9076 - val_loss: 0.3003 - val_f_2: 0.3500 - val_acc: 0.9003\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.3656 - acc: 0.9082 - val_loss: 0.3005 - val_f_2: 0.4108 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2874 - f_2: 0.3874 - acc: 0.9079 - val_loss: 0.3093 - val_f_2: 0.3656 - val_acc: 0.9012\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.3853 - acc: 0.9094 - val_loss: 0.2992 - val_f_2: 0.3822 - val_acc: 0.9012\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.3681 - acc: 0.9085 - val_loss: 0.3026 - val_f_2: 0.4144 - val_acc: 0.8985\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.3887 - acc: 0.9093 - val_loss: 0.2976 - val_f_2: 0.3936 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2822 - f_2: 0.3929 - acc: 0.9094 - val_loss: 0.2980 - val_f_2: 0.3611 - val_acc: 0.9021\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2816 - f_2: 0.3993 - acc: 0.9103 - val_loss: 0.2980 - val_f_2: 0.3745 - val_acc: 0.8991\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.3993 - acc: 0.9099 - val_loss: 0.2989 - val_f_2: 0.3663 - val_acc: 0.9018\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.4054 - acc: 0.9099 - val_loss: 0.3032 - val_f_2: 0.3882 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2779 - f_2: 0.4027 - acc: 0.9106 - val_loss: 0.3001 - val_f_2: 0.4145 - val_acc: 0.9015\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2785 - f_2: 0.4093 - acc: 0.9108 - val_loss: 0.2998 - val_f_2: 0.4041 - val_acc: 0.8985\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2790 - f_2: 0.4073 - acc: 0.9099 - val_loss: 0.2980 - val_f_2: 0.3901 - val_acc: 0.9012\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4033 - acc: 0.9103 - val_loss: 0.3037 - val_f_2: 0.4051 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2770 - f_2: 0.4241 - acc: 0.9116 - val_loss: 0.3070 - val_f_2: 0.3707 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2765 - f_2: 0.4109 - acc: 0.9110 - val_loss: 0.3004 - val_f_2: 0.4174 - val_acc: 0.8985\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2760 - f_2: 0.4136 - acc: 0.9110 - val_loss: 0.3015 - val_f_2: 0.4102 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2754 - f_2: 0.4132 - acc: 0.9119 - val_loss: 0.2984 - val_f_2: 0.4196 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2756 - f_2: 0.4112 - acc: 0.9105 - val_loss: 0.3003 - val_f_2: 0.4004 - val_acc: 0.9009\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2736 - f_2: 0.4163 - acc: 0.9109 - val_loss: 0.3003 - val_f_2: 0.3658 - val_acc: 0.8997\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2752 - f_2: 0.4039 - acc: 0.9113 - val_loss: 0.3013 - val_f_2: 0.4064 - val_acc: 0.8991\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2745 - f_2: 0.4257 - acc: 0.9132 - val_loss: 0.2995 - val_f_2: 0.4007 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2733 - f_2: 0.4322 - acc: 0.9127 - val_loss: 0.2985 - val_f_2: 0.4142 - val_acc: 0.8991\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2733 - f_2: 0.4247 - acc: 0.9123 - val_loss: 0.3015 - val_f_2: 0.4038 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2729 - f_2: 0.4234 - acc: 0.9133 - val_loss: 0.3009 - val_f_2: 0.4170 - val_acc: 0.8991\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2738 - f_2: 0.4324 - acc: 0.9118 - val_loss: 0.3004 - val_f_2: 0.3981 - val_acc: 0.8988\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2703 - f_2: 0.4294 - acc: 0.9131 - val_loss: 0.2996 - val_f_2: 0.3888 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2724 - f_2: 0.4403 - acc: 0.9127 - val_loss: 0.3016 - val_f_2: 0.3819 - val_acc: 0.8985\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2723 - f_2: 0.4340 - acc: 0.9116 - val_loss: 0.3018 - val_f_2: 0.4081 - val_acc: 0.8997\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2707 - f_2: 0.4258 - acc: 0.9111 - val_loss: 0.3050 - val_f_2: 0.3805 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2708 - f_2: 0.4323 - acc: 0.9131 - val_loss: 0.3023 - val_f_2: 0.4086 - val_acc: 0.8994\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2712 - f_2: 0.4344 - acc: 0.9133 - val_loss: 0.3050 - val_f_2: 0.4523 - val_acc: 0.8962\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2688 - f_2: 0.4449 - acc: 0.9132 - val_loss: 0.3021 - val_f_2: 0.4303 - val_acc: 0.8994\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2708 - f_2: 0.4328 - acc: 0.9124 - val_loss: 0.3024 - val_f_2: 0.4113 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2704 - f_2: 0.4313 - acc: 0.9127 - val_loss: 0.3022 - val_f_2: 0.3761 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2681 - f_2: 0.4376 - acc: 0.9145 - val_loss: 0.3020 - val_f_2: 0.4031 - val_acc: 0.9003\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2687 - f_2: 0.4492 - acc: 0.9141 - val_loss: 0.3078 - val_f_2: 0.4001 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2687 - f_2: 0.4469 - acc: 0.9136 - val_loss: 0.3043 - val_f_2: 0.3882 - val_acc: 0.8991\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2680 - f_2: 0.4451 - acc: 0.9150 - val_loss: 0.3042 - val_f_2: 0.3765 - val_acc: 0.9006\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2683 - f_2: 0.4498 - acc: 0.9134 - val_loss: 0.3086 - val_f_2: 0.3982 - val_acc: 0.8968\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4193 - f_2: 0.0050 - acc: 0.8648 - val_loss: 0.3601 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3474 - f_2: 0.0957 - acc: 0.8823 - val_loss: 0.3370 - val_f_2: 0.2354 - val_acc: 0.8947\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3297 - f_2: 0.2825 - acc: 0.9015 - val_loss: 0.3259 - val_f_2: 0.2745 - val_acc: 0.9003\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.3006 - acc: 0.9029 - val_loss: 0.3171 - val_f_2: 0.3165 - val_acc: 0.8997\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3217 - acc: 0.9042 - val_loss: 0.3112 - val_f_2: 0.3423 - val_acc: 0.9003\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3573 - acc: 0.9050 - val_loss: 0.3061 - val_f_2: 0.3409 - val_acc: 0.9003\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3785 - acc: 0.9063 - val_loss: 0.3027 - val_f_2: 0.3828 - val_acc: 0.9012\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.3771 - acc: 0.9057 - val_loss: 0.3007 - val_f_2: 0.3950 - val_acc: 0.9024\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.2932 - f_2: 0.3807 - acc: 0.9066 - val_loss: 0.2981 - val_f_2: 0.3926 - val_acc: 0.9015\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.3772 - acc: 0.9061 - val_loss: 0.2965 - val_f_2: 0.4034 - val_acc: 0.9012\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2896 - f_2: 0.3942 - acc: 0.9079 - val_loss: 0.2969 - val_f_2: 0.4267 - val_acc: 0.9024\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.2874 - f_2: 0.3969 - acc: 0.9068 - val_loss: 0.2961 - val_f_2: 0.3981 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.3927 - acc: 0.9065 - val_loss: 0.2965 - val_f_2: 0.3910 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2855 - f_2: 0.4084 - acc: 0.9065 - val_loss: 0.2941 - val_f_2: 0.4089 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2834 - f_2: 0.4132 - acc: 0.9084 - val_loss: 0.2963 - val_f_2: 0.3654 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.4148 - acc: 0.9083 - val_loss: 0.2999 - val_f_2: 0.4196 - val_acc: 0.9029\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4118 - acc: 0.9076 - val_loss: 0.2953 - val_f_2: 0.4538 - val_acc: 0.9027\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2801 - f_2: 0.4214 - acc: 0.9094 - val_loss: 0.2948 - val_f_2: 0.3881 - val_acc: 0.9021\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2765 - f_2: 0.4279 - acc: 0.9096 - val_loss: 0.3004 - val_f_2: 0.4023 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2798 - f_2: 0.4199 - acc: 0.9091 - val_loss: 0.2964 - val_f_2: 0.4277 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2799 - f_2: 0.4378 - acc: 0.9085 - val_loss: 0.2918 - val_f_2: 0.4133 - val_acc: 0.9015\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2779 - f_2: 0.4335 - acc: 0.9105 - val_loss: 0.2951 - val_f_2: 0.4247 - val_acc: 0.8991\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2769 - f_2: 0.4448 - acc: 0.9100 - val_loss: 0.2951 - val_f_2: 0.4193 - val_acc: 0.9018\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4385 - acc: 0.9100 - val_loss: 0.2969 - val_f_2: 0.3971 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2755 - f_2: 0.4492 - acc: 0.9106 - val_loss: 0.2940 - val_f_2: 0.4210 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2741 - f_2: 0.4414 - acc: 0.9121 - val_loss: 0.2973 - val_f_2: 0.4100 - val_acc: 0.9024\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2738 - f_2: 0.4402 - acc: 0.9113 - val_loss: 0.2958 - val_f_2: 0.4371 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2745 - f_2: 0.4390 - acc: 0.9102 - val_loss: 0.2976 - val_f_2: 0.3918 - val_acc: 0.8997\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2731 - f_2: 0.4523 - acc: 0.9116 - val_loss: 0.2974 - val_f_2: 0.4431 - val_acc: 0.9018\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2738 - f_2: 0.4465 - acc: 0.9118 - val_loss: 0.2949 - val_f_2: 0.4344 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2744 - f_2: 0.4523 - acc: 0.9104 - val_loss: 0.2958 - val_f_2: 0.4478 - val_acc: 0.8991\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2726 - f_2: 0.4539 - acc: 0.9116 - val_loss: 0.2964 - val_f_2: 0.4312 - val_acc: 0.8982\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2715 - f_2: 0.4479 - acc: 0.9121 - val_loss: 0.2954 - val_f_2: 0.4440 - val_acc: 0.8997\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2715 - f_2: 0.4719 - acc: 0.9132 - val_loss: 0.2941 - val_f_2: 0.4165 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2703 - f_2: 0.4627 - acc: 0.9127 - val_loss: 0.2972 - val_f_2: 0.4489 - val_acc: 0.8997\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2704 - f_2: 0.4603 - acc: 0.9136 - val_loss: 0.2993 - val_f_2: 0.4339 - val_acc: 0.8988\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2725 - f_2: 0.4526 - acc: 0.9118 - val_loss: 0.2976 - val_f_2: 0.4521 - val_acc: 0.8985\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2692 - f_2: 0.4765 - acc: 0.9134 - val_loss: 0.2960 - val_f_2: 0.4359 - val_acc: 0.8991\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2695 - f_2: 0.4595 - acc: 0.9137 - val_loss: 0.2939 - val_f_2: 0.4026 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2707 - f_2: 0.4500 - acc: 0.9122 - val_loss: 0.2964 - val_f_2: 0.4196 - val_acc: 0.9003\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2694 - f_2: 0.4637 - acc: 0.9133 - val_loss: 0.2963 - val_f_2: 0.4314 - val_acc: 0.9006\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2689 - f_2: 0.4629 - acc: 0.9124 - val_loss: 0.2993 - val_f_2: 0.4496 - val_acc: 0.8988\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2691 - f_2: 0.4636 - acc: 0.9131 - val_loss: 0.3019 - val_f_2: 0.4212 - val_acc: 0.8973\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2688 - f_2: 0.4707 - acc: 0.9134 - val_loss: 0.2979 - val_f_2: 0.4403 - val_acc: 0.8997\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2677 - f_2: 0.4667 - acc: 0.9140 - val_loss: 0.3093 - val_f_2: 0.4365 - val_acc: 0.9015\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2683 - f_2: 0.4664 - acc: 0.9136 - val_loss: 0.3013 - val_f_2: 0.4456 - val_acc: 0.8982\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2681 - f_2: 0.4746 - acc: 0.9139 - val_loss: 0.3005 - val_f_2: 0.4465 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2674 - f_2: 0.4745 - acc: 0.9138 - val_loss: 0.3001 - val_f_2: 0.4571 - val_acc: 0.8979\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2668 - f_2: 0.4811 - acc: 0.9146 - val_loss: 0.2974 - val_f_2: 0.4443 - val_acc: 0.8991\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2679 - f_2: 0.4936 - acc: 0.9130 - val_loss: 0.2983 - val_f_2: 0.4518 - val_acc: 0.8982\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4263 - f_2: 0.0161 - acc: 0.8630 - val_loss: 0.3371 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3536 - f_2: 0.0608 - acc: 0.8755 - val_loss: 0.3211 - val_f_2: 0.2520 - val_acc: 0.9056\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3388 - f_2: 0.2273 - acc: 0.8950 - val_loss: 0.3127 - val_f_2: 0.2845 - val_acc: 0.9088\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3299 - f_2: 0.2711 - acc: 0.8990 - val_loss: 0.3046 - val_f_2: 0.2976 - val_acc: 0.9094\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3224 - f_2: 0.3172 - acc: 0.8998 - val_loss: 0.3013 - val_f_2: 0.3171 - val_acc: 0.9109\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.3101 - acc: 0.9012 - val_loss: 0.2997 - val_f_2: 0.3348 - val_acc: 0.9106\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3235 - acc: 0.9014 - val_loss: 0.2959 - val_f_2: 0.3606 - val_acc: 0.9121\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.3454 - acc: 0.9031 - val_loss: 0.2932 - val_f_2: 0.3998 - val_acc: 0.9124\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3527 - acc: 0.9018 - val_loss: 0.2897 - val_f_2: 0.3655 - val_acc: 0.9118\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3732 - acc: 0.9015 - val_loss: 0.2879 - val_f_2: 0.4053 - val_acc: 0.9133\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3687 - acc: 0.9019 - val_loss: 0.2850 - val_f_2: 0.3877 - val_acc: 0.9124\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.3875 - acc: 0.9021 - val_loss: 0.2869 - val_f_2: 0.3519 - val_acc: 0.9112\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3736 - acc: 0.9023 - val_loss: 0.2831 - val_f_2: 0.3941 - val_acc: 0.9127\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.3823 - acc: 0.9021 - val_loss: 0.2836 - val_f_2: 0.4311 - val_acc: 0.9106\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.3886 - acc: 0.9040 - val_loss: 0.2837 - val_f_2: 0.4467 - val_acc: 0.9103\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2900 - f_2: 0.3931 - acc: 0.9037 - val_loss: 0.2817 - val_f_2: 0.4200 - val_acc: 0.9086\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2914 - f_2: 0.4114 - acc: 0.9047 - val_loss: 0.2820 - val_f_2: 0.4152 - val_acc: 0.9077\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.4123 - acc: 0.9036 - val_loss: 0.2816 - val_f_2: 0.4078 - val_acc: 0.9097\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2860 - f_2: 0.4128 - acc: 0.9058 - val_loss: 0.2814 - val_f_2: 0.3960 - val_acc: 0.9074\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.4171 - acc: 0.9042 - val_loss: 0.2786 - val_f_2: 0.3932 - val_acc: 0.9083\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2846 - f_2: 0.4196 - acc: 0.9059 - val_loss: 0.2871 - val_f_2: 0.4502 - val_acc: 0.9065\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.4286 - acc: 0.9059 - val_loss: 0.2814 - val_f_2: 0.4343 - val_acc: 0.9059\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.4220 - acc: 0.9066 - val_loss: 0.2846 - val_f_2: 0.4423 - val_acc: 0.9047\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4286 - acc: 0.9050 - val_loss: 0.2804 - val_f_2: 0.4066 - val_acc: 0.9059\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.4313 - acc: 0.9047 - val_loss: 0.2833 - val_f_2: 0.4465 - val_acc: 0.9032\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4338 - acc: 0.9063 - val_loss: 0.2789 - val_f_2: 0.3748 - val_acc: 0.9086\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2815 - f_2: 0.4179 - acc: 0.9065 - val_loss: 0.2784 - val_f_2: 0.3959 - val_acc: 0.9062\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2794 - f_2: 0.4253 - acc: 0.9049 - val_loss: 0.2791 - val_f_2: 0.3877 - val_acc: 0.9047\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2806 - f_2: 0.4337 - acc: 0.9055 - val_loss: 0.2789 - val_f_2: 0.4069 - val_acc: 0.9044\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2775 - f_2: 0.4414 - acc: 0.9054 - val_loss: 0.2793 - val_f_2: 0.3980 - val_acc: 0.9074\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2795 - f_2: 0.4220 - acc: 0.9051 - val_loss: 0.2784 - val_f_2: 0.4086 - val_acc: 0.9027\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2781 - f_2: 0.4455 - acc: 0.9075 - val_loss: 0.2750 - val_f_2: 0.3527 - val_acc: 0.9109\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2790 - f_2: 0.4295 - acc: 0.9062 - val_loss: 0.2793 - val_f_2: 0.4286 - val_acc: 0.9032\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2774 - f_2: 0.4508 - acc: 0.9071 - val_loss: 0.2773 - val_f_2: 0.4028 - val_acc: 0.9059\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2766 - f_2: 0.4457 - acc: 0.9064 - val_loss: 0.2765 - val_f_2: 0.3867 - val_acc: 0.9086\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2777 - f_2: 0.4344 - acc: 0.9056 - val_loss: 0.2772 - val_f_2: 0.4194 - val_acc: 0.9047\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2762 - f_2: 0.4578 - acc: 0.9068 - val_loss: 0.2828 - val_f_2: 0.3648 - val_acc: 0.9077\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2765 - f_2: 0.4530 - acc: 0.9075 - val_loss: 0.2814 - val_f_2: 0.3757 - val_acc: 0.9062\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2757 - f_2: 0.4359 - acc: 0.9075 - val_loss: 0.2822 - val_f_2: 0.4375 - val_acc: 0.9029\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2751 - f_2: 0.4610 - acc: 0.9089 - val_loss: 0.2818 - val_f_2: 0.3741 - val_acc: 0.9091\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2748 - f_2: 0.4400 - acc: 0.9077 - val_loss: 0.2850 - val_f_2: 0.4682 - val_acc: 0.9024\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2759 - f_2: 0.4565 - acc: 0.9074 - val_loss: 0.2799 - val_f_2: 0.3850 - val_acc: 0.9047\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2751 - f_2: 0.4540 - acc: 0.9059 - val_loss: 0.2787 - val_f_2: 0.4152 - val_acc: 0.9053\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2744 - f_2: 0.4573 - acc: 0.9096 - val_loss: 0.2795 - val_f_2: 0.4143 - val_acc: 0.9047\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2744 - f_2: 0.4557 - acc: 0.9081 - val_loss: 0.2784 - val_f_2: 0.4021 - val_acc: 0.9059\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2730 - f_2: 0.4690 - acc: 0.9085 - val_loss: 0.2797 - val_f_2: 0.3981 - val_acc: 0.9038\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2728 - f_2: 0.4398 - acc: 0.9071 - val_loss: 0.2806 - val_f_2: 0.4388 - val_acc: 0.9038\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2747 - f_2: 0.4693 - acc: 0.9087 - val_loss: 0.2793 - val_f_2: 0.4336 - val_acc: 0.9044\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2732 - f_2: 0.4542 - acc: 0.9098 - val_loss: 0.2841 - val_f_2: 0.4551 - val_acc: 0.9024\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2731 - f_2: 0.4587 - acc: 0.9077 - val_loss: 0.2815 - val_f_2: 0.4005 - val_acc: 0.9053\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.5960 - f_2: 0.0145 - acc: 0.8590 - val_loss: 0.4184 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3903 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3708 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3615 - f_2: 3.2048e-08 - acc: 0.8753 - val_loss: 0.3558 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3492 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3454 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3446 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3393 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3416 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3357 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3407 - f_2: 3.1885e-08 - acc: 0.8753 - val_loss: 0.3338 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3369 - f_2: 3.3325e-08 - acc: 0.8753 - val_loss: 0.3340 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3326 - f_2: 0.1015 - acc: 0.8865 - val_loss: 0.3327 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3359 - f_2: 0.1065 - acc: 0.8866 - val_loss: 0.3300 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.1448 - acc: 0.8891 - val_loss: 0.3293 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.1380 - acc: 0.8883 - val_loss: 0.3254 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.1629 - acc: 0.8905 - val_loss: 0.3262 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3303 - f_2: 0.1339 - acc: 0.8881 - val_loss: 0.3263 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.1728 - acc: 0.8917 - val_loss: 0.3233 - val_f_2: 0.0707 - val_acc: 0.8755\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.1571 - acc: 0.8899 - val_loss: 0.3232 - val_f_2: 0.0230 - val_acc: 0.8696\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.1692 - acc: 0.8891 - val_loss: 0.3256 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.1817 - acc: 0.8908 - val_loss: 0.3228 - val_f_2: 0.0522 - val_acc: 0.8729\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.1806 - acc: 0.8925 - val_loss: 0.3201 - val_f_2: 0.2704 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.2033 - acc: 0.8930 - val_loss: 0.3221 - val_f_2: 0.2638 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3227 - f_2: 0.2070 - acc: 0.8936 - val_loss: 0.3208 - val_f_2: 0.2598 - val_acc: 0.8988\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2120 - acc: 0.8941 - val_loss: 0.3199 - val_f_2: 0.2598 - val_acc: 0.8988\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3229 - f_2: 0.2377 - acc: 0.8953 - val_loss: 0.3211 - val_f_2: 0.2135 - val_acc: 0.8929\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.2352 - acc: 0.8950 - val_loss: 0.3225 - val_f_2: 0.2598 - val_acc: 0.8988\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3225 - f_2: 0.2526 - acc: 0.8975 - val_loss: 0.3184 - val_f_2: 0.2798 - val_acc: 0.8997\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.2361 - acc: 0.8952 - val_loss: 0.3175 - val_f_2: 0.2876 - val_acc: 0.9003\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.2476 - acc: 0.8956 - val_loss: 0.3192 - val_f_2: 0.2577 - val_acc: 0.8985\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.2417 - acc: 0.8967 - val_loss: 0.3177 - val_f_2: 0.2768 - val_acc: 0.9003\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3190 - f_2: 0.2471 - acc: 0.8958 - val_loss: 0.3160 - val_f_2: 0.2970 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3164 - f_2: 0.2458 - acc: 0.8968 - val_loss: 0.3167 - val_f_2: 0.2935 - val_acc: 0.9012\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.2564 - acc: 0.8961 - val_loss: 0.3176 - val_f_2: 0.2768 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.2342 - acc: 0.8933 - val_loss: 0.3176 - val_f_2: 0.2723 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.2595 - acc: 0.8955 - val_loss: 0.3166 - val_f_2: 0.2768 - val_acc: 0.9003\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.2362 - acc: 0.8950 - val_loss: 0.3161 - val_f_2: 0.3002 - val_acc: 0.9018\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.2467 - acc: 0.8957 - val_loss: 0.3146 - val_f_2: 0.3075 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2481 - acc: 0.8944 - val_loss: 0.3222 - val_f_2: 0.2768 - val_acc: 0.9003\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.2485 - acc: 0.8972 - val_loss: 0.3155 - val_f_2: 0.3077 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.2633 - acc: 0.8975 - val_loss: 0.3179 - val_f_2: 0.2897 - val_acc: 0.9006\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2338 - acc: 0.8942 - val_loss: 0.3152 - val_f_2: 0.3084 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.2570 - acc: 0.8962 - val_loss: 0.3144 - val_f_2: 0.3190 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2366 - acc: 0.8944 - val_loss: 0.3141 - val_f_2: 0.3383 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.2703 - acc: 0.8981 - val_loss: 0.3121 - val_f_2: 0.3680 - val_acc: 0.9015\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.2526 - acc: 0.8950 - val_loss: 0.3145 - val_f_2: 0.3520 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.2638 - acc: 0.8975 - val_loss: 0.3158 - val_f_2: 0.3765 - val_acc: 0.8988\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.2375 - acc: 0.8948 - val_loss: 0.3201 - val_f_2: 0.2786 - val_acc: 0.8997\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.2411 - acc: 0.8955 - val_loss: 0.3148 - val_f_2: 0.2801 - val_acc: 0.8997\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.2579 - acc: 0.8958 - val_loss: 0.3242 - val_f_2: 0.2768 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.2703 - acc: 0.8967 - val_loss: 0.3136 - val_f_2: 0.3572 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.2853 - acc: 0.8997 - val_loss: 0.3176 - val_f_2: 0.3046 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.2738 - acc: 0.8986 - val_loss: 0.3154 - val_f_2: 0.3512 - val_acc: 0.9018\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.5994 - f_2: 0.0146 - acc: 0.8565 - val_loss: 0.4156 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3899 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3700 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3635 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3541 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3542 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3437 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3452 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3394 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3433 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3361 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3420 - f_2: 0.0118 - acc: 0.8722 - val_loss: 0.3373 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3387 - f_2: 0.0859 - acc: 0.8817 - val_loss: 0.3344 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3361 - f_2: 0.1248 - acc: 0.8854 - val_loss: 0.3299 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3337 - f_2: 0.1323 - acc: 0.8865 - val_loss: 0.3293 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3323 - f_2: 0.1425 - acc: 0.8868 - val_loss: 0.3269 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3336 - f_2: 0.1545 - acc: 0.8877 - val_loss: 0.3255 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3313 - f_2: 0.1495 - acc: 0.8861 - val_loss: 0.3279 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3309 - f_2: 0.1635 - acc: 0.8879 - val_loss: 0.3237 - val_f_2: 0.0095 - val_acc: 0.8678\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3317 - f_2: 0.1680 - acc: 0.8873 - val_loss: 0.3239 - val_f_2: 0.0095 - val_acc: 0.8678\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.1691 - acc: 0.8884 - val_loss: 0.3226 - val_f_2: 0.1414 - val_acc: 0.8841\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.1620 - acc: 0.8873 - val_loss: 0.3219 - val_f_2: 0.1797 - val_acc: 0.8891\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.2056 - acc: 0.8892 - val_loss: 0.3204 - val_f_2: 0.1889 - val_acc: 0.8903\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.2006 - acc: 0.8907 - val_loss: 0.3200 - val_f_2: 0.2618 - val_acc: 0.8991\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.2339 - acc: 0.8919 - val_loss: 0.3208 - val_f_2: 0.1801 - val_acc: 0.8891\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.2427 - acc: 0.8944 - val_loss: 0.3198 - val_f_2: 0.2583 - val_acc: 0.8985\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2415 - acc: 0.8930 - val_loss: 0.3192 - val_f_2: 0.2371 - val_acc: 0.8962\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.2433 - acc: 0.8925 - val_loss: 0.3197 - val_f_2: 0.2618 - val_acc: 0.8991\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2464 - acc: 0.8948 - val_loss: 0.3179 - val_f_2: 0.3012 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2431 - acc: 0.8945 - val_loss: 0.3174 - val_f_2: 0.2754 - val_acc: 0.8997\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.2604 - acc: 0.8954 - val_loss: 0.3145 - val_f_2: 0.3679 - val_acc: 0.9024\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.2738 - acc: 0.8969 - val_loss: 0.3173 - val_f_2: 0.2863 - val_acc: 0.8997\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.2754 - acc: 0.8970 - val_loss: 0.3172 - val_f_2: 0.3239 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.2597 - acc: 0.8943 - val_loss: 0.3179 - val_f_2: 0.3357 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3193 - f_2: 0.2770 - acc: 0.8962 - val_loss: 0.3150 - val_f_2: 0.3193 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2693 - acc: 0.8956 - val_loss: 0.3154 - val_f_2: 0.3340 - val_acc: 0.9021\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.2599 - acc: 0.8945 - val_loss: 0.3140 - val_f_2: 0.3344 - val_acc: 0.9035\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2822 - acc: 0.8959 - val_loss: 0.3145 - val_f_2: 0.3515 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.2651 - acc: 0.8937 - val_loss: 0.3135 - val_f_2: 0.3057 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.2626 - acc: 0.8952 - val_loss: 0.3133 - val_f_2: 0.3499 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.2796 - acc: 0.8959 - val_loss: 0.3155 - val_f_2: 0.2991 - val_acc: 0.9009\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.2799 - acc: 0.8968 - val_loss: 0.3128 - val_f_2: 0.3141 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.2602 - acc: 0.8938 - val_loss: 0.3122 - val_f_2: 0.3658 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3160 - f_2: 0.2702 - acc: 0.8956 - val_loss: 0.3126 - val_f_2: 0.3402 - val_acc: 0.9012\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2717 - acc: 0.8956 - val_loss: 0.3158 - val_f_2: 0.3047 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.2815 - acc: 0.8950 - val_loss: 0.3132 - val_f_2: 0.3328 - val_acc: 0.9015\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.2764 - acc: 0.8956 - val_loss: 0.3113 - val_f_2: 0.3595 - val_acc: 0.9021\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3142 - f_2: 0.2777 - acc: 0.8953 - val_loss: 0.3125 - val_f_2: 0.2973 - val_acc: 0.9009\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.2679 - acc: 0.8947 - val_loss: 0.3156 - val_f_2: 0.3167 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.2864 - acc: 0.8964 - val_loss: 0.3100 - val_f_2: 0.4045 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.2971 - acc: 0.8966 - val_loss: 0.3125 - val_f_2: 0.2787 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.2895 - acc: 0.8970 - val_loss: 0.3141 - val_f_2: 0.2787 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2872 - acc: 0.8956 - val_loss: 0.3124 - val_f_2: 0.3622 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.2856 - acc: 0.8967 - val_loss: 0.3113 - val_f_2: 0.3674 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.2882 - acc: 0.8958 - val_loss: 0.3099 - val_f_2: 0.3529 - val_acc: 0.9003\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6071 - f_2: 0.0176 - acc: 0.8525 - val_loss: 0.3972 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3984 - f_2: 0.0118 - acc: 0.8691 - val_loss: 0.3507 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3705 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3357 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3597 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3264 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3539 - f_2: 3.0202e-08 - acc: 0.8691 - val_loss: 0.3220 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3497 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3197 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3457 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3185 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3443 - f_2: 0.0466 - acc: 0.8747 - val_loss: 0.3156 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3417 - f_2: 0.1119 - acc: 0.8816 - val_loss: 0.3132 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3396 - f_2: 0.1166 - acc: 0.8823 - val_loss: 0.3124 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3394 - f_2: 0.1409 - acc: 0.8844 - val_loss: 0.3110 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3367 - f_2: 0.1465 - acc: 0.8842 - val_loss: 0.3095 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3372 - f_2: 0.1551 - acc: 0.8840 - val_loss: 0.3095 - val_f_2: 0.0082 - val_acc: 0.8805\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3352 - f_2: 0.1591 - acc: 0.8846 - val_loss: 0.3072 - val_f_2: 0.0066 - val_acc: 0.8802\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3349 - f_2: 0.1754 - acc: 0.8859 - val_loss: 0.3089 - val_f_2: 0.0305 - val_acc: 0.8826\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3359 - f_2: 0.1787 - acc: 0.8860 - val_loss: 0.3071 - val_f_2: 0.1437 - val_acc: 0.8935\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3325 - f_2: 0.1731 - acc: 0.8863 - val_loss: 0.3042 - val_f_2: 0.2628 - val_acc: 0.9071\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3337 - f_2: 0.1869 - acc: 0.8857 - val_loss: 0.3045 - val_f_2: 0.0038 - val_acc: 0.8799\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3327 - f_2: 0.1705 - acc: 0.8838 - val_loss: 0.3036 - val_f_2: 0.1942 - val_acc: 0.8991\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.2202 - acc: 0.8892 - val_loss: 0.3022 - val_f_2: 0.2307 - val_acc: 0.9032\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3302 - f_2: 0.1978 - acc: 0.8874 - val_loss: 0.3047 - val_f_2: 0.2614 - val_acc: 0.9068\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3304 - f_2: 0.1958 - acc: 0.8865 - val_loss: 0.3017 - val_f_2: 0.3147 - val_acc: 0.9094\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3296 - f_2: 0.2446 - acc: 0.8916 - val_loss: 0.3025 - val_f_2: 0.3265 - val_acc: 0.9097\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3299 - f_2: 0.2448 - acc: 0.8900 - val_loss: 0.3008 - val_f_2: 0.2949 - val_acc: 0.9109\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3279 - f_2: 0.2385 - acc: 0.8905 - val_loss: 0.3003 - val_f_2: 0.2927 - val_acc: 0.9106\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3285 - f_2: 0.2385 - acc: 0.8898 - val_loss: 0.3009 - val_f_2: 0.3003 - val_acc: 0.9115\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3285 - f_2: 0.2330 - acc: 0.8892 - val_loss: 0.3010 - val_f_2: 0.2308 - val_acc: 0.9032\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3278 - f_2: 0.2207 - acc: 0.8892 - val_loss: 0.2995 - val_f_2: 0.2986 - val_acc: 0.9112\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.2419 - acc: 0.8907 - val_loss: 0.3012 - val_f_2: 0.2949 - val_acc: 0.9109\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.2512 - acc: 0.8894 - val_loss: 0.3008 - val_f_2: 0.2853 - val_acc: 0.9097\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3274 - f_2: 0.2149 - acc: 0.8880 - val_loss: 0.2988 - val_f_2: 0.2986 - val_acc: 0.9112\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2488 - acc: 0.8897 - val_loss: 0.3000 - val_f_2: 0.3316 - val_acc: 0.9088\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3262 - f_2: 0.2580 - acc: 0.8913 - val_loss: 0.2971 - val_f_2: 0.2986 - val_acc: 0.9112\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.2739 - acc: 0.8911 - val_loss: 0.2981 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.2496 - acc: 0.8893 - val_loss: 0.2973 - val_f_2: 0.3079 - val_acc: 0.9115\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.2639 - acc: 0.8919 - val_loss: 0.2992 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2725 - acc: 0.8920 - val_loss: 0.2965 - val_f_2: 0.3086 - val_acc: 0.9106\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.2792 - acc: 0.8919 - val_loss: 0.2977 - val_f_2: 0.2962 - val_acc: 0.9109\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2708 - acc: 0.8917 - val_loss: 0.2957 - val_f_2: 0.3003 - val_acc: 0.9115\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2689 - acc: 0.8902 - val_loss: 0.2951 - val_f_2: 0.3017 - val_acc: 0.9106\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3217 - f_2: 0.2757 - acc: 0.8919 - val_loss: 0.2963 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.2648 - acc: 0.8902 - val_loss: 0.2956 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2689 - acc: 0.8921 - val_loss: 0.2946 - val_f_2: 0.3979 - val_acc: 0.9083\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.2696 - acc: 0.8911 - val_loss: 0.2949 - val_f_2: 0.3040 - val_acc: 0.9103\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.2706 - acc: 0.8911 - val_loss: 0.2949 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.2615 - acc: 0.8911 - val_loss: 0.2938 - val_f_2: 0.3344 - val_acc: 0.9077\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.2925 - acc: 0.8923 - val_loss: 0.2946 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.2691 - acc: 0.8914 - val_loss: 0.2962 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.2585 - acc: 0.8900 - val_loss: 0.2929 - val_f_2: 0.3648 - val_acc: 0.9100\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2815 - acc: 0.8932 - val_loss: 0.2946 - val_f_2: 0.3617 - val_acc: 0.9086\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4356 - f_2: 0.0128 - acc: 0.8605 - val_loss: 0.3592 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3526 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3352 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3411 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3289 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3333 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3250 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3292 - f_2: 0.0230 - acc: 0.8768 - val_loss: 0.3220 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3257 - f_2: 0.1956 - acc: 0.8939 - val_loss: 0.3236 - val_f_2: 0.3134 - val_acc: 0.9015\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2084 - acc: 0.8953 - val_loss: 0.3179 - val_f_2: 0.3055 - val_acc: 0.9015\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3191 - f_2: 0.2372 - acc: 0.8951 - val_loss: 0.3184 - val_f_2: 0.2912 - val_acc: 0.8997\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.2453 - acc: 0.8975 - val_loss: 0.3170 - val_f_2: 0.3321 - val_acc: 0.9012\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.2672 - acc: 0.8983 - val_loss: 0.3147 - val_f_2: 0.3757 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.2998 - acc: 0.9006 - val_loss: 0.3134 - val_f_2: 0.3549 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.2995 - acc: 0.9015 - val_loss: 0.3150 - val_f_2: 0.3312 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3003 - acc: 0.8998 - val_loss: 0.3117 - val_f_2: 0.3683 - val_acc: 0.8985\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3012 - acc: 0.9010 - val_loss: 0.3115 - val_f_2: 0.3860 - val_acc: 0.9012\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3266 - acc: 0.9023 - val_loss: 0.3123 - val_f_2: 0.3634 - val_acc: 0.8982\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3348 - acc: 0.9026 - val_loss: 0.3123 - val_f_2: 0.4017 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.3475 - acc: 0.9029 - val_loss: 0.3124 - val_f_2: 0.3721 - val_acc: 0.8976\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.3357 - acc: 0.9023 - val_loss: 0.3103 - val_f_2: 0.3947 - val_acc: 0.8988\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2983 - f_2: 0.3757 - acc: 0.9050 - val_loss: 0.3097 - val_f_2: 0.4007 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3596 - acc: 0.9051 - val_loss: 0.3102 - val_f_2: 0.3801 - val_acc: 0.8991\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3671 - acc: 0.9051 - val_loss: 0.3129 - val_f_2: 0.3756 - val_acc: 0.8976\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.3688 - acc: 0.9048 - val_loss: 0.3112 - val_f_2: 0.3925 - val_acc: 0.8988\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2963 - f_2: 0.3704 - acc: 0.9062 - val_loss: 0.3098 - val_f_2: 0.4070 - val_acc: 0.8988\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2951 - f_2: 0.3915 - acc: 0.9072 - val_loss: 0.3108 - val_f_2: 0.3852 - val_acc: 0.8979\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2920 - f_2: 0.3768 - acc: 0.9067 - val_loss: 0.3094 - val_f_2: 0.4025 - val_acc: 0.8991\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2922 - f_2: 0.3864 - acc: 0.9080 - val_loss: 0.3103 - val_f_2: 0.3867 - val_acc: 0.8968\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3781 - acc: 0.9076 - val_loss: 0.3121 - val_f_2: 0.3848 - val_acc: 0.8973\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2936 - f_2: 0.3926 - acc: 0.9061 - val_loss: 0.3104 - val_f_2: 0.3960 - val_acc: 0.8976\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2920 - f_2: 0.3849 - acc: 0.9057 - val_loss: 0.3094 - val_f_2: 0.3735 - val_acc: 0.8982\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.3811 - acc: 0.9068 - val_loss: 0.3096 - val_f_2: 0.3836 - val_acc: 0.8979\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2914 - f_2: 0.4016 - acc: 0.9086 - val_loss: 0.3100 - val_f_2: 0.4137 - val_acc: 0.8973\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3925 - acc: 0.9066 - val_loss: 0.3100 - val_f_2: 0.4152 - val_acc: 0.8953\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2884 - f_2: 0.4088 - acc: 0.9090 - val_loss: 0.3089 - val_f_2: 0.4114 - val_acc: 0.8971\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3843 - acc: 0.9065 - val_loss: 0.3109 - val_f_2: 0.3768 - val_acc: 0.8962\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2887 - f_2: 0.4055 - acc: 0.9078 - val_loss: 0.3122 - val_f_2: 0.3990 - val_acc: 0.8959\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.4032 - acc: 0.9084 - val_loss: 0.3166 - val_f_2: 0.3971 - val_acc: 0.8965\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2880 - f_2: 0.4019 - acc: 0.9076 - val_loss: 0.3097 - val_f_2: 0.3846 - val_acc: 0.8950\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.4091 - acc: 0.9086 - val_loss: 0.3099 - val_f_2: 0.4096 - val_acc: 0.8947\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2857 - f_2: 0.4001 - acc: 0.9093 - val_loss: 0.3093 - val_f_2: 0.4270 - val_acc: 0.8950\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.3998 - acc: 0.9082 - val_loss: 0.3096 - val_f_2: 0.4218 - val_acc: 0.8962\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2880 - f_2: 0.4108 - acc: 0.9078 - val_loss: 0.3100 - val_f_2: 0.4249 - val_acc: 0.8947\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.4103 - acc: 0.9098 - val_loss: 0.3084 - val_f_2: 0.3977 - val_acc: 0.8950\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2852 - f_2: 0.4175 - acc: 0.9099 - val_loss: 0.3089 - val_f_2: 0.3999 - val_acc: 0.8947\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.4143 - acc: 0.9105 - val_loss: 0.3096 - val_f_2: 0.4175 - val_acc: 0.8941\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2846 - f_2: 0.4244 - acc: 0.9119 - val_loss: 0.3115 - val_f_2: 0.4032 - val_acc: 0.8973\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.4165 - acc: 0.9103 - val_loss: 0.3113 - val_f_2: 0.3946 - val_acc: 0.8935\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.4325 - acc: 0.9123 - val_loss: 0.3112 - val_f_2: 0.4222 - val_acc: 0.8944\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4344 - acc: 0.9114 - val_loss: 0.3091 - val_f_2: 0.4060 - val_acc: 0.8950\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.4371 - acc: 0.9127 - val_loss: 0.3128 - val_f_2: 0.4287 - val_acc: 0.8947\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.4280 - acc: 0.9112 - val_loss: 0.3102 - val_f_2: 0.4102 - val_acc: 0.8938\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4310 - f_2: 0.0174 - acc: 0.8576 - val_loss: 0.3545 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3530 - f_2: 0.0583 - acc: 0.8780 - val_loss: 0.3324 - val_f_2: 0.2431 - val_acc: 0.8959\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3352 - f_2: 0.2033 - acc: 0.8911 - val_loss: 0.3185 - val_f_2: 0.2800 - val_acc: 0.9009\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.2231 - acc: 0.8916 - val_loss: 0.3138 - val_f_2: 0.3311 - val_acc: 0.9015\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.2892 - acc: 0.8979 - val_loss: 0.3045 - val_f_2: 0.3768 - val_acc: 0.9018\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3100 - f_2: 0.3366 - acc: 0.9023 - val_loss: 0.3042 - val_f_2: 0.3273 - val_acc: 0.9012\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3075 - f_2: 0.3403 - acc: 0.9025 - val_loss: 0.3002 - val_f_2: 0.3453 - val_acc: 0.9003\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3652 - acc: 0.9031 - val_loss: 0.2992 - val_f_2: 0.3517 - val_acc: 0.8997\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3327 - acc: 0.9021 - val_loss: 0.3005 - val_f_2: 0.3546 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.3416 - acc: 0.9031 - val_loss: 0.3014 - val_f_2: 0.3915 - val_acc: 0.9006\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3770 - acc: 0.9041 - val_loss: 0.2980 - val_f_2: 0.3448 - val_acc: 0.9009\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2944 - f_2: 0.3645 - acc: 0.9041 - val_loss: 0.2991 - val_f_2: 0.3704 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3647 - acc: 0.9040 - val_loss: 0.2984 - val_f_2: 0.3506 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2932 - f_2: 0.3817 - acc: 0.9044 - val_loss: 0.2969 - val_f_2: 0.4129 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3729 - acc: 0.9046 - val_loss: 0.2972 - val_f_2: 0.4010 - val_acc: 0.9006\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2909 - f_2: 0.3831 - acc: 0.9055 - val_loss: 0.2971 - val_f_2: 0.4303 - val_acc: 0.9038\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.3890 - acc: 0.9060 - val_loss: 0.2978 - val_f_2: 0.4359 - val_acc: 0.9027\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2882 - f_2: 0.3997 - acc: 0.9071 - val_loss: 0.2964 - val_f_2: 0.4297 - val_acc: 0.9029\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2881 - f_2: 0.4048 - acc: 0.9076 - val_loss: 0.2973 - val_f_2: 0.4265 - val_acc: 0.9029\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2868 - f_2: 0.4028 - acc: 0.9067 - val_loss: 0.2957 - val_f_2: 0.4278 - val_acc: 0.9024\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2851 - f_2: 0.4094 - acc: 0.9082 - val_loss: 0.2961 - val_f_2: 0.3992 - val_acc: 0.9015\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.4148 - acc: 0.9081 - val_loss: 0.2964 - val_f_2: 0.4132 - val_acc: 0.9024\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4412 - acc: 0.9097 - val_loss: 0.2965 - val_f_2: 0.4261 - val_acc: 0.9018\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.4271 - acc: 0.9083 - val_loss: 0.2979 - val_f_2: 0.4410 - val_acc: 0.9018\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2838 - f_2: 0.4267 - acc: 0.9091 - val_loss: 0.2989 - val_f_2: 0.3706 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.4138 - acc: 0.9091 - val_loss: 0.2962 - val_f_2: 0.4494 - val_acc: 0.9021\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.4471 - acc: 0.9101 - val_loss: 0.2984 - val_f_2: 0.4419 - val_acc: 0.9018\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4338 - acc: 0.9105 - val_loss: 0.2967 - val_f_2: 0.4367 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4524 - acc: 0.9114 - val_loss: 0.2972 - val_f_2: 0.4321 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2810 - f_2: 0.4386 - acc: 0.9109 - val_loss: 0.2974 - val_f_2: 0.4348 - val_acc: 0.9018\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2801 - f_2: 0.4568 - acc: 0.9106 - val_loss: 0.2972 - val_f_2: 0.4289 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2794 - f_2: 0.4521 - acc: 0.9105 - val_loss: 0.2999 - val_f_2: 0.4375 - val_acc: 0.8988\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4593 - acc: 0.9108 - val_loss: 0.2980 - val_f_2: 0.4319 - val_acc: 0.9024\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2793 - f_2: 0.4521 - acc: 0.9110 - val_loss: 0.3006 - val_f_2: 0.4418 - val_acc: 0.8968\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4539 - acc: 0.9120 - val_loss: 0.3034 - val_f_2: 0.4374 - val_acc: 0.8973\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2778 - f_2: 0.4566 - acc: 0.9114 - val_loss: 0.3001 - val_f_2: 0.3968 - val_acc: 0.8994\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2790 - f_2: 0.4636 - acc: 0.9139 - val_loss: 0.3021 - val_f_2: 0.4109 - val_acc: 0.8991\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4668 - acc: 0.9132 - val_loss: 0.2995 - val_f_2: 0.4435 - val_acc: 0.8991\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2773 - f_2: 0.4666 - acc: 0.9118 - val_loss: 0.3005 - val_f_2: 0.4253 - val_acc: 0.8971\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2774 - f_2: 0.4707 - acc: 0.9136 - val_loss: 0.3003 - val_f_2: 0.4275 - val_acc: 0.8997\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2758 - f_2: 0.4669 - acc: 0.9136 - val_loss: 0.3016 - val_f_2: 0.4292 - val_acc: 0.8968\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2762 - f_2: 0.4788 - acc: 0.9153 - val_loss: 0.3006 - val_f_2: 0.4266 - val_acc: 0.8979\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2754 - f_2: 0.4815 - acc: 0.9137 - val_loss: 0.3004 - val_f_2: 0.4068 - val_acc: 0.8994\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4801 - acc: 0.9136 - val_loss: 0.3018 - val_f_2: 0.4197 - val_acc: 0.8985\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2736 - f_2: 0.4776 - acc: 0.9151 - val_loss: 0.3054 - val_f_2: 0.4346 - val_acc: 0.8923\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2746 - f_2: 0.4815 - acc: 0.9141 - val_loss: 0.3022 - val_f_2: 0.4184 - val_acc: 0.8985\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2763 - f_2: 0.4762 - acc: 0.9142 - val_loss: 0.3045 - val_f_2: 0.4526 - val_acc: 0.8914\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2749 - f_2: 0.4912 - acc: 0.9151 - val_loss: 0.3052 - val_f_2: 0.4220 - val_acc: 0.8973\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2739 - f_2: 0.4978 - acc: 0.9152 - val_loss: 0.3048 - val_f_2: 0.4501 - val_acc: 0.8941\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2740 - f_2: 0.4939 - acc: 0.9155 - val_loss: 0.3031 - val_f_2: 0.4294 - val_acc: 0.8968\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4383 - f_2: 0.0179 - acc: 0.8559 - val_loss: 0.3352 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3600 - f_2: 0.0515 - acc: 0.8747 - val_loss: 0.3165 - val_f_2: 0.2333 - val_acc: 0.9038\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3430 - f_2: 0.1791 - acc: 0.8868 - val_loss: 0.3039 - val_f_2: 0.3059 - val_acc: 0.9109\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3295 - f_2: 0.2354 - acc: 0.8905 - val_loss: 0.2950 - val_f_2: 0.3330 - val_acc: 0.9100\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2640 - acc: 0.8929 - val_loss: 0.2903 - val_f_2: 0.3386 - val_acc: 0.9100\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2840 - acc: 0.8947 - val_loss: 0.2869 - val_f_2: 0.3434 - val_acc: 0.9091\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.2974 - acc: 0.8957 - val_loss: 0.2847 - val_f_2: 0.3277 - val_acc: 0.9088\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.3118 - acc: 0.8965 - val_loss: 0.2872 - val_f_2: 0.3780 - val_acc: 0.9100\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.3272 - acc: 0.8974 - val_loss: 0.2869 - val_f_2: 0.3761 - val_acc: 0.9106\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3078 - f_2: 0.3376 - acc: 0.8990 - val_loss: 0.2832 - val_f_2: 0.3779 - val_acc: 0.9112\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3391 - acc: 0.9000 - val_loss: 0.2842 - val_f_2: 0.3866 - val_acc: 0.9100\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3512 - acc: 0.8989 - val_loss: 0.2830 - val_f_2: 0.3651 - val_acc: 0.9094\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3019 - f_2: 0.3567 - acc: 0.9007 - val_loss: 0.2832 - val_f_2: 0.4125 - val_acc: 0.9100\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3653 - acc: 0.9001 - val_loss: 0.2824 - val_f_2: 0.3877 - val_acc: 0.9091\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3732 - acc: 0.9004 - val_loss: 0.2814 - val_f_2: 0.4159 - val_acc: 0.9094\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.3556 - acc: 0.8995 - val_loss: 0.2814 - val_f_2: 0.4041 - val_acc: 0.9106\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3613 - acc: 0.9006 - val_loss: 0.2826 - val_f_2: 0.4137 - val_acc: 0.9094\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.3824 - acc: 0.9014 - val_loss: 0.2791 - val_f_2: 0.4088 - val_acc: 0.9103\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.3945 - acc: 0.9016 - val_loss: 0.2780 - val_f_2: 0.3939 - val_acc: 0.9109\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3945 - acc: 0.9028 - val_loss: 0.2805 - val_f_2: 0.4072 - val_acc: 0.9100\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2932 - f_2: 0.4104 - acc: 0.9035 - val_loss: 0.2805 - val_f_2: 0.4123 - val_acc: 0.9083\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2938 - f_2: 0.3997 - acc: 0.9037 - val_loss: 0.2782 - val_f_2: 0.3977 - val_acc: 0.9086\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2927 - f_2: 0.4158 - acc: 0.9051 - val_loss: 0.2814 - val_f_2: 0.4106 - val_acc: 0.9071\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2906 - f_2: 0.4224 - acc: 0.9054 - val_loss: 0.2787 - val_f_2: 0.4132 - val_acc: 0.9080\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2894 - f_2: 0.4220 - acc: 0.9060 - val_loss: 0.2800 - val_f_2: 0.4202 - val_acc: 0.9065\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2901 - f_2: 0.4202 - acc: 0.9051 - val_loss: 0.2796 - val_f_2: 0.3986 - val_acc: 0.9097\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.4433 - acc: 0.9069 - val_loss: 0.2788 - val_f_2: 0.4035 - val_acc: 0.9062\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2887 - f_2: 0.4392 - acc: 0.9060 - val_loss: 0.2800 - val_f_2: 0.4009 - val_acc: 0.9086\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2879 - f_2: 0.4437 - acc: 0.9080 - val_loss: 0.2816 - val_f_2: 0.4074 - val_acc: 0.9053\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.4433 - acc: 0.9065 - val_loss: 0.2797 - val_f_2: 0.4193 - val_acc: 0.9068\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.4602 - acc: 0.9084 - val_loss: 0.2814 - val_f_2: 0.3994 - val_acc: 0.9112\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.4539 - acc: 0.9080 - val_loss: 0.2806 - val_f_2: 0.4114 - val_acc: 0.9062\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.4557 - acc: 0.9080 - val_loss: 0.2815 - val_f_2: 0.4030 - val_acc: 0.9059\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2865 - f_2: 0.4678 - acc: 0.9090 - val_loss: 0.2820 - val_f_2: 0.4050 - val_acc: 0.9068\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2840 - f_2: 0.4723 - acc: 0.9085 - val_loss: 0.2815 - val_f_2: 0.4116 - val_acc: 0.9050\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2834 - f_2: 0.4670 - acc: 0.9101 - val_loss: 0.2853 - val_f_2: 0.4201 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2858 - f_2: 0.4661 - acc: 0.9091 - val_loss: 0.2833 - val_f_2: 0.4108 - val_acc: 0.9053\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2847 - f_2: 0.4735 - acc: 0.9099 - val_loss: 0.2831 - val_f_2: 0.4160 - val_acc: 0.9053\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2847 - f_2: 0.4649 - acc: 0.9098 - val_loss: 0.2812 - val_f_2: 0.4171 - val_acc: 0.9050\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2822 - f_2: 0.4731 - acc: 0.9096 - val_loss: 0.2881 - val_f_2: 0.4485 - val_acc: 0.9029\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2831 - f_2: 0.4737 - acc: 0.9088 - val_loss: 0.2830 - val_f_2: 0.4173 - val_acc: 0.9041\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2838 - f_2: 0.4859 - acc: 0.9092 - val_loss: 0.2831 - val_f_2: 0.4182 - val_acc: 0.9038\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4862 - acc: 0.9108 - val_loss: 0.2824 - val_f_2: 0.4152 - val_acc: 0.9038\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.4830 - acc: 0.9111 - val_loss: 0.2825 - val_f_2: 0.4053 - val_acc: 0.9053\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.4848 - acc: 0.9111 - val_loss: 0.2850 - val_f_2: 0.4342 - val_acc: 0.9038\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4811 - acc: 0.9097 - val_loss: 0.2831 - val_f_2: 0.4155 - val_acc: 0.9059\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2817 - f_2: 0.4887 - acc: 0.9108 - val_loss: 0.2862 - val_f_2: 0.4153 - val_acc: 0.9035\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2796 - f_2: 0.4953 - acc: 0.9125 - val_loss: 0.2831 - val_f_2: 0.4128 - val_acc: 0.9029\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4757 - acc: 0.9107 - val_loss: 0.2870 - val_f_2: 0.4215 - val_acc: 0.9018\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4874 - acc: 0.9110 - val_loss: 0.2845 - val_f_2: 0.4139 - val_acc: 0.9035\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6168 - f_2: 0.0193 - acc: 0.8637 - val_loss: 0.4045 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3809 - f_2: 3.2533e-08 - acc: 0.8753 - val_loss: 0.3656 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3595 - f_2: 3.0946e-08 - acc: 0.8753 - val_loss: 0.3550 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3507 - f_2: 3.2402e-08 - acc: 0.8753 - val_loss: 0.3458 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3437 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3397 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3408 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3341 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3390 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3321 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3359 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3306 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3340 - f_2: 0.0533 - acc: 0.8807 - val_loss: 0.3253 - val_f_2: 0.2684 - val_acc: 0.8994\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3316 - f_2: 0.1849 - acc: 0.8936 - val_loss: 0.3248 - val_f_2: 0.2618 - val_acc: 0.8991\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.2099 - acc: 0.8975 - val_loss: 0.3251 - val_f_2: 0.2704 - val_acc: 0.8997\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.2194 - acc: 0.8981 - val_loss: 0.3222 - val_f_2: 0.2737 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2406 - acc: 0.8995 - val_loss: 0.3239 - val_f_2: 0.2596 - val_acc: 0.8988\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2362 - acc: 0.8987 - val_loss: 0.3215 - val_f_2: 0.2704 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3257 - f_2: 0.2523 - acc: 0.8997 - val_loss: 0.3201 - val_f_2: 0.2755 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3260 - f_2: 0.2458 - acc: 0.9011 - val_loss: 0.3196 - val_f_2: 0.2704 - val_acc: 0.8997\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2420 - acc: 0.8988 - val_loss: 0.3201 - val_f_2: 0.2755 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.2577 - acc: 0.9008 - val_loss: 0.3178 - val_f_2: 0.2860 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3238 - f_2: 0.2636 - acc: 0.9017 - val_loss: 0.3185 - val_f_2: 0.2863 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3230 - f_2: 0.2433 - acc: 0.8989 - val_loss: 0.3233 - val_f_2: 0.2968 - val_acc: 0.9012\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2657 - acc: 0.9006 - val_loss: 0.3187 - val_f_2: 0.2963 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.2737 - acc: 0.9012 - val_loss: 0.3164 - val_f_2: 0.2860 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3227 - f_2: 0.2900 - acc: 0.9024 - val_loss: 0.3204 - val_f_2: 0.2768 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2829 - acc: 0.9021 - val_loss: 0.3162 - val_f_2: 0.2917 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.2895 - acc: 0.9027 - val_loss: 0.3158 - val_f_2: 0.2816 - val_acc: 0.8997\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.2663 - acc: 0.9015 - val_loss: 0.3141 - val_f_2: 0.2978 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3191 - f_2: 0.2784 - acc: 0.9025 - val_loss: 0.3150 - val_f_2: 0.2978 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.2927 - acc: 0.9017 - val_loss: 0.3177 - val_f_2: 0.2818 - val_acc: 0.9003\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.2808 - acc: 0.9031 - val_loss: 0.3190 - val_f_2: 0.2900 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2858 - acc: 0.9017 - val_loss: 0.3150 - val_f_2: 0.2860 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3064 - acc: 0.9031 - val_loss: 0.3132 - val_f_2: 0.2860 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2860 - acc: 0.9021 - val_loss: 0.3130 - val_f_2: 0.2996 - val_acc: 0.9009\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.2974 - acc: 0.9027 - val_loss: 0.3169 - val_f_2: 0.2860 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3134 - f_2: 0.3072 - acc: 0.9035 - val_loss: 0.3174 - val_f_2: 0.2978 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.3062 - acc: 0.9026 - val_loss: 0.3171 - val_f_2: 0.2996 - val_acc: 0.9009\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2924 - acc: 0.9023 - val_loss: 0.3125 - val_f_2: 0.2993 - val_acc: 0.9003\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.2854 - acc: 0.9026 - val_loss: 0.3125 - val_f_2: 0.2981 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3052 - acc: 0.9036 - val_loss: 0.3155 - val_f_2: 0.3137 - val_acc: 0.8985\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3135 - f_2: 0.3090 - acc: 0.9042 - val_loss: 0.3116 - val_f_2: 0.3087 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3052 - acc: 0.9028 - val_loss: 0.3139 - val_f_2: 0.3103 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2863 - acc: 0.9019 - val_loss: 0.3141 - val_f_2: 0.3240 - val_acc: 0.8988\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.3077 - acc: 0.9028 - val_loss: 0.3119 - val_f_2: 0.2997 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.2912 - acc: 0.9015 - val_loss: 0.3120 - val_f_2: 0.3250 - val_acc: 0.8985\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.3080 - acc: 0.9042 - val_loss: 0.3123 - val_f_2: 0.3111 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.3060 - acc: 0.9018 - val_loss: 0.3117 - val_f_2: 0.3132 - val_acc: 0.8997\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3067 - acc: 0.9030 - val_loss: 0.3144 - val_f_2: 0.2996 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3050 - acc: 0.9024 - val_loss: 0.3201 - val_f_2: 0.2981 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.2946 - acc: 0.9016 - val_loss: 0.3123 - val_f_2: 0.2913 - val_acc: 0.9009\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.2917 - acc: 0.9021 - val_loss: 0.3106 - val_f_2: 0.3014 - val_acc: 0.9012\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.2982 - acc: 0.9037 - val_loss: 0.3117 - val_f_2: 0.2996 - val_acc: 0.9009\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6151 - f_2: 0.0124 - acc: 0.8593 - val_loss: 0.4040 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3832 - f_2: 0.0118 - acc: 0.8722 - val_loss: 0.3644 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3620 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3531 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3489 - f_2: 0.0057 - acc: 0.8722 - val_loss: 0.3425 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3436 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3396 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3421 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3324 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3359 - f_2: 0.1041 - acc: 0.8836 - val_loss: 0.3312 - val_f_2: 0.1996 - val_acc: 0.8909\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3324 - f_2: 0.1944 - acc: 0.8933 - val_loss: 0.3274 - val_f_2: 0.2698 - val_acc: 0.8994\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3303 - f_2: 0.2460 - acc: 0.8964 - val_loss: 0.3348 - val_f_2: 0.2452 - val_acc: 0.8971\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.2392 - acc: 0.8972 - val_loss: 0.3236 - val_f_2: 0.2698 - val_acc: 0.8997\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3294 - f_2: 0.2498 - acc: 0.8981 - val_loss: 0.3229 - val_f_2: 0.2752 - val_acc: 0.8994\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2618 - acc: 0.8987 - val_loss: 0.3212 - val_f_2: 0.2790 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3250 - f_2: 0.2680 - acc: 0.8992 - val_loss: 0.3225 - val_f_2: 0.2788 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.2752 - acc: 0.8988 - val_loss: 0.3196 - val_f_2: 0.2777 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3250 - f_2: 0.2888 - acc: 0.9008 - val_loss: 0.3221 - val_f_2: 0.2737 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2839 - acc: 0.9006 - val_loss: 0.3200 - val_f_2: 0.2754 - val_acc: 0.8997\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.2872 - acc: 0.9008 - val_loss: 0.3181 - val_f_2: 0.2877 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3185 - f_2: 0.2938 - acc: 0.9003 - val_loss: 0.3175 - val_f_2: 0.2818 - val_acc: 0.9003\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.3039 - acc: 0.9028 - val_loss: 0.3164 - val_f_2: 0.2818 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.3014 - acc: 0.9014 - val_loss: 0.3140 - val_f_2: 0.2892 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.2998 - acc: 0.9021 - val_loss: 0.3140 - val_f_2: 0.2840 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.3073 - acc: 0.9017 - val_loss: 0.3133 - val_f_2: 0.3161 - val_acc: 0.9009\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3190 - f_2: 0.2919 - acc: 0.9003 - val_loss: 0.3124 - val_f_2: 0.2898 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3171 - f_2: 0.3267 - acc: 0.9038 - val_loss: 0.3137 - val_f_2: 0.3439 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3164 - f_2: 0.3156 - acc: 0.9027 - val_loss: 0.3202 - val_f_2: 0.2911 - val_acc: 0.9006\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3205 - acc: 0.9040 - val_loss: 0.3115 - val_f_2: 0.3121 - val_acc: 0.8994\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3190 - acc: 0.9018 - val_loss: 0.3113 - val_f_2: 0.2918 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3225 - acc: 0.9037 - val_loss: 0.3115 - val_f_2: 0.3456 - val_acc: 0.9021\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3160 - f_2: 0.3281 - acc: 0.9031 - val_loss: 0.3097 - val_f_2: 0.3093 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3261 - acc: 0.9031 - val_loss: 0.3098 - val_f_2: 0.3194 - val_acc: 0.9018\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3281 - acc: 0.9034 - val_loss: 0.3085 - val_f_2: 0.3356 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.3243 - acc: 0.9025 - val_loss: 0.3123 - val_f_2: 0.3027 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3481 - acc: 0.9056 - val_loss: 0.3088 - val_f_2: 0.3317 - val_acc: 0.9003\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3455 - acc: 0.9046 - val_loss: 0.3102 - val_f_2: 0.3167 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3349 - acc: 0.9038 - val_loss: 0.3083 - val_f_2: 0.3326 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3352 - acc: 0.9049 - val_loss: 0.3110 - val_f_2: 0.3135 - val_acc: 0.9021\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3318 - acc: 0.9053 - val_loss: 0.3109 - val_f_2: 0.3493 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3564 - acc: 0.9042 - val_loss: 0.3077 - val_f_2: 0.3425 - val_acc: 0.9006\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3597 - acc: 0.9053 - val_loss: 0.3086 - val_f_2: 0.3164 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.3420 - acc: 0.9035 - val_loss: 0.3113 - val_f_2: 0.3221 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3103 - f_2: 0.3316 - acc: 0.9033 - val_loss: 0.3129 - val_f_2: 0.3001 - val_acc: 0.9015\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3504 - acc: 0.9038 - val_loss: 0.3060 - val_f_2: 0.3393 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3331 - acc: 0.9027 - val_loss: 0.3085 - val_f_2: 0.3280 - val_acc: 0.9018\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3411 - acc: 0.9052 - val_loss: 0.3076 - val_f_2: 0.4001 - val_acc: 0.9024\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3089 - f_2: 0.3655 - acc: 0.9055 - val_loss: 0.3060 - val_f_2: 0.3428 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3365 - acc: 0.9033 - val_loss: 0.3068 - val_f_2: 0.3359 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3503 - acc: 0.9044 - val_loss: 0.3066 - val_f_2: 0.3355 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3434 - acc: 0.9031 - val_loss: 0.3140 - val_f_2: 0.2979 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3681 - acc: 0.9057 - val_loss: 0.3100 - val_f_2: 0.3384 - val_acc: 0.9012\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3464 - acc: 0.9040 - val_loss: 0.3109 - val_f_2: 0.3569 - val_acc: 0.9015\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6210 - f_2: 0.0192 - acc: 0.8579 - val_loss: 0.3840 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3910 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3477 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3646 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3347 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3585 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3270 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3508 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3232 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3450 - f_2: 2.9801e-08 - acc: 0.8691 - val_loss: 0.3170 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3418 - f_2: 0.0985 - acc: 0.8804 - val_loss: 0.3147 - val_f_2: 0.2668 - val_acc: 0.9077\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3391 - f_2: 0.1720 - acc: 0.8877 - val_loss: 0.3110 - val_f_2: 0.2788 - val_acc: 0.9091\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3385 - f_2: 0.2085 - acc: 0.8922 - val_loss: 0.3090 - val_f_2: 0.2748 - val_acc: 0.9086\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3386 - f_2: 0.2270 - acc: 0.8928 - val_loss: 0.3077 - val_f_2: 0.2970 - val_acc: 0.9109\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3319 - f_2: 0.2383 - acc: 0.8947 - val_loss: 0.3073 - val_f_2: 0.3028 - val_acc: 0.9109\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3355 - f_2: 0.2453 - acc: 0.8945 - val_loss: 0.3067 - val_f_2: 0.2910 - val_acc: 0.9103\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3338 - f_2: 0.2551 - acc: 0.8956 - val_loss: 0.3064 - val_f_2: 0.3111 - val_acc: 0.9097\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.2642 - acc: 0.8957 - val_loss: 0.3044 - val_f_2: 0.2987 - val_acc: 0.9109\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3293 - f_2: 0.2725 - acc: 0.8970 - val_loss: 0.3026 - val_f_2: 0.3011 - val_acc: 0.9106\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3281 - f_2: 0.2756 - acc: 0.8962 - val_loss: 0.3081 - val_f_2: 0.3270 - val_acc: 0.9086\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3299 - f_2: 0.2789 - acc: 0.8967 - val_loss: 0.3001 - val_f_2: 0.3043 - val_acc: 0.9112\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.2756 - acc: 0.8978 - val_loss: 0.2988 - val_f_2: 0.3015 - val_acc: 0.9109\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3285 - f_2: 0.2890 - acc: 0.8971 - val_loss: 0.2993 - val_f_2: 0.3043 - val_acc: 0.9106\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3279 - f_2: 0.2926 - acc: 0.8967 - val_loss: 0.2978 - val_f_2: 0.2999 - val_acc: 0.9106\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2790 - acc: 0.8967 - val_loss: 0.2995 - val_f_2: 0.3283 - val_acc: 0.9106\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.2974 - acc: 0.8982 - val_loss: 0.2971 - val_f_2: 0.3150 - val_acc: 0.9106\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.2930 - acc: 0.8970 - val_loss: 0.2976 - val_f_2: 0.3061 - val_acc: 0.9103\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.3077 - acc: 0.8988 - val_loss: 0.2959 - val_f_2: 0.3064 - val_acc: 0.9106\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.2834 - acc: 0.8974 - val_loss: 0.2968 - val_f_2: 0.3124 - val_acc: 0.9100\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3229 - f_2: 0.3141 - acc: 0.8987 - val_loss: 0.2971 - val_f_2: 0.3358 - val_acc: 0.9056\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.3076 - acc: 0.8983 - val_loss: 0.2982 - val_f_2: 0.3379 - val_acc: 0.9074\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.3208 - acc: 0.8982 - val_loss: 0.2952 - val_f_2: 0.3139 - val_acc: 0.9103\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.2988 - acc: 0.8978 - val_loss: 0.2957 - val_f_2: 0.3222 - val_acc: 0.9106\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.3224 - acc: 0.8992 - val_loss: 0.2930 - val_f_2: 0.3163 - val_acc: 0.9103\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.3099 - acc: 0.8988 - val_loss: 0.2923 - val_f_2: 0.3014 - val_acc: 0.9106\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.3163 - acc: 0.8987 - val_loss: 0.3022 - val_f_2: 0.3130 - val_acc: 0.9112\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.3229 - acc: 0.8982 - val_loss: 0.2975 - val_f_2: 0.2997 - val_acc: 0.9103\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.3434 - acc: 0.9004 - val_loss: 0.2944 - val_f_2: 0.3347 - val_acc: 0.9086\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.3206 - acc: 0.8989 - val_loss: 0.2937 - val_f_2: 0.3237 - val_acc: 0.9103\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.3194 - acc: 0.8988 - val_loss: 0.2920 - val_f_2: 0.2994 - val_acc: 0.9094\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.3236 - acc: 0.8997 - val_loss: 0.2929 - val_f_2: 0.3215 - val_acc: 0.9103\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.3222 - acc: 0.8987 - val_loss: 0.2920 - val_f_2: 0.3215 - val_acc: 0.9106\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.3232 - acc: 0.8972 - val_loss: 0.2918 - val_f_2: 0.2997 - val_acc: 0.9097\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.3271 - acc: 0.8988 - val_loss: 0.2942 - val_f_2: 0.3214 - val_acc: 0.9103\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.3306 - acc: 0.9012 - val_loss: 0.2913 - val_f_2: 0.3195 - val_acc: 0.9103\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3193 - f_2: 0.3230 - acc: 0.8995 - val_loss: 0.2915 - val_f_2: 0.3213 - val_acc: 0.9100\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.3317 - acc: 0.9001 - val_loss: 0.2920 - val_f_2: 0.3214 - val_acc: 0.9103\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.3304 - acc: 0.9000 - val_loss: 0.2913 - val_f_2: 0.3278 - val_acc: 0.9103\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.3198 - acc: 0.8988 - val_loss: 0.2912 - val_f_2: 0.3171 - val_acc: 0.9100\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.3395 - acc: 0.9015 - val_loss: 0.2910 - val_f_2: 0.3108 - val_acc: 0.9109\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.3227 - acc: 0.8992 - val_loss: 0.2918 - val_f_2: 0.3215 - val_acc: 0.9100\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.3332 - acc: 0.8981 - val_loss: 0.2930 - val_f_2: 0.2975 - val_acc: 0.9103\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.3365 - acc: 0.9002 - val_loss: 0.2926 - val_f_2: 0.3873 - val_acc: 0.9083\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3395 - acc: 0.8989 - val_loss: 0.2909 - val_f_2: 0.3357 - val_acc: 0.9088\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4345 - f_2: 0.0148 - acc: 0.8635 - val_loss: 0.3637 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3507 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3386 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3366 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3290 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.0418 - acc: 0.8800 - val_loss: 0.3244 - val_f_2: 0.2609 - val_acc: 0.8982\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3230 - f_2: 0.2155 - acc: 0.8979 - val_loss: 0.3218 - val_f_2: 0.2734 - val_acc: 0.8997\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2499 - acc: 0.8989 - val_loss: 0.3206 - val_f_2: 0.3148 - val_acc: 0.9003\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.2815 - acc: 0.8996 - val_loss: 0.3175 - val_f_2: 0.3245 - val_acc: 0.8988\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3126 - f_2: 0.2960 - acc: 0.9003 - val_loss: 0.3142 - val_f_2: 0.3415 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.2975 - acc: 0.9006 - val_loss: 0.3129 - val_f_2: 0.3600 - val_acc: 0.9015\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3000 - acc: 0.9028 - val_loss: 0.3120 - val_f_2: 0.3726 - val_acc: 0.9003\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3302 - acc: 0.9035 - val_loss: 0.3110 - val_f_2: 0.4103 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3253 - acc: 0.9035 - val_loss: 0.3099 - val_f_2: 0.3687 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3294 - acc: 0.9027 - val_loss: 0.3106 - val_f_2: 0.3559 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3007 - f_2: 0.3239 - acc: 0.9032 - val_loss: 0.3092 - val_f_2: 0.3750 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.3510 - acc: 0.9043 - val_loss: 0.3099 - val_f_2: 0.3785 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2968 - f_2: 0.3330 - acc: 0.9037 - val_loss: 0.3105 - val_f_2: 0.3762 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3435 - acc: 0.9037 - val_loss: 0.3085 - val_f_2: 0.4184 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2954 - f_2: 0.3457 - acc: 0.9039 - val_loss: 0.3089 - val_f_2: 0.3810 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2943 - f_2: 0.3494 - acc: 0.9051 - val_loss: 0.3123 - val_f_2: 0.4009 - val_acc: 0.8988\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2940 - f_2: 0.3439 - acc: 0.9044 - val_loss: 0.3070 - val_f_2: 0.3780 - val_acc: 0.8985\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2927 - f_2: 0.3428 - acc: 0.9044 - val_loss: 0.3080 - val_f_2: 0.3776 - val_acc: 0.8994\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.3524 - acc: 0.9045 - val_loss: 0.3099 - val_f_2: 0.3817 - val_acc: 0.8991\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2905 - f_2: 0.3673 - acc: 0.9065 - val_loss: 0.3100 - val_f_2: 0.3710 - val_acc: 0.8976\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2885 - f_2: 0.3533 - acc: 0.9063 - val_loss: 0.3099 - val_f_2: 0.4071 - val_acc: 0.8971\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.2896 - f_2: 0.3632 - acc: 0.9059 - val_loss: 0.3091 - val_f_2: 0.3925 - val_acc: 0.8979\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2905 - f_2: 0.3683 - acc: 0.9051 - val_loss: 0.3106 - val_f_2: 0.3756 - val_acc: 0.8988\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.3761 - acc: 0.9071 - val_loss: 0.3063 - val_f_2: 0.3832 - val_acc: 0.8982\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2847 - f_2: 0.3653 - acc: 0.9071 - val_loss: 0.3077 - val_f_2: 0.3757 - val_acc: 0.8979\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2867 - f_2: 0.3633 - acc: 0.9067 - val_loss: 0.3069 - val_f_2: 0.3869 - val_acc: 0.8988\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2855 - f_2: 0.3703 - acc: 0.9055 - val_loss: 0.3082 - val_f_2: 0.4201 - val_acc: 0.8991\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.3879 - acc: 0.9086 - val_loss: 0.3095 - val_f_2: 0.3873 - val_acc: 0.8973\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2857 - f_2: 0.3805 - acc: 0.9074 - val_loss: 0.3092 - val_f_2: 0.3646 - val_acc: 0.8994\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2821 - f_2: 0.3754 - acc: 0.9071 - val_loss: 0.3091 - val_f_2: 0.4212 - val_acc: 0.8965\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2829 - f_2: 0.3778 - acc: 0.9080 - val_loss: 0.3074 - val_f_2: 0.3848 - val_acc: 0.8976\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.3878 - acc: 0.9091 - val_loss: 0.3107 - val_f_2: 0.4304 - val_acc: 0.8968\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2834 - f_2: 0.3822 - acc: 0.9076 - val_loss: 0.3081 - val_f_2: 0.3890 - val_acc: 0.8971\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.3603 - acc: 0.9060 - val_loss: 0.3097 - val_f_2: 0.3853 - val_acc: 0.8973\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.3957 - acc: 0.9115 - val_loss: 0.3073 - val_f_2: 0.4061 - val_acc: 0.8997\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4103 - acc: 0.9108 - val_loss: 0.3106 - val_f_2: 0.4246 - val_acc: 0.8965\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4084 - acc: 0.9105 - val_loss: 0.3181 - val_f_2: 0.3861 - val_acc: 0.8991\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2799 - f_2: 0.4007 - acc: 0.9118 - val_loss: 0.3108 - val_f_2: 0.4065 - val_acc: 0.8962\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2804 - f_2: 0.4076 - acc: 0.9107 - val_loss: 0.3123 - val_f_2: 0.3839 - val_acc: 0.8985\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2804 - f_2: 0.4105 - acc: 0.9104 - val_loss: 0.3081 - val_f_2: 0.3830 - val_acc: 0.8988\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2790 - f_2: 0.4027 - acc: 0.9115 - val_loss: 0.3102 - val_f_2: 0.4244 - val_acc: 0.8968\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2793 - f_2: 0.4095 - acc: 0.9119 - val_loss: 0.3101 - val_f_2: 0.4171 - val_acc: 0.8968\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4106 - acc: 0.9110 - val_loss: 0.3083 - val_f_2: 0.4255 - val_acc: 0.8988\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4211 - acc: 0.9111 - val_loss: 0.3074 - val_f_2: 0.4180 - val_acc: 0.8991\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2780 - f_2: 0.4040 - acc: 0.9106 - val_loss: 0.3067 - val_f_2: 0.4092 - val_acc: 0.8973\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2779 - f_2: 0.4085 - acc: 0.9103 - val_loss: 0.3097 - val_f_2: 0.4321 - val_acc: 0.8973\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2759 - f_2: 0.4152 - acc: 0.9114 - val_loss: 0.3130 - val_f_2: 0.3932 - val_acc: 0.8976\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4343 - f_2: 0.0068 - acc: 0.8620 - val_loss: 0.3645 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3497 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3395 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3389 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3315 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3296 - f_2: 0.0995 - acc: 0.8820 - val_loss: 0.3261 - val_f_2: 0.2538 - val_acc: 0.8973\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2071 - acc: 0.8906 - val_loss: 0.3226 - val_f_2: 0.2918 - val_acc: 0.9009\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3217 - f_2: 0.2147 - acc: 0.8932 - val_loss: 0.3233 - val_f_2: 0.3157 - val_acc: 0.8997\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.2253 - acc: 0.8937 - val_loss: 0.3231 - val_f_2: 0.3396 - val_acc: 0.9015\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.2801 - acc: 0.8985 - val_loss: 0.3148 - val_f_2: 0.3289 - val_acc: 0.9009\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3279 - acc: 0.9020 - val_loss: 0.3136 - val_f_2: 0.3586 - val_acc: 0.8991\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.3388 - acc: 0.9043 - val_loss: 0.3166 - val_f_2: 0.3528 - val_acc: 0.8997\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3089 - f_2: 0.3417 - acc: 0.9034 - val_loss: 0.3124 - val_f_2: 0.3873 - val_acc: 0.9015\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.3524 - acc: 0.9044 - val_loss: 0.3099 - val_f_2: 0.3298 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3540 - acc: 0.9046 - val_loss: 0.3083 - val_f_2: 0.3981 - val_acc: 0.9012\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3598 - acc: 0.9046 - val_loss: 0.3079 - val_f_2: 0.4196 - val_acc: 0.9021\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3723 - acc: 0.9046 - val_loss: 0.3088 - val_f_2: 0.3950 - val_acc: 0.9024\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3770 - acc: 0.9054 - val_loss: 0.3088 - val_f_2: 0.3749 - val_acc: 0.9009\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.3614 - acc: 0.9055 - val_loss: 0.3047 - val_f_2: 0.3801 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3592 - acc: 0.9058 - val_loss: 0.3043 - val_f_2: 0.3987 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2983 - f_2: 0.3741 - acc: 0.9047 - val_loss: 0.3044 - val_f_2: 0.4147 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.3947 - acc: 0.9060 - val_loss: 0.3223 - val_f_2: 0.4182 - val_acc: 0.9012\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2969 - f_2: 0.3814 - acc: 0.9055 - val_loss: 0.3040 - val_f_2: 0.4066 - val_acc: 0.9024\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3914 - acc: 0.9062 - val_loss: 0.3126 - val_f_2: 0.4372 - val_acc: 0.9024\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2962 - f_2: 0.3958 - acc: 0.9058 - val_loss: 0.3027 - val_f_2: 0.4209 - val_acc: 0.9027\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.3829 - acc: 0.9060 - val_loss: 0.3039 - val_f_2: 0.4239 - val_acc: 0.9029\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2920 - f_2: 0.3867 - acc: 0.9069 - val_loss: 0.3100 - val_f_2: 0.4450 - val_acc: 0.9041\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2900 - f_2: 0.3981 - acc: 0.9079 - val_loss: 0.3019 - val_f_2: 0.4024 - val_acc: 0.9018\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3926 - acc: 0.9062 - val_loss: 0.3030 - val_f_2: 0.4171 - val_acc: 0.9018\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2920 - f_2: 0.4031 - acc: 0.9068 - val_loss: 0.3089 - val_f_2: 0.4217 - val_acc: 0.9041\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2891 - f_2: 0.3931 - acc: 0.9071 - val_loss: 0.3056 - val_f_2: 0.4477 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.3939 - acc: 0.9059 - val_loss: 0.3033 - val_f_2: 0.4324 - val_acc: 0.9035\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.3943 - acc: 0.9060 - val_loss: 0.3062 - val_f_2: 0.4436 - val_acc: 0.9009\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.4076 - acc: 0.9060 - val_loss: 0.3028 - val_f_2: 0.4156 - val_acc: 0.9021\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2865 - f_2: 0.4249 - acc: 0.9086 - val_loss: 0.3023 - val_f_2: 0.4379 - val_acc: 0.9041\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2857 - f_2: 0.4098 - acc: 0.9087 - val_loss: 0.3068 - val_f_2: 0.4461 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.4087 - acc: 0.9093 - val_loss: 0.3045 - val_f_2: 0.4405 - val_acc: 0.9032\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.4302 - acc: 0.9081 - val_loss: 0.3112 - val_f_2: 0.4554 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2858 - f_2: 0.4080 - acc: 0.9085 - val_loss: 0.3151 - val_f_2: 0.4534 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.4220 - acc: 0.9088 - val_loss: 0.3083 - val_f_2: 0.4472 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.4207 - acc: 0.9099 - val_loss: 0.3020 - val_f_2: 0.4274 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.4317 - acc: 0.9091 - val_loss: 0.3013 - val_f_2: 0.4284 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2845 - f_2: 0.4218 - acc: 0.9085 - val_loss: 0.3083 - val_f_2: 0.4434 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.4297 - acc: 0.9086 - val_loss: 0.3023 - val_f_2: 0.4302 - val_acc: 0.9021\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2828 - f_2: 0.4153 - acc: 0.9084 - val_loss: 0.3037 - val_f_2: 0.4591 - val_acc: 0.9003\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4346 - acc: 0.9090 - val_loss: 0.3040 - val_f_2: 0.4456 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.4273 - acc: 0.9091 - val_loss: 0.3015 - val_f_2: 0.4264 - val_acc: 0.9024\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4218 - acc: 0.9083 - val_loss: 0.3050 - val_f_2: 0.4483 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2827 - f_2: 0.4460 - acc: 0.9098 - val_loss: 0.3188 - val_f_2: 0.4559 - val_acc: 0.8985\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.4428 - acc: 0.9100 - val_loss: 0.3129 - val_f_2: 0.4541 - val_acc: 0.8982\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2817 - f_2: 0.4478 - acc: 0.9107 - val_loss: 0.3031 - val_f_2: 0.4303 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2834 - f_2: 0.4311 - acc: 0.9099 - val_loss: 0.3052 - val_f_2: 0.4475 - val_acc: 0.9015\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4402 - f_2: 0.0105 - acc: 0.8579 - val_loss: 0.3419 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3596 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3206 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3409 - f_2: 0.0089 - acc: 0.8691 - val_loss: 0.3131 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3345 - f_2: 0.0799 - acc: 0.8766 - val_loss: 0.3074 - val_f_2: 0.2194 - val_acc: 0.9018\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3325 - f_2: 0.1891 - acc: 0.8877 - val_loss: 0.3067 - val_f_2: 0.3108 - val_acc: 0.9097\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3281 - f_2: 0.2014 - acc: 0.8885 - val_loss: 0.3120 - val_f_2: 0.3165 - val_acc: 0.9100\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.2543 - acc: 0.8936 - val_loss: 0.3003 - val_f_2: 0.3188 - val_acc: 0.9106\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.3104 - acc: 0.8989 - val_loss: 0.3052 - val_f_2: 0.3387 - val_acc: 0.9083\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.3292 - acc: 0.8995 - val_loss: 0.3012 - val_f_2: 0.3356 - val_acc: 0.9080\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.3256 - acc: 0.8993 - val_loss: 0.2966 - val_f_2: 0.3399 - val_acc: 0.9068\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3142 - f_2: 0.3226 - acc: 0.8995 - val_loss: 0.3056 - val_f_2: 0.3567 - val_acc: 0.9086\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3381 - acc: 0.8996 - val_loss: 0.2983 - val_f_2: 0.3758 - val_acc: 0.9080\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.3517 - acc: 0.9012 - val_loss: 0.2971 - val_f_2: 0.3389 - val_acc: 0.9083\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3528 - acc: 0.9004 - val_loss: 0.3036 - val_f_2: 0.4057 - val_acc: 0.9080\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3607 - acc: 0.9013 - val_loss: 0.2926 - val_f_2: 0.3687 - val_acc: 0.9083\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.3669 - acc: 0.9011 - val_loss: 0.3023 - val_f_2: 0.3878 - val_acc: 0.9074\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3037 - f_2: 0.3679 - acc: 0.9016 - val_loss: 0.2965 - val_f_2: 0.4274 - val_acc: 0.9097\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3589 - acc: 0.8996 - val_loss: 0.2893 - val_f_2: 0.3622 - val_acc: 0.9091\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3656 - acc: 0.9018 - val_loss: 0.3034 - val_f_2: 0.4343 - val_acc: 0.9091\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2992 - f_2: 0.3856 - acc: 0.9011 - val_loss: 0.2907 - val_f_2: 0.3754 - val_acc: 0.9086\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.3762 - acc: 0.9022 - val_loss: 0.3012 - val_f_2: 0.4130 - val_acc: 0.9074\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3788 - acc: 0.9031 - val_loss: 0.2890 - val_f_2: 0.3748 - val_acc: 0.9088\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3681 - acc: 0.9018 - val_loss: 0.2918 - val_f_2: 0.4070 - val_acc: 0.9065\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.3888 - acc: 0.9015 - val_loss: 0.3070 - val_f_2: 0.4332 - val_acc: 0.9047\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3726 - acc: 0.9010 - val_loss: 0.2895 - val_f_2: 0.4063 - val_acc: 0.9062\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.3795 - acc: 0.9015 - val_loss: 0.2893 - val_f_2: 0.4017 - val_acc: 0.9074\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2935 - f_2: 0.3963 - acc: 0.9027 - val_loss: 0.3066 - val_f_2: 0.4405 - val_acc: 0.9044\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.3977 - acc: 0.9024 - val_loss: 0.2899 - val_f_2: 0.3773 - val_acc: 0.9088\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.4003 - acc: 0.9030 - val_loss: 0.2903 - val_f_2: 0.3911 - val_acc: 0.9086\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2909 - f_2: 0.3997 - acc: 0.9027 - val_loss: 0.2923 - val_f_2: 0.4526 - val_acc: 0.9059\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2922 - f_2: 0.4023 - acc: 0.9034 - val_loss: 0.3132 - val_f_2: 0.4571 - val_acc: 0.9038\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2906 - f_2: 0.4037 - acc: 0.9044 - val_loss: 0.2983 - val_f_2: 0.4499 - val_acc: 0.9041\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.4014 - acc: 0.9041 - val_loss: 0.2877 - val_f_2: 0.3892 - val_acc: 0.9059\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.3958 - acc: 0.9018 - val_loss: 0.2870 - val_f_2: 0.3918 - val_acc: 0.9077\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2891 - f_2: 0.4059 - acc: 0.9035 - val_loss: 0.2948 - val_f_2: 0.4421 - val_acc: 0.9056\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.4111 - acc: 0.9027 - val_loss: 0.3062 - val_f_2: 0.4565 - val_acc: 0.9041\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2881 - f_2: 0.4121 - acc: 0.9033 - val_loss: 0.3019 - val_f_2: 0.4502 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2883 - f_2: 0.4041 - acc: 0.9036 - val_loss: 0.3169 - val_f_2: 0.4324 - val_acc: 0.9032\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2867 - f_2: 0.4116 - acc: 0.9043 - val_loss: 0.2911 - val_f_2: 0.4200 - val_acc: 0.9053\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2880 - f_2: 0.3998 - acc: 0.9038 - val_loss: 0.3116 - val_f_2: 0.4611 - val_acc: 0.9032\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2880 - f_2: 0.4166 - acc: 0.9040 - val_loss: 0.2929 - val_f_2: 0.4481 - val_acc: 0.9047\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.4325 - acc: 0.9057 - val_loss: 0.2892 - val_f_2: 0.4229 - val_acc: 0.9059\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2882 - f_2: 0.4181 - acc: 0.9043 - val_loss: 0.2972 - val_f_2: 0.4433 - val_acc: 0.9050\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2874 - f_2: 0.4201 - acc: 0.9044 - val_loss: 0.2904 - val_f_2: 0.4079 - val_acc: 0.9044\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.3979 - acc: 0.9027 - val_loss: 0.2894 - val_f_2: 0.4036 - val_acc: 0.9038\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2862 - f_2: 0.3988 - acc: 0.9020 - val_loss: 0.2875 - val_f_2: 0.3966 - val_acc: 0.9038\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2855 - f_2: 0.4144 - acc: 0.9040 - val_loss: 0.2914 - val_f_2: 0.4494 - val_acc: 0.9041\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.4204 - acc: 0.9047 - val_loss: 0.2878 - val_f_2: 0.3986 - val_acc: 0.9021\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.4314 - acc: 0.9044 - val_loss: 0.2866 - val_f_2: 0.3937 - val_acc: 0.9062\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.4284 - acc: 0.9056 - val_loss: 0.2915 - val_f_2: 0.4576 - val_acc: 0.9035\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6282 - f_2: 0.0172 - acc: 0.8650 - val_loss: 0.4026 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3768 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3615 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3528 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3452 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3408 - f_2: 3.2196e-08 - acc: 0.8753 - val_loss: 0.3377 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3355 - f_2: 0.0517 - acc: 0.8809 - val_loss: 0.3335 - val_f_2: 0.2463 - val_acc: 0.8971\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3338 - f_2: 0.1447 - acc: 0.8902 - val_loss: 0.3301 - val_f_2: 0.2718 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3296 - f_2: 0.1610 - acc: 0.8924 - val_loss: 0.3276 - val_f_2: 0.2718 - val_acc: 0.8997\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.1938 - acc: 0.8948 - val_loss: 0.3251 - val_f_2: 0.2752 - val_acc: 0.8991\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3252 - f_2: 0.2128 - acc: 0.8966 - val_loss: 0.3263 - val_f_2: 0.2786 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2343 - acc: 0.8972 - val_loss: 0.3244 - val_f_2: 0.2768 - val_acc: 0.8994\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2458 - acc: 0.8979 - val_loss: 0.3212 - val_f_2: 0.3289 - val_acc: 0.9018\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.2582 - acc: 0.8998 - val_loss: 0.3211 - val_f_2: 0.3386 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.2597 - acc: 0.8991 - val_loss: 0.3192 - val_f_2: 0.2978 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2517 - acc: 0.8989 - val_loss: 0.3173 - val_f_2: 0.3059 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.2598 - acc: 0.8995 - val_loss: 0.3189 - val_f_2: 0.3027 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.2827 - acc: 0.9002 - val_loss: 0.3153 - val_f_2: 0.3097 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.2815 - acc: 0.9003 - val_loss: 0.3217 - val_f_2: 0.2961 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.2728 - acc: 0.9008 - val_loss: 0.3161 - val_f_2: 0.3048 - val_acc: 0.9006\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3136 - f_2: 0.2918 - acc: 0.9025 - val_loss: 0.3141 - val_f_2: 0.3143 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.2788 - acc: 0.9009 - val_loss: 0.3134 - val_f_2: 0.3124 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3041 - acc: 0.9023 - val_loss: 0.3151 - val_f_2: 0.3032 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.2758 - acc: 0.9009 - val_loss: 0.3135 - val_f_2: 0.3550 - val_acc: 0.9012\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.3037 - acc: 0.9034 - val_loss: 0.3119 - val_f_2: 0.3631 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.2917 - acc: 0.9022 - val_loss: 0.3112 - val_f_2: 0.3258 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.2968 - acc: 0.9015 - val_loss: 0.3098 - val_f_2: 0.3230 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3048 - acc: 0.9023 - val_loss: 0.3106 - val_f_2: 0.3587 - val_acc: 0.9018\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3003 - acc: 0.9032 - val_loss: 0.3108 - val_f_2: 0.3606 - val_acc: 0.9009\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.2971 - acc: 0.9022 - val_loss: 0.3109 - val_f_2: 0.3804 - val_acc: 0.9027\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3019 - acc: 0.9011 - val_loss: 0.3094 - val_f_2: 0.3583 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.2946 - acc: 0.9026 - val_loss: 0.3130 - val_f_2: 0.3980 - val_acc: 0.9021\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3229 - acc: 0.9020 - val_loss: 0.3080 - val_f_2: 0.3261 - val_acc: 0.8994\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.3211 - acc: 0.9036 - val_loss: 0.3095 - val_f_2: 0.3783 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3168 - acc: 0.9023 - val_loss: 0.3280 - val_f_2: 0.3108 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3032 - f_2: 0.3044 - acc: 0.9031 - val_loss: 0.3165 - val_f_2: 0.3100 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3266 - acc: 0.9049 - val_loss: 0.3077 - val_f_2: 0.3443 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3081 - acc: 0.9021 - val_loss: 0.3081 - val_f_2: 0.3806 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3106 - acc: 0.9027 - val_loss: 0.3102 - val_f_2: 0.3701 - val_acc: 0.9021\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3183 - acc: 0.9030 - val_loss: 0.3175 - val_f_2: 0.2973 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.3106 - acc: 0.9026 - val_loss: 0.3075 - val_f_2: 0.3757 - val_acc: 0.9029\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.3056 - acc: 0.9030 - val_loss: 0.3090 - val_f_2: 0.3810 - val_acc: 0.9029\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.3112 - acc: 0.9017 - val_loss: 0.3123 - val_f_2: 0.3151 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2991 - f_2: 0.3323 - acc: 0.9044 - val_loss: 0.3086 - val_f_2: 0.3105 - val_acc: 0.8988\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3043 - acc: 0.9026 - val_loss: 0.3089 - val_f_2: 0.3206 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3276 - acc: 0.9025 - val_loss: 0.3081 - val_f_2: 0.3659 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2996 - f_2: 0.3313 - acc: 0.9040 - val_loss: 0.3076 - val_f_2: 0.3633 - val_acc: 0.9024\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3309 - acc: 0.9031 - val_loss: 0.3081 - val_f_2: 0.3280 - val_acc: 0.9003\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3306 - acc: 0.9031 - val_loss: 0.3074 - val_f_2: 0.3750 - val_acc: 0.9018\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.3284 - acc: 0.9035 - val_loss: 0.3062 - val_f_2: 0.3782 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.3135 - acc: 0.9019 - val_loss: 0.3073 - val_f_2: 0.3237 - val_acc: 0.8994\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.3315 - acc: 0.9037 - val_loss: 0.3087 - val_f_2: 0.3676 - val_acc: 0.9018\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6277 - f_2: 0.0053 - acc: 0.8624 - val_loss: 0.3993 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3769 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3600 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3525 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3447 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3438 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3385 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3367 - f_2: 0.1271 - acc: 0.8849 - val_loss: 0.3369 - val_f_2: 0.2442 - val_acc: 0.8968\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3328 - f_2: 0.1831 - acc: 0.8916 - val_loss: 0.3335 - val_f_2: 0.2684 - val_acc: 0.8994\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.2342 - acc: 0.8958 - val_loss: 0.3268 - val_f_2: 0.2698 - val_acc: 0.8997\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3264 - f_2: 0.2322 - acc: 0.8960 - val_loss: 0.3297 - val_f_2: 0.2718 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.2492 - acc: 0.8974 - val_loss: 0.3240 - val_f_2: 0.2737 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.2810 - acc: 0.8995 - val_loss: 0.3327 - val_f_2: 0.2737 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.2655 - acc: 0.8986 - val_loss: 0.3214 - val_f_2: 0.2737 - val_acc: 0.8997\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.2649 - acc: 0.8994 - val_loss: 0.3176 - val_f_2: 0.2903 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.2827 - acc: 0.8997 - val_loss: 0.3181 - val_f_2: 0.2736 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2590 - acc: 0.8967 - val_loss: 0.3178 - val_f_2: 0.3423 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.2953 - acc: 0.8994 - val_loss: 0.3164 - val_f_2: 0.3152 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.2987 - acc: 0.9006 - val_loss: 0.3142 - val_f_2: 0.3168 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.2876 - acc: 0.9002 - val_loss: 0.3138 - val_f_2: 0.3495 - val_acc: 0.9015\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.3069 - acc: 0.9017 - val_loss: 0.3133 - val_f_2: 0.3540 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3039 - acc: 0.9001 - val_loss: 0.3116 - val_f_2: 0.3452 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3122 - f_2: 0.3157 - acc: 0.9003 - val_loss: 0.3111 - val_f_2: 0.2995 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.3052 - acc: 0.9006 - val_loss: 0.3096 - val_f_2: 0.3383 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3009 - acc: 0.9000 - val_loss: 0.3099 - val_f_2: 0.3411 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.2973 - acc: 0.9000 - val_loss: 0.3090 - val_f_2: 0.3298 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3292 - acc: 0.9000 - val_loss: 0.3082 - val_f_2: 0.3225 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3006 - acc: 0.9002 - val_loss: 0.3089 - val_f_2: 0.3160 - val_acc: 0.9015\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3192 - acc: 0.9020 - val_loss: 0.3116 - val_f_2: 0.4149 - val_acc: 0.9032\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.3200 - acc: 0.9010 - val_loss: 0.3065 - val_f_2: 0.2904 - val_acc: 0.9009\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3122 - acc: 0.9018 - val_loss: 0.3062 - val_f_2: 0.3132 - val_acc: 0.9018\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3050 - f_2: 0.3070 - acc: 0.9010 - val_loss: 0.3081 - val_f_2: 0.4073 - val_acc: 0.9027\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3173 - acc: 0.9025 - val_loss: 0.3057 - val_f_2: 0.3363 - val_acc: 0.9003\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3098 - acc: 0.9010 - val_loss: 0.3116 - val_f_2: 0.3166 - val_acc: 0.9027\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3243 - acc: 0.9009 - val_loss: 0.3050 - val_f_2: 0.2943 - val_acc: 0.9012\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3157 - acc: 0.9013 - val_loss: 0.3090 - val_f_2: 0.3243 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3032 - f_2: 0.3256 - acc: 0.9011 - val_loss: 0.3087 - val_f_2: 0.3105 - val_acc: 0.9021\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3231 - acc: 0.9015 - val_loss: 0.3083 - val_f_2: 0.4279 - val_acc: 0.9027\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.3382 - acc: 0.9015 - val_loss: 0.3061 - val_f_2: 0.3349 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3285 - acc: 0.9029 - val_loss: 0.3044 - val_f_2: 0.3881 - val_acc: 0.9024\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3119 - acc: 0.9013 - val_loss: 0.3056 - val_f_2: 0.3198 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3295 - acc: 0.9034 - val_loss: 0.3044 - val_f_2: 0.3255 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3340 - acc: 0.9023 - val_loss: 0.3040 - val_f_2: 0.3195 - val_acc: 0.8997\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.3319 - acc: 0.9020 - val_loss: 0.3037 - val_f_2: 0.3838 - val_acc: 0.9021\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3005 - f_2: 0.3307 - acc: 0.9012 - val_loss: 0.3061 - val_f_2: 0.3528 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3399 - acc: 0.9042 - val_loss: 0.3031 - val_f_2: 0.3504 - val_acc: 0.9009\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3211 - acc: 0.9001 - val_loss: 0.3020 - val_f_2: 0.3154 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.3304 - acc: 0.9020 - val_loss: 0.3024 - val_f_2: 0.3428 - val_acc: 0.9015\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.3084 - acc: 0.9001 - val_loss: 0.3046 - val_f_2: 0.4267 - val_acc: 0.9018\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.3574 - acc: 0.9033 - val_loss: 0.3033 - val_f_2: 0.3699 - val_acc: 0.9018\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2954 - f_2: 0.3272 - acc: 0.9005 - val_loss: 0.3023 - val_f_2: 0.3789 - val_acc: 0.9015\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3469 - acc: 0.9026 - val_loss: 0.3026 - val_f_2: 0.3339 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.3383 - acc: 0.9026 - val_loss: 0.3013 - val_f_2: 0.3437 - val_acc: 0.9012\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6305 - f_2: 0.0102 - acc: 0.8616 - val_loss: 0.3785 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3832 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3405 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3577 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3256 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3488 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3222 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3421 - f_2: 0.1270 - acc: 0.8822 - val_loss: 0.3148 - val_f_2: 0.2670 - val_acc: 0.9074\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3376 - f_2: 0.1953 - acc: 0.8896 - val_loss: 0.3113 - val_f_2: 0.2720 - val_acc: 0.9083\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3352 - f_2: 0.2218 - acc: 0.8919 - val_loss: 0.3099 - val_f_2: 0.2952 - val_acc: 0.9109\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3308 - f_2: 0.2430 - acc: 0.8936 - val_loss: 0.3071 - val_f_2: 0.2960 - val_acc: 0.9103\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3311 - f_2: 0.2403 - acc: 0.8944 - val_loss: 0.3068 - val_f_2: 0.2960 - val_acc: 0.9103\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2648 - acc: 0.8956 - val_loss: 0.3038 - val_f_2: 0.3133 - val_acc: 0.9121\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2550 - acc: 0.8942 - val_loss: 0.3013 - val_f_2: 0.3089 - val_acc: 0.9109\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3254 - f_2: 0.2857 - acc: 0.8973 - val_loss: 0.3003 - val_f_2: 0.3129 - val_acc: 0.9112\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3229 - f_2: 0.2810 - acc: 0.8971 - val_loss: 0.2979 - val_f_2: 0.2993 - val_acc: 0.9109\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3225 - f_2: 0.2779 - acc: 0.8950 - val_loss: 0.3026 - val_f_2: 0.3421 - val_acc: 0.9094\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.3006 - acc: 0.8974 - val_loss: 0.2990 - val_f_2: 0.3242 - val_acc: 0.9100\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2946 - acc: 0.8981 - val_loss: 0.2965 - val_f_2: 0.3431 - val_acc: 0.9094\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3023 - acc: 0.8971 - val_loss: 0.2959 - val_f_2: 0.3066 - val_acc: 0.9106\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.3029 - acc: 0.8976 - val_loss: 0.2970 - val_f_2: 0.3572 - val_acc: 0.9094\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3049 - acc: 0.8986 - val_loss: 0.2928 - val_f_2: 0.3108 - val_acc: 0.9109\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3010 - acc: 0.8968 - val_loss: 0.2922 - val_f_2: 0.3202 - val_acc: 0.9109\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3282 - acc: 0.8991 - val_loss: 0.2912 - val_f_2: 0.3210 - val_acc: 0.9115\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3150 - acc: 0.8989 - val_loss: 0.2909 - val_f_2: 0.3181 - val_acc: 0.9097\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3192 - acc: 0.8984 - val_loss: 0.2899 - val_f_2: 0.3293 - val_acc: 0.9103\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3158 - acc: 0.8976 - val_loss: 0.2893 - val_f_2: 0.3151 - val_acc: 0.9106\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3247 - acc: 0.8990 - val_loss: 0.2896 - val_f_2: 0.3232 - val_acc: 0.9118\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3086 - f_2: 0.3279 - acc: 0.8992 - val_loss: 0.2887 - val_f_2: 0.3280 - val_acc: 0.9106\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3086 - f_2: 0.3269 - acc: 0.8984 - val_loss: 0.2878 - val_f_2: 0.3351 - val_acc: 0.9091\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.3198 - acc: 0.8983 - val_loss: 0.2974 - val_f_2: 0.4111 - val_acc: 0.9083\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3083 - acc: 0.8970 - val_loss: 0.2934 - val_f_2: 0.3856 - val_acc: 0.9086\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3039 - f_2: 0.3279 - acc: 0.8992 - val_loss: 0.2948 - val_f_2: 0.3265 - val_acc: 0.9106\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3233 - acc: 0.8980 - val_loss: 0.2872 - val_f_2: 0.3308 - val_acc: 0.9109\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3480 - acc: 0.9009 - val_loss: 0.2866 - val_f_2: 0.3096 - val_acc: 0.9115\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.3344 - acc: 0.8987 - val_loss: 0.2876 - val_f_2: 0.3642 - val_acc: 0.9077\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3037 - f_2: 0.3499 - acc: 0.9003 - val_loss: 0.2865 - val_f_2: 0.3664 - val_acc: 0.9088\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3218 - acc: 0.8981 - val_loss: 0.2855 - val_f_2: 0.3364 - val_acc: 0.9091\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3237 - acc: 0.8981 - val_loss: 0.2871 - val_f_2: 0.3482 - val_acc: 0.9088\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3020 - f_2: 0.3454 - acc: 0.9012 - val_loss: 0.2964 - val_f_2: 0.4030 - val_acc: 0.9068\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3396 - acc: 0.8985 - val_loss: 0.2852 - val_f_2: 0.3428 - val_acc: 0.9080\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3407 - acc: 0.8996 - val_loss: 0.2864 - val_f_2: 0.3299 - val_acc: 0.9115\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3020 - f_2: 0.3462 - acc: 0.8992 - val_loss: 0.2845 - val_f_2: 0.3318 - val_acc: 0.9097\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3007 - f_2: 0.3278 - acc: 0.8995 - val_loss: 0.2855 - val_f_2: 0.3527 - val_acc: 0.9100\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3278 - acc: 0.8995 - val_loss: 0.2835 - val_f_2: 0.3433 - val_acc: 0.9103\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3481 - acc: 0.9012 - val_loss: 0.2830 - val_f_2: 0.3205 - val_acc: 0.9112\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3385 - acc: 0.9013 - val_loss: 0.2837 - val_f_2: 0.3454 - val_acc: 0.9106\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2991 - f_2: 0.3387 - acc: 0.9012 - val_loss: 0.2851 - val_f_2: 0.3809 - val_acc: 0.9100\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2992 - f_2: 0.3569 - acc: 0.9015 - val_loss: 0.2825 - val_f_2: 0.3409 - val_acc: 0.9103\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.3412 - acc: 0.8997 - val_loss: 0.2837 - val_f_2: 0.3640 - val_acc: 0.9103\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3454 - acc: 0.9009 - val_loss: 0.2826 - val_f_2: 0.3090 - val_acc: 0.9112\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.3352 - acc: 0.8996 - val_loss: 0.2835 - val_f_2: 0.3693 - val_acc: 0.9112\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2977 - f_2: 0.3453 - acc: 0.8995 - val_loss: 0.2830 - val_f_2: 0.3628 - val_acc: 0.9121\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4287 - f_2: 0.0111 - acc: 0.8676 - val_loss: 0.3659 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3558 - f_2: 3.2423e-08 - acc: 0.8753 - val_loss: 0.3426 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3415 - f_2: 0.0237 - acc: 0.8776 - val_loss: 0.3333 - val_f_2: 0.2155 - val_acc: 0.8929\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3313 - f_2: 0.1968 - acc: 0.8961 - val_loss: 0.3292 - val_f_2: 0.2869 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2550 - acc: 0.9024 - val_loss: 0.3232 - val_f_2: 0.2868 - val_acc: 0.8997\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.2917 - acc: 0.9047 - val_loss: 0.3208 - val_f_2: 0.2985 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3172 - f_2: 0.2943 - acc: 0.9051 - val_loss: 0.3216 - val_f_2: 0.3356 - val_acc: 0.9006\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3125 - f_2: 0.3172 - acc: 0.9053 - val_loss: 0.3167 - val_f_2: 0.3356 - val_acc: 0.9012\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3214 - acc: 0.9058 - val_loss: 0.3174 - val_f_2: 0.3160 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.3254 - acc: 0.9061 - val_loss: 0.3247 - val_f_2: 0.3343 - val_acc: 0.9015\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3057 - f_2: 0.3395 - acc: 0.9065 - val_loss: 0.3131 - val_f_2: 0.3399 - val_acc: 0.9015\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3062 - f_2: 0.3460 - acc: 0.9058 - val_loss: 0.3162 - val_f_2: 0.3466 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3603 - acc: 0.9069 - val_loss: 0.3120 - val_f_2: 0.3691 - val_acc: 0.9018\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3682 - acc: 0.9072 - val_loss: 0.3138 - val_f_2: 0.3457 - val_acc: 0.9006\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2996 - f_2: 0.3716 - acc: 0.9071 - val_loss: 0.3112 - val_f_2: 0.3890 - val_acc: 0.9021\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2983 - f_2: 0.3671 - acc: 0.9072 - val_loss: 0.3117 - val_f_2: 0.3930 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2991 - f_2: 0.3903 - acc: 0.9081 - val_loss: 0.3164 - val_f_2: 0.3574 - val_acc: 0.9006\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2971 - f_2: 0.3708 - acc: 0.9083 - val_loss: 0.3174 - val_f_2: 0.3870 - val_acc: 0.9015\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.3920 - acc: 0.9088 - val_loss: 0.3107 - val_f_2: 0.3753 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.3858 - acc: 0.9091 - val_loss: 0.3104 - val_f_2: 0.3775 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2936 - f_2: 0.3875 - acc: 0.9088 - val_loss: 0.3115 - val_f_2: 0.3898 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2934 - f_2: 0.3873 - acc: 0.9083 - val_loss: 0.3117 - val_f_2: 0.4059 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2938 - f_2: 0.4013 - acc: 0.9089 - val_loss: 0.3179 - val_f_2: 0.3786 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2940 - f_2: 0.3848 - acc: 0.9093 - val_loss: 0.3122 - val_f_2: 0.3995 - val_acc: 0.8988\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2918 - f_2: 0.3920 - acc: 0.9093 - val_loss: 0.3111 - val_f_2: 0.4047 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2901 - f_2: 0.3943 - acc: 0.9096 - val_loss: 0.3129 - val_f_2: 0.4098 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2910 - f_2: 0.4012 - acc: 0.9110 - val_loss: 0.3156 - val_f_2: 0.4063 - val_acc: 0.8979\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2887 - f_2: 0.4026 - acc: 0.9100 - val_loss: 0.3112 - val_f_2: 0.3978 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2903 - f_2: 0.4100 - acc: 0.9098 - val_loss: 0.3130 - val_f_2: 0.3834 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2885 - f_2: 0.4048 - acc: 0.9102 - val_loss: 0.3109 - val_f_2: 0.4006 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2884 - f_2: 0.3973 - acc: 0.9106 - val_loss: 0.3122 - val_f_2: 0.4038 - val_acc: 0.8991\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2873 - f_2: 0.4002 - acc: 0.9113 - val_loss: 0.3108 - val_f_2: 0.3995 - val_acc: 0.8985\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2862 - f_2: 0.4080 - acc: 0.9110 - val_loss: 0.3115 - val_f_2: 0.3895 - val_acc: 0.8991\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2872 - f_2: 0.4150 - acc: 0.9102 - val_loss: 0.3131 - val_f_2: 0.4007 - val_acc: 0.8994\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.4044 - acc: 0.9109 - val_loss: 0.3121 - val_f_2: 0.3969 - val_acc: 0.8971\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.4141 - acc: 0.9113 - val_loss: 0.3177 - val_f_2: 0.4132 - val_acc: 0.8979\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.4182 - acc: 0.9112 - val_loss: 0.3121 - val_f_2: 0.3994 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.4131 - acc: 0.9117 - val_loss: 0.3134 - val_f_2: 0.4014 - val_acc: 0.8982\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2836 - f_2: 0.4324 - acc: 0.9121 - val_loss: 0.3127 - val_f_2: 0.3780 - val_acc: 0.8973\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.4089 - acc: 0.9109 - val_loss: 0.3113 - val_f_2: 0.3993 - val_acc: 0.8991\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.4262 - acc: 0.9113 - val_loss: 0.3126 - val_f_2: 0.3996 - val_acc: 0.8985\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4256 - acc: 0.9109 - val_loss: 0.3128 - val_f_2: 0.3989 - val_acc: 0.8962\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4301 - acc: 0.9109 - val_loss: 0.3115 - val_f_2: 0.3970 - val_acc: 0.8985\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2827 - f_2: 0.4251 - acc: 0.9118 - val_loss: 0.3122 - val_f_2: 0.3928 - val_acc: 0.8973\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.4220 - acc: 0.9114 - val_loss: 0.3131 - val_f_2: 0.3979 - val_acc: 0.8979\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2817 - f_2: 0.4280 - acc: 0.9122 - val_loss: 0.3119 - val_f_2: 0.4023 - val_acc: 0.8997\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.4251 - acc: 0.9104 - val_loss: 0.3119 - val_f_2: 0.4027 - val_acc: 0.8991\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2815 - f_2: 0.4250 - acc: 0.9118 - val_loss: 0.3138 - val_f_2: 0.4104 - val_acc: 0.8962\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2816 - f_2: 0.4305 - acc: 0.9112 - val_loss: 0.3112 - val_f_2: 0.3937 - val_acc: 0.8982\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2800 - f_2: 0.4212 - acc: 0.9119 - val_loss: 0.3154 - val_f_2: 0.4051 - val_acc: 0.8988\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4303 - f_2: 0.0126 - acc: 0.8640 - val_loss: 0.3659 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3539 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3416 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3382 - f_2: 0.1483 - acc: 0.8885 - val_loss: 0.3387 - val_f_2: 0.2246 - val_acc: 0.8938\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3312 - f_2: 0.2460 - acc: 0.8988 - val_loss: 0.3266 - val_f_2: 0.2757 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3268 - f_2: 0.2642 - acc: 0.9006 - val_loss: 0.3206 - val_f_2: 0.2737 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.2981 - acc: 0.9029 - val_loss: 0.3158 - val_f_2: 0.3207 - val_acc: 0.9009\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3468 - acc: 0.9045 - val_loss: 0.3113 - val_f_2: 0.3646 - val_acc: 0.8997\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3081 - f_2: 0.3676 - acc: 0.9056 - val_loss: 0.3091 - val_f_2: 0.3683 - val_acc: 0.9009\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3728 - acc: 0.9070 - val_loss: 0.3048 - val_f_2: 0.3876 - val_acc: 0.9012\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3704 - acc: 0.9065 - val_loss: 0.3022 - val_f_2: 0.3871 - val_acc: 0.9018\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2971 - f_2: 0.4002 - acc: 0.9075 - val_loss: 0.3036 - val_f_2: 0.3829 - val_acc: 0.9009\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3853 - acc: 0.9056 - val_loss: 0.2990 - val_f_2: 0.4057 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3990 - acc: 0.9076 - val_loss: 0.2987 - val_f_2: 0.4033 - val_acc: 0.9015\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2909 - f_2: 0.4060 - acc: 0.9067 - val_loss: 0.2993 - val_f_2: 0.3497 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2914 - f_2: 0.3972 - acc: 0.9067 - val_loss: 0.2996 - val_f_2: 0.4019 - val_acc: 0.9021\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.4160 - acc: 0.9077 - val_loss: 0.2969 - val_f_2: 0.4215 - val_acc: 0.9024\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2855 - f_2: 0.4107 - acc: 0.9079 - val_loss: 0.2991 - val_f_2: 0.3794 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.4100 - acc: 0.9072 - val_loss: 0.2985 - val_f_2: 0.3955 - val_acc: 0.9015\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.4119 - acc: 0.9085 - val_loss: 0.3021 - val_f_2: 0.3506 - val_acc: 0.9009\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2867 - f_2: 0.4051 - acc: 0.9071 - val_loss: 0.3067 - val_f_2: 0.3915 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2818 - f_2: 0.4250 - acc: 0.9090 - val_loss: 0.2995 - val_f_2: 0.4128 - val_acc: 0.8991\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2834 - f_2: 0.4282 - acc: 0.9088 - val_loss: 0.3032 - val_f_2: 0.3901 - val_acc: 0.8997\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2828 - f_2: 0.4296 - acc: 0.9101 - val_loss: 0.2971 - val_f_2: 0.4185 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2814 - f_2: 0.4347 - acc: 0.9099 - val_loss: 0.2986 - val_f_2: 0.4332 - val_acc: 0.9015\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2816 - f_2: 0.4457 - acc: 0.9108 - val_loss: 0.2988 - val_f_2: 0.3764 - val_acc: 0.9021\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4380 - acc: 0.9097 - val_loss: 0.3012 - val_f_2: 0.4104 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2783 - f_2: 0.4347 - acc: 0.9108 - val_loss: 0.3017 - val_f_2: 0.4180 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2784 - f_2: 0.4407 - acc: 0.9109 - val_loss: 0.2992 - val_f_2: 0.4174 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2791 - f_2: 0.4321 - acc: 0.9116 - val_loss: 0.2996 - val_f_2: 0.4299 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2775 - f_2: 0.4500 - acc: 0.9116 - val_loss: 0.2993 - val_f_2: 0.4337 - val_acc: 0.9015\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2774 - f_2: 0.4456 - acc: 0.9119 - val_loss: 0.3006 - val_f_2: 0.4280 - val_acc: 0.8994\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4434 - acc: 0.9106 - val_loss: 0.2995 - val_f_2: 0.4180 - val_acc: 0.8997\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2758 - f_2: 0.4511 - acc: 0.9110 - val_loss: 0.3014 - val_f_2: 0.4007 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2763 - f_2: 0.4460 - acc: 0.9124 - val_loss: 0.3007 - val_f_2: 0.4311 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2762 - f_2: 0.4668 - acc: 0.9118 - val_loss: 0.3009 - val_f_2: 0.4225 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2769 - f_2: 0.4538 - acc: 0.9109 - val_loss: 0.3030 - val_f_2: 0.4339 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2736 - f_2: 0.4586 - acc: 0.9129 - val_loss: 0.3015 - val_f_2: 0.4095 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2749 - f_2: 0.4599 - acc: 0.9127 - val_loss: 0.3030 - val_f_2: 0.4330 - val_acc: 0.8985\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2737 - f_2: 0.4831 - acc: 0.9138 - val_loss: 0.3056 - val_f_2: 0.4257 - val_acc: 0.8997\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2729 - f_2: 0.4593 - acc: 0.9138 - val_loss: 0.3040 - val_f_2: 0.4198 - val_acc: 0.8994\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2740 - f_2: 0.4734 - acc: 0.9130 - val_loss: 0.3041 - val_f_2: 0.4260 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2723 - f_2: 0.4654 - acc: 0.9144 - val_loss: 0.3018 - val_f_2: 0.4379 - val_acc: 0.8997\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2734 - f_2: 0.4666 - acc: 0.9134 - val_loss: 0.3034 - val_f_2: 0.4176 - val_acc: 0.8985\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2736 - f_2: 0.4688 - acc: 0.9127 - val_loss: 0.3037 - val_f_2: 0.4369 - val_acc: 0.8979\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2728 - f_2: 0.4824 - acc: 0.9147 - val_loss: 0.3032 - val_f_2: 0.4339 - val_acc: 0.8973\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2734 - f_2: 0.4695 - acc: 0.9116 - val_loss: 0.3075 - val_f_2: 0.4142 - val_acc: 0.8982\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2713 - f_2: 0.4764 - acc: 0.9152 - val_loss: 0.3021 - val_f_2: 0.4349 - val_acc: 0.8997\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2710 - f_2: 0.4910 - acc: 0.9150 - val_loss: 0.3026 - val_f_2: 0.4102 - val_acc: 0.8985\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2708 - f_2: 0.4774 - acc: 0.9141 - val_loss: 0.3036 - val_f_2: 0.4161 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2723 - f_2: 0.4745 - acc: 0.9128 - val_loss: 0.3055 - val_f_2: 0.4416 - val_acc: 0.8976\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4371 - f_2: 0.0099 - acc: 0.8616 - val_loss: 0.3405 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3625 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3215 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3455 - f_2: 0.0911 - acc: 0.8789 - val_loss: 0.3141 - val_f_2: 0.2655 - val_acc: 0.9071\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3366 - f_2: 0.2389 - acc: 0.8951 - val_loss: 0.3080 - val_f_2: 0.2868 - val_acc: 0.9091\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.2659 - acc: 0.8983 - val_loss: 0.3046 - val_f_2: 0.3061 - val_acc: 0.9097\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2813 - acc: 0.8986 - val_loss: 0.3036 - val_f_2: 0.3160 - val_acc: 0.9103\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.3223 - acc: 0.9009 - val_loss: 0.3017 - val_f_2: 0.3114 - val_acc: 0.9106\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3189 - f_2: 0.3158 - acc: 0.9011 - val_loss: 0.2992 - val_f_2: 0.3297 - val_acc: 0.9097\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.3356 - acc: 0.9010 - val_loss: 0.2973 - val_f_2: 0.3412 - val_acc: 0.9094\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3226 - acc: 0.9007 - val_loss: 0.2993 - val_f_2: 0.3659 - val_acc: 0.9109\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3436 - acc: 0.9019 - val_loss: 0.2941 - val_f_2: 0.3528 - val_acc: 0.9097\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3541 - acc: 0.9019 - val_loss: 0.3022 - val_f_2: 0.3408 - val_acc: 0.9103\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3089 - f_2: 0.3468 - acc: 0.9021 - val_loss: 0.2972 - val_f_2: 0.4053 - val_acc: 0.9100\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3820 - acc: 0.9035 - val_loss: 0.2915 - val_f_2: 0.3693 - val_acc: 0.9088\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3787 - acc: 0.9028 - val_loss: 0.2902 - val_f_2: 0.3696 - val_acc: 0.9112\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3791 - acc: 0.9032 - val_loss: 0.2929 - val_f_2: 0.4109 - val_acc: 0.9086\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3920 - acc: 0.9036 - val_loss: 0.2899 - val_f_2: 0.4178 - val_acc: 0.9091\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3005 - f_2: 0.3835 - acc: 0.9029 - val_loss: 0.2894 - val_f_2: 0.4192 - val_acc: 0.9094\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.4075 - acc: 0.9045 - val_loss: 0.2899 - val_f_2: 0.4247 - val_acc: 0.9100\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.4008 - acc: 0.9040 - val_loss: 0.2896 - val_f_2: 0.4066 - val_acc: 0.9097\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2992 - f_2: 0.4027 - acc: 0.9040 - val_loss: 0.2899 - val_f_2: 0.3923 - val_acc: 0.9088\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3967 - acc: 0.9029 - val_loss: 0.2889 - val_f_2: 0.4254 - val_acc: 0.9080\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.4110 - acc: 0.9037 - val_loss: 0.2883 - val_f_2: 0.3923 - val_acc: 0.9094\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.4121 - acc: 0.9049 - val_loss: 0.2864 - val_f_2: 0.4078 - val_acc: 0.9083\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.4144 - acc: 0.9051 - val_loss: 0.2869 - val_f_2: 0.4013 - val_acc: 0.9109\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.4217 - acc: 0.9062 - val_loss: 0.2877 - val_f_2: 0.4373 - val_acc: 0.9097\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.4350 - acc: 0.9059 - val_loss: 0.2882 - val_f_2: 0.4375 - val_acc: 0.9068\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.4267 - acc: 0.9055 - val_loss: 0.2886 - val_f_2: 0.4167 - val_acc: 0.9077\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2930 - f_2: 0.4441 - acc: 0.9049 - val_loss: 0.2863 - val_f_2: 0.3968 - val_acc: 0.9097\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2936 - f_2: 0.4294 - acc: 0.9059 - val_loss: 0.2889 - val_f_2: 0.4020 - val_acc: 0.9083\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2918 - f_2: 0.4414 - acc: 0.9057 - val_loss: 0.2885 - val_f_2: 0.4234 - val_acc: 0.9047\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.4525 - acc: 0.9070 - val_loss: 0.2876 - val_f_2: 0.4277 - val_acc: 0.9053\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.4478 - acc: 0.9063 - val_loss: 0.2868 - val_f_2: 0.4049 - val_acc: 0.9100\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.4363 - acc: 0.9065 - val_loss: 0.2873 - val_f_2: 0.4167 - val_acc: 0.9065\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.4294 - acc: 0.9070 - val_loss: 0.2877 - val_f_2: 0.4201 - val_acc: 0.9080\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2885 - f_2: 0.4602 - acc: 0.9075 - val_loss: 0.2918 - val_f_2: 0.4447 - val_acc: 0.9038\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2896 - f_2: 0.4386 - acc: 0.9074 - val_loss: 0.2885 - val_f_2: 0.4154 - val_acc: 0.9088\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2873 - f_2: 0.4591 - acc: 0.9079 - val_loss: 0.2883 - val_f_2: 0.4391 - val_acc: 0.9050\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.4596 - acc: 0.9085 - val_loss: 0.2891 - val_f_2: 0.4250 - val_acc: 0.9086\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.4711 - acc: 0.9085 - val_loss: 0.2867 - val_f_2: 0.4055 - val_acc: 0.9062\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2873 - f_2: 0.4491 - acc: 0.9074 - val_loss: 0.2930 - val_f_2: 0.3953 - val_acc: 0.9059\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.4708 - acc: 0.9084 - val_loss: 0.2909 - val_f_2: 0.4298 - val_acc: 0.9029\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4674 - acc: 0.9074 - val_loss: 0.2877 - val_f_2: 0.4180 - val_acc: 0.9083\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4680 - acc: 0.9086 - val_loss: 0.2918 - val_f_2: 0.4528 - val_acc: 0.9021\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.4833 - acc: 0.9093 - val_loss: 0.2864 - val_f_2: 0.4165 - val_acc: 0.9035\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2863 - f_2: 0.4450 - acc: 0.9079 - val_loss: 0.2896 - val_f_2: 0.4432 - val_acc: 0.9038\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2860 - f_2: 0.4689 - acc: 0.9088 - val_loss: 0.2909 - val_f_2: 0.4154 - val_acc: 0.9068\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2831 - f_2: 0.4657 - acc: 0.9086 - val_loss: 0.2885 - val_f_2: 0.4318 - val_acc: 0.9015\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2862 - f_2: 0.4638 - acc: 0.9085 - val_loss: 0.2874 - val_f_2: 0.4241 - val_acc: 0.9032\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2845 - f_2: 0.4758 - acc: 0.9095 - val_loss: 0.2901 - val_f_2: 0.4274 - val_acc: 0.9044\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7137 - f_2: 0.0269 - acc: 0.8467 - val_loss: 0.5064 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4403 - f_2: 1.4048e-08 - acc: 0.8753 - val_loss: 0.3936 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3779 - f_2: 1.3856e-08 - acc: 0.8753 - val_loss: 0.3653 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3575 - f_2: 1.3957e-08 - acc: 0.8753 - val_loss: 0.3503 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3486 - f_2: 1.4255e-08 - acc: 0.8753 - val_loss: 0.3441 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3413 - f_2: 1.3815e-08 - acc: 0.8753 - val_loss: 0.3384 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3366 - f_2: 1.3845e-08 - acc: 0.8753 - val_loss: 0.3354 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3346 - f_2: 1.4236e-08 - acc: 0.8753 - val_loss: 0.3331 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3314 - f_2: 0.1328 - acc: 0.8884 - val_loss: 0.3297 - val_f_2: 0.2713 - val_acc: 0.8985\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3270 - f_2: 0.1865 - acc: 0.8950 - val_loss: 0.3275 - val_f_2: 0.2713 - val_acc: 0.8985\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3270 - f_2: 0.1916 - acc: 0.8953 - val_loss: 0.3268 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3238 - f_2: 0.2177 - acc: 0.8971 - val_loss: 0.3248 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2294 - acc: 0.8984 - val_loss: 0.3230 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3227 - f_2: 0.2176 - acc: 0.8971 - val_loss: 0.3253 - val_f_2: 0.2890 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2221 - acc: 0.8968 - val_loss: 0.3211 - val_f_2: 0.2918 - val_acc: 0.8994\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2281 - acc: 0.8985 - val_loss: 0.3193 - val_f_2: 0.2891 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2396 - acc: 0.8982 - val_loss: 0.3203 - val_f_2: 0.2918 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.2518 - acc: 0.8988 - val_loss: 0.3179 - val_f_2: 0.2891 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.2666 - acc: 0.9001 - val_loss: 0.3162 - val_f_2: 0.2999 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.2617 - acc: 0.9001 - val_loss: 0.3167 - val_f_2: 0.2942 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.2554 - acc: 0.8993 - val_loss: 0.3153 - val_f_2: 0.3035 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3126 - f_2: 0.2686 - acc: 0.9010 - val_loss: 0.3143 - val_f_2: 0.3067 - val_acc: 0.9009\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.2705 - acc: 0.8998 - val_loss: 0.3139 - val_f_2: 0.3151 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.2713 - acc: 0.9000 - val_loss: 0.3139 - val_f_2: 0.3378 - val_acc: 0.8997\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.2669 - acc: 0.9000 - val_loss: 0.3129 - val_f_2: 0.3222 - val_acc: 0.9006\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.2943 - acc: 0.9022 - val_loss: 0.3125 - val_f_2: 0.3225 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.2866 - acc: 0.9021 - val_loss: 0.3115 - val_f_2: 0.3262 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.2939 - acc: 0.9020 - val_loss: 0.3112 - val_f_2: 0.3403 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.2891 - acc: 0.9013 - val_loss: 0.3111 - val_f_2: 0.3704 - val_acc: 0.8994\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.2843 - acc: 0.9013 - val_loss: 0.3112 - val_f_2: 0.3143 - val_acc: 0.9012\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.2932 - acc: 0.9023 - val_loss: 0.3106 - val_f_2: 0.3143 - val_acc: 0.9012\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.2895 - acc: 0.9019 - val_loss: 0.3091 - val_f_2: 0.3241 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3061 - acc: 0.9041 - val_loss: 0.3101 - val_f_2: 0.3106 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.2968 - acc: 0.9029 - val_loss: 0.3080 - val_f_2: 0.3241 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3050 - f_2: 0.3123 - acc: 0.9029 - val_loss: 0.3081 - val_f_2: 0.3224 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.2901 - acc: 0.9031 - val_loss: 0.3081 - val_f_2: 0.3305 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3023 - acc: 0.9022 - val_loss: 0.3094 - val_f_2: 0.3124 - val_acc: 0.9012\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.2990 - acc: 0.9034 - val_loss: 0.3071 - val_f_2: 0.3191 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3034 - acc: 0.9029 - val_loss: 0.3075 - val_f_2: 0.3576 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3032 - f_2: 0.3014 - acc: 0.9026 - val_loss: 0.3065 - val_f_2: 0.3225 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.2968 - acc: 0.9021 - val_loss: 0.3072 - val_f_2: 0.3308 - val_acc: 0.8997\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.2969 - acc: 0.9015 - val_loss: 0.3052 - val_f_2: 0.3182 - val_acc: 0.9006\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3153 - acc: 0.9049 - val_loss: 0.3114 - val_f_2: 0.4194 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3187 - acc: 0.9034 - val_loss: 0.3051 - val_f_2: 0.3149 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3186 - acc: 0.9031 - val_loss: 0.3062 - val_f_2: 0.3339 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.3031 - acc: 0.9020 - val_loss: 0.3050 - val_f_2: 0.3665 - val_acc: 0.9015\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3014 - f_2: 0.3048 - acc: 0.9026 - val_loss: 0.3047 - val_f_2: 0.3916 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2989 - f_2: 0.3287 - acc: 0.9037 - val_loss: 0.3043 - val_f_2: 0.3592 - val_acc: 0.8988\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.3032 - acc: 0.9017 - val_loss: 0.3039 - val_f_2: 0.3324 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2996 - f_2: 0.3134 - acc: 0.9041 - val_loss: 0.3054 - val_f_2: 0.3183 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.01, total=  31.8s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7147 - f_2: 0.0272 - acc: 0.8410 - val_loss: 0.5030 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4403 - f_2: 1.3385e-08 - acc: 0.8722 - val_loss: 0.3920 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3777 - f_2: 1.3157e-08 - acc: 0.8722 - val_loss: 0.3652 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3591 - f_2: 1.3366e-08 - acc: 0.8722 - val_loss: 0.3508 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3498 - f_2: 1.3681e-08 - acc: 0.8722 - val_loss: 0.3434 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3437 - f_2: 1.3673e-08 - acc: 0.8722 - val_loss: 0.3403 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3387 - f_2: 1.3448e-08 - acc: 0.8722 - val_loss: 0.3361 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3358 - f_2: 0.1105 - acc: 0.8844 - val_loss: 0.3324 - val_f_2: 0.0404 - val_acc: 0.8714\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3341 - f_2: 0.1514 - acc: 0.8880 - val_loss: 0.3298 - val_f_2: 0.2026 - val_acc: 0.8900\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3308 - f_2: 0.1626 - acc: 0.8898 - val_loss: 0.3287 - val_f_2: 0.1811 - val_acc: 0.8873\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3281 - f_2: 0.1608 - acc: 0.8888 - val_loss: 0.3262 - val_f_2: 0.2451 - val_acc: 0.8953\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.1867 - acc: 0.8905 - val_loss: 0.3244 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3260 - f_2: 0.2156 - acc: 0.8929 - val_loss: 0.3232 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.2281 - acc: 0.8952 - val_loss: 0.3216 - val_f_2: 0.2850 - val_acc: 0.8988\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3250 - f_2: 0.2270 - acc: 0.8956 - val_loss: 0.3247 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.2472 - acc: 0.8958 - val_loss: 0.3196 - val_f_2: 0.2883 - val_acc: 0.8991\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.2437 - acc: 0.8956 - val_loss: 0.3188 - val_f_2: 0.2935 - val_acc: 0.8985\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.2419 - acc: 0.8960 - val_loss: 0.3197 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.2528 - acc: 0.8971 - val_loss: 0.3175 - val_f_2: 0.3390 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.2664 - acc: 0.8965 - val_loss: 0.3165 - val_f_2: 0.3337 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2700 - acc: 0.8977 - val_loss: 0.3156 - val_f_2: 0.3326 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.2776 - acc: 0.8975 - val_loss: 0.3154 - val_f_2: 0.3357 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.2749 - acc: 0.8971 - val_loss: 0.3138 - val_f_2: 0.3344 - val_acc: 0.9012\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.2784 - acc: 0.8970 - val_loss: 0.3156 - val_f_2: 0.2887 - val_acc: 0.8994\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2723 - acc: 0.8967 - val_loss: 0.3128 - val_f_2: 0.3282 - val_acc: 0.9015\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.2837 - acc: 0.8987 - val_loss: 0.3120 - val_f_2: 0.3177 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.2750 - acc: 0.8975 - val_loss: 0.3140 - val_f_2: 0.3622 - val_acc: 0.9021\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.2921 - acc: 0.8995 - val_loss: 0.3113 - val_f_2: 0.3417 - val_acc: 0.9003\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3013 - acc: 0.8989 - val_loss: 0.3157 - val_f_2: 0.3195 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.3053 - acc: 0.8994 - val_loss: 0.3117 - val_f_2: 0.3828 - val_acc: 0.9035\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.3139 - acc: 0.9005 - val_loss: 0.3111 - val_f_2: 0.3457 - val_acc: 0.9012\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3113 - f_2: 0.3137 - acc: 0.8998 - val_loss: 0.3092 - val_f_2: 0.3693 - val_acc: 0.9021\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3183 - acc: 0.9012 - val_loss: 0.3114 - val_f_2: 0.3203 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3078 - f_2: 0.3193 - acc: 0.9020 - val_loss: 0.3087 - val_f_2: 0.3666 - val_acc: 0.9021\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.3289 - acc: 0.9016 - val_loss: 0.3083 - val_f_2: 0.3718 - val_acc: 0.9024\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.3086 - acc: 0.8997 - val_loss: 0.3092 - val_f_2: 0.3220 - val_acc: 0.9015\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3353 - acc: 0.9029 - val_loss: 0.3075 - val_f_2: 0.3466 - val_acc: 0.9024\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3075 - f_2: 0.3331 - acc: 0.9031 - val_loss: 0.3071 - val_f_2: 0.3532 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3167 - acc: 0.9014 - val_loss: 0.3072 - val_f_2: 0.3902 - val_acc: 0.9027\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3353 - acc: 0.9016 - val_loss: 0.3060 - val_f_2: 0.3547 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3311 - acc: 0.9024 - val_loss: 0.3086 - val_f_2: 0.3131 - val_acc: 0.9012\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3286 - acc: 0.9012 - val_loss: 0.3104 - val_f_2: 0.3180 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3078 - f_2: 0.3127 - acc: 0.9003 - val_loss: 0.3065 - val_f_2: 0.3545 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3239 - acc: 0.9006 - val_loss: 0.3051 - val_f_2: 0.3519 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3173 - acc: 0.9011 - val_loss: 0.3047 - val_f_2: 0.3592 - val_acc: 0.9024\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3245 - acc: 0.9009 - val_loss: 0.3045 - val_f_2: 0.3439 - val_acc: 0.9015\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3213 - acc: 0.9003 - val_loss: 0.3097 - val_f_2: 0.3032 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3038 - f_2: 0.3167 - acc: 0.9015 - val_loss: 0.3061 - val_f_2: 0.3489 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3389 - acc: 0.9025 - val_loss: 0.3070 - val_f_2: 0.4057 - val_acc: 0.9018\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3307 - acc: 0.9017 - val_loss: 0.3048 - val_f_2: 0.3676 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.01, total=  31.3s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7161 - f_2: 0.0265 - acc: 0.8408 - val_loss: 0.4856 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4468 - f_2: 1.3241e-08 - acc: 0.8691 - val_loss: 0.3729 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3842 - f_2: 1.3581e-08 - acc: 0.8691 - val_loss: 0.3465 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3664 - f_2: 1.3300e-08 - acc: 0.8691 - val_loss: 0.3343 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3557 - f_2: 1.3364e-08 - acc: 0.8691 - val_loss: 0.3271 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3501 - f_2: 1.3041e-08 - acc: 0.8691 - val_loss: 0.3222 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3450 - f_2: 1.2928e-08 - acc: 0.8691 - val_loss: 0.3185 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3418 - f_2: 0.0680 - acc: 0.8768 - val_loss: 0.3162 - val_f_2: 0.2106 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3388 - f_2: 0.1446 - acc: 0.8852 - val_loss: 0.3183 - val_f_2: 0.2816 - val_acc: 0.9086\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3379 - f_2: 0.1871 - acc: 0.8889 - val_loss: 0.3119 - val_f_2: 0.1234 - val_acc: 0.8917\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3348 - f_2: 0.1846 - acc: 0.8888 - val_loss: 0.3113 - val_f_2: 0.2775 - val_acc: 0.9080\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3325 - f_2: 0.2119 - acc: 0.8917 - val_loss: 0.3096 - val_f_2: 0.3007 - val_acc: 0.9106\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3320 - f_2: 0.2198 - acc: 0.8925 - val_loss: 0.3081 - val_f_2: 0.3062 - val_acc: 0.9109\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3308 - f_2: 0.2172 - acc: 0.8911 - val_loss: 0.3054 - val_f_2: 0.2775 - val_acc: 0.9080\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3302 - f_2: 0.2189 - acc: 0.8914 - val_loss: 0.3042 - val_f_2: 0.2874 - val_acc: 0.9091\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2161 - acc: 0.8909 - val_loss: 0.3039 - val_f_2: 0.3081 - val_acc: 0.9112\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3257 - f_2: 0.2269 - acc: 0.8913 - val_loss: 0.3022 - val_f_2: 0.3078 - val_acc: 0.9109\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3257 - f_2: 0.2380 - acc: 0.8929 - val_loss: 0.3025 - val_f_2: 0.3081 - val_acc: 0.9112\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3258 - f_2: 0.2362 - acc: 0.8934 - val_loss: 0.3012 - val_f_2: 0.3170 - val_acc: 0.9097\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2366 - acc: 0.8929 - val_loss: 0.2993 - val_f_2: 0.3100 - val_acc: 0.9109\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.2423 - acc: 0.8917 - val_loss: 0.3042 - val_f_2: 0.3325 - val_acc: 0.9091\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.2528 - acc: 0.8933 - val_loss: 0.2983 - val_f_2: 0.3264 - val_acc: 0.9100\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2505 - acc: 0.8925 - val_loss: 0.2983 - val_f_2: 0.3399 - val_acc: 0.9088\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.2416 - acc: 0.8909 - val_loss: 0.2977 - val_f_2: 0.3376 - val_acc: 0.9094\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.2550 - acc: 0.8931 - val_loss: 0.2970 - val_f_2: 0.3329 - val_acc: 0.9086\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3189 - f_2: 0.2663 - acc: 0.8931 - val_loss: 0.2964 - val_f_2: 0.3456 - val_acc: 0.9088\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3191 - f_2: 0.2734 - acc: 0.8934 - val_loss: 0.2977 - val_f_2: 0.3401 - val_acc: 0.9074\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2651 - acc: 0.8933 - val_loss: 0.2948 - val_f_2: 0.3321 - val_acc: 0.9091\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.2859 - acc: 0.8953 - val_loss: 0.2934 - val_f_2: 0.3295 - val_acc: 0.9097\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.2827 - acc: 0.8950 - val_loss: 0.2947 - val_f_2: 0.3341 - val_acc: 0.9077\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2768 - acc: 0.8944 - val_loss: 0.2944 - val_f_2: 0.3551 - val_acc: 0.9086\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.2908 - acc: 0.8956 - val_loss: 0.2927 - val_f_2: 0.3334 - val_acc: 0.9088\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3022 - acc: 0.8963 - val_loss: 0.2922 - val_f_2: 0.3238 - val_acc: 0.9112\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.2879 - acc: 0.8951 - val_loss: 0.2934 - val_f_2: 0.3370 - val_acc: 0.9086\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.3143 - acc: 0.8961 - val_loss: 0.2903 - val_f_2: 0.3241 - val_acc: 0.9103\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3139 - f_2: 0.2863 - acc: 0.8937 - val_loss: 0.2926 - val_f_2: 0.3243 - val_acc: 0.9121\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.2908 - acc: 0.8945 - val_loss: 0.2921 - val_f_2: 0.3217 - val_acc: 0.9112\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3005 - acc: 0.8964 - val_loss: 0.2901 - val_f_2: 0.3326 - val_acc: 0.9094\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.2934 - acc: 0.8945 - val_loss: 0.2909 - val_f_2: 0.3515 - val_acc: 0.9074\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3122 - f_2: 0.3139 - acc: 0.8977 - val_loss: 0.2906 - val_f_2: 0.3379 - val_acc: 0.9091\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3053 - acc: 0.8963 - val_loss: 0.2892 - val_f_2: 0.3439 - val_acc: 0.9071\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.3075 - acc: 0.8950 - val_loss: 0.2912 - val_f_2: 0.3350 - val_acc: 0.9088\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3063 - acc: 0.8964 - val_loss: 0.2877 - val_f_2: 0.3423 - val_acc: 0.9074\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3124 - acc: 0.8951 - val_loss: 0.2882 - val_f_2: 0.3300 - val_acc: 0.9109\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3140 - acc: 0.8978 - val_loss: 0.2910 - val_f_2: 0.3969 - val_acc: 0.9091\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3197 - acc: 0.8960 - val_loss: 0.2896 - val_f_2: 0.3492 - val_acc: 0.9074\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3087 - f_2: 0.3178 - acc: 0.8979 - val_loss: 0.2870 - val_f_2: 0.3537 - val_acc: 0.9080\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3261 - acc: 0.8973 - val_loss: 0.2870 - val_f_2: 0.3649 - val_acc: 0.9094\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.3088 - acc: 0.8960 - val_loss: 0.2892 - val_f_2: 0.3786 - val_acc: 0.9083\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3233 - acc: 0.8972 - val_loss: 0.2859 - val_f_2: 0.3460 - val_acc: 0.9094\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.01, total=  32.4s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4639 - f_2: 0.0217 - acc: 0.8475 - val_loss: 0.3692 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3593 - f_2: 0.0585 - acc: 0.8812 - val_loss: 0.3375 - val_f_2: 0.2459 - val_acc: 0.8947\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3364 - f_2: 0.2038 - acc: 0.8956 - val_loss: 0.3209 - val_f_2: 0.2943 - val_acc: 0.9012\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.2522 - acc: 0.8964 - val_loss: 0.3113 - val_f_2: 0.3116 - val_acc: 0.9009\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.2768 - acc: 0.8994 - val_loss: 0.3053 - val_f_2: 0.3264 - val_acc: 0.9003\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.3114 - acc: 0.9017 - val_loss: 0.3033 - val_f_2: 0.3288 - val_acc: 0.9006\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3117 - acc: 0.9025 - val_loss: 0.3007 - val_f_2: 0.3543 - val_acc: 0.9015\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3187 - acc: 0.9023 - val_loss: 0.2989 - val_f_2: 0.3592 - val_acc: 0.9018\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3347 - acc: 0.9039 - val_loss: 0.2985 - val_f_2: 0.3393 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3350 - acc: 0.9048 - val_loss: 0.2978 - val_f_2: 0.3541 - val_acc: 0.9012\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3487 - acc: 0.9062 - val_loss: 0.2974 - val_f_2: 0.3631 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.3522 - acc: 0.9064 - val_loss: 0.2968 - val_f_2: 0.3587 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3546 - acc: 0.9069 - val_loss: 0.2954 - val_f_2: 0.3770 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2919 - f_2: 0.3622 - acc: 0.9071 - val_loss: 0.2960 - val_f_2: 0.3602 - val_acc: 0.9009\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.3495 - acc: 0.9060 - val_loss: 0.2967 - val_f_2: 0.4135 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2900 - f_2: 0.3791 - acc: 0.9085 - val_loss: 0.2956 - val_f_2: 0.4053 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.3595 - acc: 0.9062 - val_loss: 0.2952 - val_f_2: 0.3900 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2881 - f_2: 0.3650 - acc: 0.9077 - val_loss: 0.2951 - val_f_2: 0.3903 - val_acc: 0.9018\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.3737 - acc: 0.9084 - val_loss: 0.2947 - val_f_2: 0.4081 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.3831 - acc: 0.9093 - val_loss: 0.2965 - val_f_2: 0.3788 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2851 - f_2: 0.3852 - acc: 0.9089 - val_loss: 0.2964 - val_f_2: 0.3960 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2848 - f_2: 0.3842 - acc: 0.9091 - val_loss: 0.2952 - val_f_2: 0.4026 - val_acc: 0.8988\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2845 - f_2: 0.3889 - acc: 0.9089 - val_loss: 0.2976 - val_f_2: 0.3760 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.3842 - acc: 0.9078 - val_loss: 0.2962 - val_f_2: 0.3914 - val_acc: 0.8997\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2821 - f_2: 0.3931 - acc: 0.9080 - val_loss: 0.2953 - val_f_2: 0.3683 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2831 - f_2: 0.3849 - acc: 0.9079 - val_loss: 0.2957 - val_f_2: 0.3806 - val_acc: 0.8985\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2803 - f_2: 0.4014 - acc: 0.9107 - val_loss: 0.2960 - val_f_2: 0.4110 - val_acc: 0.8997\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.3959 - acc: 0.9093 - val_loss: 0.2970 - val_f_2: 0.4258 - val_acc: 0.8985\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2815 - f_2: 0.4153 - acc: 0.9100 - val_loss: 0.2979 - val_f_2: 0.3654 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2806 - f_2: 0.3996 - acc: 0.9099 - val_loss: 0.2951 - val_f_2: 0.4183 - val_acc: 0.8997\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2790 - f_2: 0.4090 - acc: 0.9096 - val_loss: 0.2963 - val_f_2: 0.4071 - val_acc: 0.8979\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2780 - f_2: 0.4144 - acc: 0.9105 - val_loss: 0.2959 - val_f_2: 0.4093 - val_acc: 0.8976\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4091 - acc: 0.9119 - val_loss: 0.2954 - val_f_2: 0.3802 - val_acc: 0.8991\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2790 - f_2: 0.4138 - acc: 0.9114 - val_loss: 0.2959 - val_f_2: 0.4016 - val_acc: 0.8994\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2780 - f_2: 0.4145 - acc: 0.9110 - val_loss: 0.2958 - val_f_2: 0.4098 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2781 - f_2: 0.4199 - acc: 0.9122 - val_loss: 0.2964 - val_f_2: 0.4082 - val_acc: 0.8991\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2759 - f_2: 0.4252 - acc: 0.9107 - val_loss: 0.2961 - val_f_2: 0.4042 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2758 - f_2: 0.4229 - acc: 0.9117 - val_loss: 0.2986 - val_f_2: 0.3854 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4130 - acc: 0.9110 - val_loss: 0.2955 - val_f_2: 0.4104 - val_acc: 0.8985\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4276 - acc: 0.9116 - val_loss: 0.2959 - val_f_2: 0.3808 - val_acc: 0.8997\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2740 - f_2: 0.4261 - acc: 0.9126 - val_loss: 0.2975 - val_f_2: 0.3836 - val_acc: 0.8991\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2759 - f_2: 0.4164 - acc: 0.9116 - val_loss: 0.2967 - val_f_2: 0.4104 - val_acc: 0.8982\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2740 - f_2: 0.4127 - acc: 0.9112 - val_loss: 0.3011 - val_f_2: 0.3977 - val_acc: 0.8979\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2741 - f_2: 0.4370 - acc: 0.9131 - val_loss: 0.2991 - val_f_2: 0.3862 - val_acc: 0.8997\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2737 - f_2: 0.4258 - acc: 0.9111 - val_loss: 0.2979 - val_f_2: 0.3965 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2729 - f_2: 0.4356 - acc: 0.9123 - val_loss: 0.3002 - val_f_2: 0.4371 - val_acc: 0.8944\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2729 - f_2: 0.4379 - acc: 0.9116 - val_loss: 0.2981 - val_f_2: 0.4050 - val_acc: 0.8965\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2741 - f_2: 0.4394 - acc: 0.9119 - val_loss: 0.2968 - val_f_2: 0.3759 - val_acc: 0.8976\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2723 - f_2: 0.4403 - acc: 0.9119 - val_loss: 0.2978 - val_f_2: 0.3971 - val_acc: 0.8988\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2720 - f_2: 0.4362 - acc: 0.9123 - val_loss: 0.2991 - val_f_2: 0.4197 - val_acc: 0.8965\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.001, total=  31.2s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4640 - f_2: 0.0233 - acc: 0.8461 - val_loss: 0.3684 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3549 - f_2: 0.1356 - acc: 0.8867 - val_loss: 0.3314 - val_f_2: 0.2813 - val_acc: 0.8994\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3342 - f_2: 0.2460 - acc: 0.8954 - val_loss: 0.3169 - val_f_2: 0.3035 - val_acc: 0.9009\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2905 - acc: 0.8973 - val_loss: 0.3104 - val_f_2: 0.3447 - val_acc: 0.9018\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3142 - f_2: 0.3100 - acc: 0.8992 - val_loss: 0.3061 - val_f_2: 0.3291 - val_acc: 0.9006\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.3295 - acc: 0.9012 - val_loss: 0.3039 - val_f_2: 0.3294 - val_acc: 0.9015\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3466 - acc: 0.9029 - val_loss: 0.3004 - val_f_2: 0.3565 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3632 - acc: 0.9041 - val_loss: 0.2997 - val_f_2: 0.3532 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3759 - acc: 0.9040 - val_loss: 0.2973 - val_f_2: 0.3782 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.3678 - acc: 0.9051 - val_loss: 0.2986 - val_f_2: 0.3688 - val_acc: 0.9009\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.3715 - acc: 0.9052 - val_loss: 0.2973 - val_f_2: 0.3794 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2960 - f_2: 0.3703 - acc: 0.9048 - val_loss: 0.2968 - val_f_2: 0.3985 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.3981 - acc: 0.9082 - val_loss: 0.2978 - val_f_2: 0.4308 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2913 - f_2: 0.3932 - acc: 0.9062 - val_loss: 0.2958 - val_f_2: 0.3783 - val_acc: 0.9012\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.3958 - acc: 0.9078 - val_loss: 0.2955 - val_f_2: 0.3957 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2889 - f_2: 0.3902 - acc: 0.9060 - val_loss: 0.2961 - val_f_2: 0.3649 - val_acc: 0.9009\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.3912 - acc: 0.9064 - val_loss: 0.2942 - val_f_2: 0.4046 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.4119 - acc: 0.9079 - val_loss: 0.2948 - val_f_2: 0.3871 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2872 - f_2: 0.3933 - acc: 0.9062 - val_loss: 0.2946 - val_f_2: 0.4267 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2851 - f_2: 0.4185 - acc: 0.9084 - val_loss: 0.2947 - val_f_2: 0.3896 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2855 - f_2: 0.4128 - acc: 0.9083 - val_loss: 0.2961 - val_f_2: 0.3784 - val_acc: 0.8988\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.4022 - acc: 0.9076 - val_loss: 0.2958 - val_f_2: 0.4272 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.4027 - acc: 0.9072 - val_loss: 0.2974 - val_f_2: 0.3450 - val_acc: 0.8982\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2820 - f_2: 0.4217 - acc: 0.9091 - val_loss: 0.2951 - val_f_2: 0.4005 - val_acc: 0.8988\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4214 - acc: 0.9096 - val_loss: 0.2976 - val_f_2: 0.3828 - val_acc: 0.8982\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2824 - f_2: 0.4228 - acc: 0.9084 - val_loss: 0.2948 - val_f_2: 0.3797 - val_acc: 0.8985\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2799 - f_2: 0.4331 - acc: 0.9101 - val_loss: 0.2947 - val_f_2: 0.4398 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4270 - acc: 0.9098 - val_loss: 0.2954 - val_f_2: 0.4243 - val_acc: 0.9012\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2795 - f_2: 0.4259 - acc: 0.9097 - val_loss: 0.2957 - val_f_2: 0.4260 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2785 - f_2: 0.4334 - acc: 0.9085 - val_loss: 0.2971 - val_f_2: 0.3637 - val_acc: 0.8973\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2780 - f_2: 0.4305 - acc: 0.9104 - val_loss: 0.2964 - val_f_2: 0.4450 - val_acc: 0.9009\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2768 - f_2: 0.4426 - acc: 0.9108 - val_loss: 0.2949 - val_f_2: 0.4264 - val_acc: 0.9012\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2762 - f_2: 0.4479 - acc: 0.9091 - val_loss: 0.2950 - val_f_2: 0.4349 - val_acc: 0.9018\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2752 - f_2: 0.4430 - acc: 0.9107 - val_loss: 0.2951 - val_f_2: 0.4292 - val_acc: 0.9012\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2759 - f_2: 0.4442 - acc: 0.9103 - val_loss: 0.2973 - val_f_2: 0.4559 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2742 - f_2: 0.4544 - acc: 0.9113 - val_loss: 0.2994 - val_f_2: 0.4008 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2753 - f_2: 0.4496 - acc: 0.9107 - val_loss: 0.2970 - val_f_2: 0.4189 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2755 - f_2: 0.4449 - acc: 0.9105 - val_loss: 0.2953 - val_f_2: 0.4306 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2754 - f_2: 0.4503 - acc: 0.9104 - val_loss: 0.2968 - val_f_2: 0.4242 - val_acc: 0.8979\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2742 - f_2: 0.4524 - acc: 0.9104 - val_loss: 0.2981 - val_f_2: 0.4565 - val_acc: 0.9003\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2733 - f_2: 0.4645 - acc: 0.9115 - val_loss: 0.2979 - val_f_2: 0.4438 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2728 - f_2: 0.4567 - acc: 0.9113 - val_loss: 0.3013 - val_f_2: 0.4698 - val_acc: 0.8968\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2724 - f_2: 0.4623 - acc: 0.9122 - val_loss: 0.2955 - val_f_2: 0.4245 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2715 - f_2: 0.4621 - acc: 0.9118 - val_loss: 0.2989 - val_f_2: 0.4319 - val_acc: 0.8988\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2719 - f_2: 0.4620 - acc: 0.9126 - val_loss: 0.2970 - val_f_2: 0.4597 - val_acc: 0.9012\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2728 - f_2: 0.4690 - acc: 0.9125 - val_loss: 0.2971 - val_f_2: 0.4543 - val_acc: 0.8997\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2720 - f_2: 0.4603 - acc: 0.9125 - val_loss: 0.2962 - val_f_2: 0.4286 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2712 - f_2: 0.4742 - acc: 0.9121 - val_loss: 0.2966 - val_f_2: 0.4522 - val_acc: 0.8991\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2708 - f_2: 0.4718 - acc: 0.9132 - val_loss: 0.2970 - val_f_2: 0.4474 - val_acc: 0.8973\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2699 - f_2: 0.4725 - acc: 0.9133 - val_loss: 0.2988 - val_f_2: 0.4563 - val_acc: 0.8962\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.001, total=  31.5s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4676 - f_2: 0.0254 - acc: 0.8428 - val_loss: 0.3475 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3650 - f_2: 0.1026 - acc: 0.8798 - val_loss: 0.3186 - val_f_2: 0.2610 - val_acc: 0.9065\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3395 - f_2: 0.2353 - acc: 0.8922 - val_loss: 0.3017 - val_f_2: 0.3291 - val_acc: 0.9106\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3287 - f_2: 0.2668 - acc: 0.8942 - val_loss: 0.2959 - val_f_2: 0.3644 - val_acc: 0.9109\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.3145 - acc: 0.8978 - val_loss: 0.2928 - val_f_2: 0.3694 - val_acc: 0.9106\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.3470 - acc: 0.8999 - val_loss: 0.2903 - val_f_2: 0.3607 - val_acc: 0.9106\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3355 - acc: 0.8998 - val_loss: 0.2870 - val_f_2: 0.3724 - val_acc: 0.9088\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3546 - acc: 0.9012 - val_loss: 0.2856 - val_f_2: 0.3818 - val_acc: 0.9109\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3567 - acc: 0.9015 - val_loss: 0.2839 - val_f_2: 0.3683 - val_acc: 0.9088\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3618 - acc: 0.9015 - val_loss: 0.2815 - val_f_2: 0.3734 - val_acc: 0.9091\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3533 - acc: 0.9021 - val_loss: 0.2798 - val_f_2: 0.3722 - val_acc: 0.9094\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3561 - acc: 0.9005 - val_loss: 0.2795 - val_f_2: 0.3587 - val_acc: 0.9103\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3760 - acc: 0.9027 - val_loss: 0.2787 - val_f_2: 0.3671 - val_acc: 0.9103\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.3843 - acc: 0.9022 - val_loss: 0.2777 - val_f_2: 0.3513 - val_acc: 0.9091\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3729 - acc: 0.9016 - val_loss: 0.2778 - val_f_2: 0.3661 - val_acc: 0.9088\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2954 - f_2: 0.3836 - acc: 0.9026 - val_loss: 0.2767 - val_f_2: 0.3610 - val_acc: 0.9080\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2937 - f_2: 0.3782 - acc: 0.9032 - val_loss: 0.2795 - val_f_2: 0.4172 - val_acc: 0.9109\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2923 - f_2: 0.3921 - acc: 0.9020 - val_loss: 0.2765 - val_f_2: 0.3705 - val_acc: 0.9088\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.3920 - acc: 0.9042 - val_loss: 0.2768 - val_f_2: 0.3902 - val_acc: 0.9100\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.4029 - acc: 0.9034 - val_loss: 0.2764 - val_f_2: 0.3875 - val_acc: 0.9100\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3887 - acc: 0.9035 - val_loss: 0.2754 - val_f_2: 0.3960 - val_acc: 0.9109\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2894 - f_2: 0.3856 - acc: 0.9030 - val_loss: 0.2771 - val_f_2: 0.4123 - val_acc: 0.9097\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2890 - f_2: 0.4079 - acc: 0.9048 - val_loss: 0.2770 - val_f_2: 0.4145 - val_acc: 0.9097\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2880 - f_2: 0.4112 - acc: 0.9036 - val_loss: 0.2740 - val_f_2: 0.3911 - val_acc: 0.9106\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2855 - f_2: 0.4087 - acc: 0.9046 - val_loss: 0.2778 - val_f_2: 0.4095 - val_acc: 0.9077\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.4045 - acc: 0.9040 - val_loss: 0.2743 - val_f_2: 0.3985 - val_acc: 0.9094\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4107 - acc: 0.9035 - val_loss: 0.2771 - val_f_2: 0.4137 - val_acc: 0.9088\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.4195 - acc: 0.9039 - val_loss: 0.2762 - val_f_2: 0.4255 - val_acc: 0.9074\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.4147 - acc: 0.9044 - val_loss: 0.2740 - val_f_2: 0.3955 - val_acc: 0.9091\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2828 - f_2: 0.4260 - acc: 0.9048 - val_loss: 0.2752 - val_f_2: 0.4010 - val_acc: 0.9094\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4382 - acc: 0.9064 - val_loss: 0.2809 - val_f_2: 0.4225 - val_acc: 0.9041\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2821 - f_2: 0.4328 - acc: 0.9053 - val_loss: 0.2794 - val_f_2: 0.4482 - val_acc: 0.9056\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2818 - f_2: 0.4281 - acc: 0.9050 - val_loss: 0.2776 - val_f_2: 0.4305 - val_acc: 0.9056\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2823 - f_2: 0.4358 - acc: 0.9047 - val_loss: 0.2743 - val_f_2: 0.4025 - val_acc: 0.9088\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2803 - f_2: 0.4337 - acc: 0.9060 - val_loss: 0.2742 - val_f_2: 0.4048 - val_acc: 0.9088\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2806 - f_2: 0.4409 - acc: 0.9070 - val_loss: 0.2747 - val_f_2: 0.4243 - val_acc: 0.9053\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2793 - f_2: 0.4515 - acc: 0.9084 - val_loss: 0.2736 - val_f_2: 0.4071 - val_acc: 0.9083\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2796 - f_2: 0.4464 - acc: 0.9057 - val_loss: 0.2748 - val_f_2: 0.4138 - val_acc: 0.9041\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2792 - f_2: 0.4501 - acc: 0.9067 - val_loss: 0.2751 - val_f_2: 0.3987 - val_acc: 0.9097\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2794 - f_2: 0.4399 - acc: 0.9057 - val_loss: 0.2751 - val_f_2: 0.4070 - val_acc: 0.9047\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2787 - f_2: 0.4463 - acc: 0.9065 - val_loss: 0.2740 - val_f_2: 0.4140 - val_acc: 0.9050\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2791 - f_2: 0.4496 - acc: 0.9063 - val_loss: 0.2746 - val_f_2: 0.3904 - val_acc: 0.9077\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2784 - f_2: 0.4443 - acc: 0.9072 - val_loss: 0.2752 - val_f_2: 0.4237 - val_acc: 0.9041\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2777 - f_2: 0.4380 - acc: 0.9050 - val_loss: 0.2752 - val_f_2: 0.4046 - val_acc: 0.9044\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2777 - f_2: 0.4615 - acc: 0.9080 - val_loss: 0.2752 - val_f_2: 0.3977 - val_acc: 0.9086\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2779 - f_2: 0.4618 - acc: 0.9077 - val_loss: 0.2748 - val_f_2: 0.4054 - val_acc: 0.9062\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4621 - acc: 0.9064 - val_loss: 0.2779 - val_f_2: 0.4252 - val_acc: 0.9029\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2770 - f_2: 0.4671 - acc: 0.9065 - val_loss: 0.2737 - val_f_2: 0.4073 - val_acc: 0.9074\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2753 - f_2: 0.4672 - acc: 0.9076 - val_loss: 0.2761 - val_f_2: 0.4081 - val_acc: 0.9056\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2756 - f_2: 0.4575 - acc: 0.9072 - val_loss: 0.2767 - val_f_2: 0.4243 - val_acc: 0.9053\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.001, total=  32.5s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7541 - f_2: 0.0194 - acc: 0.8562 - val_loss: 0.5040 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4259 - f_2: 1.3985e-08 - acc: 0.8753 - val_loss: 0.3838 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3703 - f_2: 1.3720e-08 - acc: 0.8753 - val_loss: 0.3614 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3552 - f_2: 1.3995e-08 - acc: 0.8753 - val_loss: 0.3500 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3454 - f_2: 1.3806e-08 - acc: 0.8753 - val_loss: 0.3408 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3403 - f_2: 1.4601e-08 - acc: 0.8753 - val_loss: 0.3374 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3359 - f_2: 1.3974e-08 - acc: 0.8753 - val_loss: 0.3335 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3317 - f_2: 0.1000 - acc: 0.8861 - val_loss: 0.3308 - val_f_2: 0.1050 - val_acc: 0.8788\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.1521 - acc: 0.8908 - val_loss: 0.3288 - val_f_2: 0.1930 - val_acc: 0.8888\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.1864 - acc: 0.8934 - val_loss: 0.3278 - val_f_2: 0.2454 - val_acc: 0.8953\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.1940 - acc: 0.8948 - val_loss: 0.3259 - val_f_2: 0.2553 - val_acc: 0.8962\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3252 - f_2: 0.1953 - acc: 0.8945 - val_loss: 0.3237 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.2099 - acc: 0.8964 - val_loss: 0.3225 - val_f_2: 0.2834 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3207 - f_2: 0.2179 - acc: 0.8971 - val_loss: 0.3215 - val_f_2: 0.2834 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.2029 - acc: 0.8957 - val_loss: 0.3207 - val_f_2: 0.2996 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.2257 - acc: 0.8974 - val_loss: 0.3209 - val_f_2: 0.2885 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.2210 - acc: 0.8967 - val_loss: 0.3201 - val_f_2: 0.2866 - val_acc: 0.8988\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.2449 - acc: 0.8991 - val_loss: 0.3173 - val_f_2: 0.2957 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3164 - f_2: 0.2274 - acc: 0.8972 - val_loss: 0.3167 - val_f_2: 0.3127 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.2484 - acc: 0.8978 - val_loss: 0.3157 - val_f_2: 0.3020 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.2326 - acc: 0.8971 - val_loss: 0.3156 - val_f_2: 0.3035 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.2360 - acc: 0.8967 - val_loss: 0.3162 - val_f_2: 0.3287 - val_acc: 0.8988\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.2359 - acc: 0.8980 - val_loss: 0.3139 - val_f_2: 0.3095 - val_acc: 0.9015\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.2410 - acc: 0.8975 - val_loss: 0.3136 - val_f_2: 0.3035 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.2442 - acc: 0.8972 - val_loss: 0.3148 - val_f_2: 0.3161 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.2532 - acc: 0.8984 - val_loss: 0.3130 - val_f_2: 0.3114 - val_acc: 0.9018\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.2684 - acc: 0.9005 - val_loss: 0.3114 - val_f_2: 0.3113 - val_acc: 0.9015\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.2940 - acc: 0.9019 - val_loss: 0.3113 - val_f_2: 0.3405 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.2918 - acc: 0.9031 - val_loss: 0.3104 - val_f_2: 0.3283 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.2976 - acc: 0.9034 - val_loss: 0.3125 - val_f_2: 0.3114 - val_acc: 0.9018\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3167 - acc: 0.9046 - val_loss: 0.3107 - val_f_2: 0.3676 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3065 - acc: 0.9037 - val_loss: 0.3092 - val_f_2: 0.3309 - val_acc: 0.8994\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3146 - acc: 0.9042 - val_loss: 0.3095 - val_f_2: 0.3226 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3074 - acc: 0.9045 - val_loss: 0.3086 - val_f_2: 0.3370 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3245 - acc: 0.9043 - val_loss: 0.3085 - val_f_2: 0.3255 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3197 - acc: 0.9050 - val_loss: 0.3084 - val_f_2: 0.3552 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3058 - acc: 0.9023 - val_loss: 0.3082 - val_f_2: 0.3207 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3089 - acc: 0.9031 - val_loss: 0.3133 - val_f_2: 0.3113 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3070 - acc: 0.9037 - val_loss: 0.3075 - val_f_2: 0.3241 - val_acc: 0.9018\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.3188 - acc: 0.9051 - val_loss: 0.3090 - val_f_2: 0.3618 - val_acc: 0.8994\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3124 - acc: 0.9037 - val_loss: 0.3066 - val_f_2: 0.3368 - val_acc: 0.8997\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3272 - acc: 0.9064 - val_loss: 0.3073 - val_f_2: 0.3261 - val_acc: 0.9021\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3196 - acc: 0.9039 - val_loss: 0.3068 - val_f_2: 0.3192 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3144 - acc: 0.9056 - val_loss: 0.3061 - val_f_2: 0.3398 - val_acc: 0.8994\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3306 - acc: 0.9059 - val_loss: 0.3053 - val_f_2: 0.3235 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.3359 - acc: 0.9054 - val_loss: 0.3070 - val_f_2: 0.3600 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3391 - acc: 0.9054 - val_loss: 0.3067 - val_f_2: 0.3163 - val_acc: 0.9015\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3259 - acc: 0.9054 - val_loss: 0.3089 - val_f_2: 0.4132 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3351 - acc: 0.9048 - val_loss: 0.3054 - val_f_2: 0.3589 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.3335 - acc: 0.9056 - val_loss: 0.3047 - val_f_2: 0.3465 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.01, total=  32.1s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7585 - f_2: 0.0167 - acc: 0.8489 - val_loss: 0.5031 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4280 - f_2: 1.3330e-08 - acc: 0.8722 - val_loss: 0.3815 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3714 - f_2: 1.3464e-08 - acc: 0.8722 - val_loss: 0.3604 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3550 - f_2: 1.3498e-08 - acc: 0.8722 - val_loss: 0.3482 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3442 - f_2: 1.3739e-08 - acc: 0.8722 - val_loss: 0.3406 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3377 - f_2: 1.3461e-08 - acc: 0.8722 - val_loss: 0.3359 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3351 - f_2: 0.0885 - acc: 0.8814 - val_loss: 0.3328 - val_f_2: 0.2089 - val_acc: 0.8909\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3332 - f_2: 0.1625 - acc: 0.8894 - val_loss: 0.3311 - val_f_2: 0.2772 - val_acc: 0.8988\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3294 - f_2: 0.1968 - acc: 0.8925 - val_loss: 0.3281 - val_f_2: 0.2793 - val_acc: 0.8991\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3259 - f_2: 0.2023 - acc: 0.8921 - val_loss: 0.3261 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.2127 - acc: 0.8942 - val_loss: 0.3257 - val_f_2: 0.2969 - val_acc: 0.8997\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.2529 - acc: 0.8981 - val_loss: 0.3232 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.2580 - acc: 0.8983 - val_loss: 0.3218 - val_f_2: 0.2993 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.2722 - acc: 0.8996 - val_loss: 0.3206 - val_f_2: 0.2995 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3193 - f_2: 0.2790 - acc: 0.9001 - val_loss: 0.3191 - val_f_2: 0.2889 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3190 - f_2: 0.2661 - acc: 0.8986 - val_loss: 0.3180 - val_f_2: 0.3017 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3172 - f_2: 0.2869 - acc: 0.9009 - val_loss: 0.3169 - val_f_2: 0.2975 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3035 - acc: 0.9019 - val_loss: 0.3171 - val_f_2: 0.2946 - val_acc: 0.9003\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.2972 - acc: 0.9015 - val_loss: 0.3151 - val_f_2: 0.3205 - val_acc: 0.9006\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3055 - acc: 0.9016 - val_loss: 0.3151 - val_f_2: 0.3327 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3151 - acc: 0.9013 - val_loss: 0.3154 - val_f_2: 0.2947 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3029 - acc: 0.9019 - val_loss: 0.3178 - val_f_2: 0.3404 - val_acc: 0.9009\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3070 - acc: 0.9008 - val_loss: 0.3128 - val_f_2: 0.3324 - val_acc: 0.9018\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.3146 - acc: 0.9021 - val_loss: 0.3115 - val_f_2: 0.2967 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3026 - acc: 0.9012 - val_loss: 0.3109 - val_f_2: 0.2933 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3202 - acc: 0.9029 - val_loss: 0.3105 - val_f_2: 0.3245 - val_acc: 0.9012\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3072 - f_2: 0.3262 - acc: 0.9025 - val_loss: 0.3107 - val_f_2: 0.3517 - val_acc: 0.9021\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3148 - acc: 0.9011 - val_loss: 0.3117 - val_f_2: 0.3452 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3195 - acc: 0.9015 - val_loss: 0.3090 - val_f_2: 0.2965 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3143 - acc: 0.9020 - val_loss: 0.3080 - val_f_2: 0.3128 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3211 - acc: 0.9023 - val_loss: 0.3073 - val_f_2: 0.3029 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3216 - acc: 0.9022 - val_loss: 0.3076 - val_f_2: 0.3031 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3062 - f_2: 0.3077 - acc: 0.9012 - val_loss: 0.3069 - val_f_2: 0.3032 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3057 - f_2: 0.3140 - acc: 0.9012 - val_loss: 0.3061 - val_f_2: 0.3353 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3210 - acc: 0.9017 - val_loss: 0.3074 - val_f_2: 0.3798 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3389 - acc: 0.9031 - val_loss: 0.3065 - val_f_2: 0.3445 - val_acc: 0.9015\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3185 - acc: 0.9012 - val_loss: 0.3077 - val_f_2: 0.3851 - val_acc: 0.9027\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3314 - acc: 0.9026 - val_loss: 0.3043 - val_f_2: 0.3631 - val_acc: 0.9021\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3264 - acc: 0.9025 - val_loss: 0.3043 - val_f_2: 0.3388 - val_acc: 0.9012\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3278 - acc: 0.9026 - val_loss: 0.3069 - val_f_2: 0.3072 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2983 - f_2: 0.3382 - acc: 0.9043 - val_loss: 0.3050 - val_f_2: 0.3408 - val_acc: 0.9021\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3339 - acc: 0.9031 - val_loss: 0.3029 - val_f_2: 0.3481 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3265 - acc: 0.9024 - val_loss: 0.3035 - val_f_2: 0.3267 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3387 - acc: 0.9043 - val_loss: 0.3032 - val_f_2: 0.3109 - val_acc: 0.9021\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2989 - f_2: 0.3424 - acc: 0.9037 - val_loss: 0.3023 - val_f_2: 0.3506 - val_acc: 0.9027\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.3440 - acc: 0.9037 - val_loss: 0.3052 - val_f_2: 0.3160 - val_acc: 0.9015\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.3408 - acc: 0.9040 - val_loss: 0.3031 - val_f_2: 0.3409 - val_acc: 0.9035\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3409 - acc: 0.9043 - val_loss: 0.3036 - val_f_2: 0.4163 - val_acc: 0.9029\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.3336 - acc: 0.9035 - val_loss: 0.3045 - val_f_2: 0.3427 - val_acc: 0.9021\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2991 - f_2: 0.3333 - acc: 0.9036 - val_loss: 0.3011 - val_f_2: 0.3454 - val_acc: 0.9018\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.01, total=  32.2s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7605 - f_2: 0.0166 - acc: 0.8506 - val_loss: 0.4872 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4329 - f_2: 1.3629e-08 - acc: 0.8691 - val_loss: 0.3630 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3768 - f_2: 1.3479e-08 - acc: 0.8691 - val_loss: 0.3423 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3614 - f_2: 1.3532e-08 - acc: 0.8691 - val_loss: 0.3327 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3529 - f_2: 1.3238e-08 - acc: 0.8691 - val_loss: 0.3248 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3451 - f_2: 1.3455e-08 - acc: 0.8691 - val_loss: 0.3217 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3434 - f_2: 0.0630 - acc: 0.8758 - val_loss: 0.3184 - val_f_2: 0.0646 - val_acc: 0.8861\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3372 - f_2: 0.1480 - acc: 0.8849 - val_loss: 0.3152 - val_f_2: 0.2799 - val_acc: 0.9083\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3380 - f_2: 0.1738 - acc: 0.8872 - val_loss: 0.3120 - val_f_2: 0.2841 - val_acc: 0.9088\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3341 - f_2: 0.1978 - acc: 0.8895 - val_loss: 0.3102 - val_f_2: 0.2771 - val_acc: 0.9080\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3322 - f_2: 0.2056 - acc: 0.8908 - val_loss: 0.3078 - val_f_2: 0.2739 - val_acc: 0.9077\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3311 - f_2: 0.2320 - acc: 0.8934 - val_loss: 0.3065 - val_f_2: 0.3049 - val_acc: 0.9103\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.2473 - acc: 0.8942 - val_loss: 0.3047 - val_f_2: 0.3054 - val_acc: 0.9109\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3285 - f_2: 0.2545 - acc: 0.8960 - val_loss: 0.3054 - val_f_2: 0.3186 - val_acc: 0.9106\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3284 - f_2: 0.2504 - acc: 0.8937 - val_loss: 0.3034 - val_f_2: 0.3049 - val_acc: 0.9103\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.2635 - acc: 0.8967 - val_loss: 0.3013 - val_f_2: 0.3076 - val_acc: 0.9106\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3227 - f_2: 0.2762 - acc: 0.8967 - val_loss: 0.3010 - val_f_2: 0.3124 - val_acc: 0.9106\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.2784 - acc: 0.8970 - val_loss: 0.3014 - val_f_2: 0.3074 - val_acc: 0.9103\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3230 - f_2: 0.2787 - acc: 0.8967 - val_loss: 0.3006 - val_f_2: 0.3342 - val_acc: 0.9094\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2893 - acc: 0.8980 - val_loss: 0.2977 - val_f_2: 0.3189 - val_acc: 0.9109\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.3107 - acc: 0.8977 - val_loss: 0.2985 - val_f_2: 0.3231 - val_acc: 0.9109\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3185 - f_2: 0.3039 - acc: 0.8982 - val_loss: 0.2958 - val_f_2: 0.3121 - val_acc: 0.9109\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3191 - f_2: 0.2918 - acc: 0.8975 - val_loss: 0.2969 - val_f_2: 0.3252 - val_acc: 0.9103\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.3045 - acc: 0.8970 - val_loss: 0.2974 - val_f_2: 0.3255 - val_acc: 0.9109\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.3060 - acc: 0.8992 - val_loss: 0.2945 - val_f_2: 0.3168 - val_acc: 0.9109\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3171 - f_2: 0.2978 - acc: 0.8976 - val_loss: 0.2935 - val_f_2: 0.3186 - val_acc: 0.9106\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.2986 - acc: 0.8965 - val_loss: 0.2979 - val_f_2: 0.3490 - val_acc: 0.9091\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3144 - acc: 0.8989 - val_loss: 0.2936 - val_f_2: 0.3296 - val_acc: 0.9097\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3053 - acc: 0.8977 - val_loss: 0.2932 - val_f_2: 0.3123 - val_acc: 0.9106\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.3041 - acc: 0.8974 - val_loss: 0.2977 - val_f_2: 0.3630 - val_acc: 0.9077\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3126 - f_2: 0.3111 - acc: 0.8982 - val_loss: 0.2928 - val_f_2: 0.3365 - val_acc: 0.9088\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3135 - f_2: 0.3067 - acc: 0.8981 - val_loss: 0.2907 - val_f_2: 0.3250 - val_acc: 0.9100\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3059 - acc: 0.8984 - val_loss: 0.2905 - val_f_2: 0.3164 - val_acc: 0.9100\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3191 - acc: 0.8995 - val_loss: 0.2904 - val_f_2: 0.3369 - val_acc: 0.9094\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3187 - acc: 0.8982 - val_loss: 0.2902 - val_f_2: 0.3342 - val_acc: 0.9106\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.3282 - acc: 0.8990 - val_loss: 0.2911 - val_f_2: 0.3380 - val_acc: 0.9080\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.3279 - acc: 0.8983 - val_loss: 0.2886 - val_f_2: 0.3269 - val_acc: 0.9100\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3132 - acc: 0.8996 - val_loss: 0.2886 - val_f_2: 0.3392 - val_acc: 0.9097\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3179 - acc: 0.8987 - val_loss: 0.2878 - val_f_2: 0.3237 - val_acc: 0.9112\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3090 - f_2: 0.3158 - acc: 0.8981 - val_loss: 0.2879 - val_f_2: 0.3342 - val_acc: 0.9109\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.3211 - acc: 0.8993 - val_loss: 0.2878 - val_f_2: 0.3408 - val_acc: 0.9100\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3212 - acc: 0.8981 - val_loss: 0.2884 - val_f_2: 0.3419 - val_acc: 0.9088\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3232 - acc: 0.8987 - val_loss: 0.2864 - val_f_2: 0.3147 - val_acc: 0.9109\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3192 - acc: 0.8987 - val_loss: 0.2977 - val_f_2: 0.3904 - val_acc: 0.9091\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3089 - f_2: 0.3111 - acc: 0.8978 - val_loss: 0.2874 - val_f_2: 0.3396 - val_acc: 0.9091\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3204 - acc: 0.8984 - val_loss: 0.2868 - val_f_2: 0.3173 - val_acc: 0.9112\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3343 - acc: 0.8998 - val_loss: 0.2858 - val_f_2: 0.3416 - val_acc: 0.9097\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3050 - f_2: 0.3333 - acc: 0.9007 - val_loss: 0.2868 - val_f_2: 0.3669 - val_acc: 0.9094\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.3266 - acc: 0.8978 - val_loss: 0.2849 - val_f_2: 0.3334 - val_acc: 0.9091\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.3278 - acc: 0.8991 - val_loss: 0.2890 - val_f_2: 0.3801 - val_acc: 0.9086\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.01, total=  31.2s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4673 - f_2: 0.0168 - acc: 0.8575 - val_loss: 0.3777 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3587 - f_2: 1.3938e-08 - acc: 0.8753 - val_loss: 0.3423 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3383 - f_2: 1.3881e-08 - acc: 0.8753 - val_loss: 0.3317 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.0140 - acc: 0.8767 - val_loss: 0.3298 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.2111 - acc: 0.8973 - val_loss: 0.3242 - val_f_2: 0.3025 - val_acc: 0.8994\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.2532 - acc: 0.9015 - val_loss: 0.3216 - val_f_2: 0.3047 - val_acc: 0.9003\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.2703 - acc: 0.9015 - val_loss: 0.3196 - val_f_2: 0.3163 - val_acc: 0.9006\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3149 - f_2: 0.2860 - acc: 0.9035 - val_loss: 0.3179 - val_f_2: 0.3294 - val_acc: 0.8994\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.2987 - acc: 0.9031 - val_loss: 0.3171 - val_f_2: 0.3322 - val_acc: 0.9003\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.3191 - acc: 0.9035 - val_loss: 0.3175 - val_f_2: 0.3184 - val_acc: 0.8991\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3223 - acc: 0.9046 - val_loss: 0.3139 - val_f_2: 0.3777 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3347 - acc: 0.9049 - val_loss: 0.3121 - val_f_2: 0.3605 - val_acc: 0.9006\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3364 - acc: 0.9048 - val_loss: 0.3168 - val_f_2: 0.3333 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3005 - f_2: 0.3350 - acc: 0.9046 - val_loss: 0.3098 - val_f_2: 0.3609 - val_acc: 0.8982\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3511 - acc: 0.9056 - val_loss: 0.3105 - val_f_2: 0.3968 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3465 - acc: 0.9057 - val_loss: 0.3111 - val_f_2: 0.3567 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.3461 - acc: 0.9055 - val_loss: 0.3133 - val_f_2: 0.3663 - val_acc: 0.8991\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.3606 - acc: 0.9048 - val_loss: 0.3080 - val_f_2: 0.3821 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2927 - f_2: 0.3639 - acc: 0.9062 - val_loss: 0.3090 - val_f_2: 0.3910 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2916 - f_2: 0.3619 - acc: 0.9059 - val_loss: 0.3075 - val_f_2: 0.3850 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.3610 - acc: 0.9079 - val_loss: 0.3079 - val_f_2: 0.3877 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.3752 - acc: 0.9081 - val_loss: 0.3058 - val_f_2: 0.4006 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2896 - f_2: 0.3724 - acc: 0.9070 - val_loss: 0.3055 - val_f_2: 0.3832 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2891 - f_2: 0.3634 - acc: 0.9065 - val_loss: 0.3070 - val_f_2: 0.3752 - val_acc: 0.8994\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2883 - f_2: 0.3731 - acc: 0.9077 - val_loss: 0.3072 - val_f_2: 0.3597 - val_acc: 0.8997\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.3771 - acc: 0.9092 - val_loss: 0.3162 - val_f_2: 0.3544 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.3740 - acc: 0.9082 - val_loss: 0.3065 - val_f_2: 0.4100 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.3847 - acc: 0.9075 - val_loss: 0.3083 - val_f_2: 0.3723 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.3809 - acc: 0.9085 - val_loss: 0.3135 - val_f_2: 0.3835 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2848 - f_2: 0.3852 - acc: 0.9071 - val_loss: 0.3068 - val_f_2: 0.3874 - val_acc: 0.8985\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4073 - acc: 0.9097 - val_loss: 0.3060 - val_f_2: 0.4039 - val_acc: 0.8985\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2817 - f_2: 0.3878 - acc: 0.9085 - val_loss: 0.3073 - val_f_2: 0.3965 - val_acc: 0.8997\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2810 - f_2: 0.3938 - acc: 0.9089 - val_loss: 0.3046 - val_f_2: 0.3990 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2810 - f_2: 0.3980 - acc: 0.9092 - val_loss: 0.3045 - val_f_2: 0.3976 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.3793 - acc: 0.9085 - val_loss: 0.3055 - val_f_2: 0.3960 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2794 - f_2: 0.3950 - acc: 0.9096 - val_loss: 0.3070 - val_f_2: 0.4030 - val_acc: 0.9003\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2798 - f_2: 0.3996 - acc: 0.9088 - val_loss: 0.3043 - val_f_2: 0.4013 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.3878 - acc: 0.9087 - val_loss: 0.3046 - val_f_2: 0.4183 - val_acc: 0.9003\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2781 - f_2: 0.3964 - acc: 0.9087 - val_loss: 0.3042 - val_f_2: 0.4042 - val_acc: 0.8994\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2771 - f_2: 0.3904 - acc: 0.9095 - val_loss: 0.3048 - val_f_2: 0.3982 - val_acc: 0.8994\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2783 - f_2: 0.4010 - acc: 0.9089 - val_loss: 0.3046 - val_f_2: 0.3914 - val_acc: 0.8982\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2767 - f_2: 0.3913 - acc: 0.9088 - val_loss: 0.3050 - val_f_2: 0.3851 - val_acc: 0.9006\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2777 - f_2: 0.4043 - acc: 0.9091 - val_loss: 0.3068 - val_f_2: 0.3974 - val_acc: 0.9003\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2762 - f_2: 0.4014 - acc: 0.9105 - val_loss: 0.3035 - val_f_2: 0.4084 - val_acc: 0.8991\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2755 - f_2: 0.3973 - acc: 0.9110 - val_loss: 0.3059 - val_f_2: 0.4046 - val_acc: 0.8991\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2745 - f_2: 0.4148 - acc: 0.9106 - val_loss: 0.3026 - val_f_2: 0.4123 - val_acc: 0.8976\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2757 - f_2: 0.4064 - acc: 0.9102 - val_loss: 0.3036 - val_f_2: 0.4061 - val_acc: 0.8982\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2765 - f_2: 0.4063 - acc: 0.9093 - val_loss: 0.3111 - val_f_2: 0.3729 - val_acc: 0.8991\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2750 - f_2: 0.3982 - acc: 0.9087 - val_loss: 0.3103 - val_f_2: 0.4044 - val_acc: 0.8997\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2749 - f_2: 0.3993 - acc: 0.9089 - val_loss: 0.3043 - val_f_2: 0.4151 - val_acc: 0.8982\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.001, total=  31.2s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4723 - f_2: 0.0156 - acc: 0.8538 - val_loss: 0.3794 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3612 - f_2: 1.3775e-08 - acc: 0.8722 - val_loss: 0.3445 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3404 - f_2: 1.3703e-08 - acc: 0.8722 - val_loss: 0.3334 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3297 - f_2: 0.0444 - acc: 0.8772 - val_loss: 0.3277 - val_f_2: 0.2697 - val_acc: 0.8979\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2257 - acc: 0.8953 - val_loss: 0.3245 - val_f_2: 0.2994 - val_acc: 0.9012\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.2684 - acc: 0.8992 - val_loss: 0.3212 - val_f_2: 0.3179 - val_acc: 0.9006\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2943 - acc: 0.9014 - val_loss: 0.3188 - val_f_2: 0.3186 - val_acc: 0.9006\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.3086 - acc: 0.9011 - val_loss: 0.3170 - val_f_2: 0.3301 - val_acc: 0.9021\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.3219 - acc: 0.9013 - val_loss: 0.3149 - val_f_2: 0.3725 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3466 - acc: 0.9018 - val_loss: 0.3136 - val_f_2: 0.3785 - val_acc: 0.9006\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3087 - f_2: 0.3428 - acc: 0.9023 - val_loss: 0.3129 - val_f_2: 0.3572 - val_acc: 0.8997\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3578 - acc: 0.9040 - val_loss: 0.3098 - val_f_2: 0.3873 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.3609 - acc: 0.9040 - val_loss: 0.3091 - val_f_2: 0.3771 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3523 - acc: 0.9028 - val_loss: 0.3070 - val_f_2: 0.3781 - val_acc: 0.9003\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2988 - f_2: 0.3598 - acc: 0.9037 - val_loss: 0.3074 - val_f_2: 0.3693 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.3596 - acc: 0.9044 - val_loss: 0.3062 - val_f_2: 0.3787 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3609 - acc: 0.9041 - val_loss: 0.3078 - val_f_2: 0.3675 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2964 - f_2: 0.3603 - acc: 0.9040 - val_loss: 0.3054 - val_f_2: 0.4203 - val_acc: 0.9027\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.3830 - acc: 0.9053 - val_loss: 0.3041 - val_f_2: 0.3880 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.3759 - acc: 0.9050 - val_loss: 0.3054 - val_f_2: 0.3812 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2923 - f_2: 0.3605 - acc: 0.9043 - val_loss: 0.3029 - val_f_2: 0.3960 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3761 - acc: 0.9052 - val_loss: 0.3055 - val_f_2: 0.3695 - val_acc: 0.9015\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.3836 - acc: 0.9065 - val_loss: 0.3044 - val_f_2: 0.3919 - val_acc: 0.9012\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.3896 - acc: 0.9060 - val_loss: 0.3051 - val_f_2: 0.3958 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2853 - f_2: 0.3919 - acc: 0.9068 - val_loss: 0.3093 - val_f_2: 0.3987 - val_acc: 0.9027\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2866 - f_2: 0.3981 - acc: 0.9082 - val_loss: 0.3028 - val_f_2: 0.4107 - val_acc: 0.8994\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.3895 - acc: 0.9055 - val_loss: 0.3043 - val_f_2: 0.3932 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2847 - f_2: 0.3913 - acc: 0.9060 - val_loss: 0.3017 - val_f_2: 0.4117 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2825 - f_2: 0.3967 - acc: 0.9074 - val_loss: 0.3018 - val_f_2: 0.3853 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.3822 - acc: 0.9053 - val_loss: 0.3003 - val_f_2: 0.4081 - val_acc: 0.9012\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4079 - acc: 0.9076 - val_loss: 0.3003 - val_f_2: 0.4257 - val_acc: 0.9018\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2824 - f_2: 0.4020 - acc: 0.9069 - val_loss: 0.3012 - val_f_2: 0.4014 - val_acc: 0.9021\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4001 - acc: 0.9065 - val_loss: 0.2998 - val_f_2: 0.4159 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2818 - f_2: 0.4067 - acc: 0.9069 - val_loss: 0.2998 - val_f_2: 0.4013 - val_acc: 0.9018\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2827 - f_2: 0.3945 - acc: 0.9049 - val_loss: 0.3053 - val_f_2: 0.3936 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2789 - f_2: 0.4016 - acc: 0.9068 - val_loss: 0.3001 - val_f_2: 0.3941 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2786 - f_2: 0.4114 - acc: 0.9069 - val_loss: 0.3038 - val_f_2: 0.3847 - val_acc: 0.9021\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2800 - f_2: 0.4026 - acc: 0.9065 - val_loss: 0.3005 - val_f_2: 0.4119 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2778 - f_2: 0.4194 - acc: 0.9086 - val_loss: 0.3052 - val_f_2: 0.4014 - val_acc: 0.9024\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4208 - acc: 0.9085 - val_loss: 0.3035 - val_f_2: 0.3894 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2767 - f_2: 0.4256 - acc: 0.9086 - val_loss: 0.3010 - val_f_2: 0.4250 - val_acc: 0.8991\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2768 - f_2: 0.4129 - acc: 0.9069 - val_loss: 0.2987 - val_f_2: 0.4156 - val_acc: 0.9003\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2782 - f_2: 0.4075 - acc: 0.9075 - val_loss: 0.3003 - val_f_2: 0.4127 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4155 - acc: 0.9079 - val_loss: 0.3013 - val_f_2: 0.4112 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2763 - f_2: 0.4266 - acc: 0.9082 - val_loss: 0.3046 - val_f_2: 0.4065 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2766 - f_2: 0.4153 - acc: 0.9083 - val_loss: 0.3022 - val_f_2: 0.4120 - val_acc: 0.8994\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2752 - f_2: 0.4199 - acc: 0.9087 - val_loss: 0.2999 - val_f_2: 0.4291 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4115 - acc: 0.9074 - val_loss: 0.3009 - val_f_2: 0.4405 - val_acc: 0.8994\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2734 - f_2: 0.4274 - acc: 0.9091 - val_loss: 0.3033 - val_f_2: 0.4003 - val_acc: 0.9021\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2735 - f_2: 0.4202 - acc: 0.9083 - val_loss: 0.3012 - val_f_2: 0.4212 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.001, total=  32.1s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4736 - f_2: 0.0162 - acc: 0.8517 - val_loss: 0.3571 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3649 - f_2: 1.3055e-08 - acc: 0.8691 - val_loss: 0.3240 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3451 - f_2: 1.3742e-08 - acc: 0.8691 - val_loss: 0.3135 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3369 - f_2: 0.0517 - acc: 0.8741 - val_loss: 0.3088 - val_f_2: 0.2343 - val_acc: 0.9032\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3295 - f_2: 0.2380 - acc: 0.8940 - val_loss: 0.3079 - val_f_2: 0.3477 - val_acc: 0.9106\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3274 - f_2: 0.2776 - acc: 0.8970 - val_loss: 0.3030 - val_f_2: 0.3070 - val_acc: 0.9094\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2913 - acc: 0.8976 - val_loss: 0.3020 - val_f_2: 0.3455 - val_acc: 0.9094\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3190 - f_2: 0.3074 - acc: 0.8982 - val_loss: 0.2985 - val_f_2: 0.3305 - val_acc: 0.9097\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.3149 - acc: 0.8990 - val_loss: 0.2983 - val_f_2: 0.3691 - val_acc: 0.9083\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3139 - f_2: 0.3325 - acc: 0.8987 - val_loss: 0.2957 - val_f_2: 0.3441 - val_acc: 0.9074\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3344 - acc: 0.8995 - val_loss: 0.2941 - val_f_2: 0.3538 - val_acc: 0.9071\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3540 - acc: 0.9007 - val_loss: 0.2924 - val_f_2: 0.3503 - val_acc: 0.9080\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3535 - acc: 0.9011 - val_loss: 0.2941 - val_f_2: 0.4005 - val_acc: 0.9088\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3691 - acc: 0.8993 - val_loss: 0.2928 - val_f_2: 0.3812 - val_acc: 0.9077\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.3683 - acc: 0.9009 - val_loss: 0.2889 - val_f_2: 0.3475 - val_acc: 0.9074\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3514 - acc: 0.9007 - val_loss: 0.2895 - val_f_2: 0.3582 - val_acc: 0.9071\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3700 - acc: 0.9015 - val_loss: 0.2901 - val_f_2: 0.3774 - val_acc: 0.9083\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.3747 - acc: 0.9011 - val_loss: 0.2887 - val_f_2: 0.4180 - val_acc: 0.9097\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3841 - acc: 0.9015 - val_loss: 0.2886 - val_f_2: 0.4147 - val_acc: 0.9091\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.3707 - acc: 0.9004 - val_loss: 0.2888 - val_f_2: 0.4244 - val_acc: 0.9088\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2940 - f_2: 0.3875 - acc: 0.9028 - val_loss: 0.2877 - val_f_2: 0.4119 - val_acc: 0.9094\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2968 - f_2: 0.3789 - acc: 0.8998 - val_loss: 0.2869 - val_f_2: 0.3807 - val_acc: 0.9062\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.3890 - acc: 0.9009 - val_loss: 0.2883 - val_f_2: 0.4121 - val_acc: 0.9068\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3909 - acc: 0.9023 - val_loss: 0.2853 - val_f_2: 0.3841 - val_acc: 0.9086\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3790 - acc: 0.9009 - val_loss: 0.2869 - val_f_2: 0.4183 - val_acc: 0.9086\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2914 - f_2: 0.3871 - acc: 0.9021 - val_loss: 0.2859 - val_f_2: 0.3995 - val_acc: 0.9071\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2889 - f_2: 0.4003 - acc: 0.9026 - val_loss: 0.2861 - val_f_2: 0.4134 - val_acc: 0.9074\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.4088 - acc: 0.9025 - val_loss: 0.2847 - val_f_2: 0.4066 - val_acc: 0.9077\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2881 - f_2: 0.3983 - acc: 0.9032 - val_loss: 0.2845 - val_f_2: 0.3978 - val_acc: 0.9077\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2880 - f_2: 0.3982 - acc: 0.9026 - val_loss: 0.2842 - val_f_2: 0.4010 - val_acc: 0.9080\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2863 - f_2: 0.4031 - acc: 0.9031 - val_loss: 0.2853 - val_f_2: 0.4066 - val_acc: 0.9050\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2851 - f_2: 0.4085 - acc: 0.9032 - val_loss: 0.2864 - val_f_2: 0.3924 - val_acc: 0.9071\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.4129 - acc: 0.9042 - val_loss: 0.2867 - val_f_2: 0.3971 - val_acc: 0.9056\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2869 - f_2: 0.3948 - acc: 0.9017 - val_loss: 0.2849 - val_f_2: 0.4073 - val_acc: 0.9059\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4130 - acc: 0.9031 - val_loss: 0.2847 - val_f_2: 0.4201 - val_acc: 0.9062\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.4144 - acc: 0.9030 - val_loss: 0.2838 - val_f_2: 0.4053 - val_acc: 0.9077\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2827 - f_2: 0.4146 - acc: 0.9043 - val_loss: 0.2831 - val_f_2: 0.3991 - val_acc: 0.9074\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.4042 - acc: 0.9027 - val_loss: 0.2854 - val_f_2: 0.4398 - val_acc: 0.9044\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2821 - f_2: 0.4238 - acc: 0.9035 - val_loss: 0.2821 - val_f_2: 0.3950 - val_acc: 0.9071\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2824 - f_2: 0.4128 - acc: 0.9026 - val_loss: 0.2827 - val_f_2: 0.4233 - val_acc: 0.9041\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2824 - f_2: 0.4165 - acc: 0.9035 - val_loss: 0.2823 - val_f_2: 0.3998 - val_acc: 0.9053\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4165 - acc: 0.9030 - val_loss: 0.2828 - val_f_2: 0.4168 - val_acc: 0.9041\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4216 - acc: 0.9040 - val_loss: 0.2850 - val_f_2: 0.4335 - val_acc: 0.9029\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2791 - f_2: 0.4409 - acc: 0.9051 - val_loss: 0.2830 - val_f_2: 0.4067 - val_acc: 0.9035\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2786 - f_2: 0.4228 - acc: 0.9026 - val_loss: 0.2815 - val_f_2: 0.3914 - val_acc: 0.9074\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2787 - f_2: 0.4192 - acc: 0.9040 - val_loss: 0.2847 - val_f_2: 0.4293 - val_acc: 0.9032\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4322 - acc: 0.9031 - val_loss: 0.2828 - val_f_2: 0.3842 - val_acc: 0.9080\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2785 - f_2: 0.4109 - acc: 0.9037 - val_loss: 0.2813 - val_f_2: 0.4169 - val_acc: 0.9027\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2782 - f_2: 0.4282 - acc: 0.9048 - val_loss: 0.2842 - val_f_2: 0.4326 - val_acc: 0.9018\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2762 - f_2: 0.4379 - acc: 0.9051 - val_loss: 0.2807 - val_f_2: 0.4117 - val_acc: 0.9053\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.001, total=  32.1s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7806 - f_2: 0.0109 - acc: 0.8592 - val_loss: 0.5015 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4206 - f_2: 1.4136e-08 - acc: 0.8753 - val_loss: 0.3791 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3661 - f_2: 1.4134e-08 - acc: 0.8753 - val_loss: 0.3583 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3498 - f_2: 1.4052e-08 - acc: 0.8753 - val_loss: 0.3468 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3421 - f_2: 1.4463e-08 - acc: 0.8753 - val_loss: 0.3448 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3357 - f_2: 0.0111 - acc: 0.8767 - val_loss: 0.3398 - val_f_2: 0.2853 - val_acc: 0.8997\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3324 - f_2: 0.1591 - acc: 0.8924 - val_loss: 0.3335 - val_f_2: 0.2849 - val_acc: 0.8988\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3293 - f_2: 0.1899 - acc: 0.8944 - val_loss: 0.3299 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.2166 - acc: 0.8968 - val_loss: 0.3279 - val_f_2: 0.2855 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3242 - f_2: 0.2268 - acc: 0.8980 - val_loss: 0.3258 - val_f_2: 0.2868 - val_acc: 0.8994\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.2346 - acc: 0.8995 - val_loss: 0.3242 - val_f_2: 0.2920 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3185 - f_2: 0.2465 - acc: 0.9002 - val_loss: 0.3304 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.2530 - acc: 0.9006 - val_loss: 0.3207 - val_f_2: 0.3005 - val_acc: 0.8988\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2741 - acc: 0.9020 - val_loss: 0.3198 - val_f_2: 0.3041 - val_acc: 0.8991\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.2666 - acc: 0.9015 - val_loss: 0.3188 - val_f_2: 0.2997 - val_acc: 0.8991\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3139 - f_2: 0.2878 - acc: 0.9023 - val_loss: 0.3180 - val_f_2: 0.2936 - val_acc: 0.8997\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.2767 - acc: 0.9021 - val_loss: 0.3190 - val_f_2: 0.3024 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.2881 - acc: 0.9022 - val_loss: 0.3161 - val_f_2: 0.3124 - val_acc: 0.9006\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.2864 - acc: 0.9031 - val_loss: 0.3163 - val_f_2: 0.3602 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3089 - f_2: 0.3041 - acc: 0.9031 - val_loss: 0.3144 - val_f_2: 0.3458 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3009 - acc: 0.9034 - val_loss: 0.3133 - val_f_2: 0.3195 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3013 - acc: 0.9041 - val_loss: 0.3169 - val_f_2: 0.3106 - val_acc: 0.9009\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.2969 - acc: 0.9029 - val_loss: 0.3131 - val_f_2: 0.3553 - val_acc: 0.9015\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.2986 - acc: 0.9029 - val_loss: 0.3144 - val_f_2: 0.3066 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.2982 - acc: 0.9029 - val_loss: 0.3142 - val_f_2: 0.3320 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.3152 - acc: 0.9046 - val_loss: 0.3128 - val_f_2: 0.2960 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3038 - f_2: 0.3093 - acc: 0.9040 - val_loss: 0.3111 - val_f_2: 0.3142 - val_acc: 0.9009\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3015 - acc: 0.9031 - val_loss: 0.3103 - val_f_2: 0.3276 - val_acc: 0.9003\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3095 - acc: 0.9040 - val_loss: 0.3114 - val_f_2: 0.3717 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3291 - acc: 0.9050 - val_loss: 0.3214 - val_f_2: 0.2963 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.3083 - acc: 0.9037 - val_loss: 0.3088 - val_f_2: 0.3539 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3200 - acc: 0.9032 - val_loss: 0.3092 - val_f_2: 0.3084 - val_acc: 0.9009\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3005 - f_2: 0.3135 - acc: 0.9034 - val_loss: 0.3071 - val_f_2: 0.3208 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3106 - acc: 0.9032 - val_loss: 0.3086 - val_f_2: 0.3259 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.3230 - acc: 0.9046 - val_loss: 0.3095 - val_f_2: 0.3232 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3224 - acc: 0.9040 - val_loss: 0.3092 - val_f_2: 0.3958 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2977 - f_2: 0.3209 - acc: 0.9033 - val_loss: 0.3084 - val_f_2: 0.3357 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3315 - acc: 0.9062 - val_loss: 0.3147 - val_f_2: 0.3124 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2983 - f_2: 0.3319 - acc: 0.9046 - val_loss: 0.3093 - val_f_2: 0.3743 - val_acc: 0.9012\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3381 - acc: 0.9051 - val_loss: 0.3067 - val_f_2: 0.3829 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2968 - f_2: 0.3278 - acc: 0.9045 - val_loss: 0.3059 - val_f_2: 0.3514 - val_acc: 0.8994\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2956 - f_2: 0.3440 - acc: 0.9066 - val_loss: 0.3060 - val_f_2: 0.3714 - val_acc: 0.9006\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.3341 - acc: 0.9045 - val_loss: 0.3061 - val_f_2: 0.3704 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2951 - f_2: 0.3461 - acc: 0.9060 - val_loss: 0.3049 - val_f_2: 0.3147 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.3353 - acc: 0.9057 - val_loss: 0.3059 - val_f_2: 0.3912 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2928 - f_2: 0.3432 - acc: 0.9055 - val_loss: 0.3049 - val_f_2: 0.3350 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.3302 - acc: 0.9049 - val_loss: 0.3055 - val_f_2: 0.3184 - val_acc: 0.9012\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.3330 - acc: 0.9062 - val_loss: 0.3066 - val_f_2: 0.4246 - val_acc: 0.9018\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2934 - f_2: 0.3493 - acc: 0.9059 - val_loss: 0.3089 - val_f_2: 0.3141 - val_acc: 0.9006\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.3372 - acc: 0.9049 - val_loss: 0.3059 - val_f_2: 0.3921 - val_acc: 0.9003\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.01, total=  32.4s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7836 - f_2: 0.0128 - acc: 0.8563 - val_loss: 0.5025 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4206 - f_2: 1.3603e-08 - acc: 0.8722 - val_loss: 0.3783 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3677 - f_2: 1.4095e-08 - acc: 0.8722 - val_loss: 0.3561 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3491 - f_2: 1.3905e-08 - acc: 0.8722 - val_loss: 0.3458 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3424 - f_2: 1.3655e-08 - acc: 0.8722 - val_loss: 0.3395 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3345 - f_2: 0.1500 - acc: 0.8881 - val_loss: 0.3365 - val_f_2: 0.2707 - val_acc: 0.8979\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.2000 - acc: 0.8941 - val_loss: 0.3324 - val_f_2: 0.2555 - val_acc: 0.8962\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3293 - f_2: 0.2104 - acc: 0.8951 - val_loss: 0.3290 - val_f_2: 0.2865 - val_acc: 0.8991\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.2299 - acc: 0.8964 - val_loss: 0.3270 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.2586 - acc: 0.8989 - val_loss: 0.3286 - val_f_2: 0.3055 - val_acc: 0.8994\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2718 - acc: 0.9001 - val_loss: 0.3235 - val_f_2: 0.2970 - val_acc: 0.8991\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.2737 - acc: 0.9006 - val_loss: 0.3218 - val_f_2: 0.2883 - val_acc: 0.8991\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.2889 - acc: 0.9009 - val_loss: 0.3243 - val_f_2: 0.2953 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.2840 - acc: 0.9011 - val_loss: 0.3234 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.2913 - acc: 0.9024 - val_loss: 0.3182 - val_f_2: 0.3240 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.2941 - acc: 0.9015 - val_loss: 0.3171 - val_f_2: 0.2870 - val_acc: 0.8991\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.2956 - acc: 0.9022 - val_loss: 0.3168 - val_f_2: 0.3085 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.2987 - acc: 0.9020 - val_loss: 0.3159 - val_f_2: 0.3263 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3142 - acc: 0.9023 - val_loss: 0.3205 - val_f_2: 0.3046 - val_acc: 0.8994\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3219 - acc: 0.9037 - val_loss: 0.3152 - val_f_2: 0.3336 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3270 - acc: 0.9037 - val_loss: 0.3145 - val_f_2: 0.3144 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3042 - acc: 0.9012 - val_loss: 0.3123 - val_f_2: 0.3253 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3198 - acc: 0.9025 - val_loss: 0.3131 - val_f_2: 0.3106 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3324 - acc: 0.9034 - val_loss: 0.3108 - val_f_2: 0.3127 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.3251 - acc: 0.9031 - val_loss: 0.3099 - val_f_2: 0.3089 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3150 - acc: 0.9020 - val_loss: 0.3123 - val_f_2: 0.2964 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3164 - acc: 0.9022 - val_loss: 0.3086 - val_f_2: 0.3542 - val_acc: 0.9018\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3249 - acc: 0.9029 - val_loss: 0.3122 - val_f_2: 0.2998 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.3209 - acc: 0.9030 - val_loss: 0.3078 - val_f_2: 0.3075 - val_acc: 0.8994\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3197 - acc: 0.9023 - val_loss: 0.3157 - val_f_2: 0.3159 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3318 - acc: 0.9025 - val_loss: 0.3060 - val_f_2: 0.3162 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3384 - acc: 0.9041 - val_loss: 0.3079 - val_f_2: 0.3767 - val_acc: 0.9012\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.3391 - acc: 0.9035 - val_loss: 0.3059 - val_f_2: 0.3260 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3014 - f_2: 0.3462 - acc: 0.9047 - val_loss: 0.3084 - val_f_2: 0.4065 - val_acc: 0.9018\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3474 - acc: 0.9042 - val_loss: 0.3050 - val_f_2: 0.3276 - val_acc: 0.9018\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3471 - acc: 0.9048 - val_loss: 0.3060 - val_f_2: 0.3764 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3400 - acc: 0.9038 - val_loss: 0.3041 - val_f_2: 0.3499 - val_acc: 0.9021\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3448 - acc: 0.9051 - val_loss: 0.3046 - val_f_2: 0.3282 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2983 - f_2: 0.3419 - acc: 0.9050 - val_loss: 0.3059 - val_f_2: 0.3373 - val_acc: 0.9027\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3596 - acc: 0.9057 - val_loss: 0.3054 - val_f_2: 0.3130 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3548 - acc: 0.9046 - val_loss: 0.3173 - val_f_2: 0.3000 - val_acc: 0.9012\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3439 - acc: 0.9037 - val_loss: 0.3066 - val_f_2: 0.4163 - val_acc: 0.9015\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2963 - f_2: 0.3451 - acc: 0.9041 - val_loss: 0.3035 - val_f_2: 0.3591 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3568 - acc: 0.9045 - val_loss: 0.3051 - val_f_2: 0.3966 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.3546 - acc: 0.9043 - val_loss: 0.3025 - val_f_2: 0.3781 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2936 - f_2: 0.3613 - acc: 0.9051 - val_loss: 0.3014 - val_f_2: 0.3594 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3565 - acc: 0.9050 - val_loss: 0.3025 - val_f_2: 0.3789 - val_acc: 0.9015\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3466 - acc: 0.9043 - val_loss: 0.3047 - val_f_2: 0.4120 - val_acc: 0.9015\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2935 - f_2: 0.3587 - acc: 0.9039 - val_loss: 0.3017 - val_f_2: 0.3593 - val_acc: 0.9012\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.3627 - acc: 0.9051 - val_loss: 0.3018 - val_f_2: 0.3495 - val_acc: 0.9012\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.01, total=  31.7s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7909 - f_2: 0.0097 - acc: 0.8506 - val_loss: 0.4810 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4293 - f_2: 1.3154e-08 - acc: 0.8691 - val_loss: 0.3573 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3735 - f_2: 1.3106e-08 - acc: 0.8691 - val_loss: 0.3374 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3577 - f_2: 1.3126e-08 - acc: 0.8691 - val_loss: 0.3342 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3480 - f_2: 1.3092e-08 - acc: 0.8691 - val_loss: 0.3232 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3429 - f_2: 0.1305 - acc: 0.8835 - val_loss: 0.3174 - val_f_2: 0.2112 - val_acc: 0.9006\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3389 - f_2: 0.1852 - acc: 0.8888 - val_loss: 0.3175 - val_f_2: 0.3031 - val_acc: 0.9097\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3343 - f_2: 0.2043 - acc: 0.8915 - val_loss: 0.3126 - val_f_2: 0.2694 - val_acc: 0.9071\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3328 - f_2: 0.2282 - acc: 0.8936 - val_loss: 0.3103 - val_f_2: 0.2878 - val_acc: 0.9091\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.2379 - acc: 0.8943 - val_loss: 0.3091 - val_f_2: 0.3049 - val_acc: 0.9103\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3262 - f_2: 0.2593 - acc: 0.8966 - val_loss: 0.3059 - val_f_2: 0.3026 - val_acc: 0.9100\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3258 - f_2: 0.2699 - acc: 0.8968 - val_loss: 0.3043 - val_f_2: 0.3033 - val_acc: 0.9100\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.2714 - acc: 0.8971 - val_loss: 0.3030 - val_f_2: 0.3049 - val_acc: 0.9103\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3229 - f_2: 0.2728 - acc: 0.8970 - val_loss: 0.3035 - val_f_2: 0.3076 - val_acc: 0.9106\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2705 - acc: 0.8976 - val_loss: 0.3008 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.2879 - acc: 0.8978 - val_loss: 0.3008 - val_f_2: 0.3170 - val_acc: 0.9109\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3191 - f_2: 0.2852 - acc: 0.8969 - val_loss: 0.2984 - val_f_2: 0.3074 - val_acc: 0.9103\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.2988 - acc: 0.8995 - val_loss: 0.2980 - val_f_2: 0.3254 - val_acc: 0.9106\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.2927 - acc: 0.8987 - val_loss: 0.2976 - val_f_2: 0.3142 - val_acc: 0.9109\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3020 - acc: 0.8995 - val_loss: 0.2964 - val_f_2: 0.3235 - val_acc: 0.9109\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.3064 - acc: 0.8989 - val_loss: 0.2981 - val_f_2: 0.3404 - val_acc: 0.9080\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.3055 - acc: 0.8981 - val_loss: 0.2941 - val_f_2: 0.3218 - val_acc: 0.9118\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3058 - acc: 0.8988 - val_loss: 0.2946 - val_f_2: 0.3235 - val_acc: 0.9109\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3164 - acc: 0.8996 - val_loss: 0.2944 - val_f_2: 0.3389 - val_acc: 0.9077\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.3025 - acc: 0.8978 - val_loss: 0.2943 - val_f_2: 0.3520 - val_acc: 0.9083\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3114 - f_2: 0.3185 - acc: 0.8989 - val_loss: 0.2920 - val_f_2: 0.3299 - val_acc: 0.9103\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3168 - acc: 0.8997 - val_loss: 0.2914 - val_f_2: 0.3324 - val_acc: 0.9106\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.3250 - acc: 0.9005 - val_loss: 0.2915 - val_f_2: 0.3323 - val_acc: 0.9103\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3111 - acc: 0.8990 - val_loss: 0.2901 - val_f_2: 0.3287 - val_acc: 0.9118\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.3141 - acc: 0.8986 - val_loss: 0.2916 - val_f_2: 0.3395 - val_acc: 0.9074\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3304 - acc: 0.9001 - val_loss: 0.2929 - val_f_2: 0.3658 - val_acc: 0.9086\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3202 - acc: 0.8990 - val_loss: 0.2894 - val_f_2: 0.3324 - val_acc: 0.9106\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3058 - f_2: 0.3268 - acc: 0.9006 - val_loss: 0.2882 - val_f_2: 0.3322 - val_acc: 0.9103\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3197 - acc: 0.8986 - val_loss: 0.2909 - val_f_2: 0.3692 - val_acc: 0.9086\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3440 - acc: 0.9006 - val_loss: 0.2893 - val_f_2: 0.3095 - val_acc: 0.9103\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3245 - acc: 0.9002 - val_loss: 0.2886 - val_f_2: 0.3630 - val_acc: 0.9077\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3340 - acc: 0.8997 - val_loss: 0.2869 - val_f_2: 0.3329 - val_acc: 0.9109\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3257 - acc: 0.8992 - val_loss: 0.2869 - val_f_2: 0.3235 - val_acc: 0.9109\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3395 - acc: 0.9012 - val_loss: 0.2884 - val_f_2: 0.3704 - val_acc: 0.9080\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.3289 - acc: 0.8987 - val_loss: 0.2873 - val_f_2: 0.3523 - val_acc: 0.9080\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3369 - acc: 0.8990 - val_loss: 0.2876 - val_f_2: 0.3366 - val_acc: 0.9100\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3200 - acc: 0.8987 - val_loss: 0.2880 - val_f_2: 0.3714 - val_acc: 0.9071\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.3461 - acc: 0.9007 - val_loss: 0.2872 - val_f_2: 0.3519 - val_acc: 0.9094\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3376 - acc: 0.8997 - val_loss: 0.2868 - val_f_2: 0.3456 - val_acc: 0.9103\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3357 - acc: 0.8998 - val_loss: 0.2866 - val_f_2: 0.3843 - val_acc: 0.9077\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.3527 - acc: 0.9006 - val_loss: 0.2864 - val_f_2: 0.3398 - val_acc: 0.9088\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.3412 - acc: 0.8995 - val_loss: 0.2868 - val_f_2: 0.3858 - val_acc: 0.9077\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.3538 - acc: 0.9011 - val_loss: 0.2910 - val_f_2: 0.4162 - val_acc: 0.9071\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3522 - acc: 0.9001 - val_loss: 0.2848 - val_f_2: 0.3374 - val_acc: 0.9103\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.3430 - acc: 0.8995 - val_loss: 0.2857 - val_f_2: 0.3114 - val_acc: 0.9100\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.01, total=  32.4s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4537 - f_2: 0.0105 - acc: 0.8612 - val_loss: 0.3842 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3654 - f_2: 1.4291e-08 - acc: 0.8753 - val_loss: 0.3515 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3425 - f_2: 0.1259 - acc: 0.8882 - val_loss: 0.3365 - val_f_2: 0.2613 - val_acc: 0.8968\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3318 - f_2: 0.2690 - acc: 0.9022 - val_loss: 0.3279 - val_f_2: 0.2966 - val_acc: 0.8997\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2814 - acc: 0.9037 - val_loss: 0.3196 - val_f_2: 0.3242 - val_acc: 0.9009\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.3138 - acc: 0.9048 - val_loss: 0.3139 - val_f_2: 0.3401 - val_acc: 0.9018\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3114 - f_2: 0.3286 - acc: 0.9054 - val_loss: 0.3105 - val_f_2: 0.3600 - val_acc: 0.9003\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3521 - acc: 0.9062 - val_loss: 0.3107 - val_f_2: 0.3318 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3588 - acc: 0.9055 - val_loss: 0.3070 - val_f_2: 0.3369 - val_acc: 0.9012\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.3745 - acc: 0.9074 - val_loss: 0.3035 - val_f_2: 0.3564 - val_acc: 0.9015\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.3770 - acc: 0.9063 - val_loss: 0.3021 - val_f_2: 0.3571 - val_acc: 0.9018\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3806 - acc: 0.9074 - val_loss: 0.2999 - val_f_2: 0.3914 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2930 - f_2: 0.3703 - acc: 0.9080 - val_loss: 0.2990 - val_f_2: 0.3766 - val_acc: 0.9018\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2889 - f_2: 0.3749 - acc: 0.9075 - val_loss: 0.2978 - val_f_2: 0.3900 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2874 - f_2: 0.3904 - acc: 0.9088 - val_loss: 0.3025 - val_f_2: 0.3784 - val_acc: 0.9027\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2863 - f_2: 0.3920 - acc: 0.9093 - val_loss: 0.2986 - val_f_2: 0.3832 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2847 - f_2: 0.3930 - acc: 0.9088 - val_loss: 0.2963 - val_f_2: 0.3775 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.3926 - acc: 0.9095 - val_loss: 0.2957 - val_f_2: 0.3861 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.3843 - acc: 0.9091 - val_loss: 0.2958 - val_f_2: 0.3936 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2801 - f_2: 0.4048 - acc: 0.9100 - val_loss: 0.2951 - val_f_2: 0.3963 - val_acc: 0.9012\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.3998 - acc: 0.9105 - val_loss: 0.2967 - val_f_2: 0.4186 - val_acc: 0.9027\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2795 - f_2: 0.4118 - acc: 0.9101 - val_loss: 0.2962 - val_f_2: 0.3804 - val_acc: 0.9009\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.4132 - acc: 0.9107 - val_loss: 0.2961 - val_f_2: 0.4010 - val_acc: 0.9029\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4040 - acc: 0.9102 - val_loss: 0.2967 - val_f_2: 0.4219 - val_acc: 0.9024\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2787 - f_2: 0.4081 - acc: 0.9114 - val_loss: 0.2966 - val_f_2: 0.4041 - val_acc: 0.8997\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4176 - acc: 0.9112 - val_loss: 0.2951 - val_f_2: 0.4119 - val_acc: 0.9029\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2767 - f_2: 0.4159 - acc: 0.9113 - val_loss: 0.2971 - val_f_2: 0.4069 - val_acc: 0.9029\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2757 - f_2: 0.4243 - acc: 0.9119 - val_loss: 0.2958 - val_f_2: 0.4039 - val_acc: 0.9018\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2758 - f_2: 0.4192 - acc: 0.9108 - val_loss: 0.2957 - val_f_2: 0.3921 - val_acc: 0.9009\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2778 - f_2: 0.4118 - acc: 0.9112 - val_loss: 0.2962 - val_f_2: 0.3964 - val_acc: 0.9015\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2751 - f_2: 0.4349 - acc: 0.9127 - val_loss: 0.2962 - val_f_2: 0.4021 - val_acc: 0.9027\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2713 - f_2: 0.4351 - acc: 0.9135 - val_loss: 0.2955 - val_f_2: 0.4112 - val_acc: 0.9021\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2757 - f_2: 0.4255 - acc: 0.9123 - val_loss: 0.2941 - val_f_2: 0.4149 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2738 - f_2: 0.4279 - acc: 0.9130 - val_loss: 0.2959 - val_f_2: 0.4059 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2730 - f_2: 0.4243 - acc: 0.9113 - val_loss: 0.3011 - val_f_2: 0.3992 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2733 - f_2: 0.4324 - acc: 0.9128 - val_loss: 0.2964 - val_f_2: 0.4221 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2721 - f_2: 0.4352 - acc: 0.9117 - val_loss: 0.2987 - val_f_2: 0.4015 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2717 - f_2: 0.4318 - acc: 0.9131 - val_loss: 0.2973 - val_f_2: 0.3927 - val_acc: 0.9018\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2722 - f_2: 0.4389 - acc: 0.9123 - val_loss: 0.2950 - val_f_2: 0.3987 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2714 - f_2: 0.4334 - acc: 0.9123 - val_loss: 0.2962 - val_f_2: 0.4052 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2710 - f_2: 0.4308 - acc: 0.9115 - val_loss: 0.2987 - val_f_2: 0.4085 - val_acc: 0.8994\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2701 - f_2: 0.4366 - acc: 0.9123 - val_loss: 0.2967 - val_f_2: 0.3971 - val_acc: 0.8979\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2681 - f_2: 0.4482 - acc: 0.9144 - val_loss: 0.2955 - val_f_2: 0.4190 - val_acc: 0.8988\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2700 - f_2: 0.4333 - acc: 0.9120 - val_loss: 0.2982 - val_f_2: 0.4287 - val_acc: 0.9003\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2702 - f_2: 0.4423 - acc: 0.9127 - val_loss: 0.2984 - val_f_2: 0.4030 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2703 - f_2: 0.4431 - acc: 0.9129 - val_loss: 0.3067 - val_f_2: 0.3925 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2688 - f_2: 0.4574 - acc: 0.9146 - val_loss: 0.2997 - val_f_2: 0.4493 - val_acc: 0.8973\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2687 - f_2: 0.4528 - acc: 0.9144 - val_loss: 0.2992 - val_f_2: 0.3961 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2686 - f_2: 0.4445 - acc: 0.9128 - val_loss: 0.2954 - val_f_2: 0.4286 - val_acc: 0.8994\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2682 - f_2: 0.4524 - acc: 0.9136 - val_loss: 0.2991 - val_f_2: 0.4459 - val_acc: 0.8965\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.001, total=  32.0s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4627 - f_2: 0.0101 - acc: 0.8581 - val_loss: 0.3799 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3634 - f_2: 1.3719e-08 - acc: 0.8722 - val_loss: 0.3499 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3402 - f_2: 0.1393 - acc: 0.8867 - val_loss: 0.3363 - val_f_2: 0.2551 - val_acc: 0.8959\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3326 - f_2: 0.2610 - acc: 0.9012 - val_loss: 0.3281 - val_f_2: 0.2931 - val_acc: 0.8994\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2880 - acc: 0.9020 - val_loss: 0.3206 - val_f_2: 0.3163 - val_acc: 0.9009\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3343 - acc: 0.9043 - val_loss: 0.3155 - val_f_2: 0.3334 - val_acc: 0.9012\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3416 - acc: 0.9046 - val_loss: 0.3094 - val_f_2: 0.3317 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3625 - acc: 0.9054 - val_loss: 0.3077 - val_f_2: 0.3333 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3629 - acc: 0.9048 - val_loss: 0.3011 - val_f_2: 0.3675 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2963 - f_2: 0.3777 - acc: 0.9056 - val_loss: 0.2984 - val_f_2: 0.3647 - val_acc: 0.8997\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2943 - f_2: 0.3778 - acc: 0.9060 - val_loss: 0.2971 - val_f_2: 0.3790 - val_acc: 0.9015\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3827 - acc: 0.9065 - val_loss: 0.2955 - val_f_2: 0.3628 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2900 - f_2: 0.3902 - acc: 0.9067 - val_loss: 0.3008 - val_f_2: 0.3502 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2874 - f_2: 0.3906 - acc: 0.9079 - val_loss: 0.2983 - val_f_2: 0.3424 - val_acc: 0.9006\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.3827 - acc: 0.9071 - val_loss: 0.2933 - val_f_2: 0.3910 - val_acc: 0.9012\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2858 - f_2: 0.3867 - acc: 0.9067 - val_loss: 0.2930 - val_f_2: 0.4136 - val_acc: 0.9015\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2853 - f_2: 0.4112 - acc: 0.9080 - val_loss: 0.2919 - val_f_2: 0.3822 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.3936 - acc: 0.9069 - val_loss: 0.2924 - val_f_2: 0.3849 - val_acc: 0.9018\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.4001 - acc: 0.9100 - val_loss: 0.2915 - val_f_2: 0.4222 - val_acc: 0.9021\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2822 - f_2: 0.4112 - acc: 0.9084 - val_loss: 0.2921 - val_f_2: 0.3821 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4086 - acc: 0.9087 - val_loss: 0.2906 - val_f_2: 0.4005 - val_acc: 0.9021\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2802 - f_2: 0.4180 - acc: 0.9089 - val_loss: 0.2935 - val_f_2: 0.3922 - val_acc: 0.9029\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2791 - f_2: 0.4307 - acc: 0.9099 - val_loss: 0.2908 - val_f_2: 0.4007 - val_acc: 0.9015\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2786 - f_2: 0.4191 - acc: 0.9079 - val_loss: 0.2966 - val_f_2: 0.3977 - val_acc: 0.9024\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2758 - f_2: 0.4251 - acc: 0.9113 - val_loss: 0.2896 - val_f_2: 0.4195 - val_acc: 0.9032\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2762 - f_2: 0.4170 - acc: 0.9088 - val_loss: 0.2923 - val_f_2: 0.4008 - val_acc: 0.9024\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2768 - f_2: 0.4277 - acc: 0.9105 - val_loss: 0.2922 - val_f_2: 0.3888 - val_acc: 0.9018\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2758 - f_2: 0.4353 - acc: 0.9102 - val_loss: 0.2909 - val_f_2: 0.4497 - val_acc: 0.9018\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2768 - f_2: 0.4361 - acc: 0.9092 - val_loss: 0.2930 - val_f_2: 0.4139 - val_acc: 0.9024\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2743 - f_2: 0.4364 - acc: 0.9112 - val_loss: 0.2947 - val_f_2: 0.3918 - val_acc: 0.9029\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2729 - f_2: 0.4380 - acc: 0.9099 - val_loss: 0.2914 - val_f_2: 0.4403 - val_acc: 0.9024\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2736 - f_2: 0.4290 - acc: 0.9114 - val_loss: 0.2919 - val_f_2: 0.4192 - val_acc: 0.9029\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2743 - f_2: 0.4476 - acc: 0.9109 - val_loss: 0.2934 - val_f_2: 0.4570 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2723 - f_2: 0.4533 - acc: 0.9114 - val_loss: 0.2907 - val_f_2: 0.4466 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2723 - f_2: 0.4488 - acc: 0.9112 - val_loss: 0.2939 - val_f_2: 0.4338 - val_acc: 0.9024\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2725 - f_2: 0.4529 - acc: 0.9115 - val_loss: 0.2925 - val_f_2: 0.4418 - val_acc: 0.9029\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2712 - f_2: 0.4537 - acc: 0.9112 - val_loss: 0.2922 - val_f_2: 0.4221 - val_acc: 0.9029\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2710 - f_2: 0.4522 - acc: 0.9125 - val_loss: 0.2924 - val_f_2: 0.4410 - val_acc: 0.9018\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2712 - f_2: 0.4624 - acc: 0.9125 - val_loss: 0.2932 - val_f_2: 0.4321 - val_acc: 0.9027\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2697 - f_2: 0.4639 - acc: 0.9121 - val_loss: 0.2912 - val_f_2: 0.4338 - val_acc: 0.9009\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2688 - f_2: 0.4615 - acc: 0.9119 - val_loss: 0.2933 - val_f_2: 0.4285 - val_acc: 0.9006\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2687 - f_2: 0.4661 - acc: 0.9127 - val_loss: 0.2912 - val_f_2: 0.4521 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2686 - f_2: 0.4591 - acc: 0.9126 - val_loss: 0.2922 - val_f_2: 0.4495 - val_acc: 0.8997\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2695 - f_2: 0.4685 - acc: 0.9131 - val_loss: 0.2929 - val_f_2: 0.4432 - val_acc: 0.8994\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2690 - f_2: 0.4672 - acc: 0.9112 - val_loss: 0.2939 - val_f_2: 0.4476 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2674 - f_2: 0.4785 - acc: 0.9136 - val_loss: 0.2979 - val_f_2: 0.3908 - val_acc: 0.9018\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2685 - f_2: 0.4690 - acc: 0.9127 - val_loss: 0.2924 - val_f_2: 0.4364 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2690 - f_2: 0.4708 - acc: 0.9129 - val_loss: 0.2931 - val_f_2: 0.4525 - val_acc: 0.9009\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2670 - f_2: 0.4599 - acc: 0.9130 - val_loss: 0.2920 - val_f_2: 0.4414 - val_acc: 0.9006\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2684 - f_2: 0.4770 - acc: 0.9137 - val_loss: 0.2938 - val_f_2: 0.4548 - val_acc: 0.9021\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.001, total=  32.9s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4598 - f_2: 0.0098 - acc: 0.8550 - val_loss: 0.3563 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3691 - f_2: 0.0011 - acc: 0.8692 - val_loss: 0.3264 - val_f_2: 0.1476 - val_acc: 0.8947\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3472 - f_2: 0.2187 - acc: 0.8930 - val_loss: 0.3135 - val_f_2: 0.2992 - val_acc: 0.9086\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3332 - f_2: 0.2960 - acc: 0.8991 - val_loss: 0.3050 - val_f_2: 0.3547 - val_acc: 0.9112\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.3329 - acc: 0.9005 - val_loss: 0.2948 - val_f_2: 0.3512 - val_acc: 0.9124\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.3508 - acc: 0.9012 - val_loss: 0.2901 - val_f_2: 0.3599 - val_acc: 0.9121\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3600 - acc: 0.9010 - val_loss: 0.2889 - val_f_2: 0.3953 - val_acc: 0.9103\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3654 - acc: 0.9009 - val_loss: 0.2857 - val_f_2: 0.3515 - val_acc: 0.9124\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3690 - acc: 0.9019 - val_loss: 0.2825 - val_f_2: 0.3525 - val_acc: 0.9109\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3685 - acc: 0.9025 - val_loss: 0.2865 - val_f_2: 0.4162 - val_acc: 0.9109\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3805 - acc: 0.9036 - val_loss: 0.2812 - val_f_2: 0.3930 - val_acc: 0.9130\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2963 - f_2: 0.3753 - acc: 0.9033 - val_loss: 0.2788 - val_f_2: 0.3732 - val_acc: 0.9115\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2930 - f_2: 0.3771 - acc: 0.9043 - val_loss: 0.2783 - val_f_2: 0.3918 - val_acc: 0.9118\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3795 - acc: 0.9031 - val_loss: 0.2823 - val_f_2: 0.4329 - val_acc: 0.9083\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.4054 - acc: 0.9050 - val_loss: 0.2788 - val_f_2: 0.3539 - val_acc: 0.9086\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2896 - f_2: 0.3907 - acc: 0.9040 - val_loss: 0.2769 - val_f_2: 0.3875 - val_acc: 0.9106\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.3971 - acc: 0.9053 - val_loss: 0.2793 - val_f_2: 0.3516 - val_acc: 0.9106\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.3898 - acc: 0.9036 - val_loss: 0.2771 - val_f_2: 0.3637 - val_acc: 0.9086\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.4114 - acc: 0.9060 - val_loss: 0.2770 - val_f_2: 0.4054 - val_acc: 0.9083\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.3970 - acc: 0.9038 - val_loss: 0.2771 - val_f_2: 0.3929 - val_acc: 0.9059\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.4055 - acc: 0.9050 - val_loss: 0.2763 - val_f_2: 0.3630 - val_acc: 0.9094\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2820 - f_2: 0.4059 - acc: 0.9060 - val_loss: 0.2771 - val_f_2: 0.4018 - val_acc: 0.9077\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2822 - f_2: 0.4124 - acc: 0.9065 - val_loss: 0.2771 - val_f_2: 0.4024 - val_acc: 0.9065\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4202 - acc: 0.9053 - val_loss: 0.2765 - val_f_2: 0.3497 - val_acc: 0.9086\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2798 - f_2: 0.4002 - acc: 0.9052 - val_loss: 0.2824 - val_f_2: 0.4377 - val_acc: 0.9044\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2804 - f_2: 0.4209 - acc: 0.9059 - val_loss: 0.2763 - val_f_2: 0.3963 - val_acc: 0.9062\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2774 - f_2: 0.4252 - acc: 0.9066 - val_loss: 0.2776 - val_f_2: 0.4026 - val_acc: 0.9053\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2787 - f_2: 0.4122 - acc: 0.9059 - val_loss: 0.2766 - val_f_2: 0.4084 - val_acc: 0.9053\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2771 - f_2: 0.4162 - acc: 0.9057 - val_loss: 0.2816 - val_f_2: 0.4129 - val_acc: 0.9044\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2766 - f_2: 0.4192 - acc: 0.9065 - val_loss: 0.2757 - val_f_2: 0.3904 - val_acc: 0.9059\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4162 - acc: 0.9080 - val_loss: 0.2749 - val_f_2: 0.4051 - val_acc: 0.9053\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4300 - acc: 0.9071 - val_loss: 0.2753 - val_f_2: 0.4071 - val_acc: 0.9050\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2750 - f_2: 0.4219 - acc: 0.9070 - val_loss: 0.2745 - val_f_2: 0.3922 - val_acc: 0.9053\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2758 - f_2: 0.4293 - acc: 0.9065 - val_loss: 0.2732 - val_f_2: 0.3810 - val_acc: 0.9071\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2756 - f_2: 0.4262 - acc: 0.9074 - val_loss: 0.2762 - val_f_2: 0.4178 - val_acc: 0.9053\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2749 - f_2: 0.4451 - acc: 0.9084 - val_loss: 0.2754 - val_f_2: 0.3978 - val_acc: 0.9038\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2743 - f_2: 0.4443 - acc: 0.9087 - val_loss: 0.2757 - val_f_2: 0.3916 - val_acc: 0.9074\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2730 - f_2: 0.4382 - acc: 0.9075 - val_loss: 0.2734 - val_f_2: 0.3713 - val_acc: 0.9094\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2740 - f_2: 0.4445 - acc: 0.9092 - val_loss: 0.2748 - val_f_2: 0.3698 - val_acc: 0.9083\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2728 - f_2: 0.4291 - acc: 0.9070 - val_loss: 0.2766 - val_f_2: 0.4119 - val_acc: 0.9053\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2722 - f_2: 0.4452 - acc: 0.9079 - val_loss: 0.2763 - val_f_2: 0.3964 - val_acc: 0.9050\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2731 - f_2: 0.4356 - acc: 0.9080 - val_loss: 0.2813 - val_f_2: 0.4385 - val_acc: 0.9029\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2728 - f_2: 0.4457 - acc: 0.9087 - val_loss: 0.2747 - val_f_2: 0.3736 - val_acc: 0.9059\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2721 - f_2: 0.4331 - acc: 0.9071 - val_loss: 0.2764 - val_f_2: 0.4150 - val_acc: 0.9050\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2714 - f_2: 0.4546 - acc: 0.9102 - val_loss: 0.2768 - val_f_2: 0.4151 - val_acc: 0.9053\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2713 - f_2: 0.4497 - acc: 0.9082 - val_loss: 0.2785 - val_f_2: 0.3709 - val_acc: 0.9086\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2712 - f_2: 0.4612 - acc: 0.9101 - val_loss: 0.2740 - val_f_2: 0.3979 - val_acc: 0.9068\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2717 - f_2: 0.4482 - acc: 0.9082 - val_loss: 0.2736 - val_f_2: 0.3802 - val_acc: 0.9059\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2701 - f_2: 0.4496 - acc: 0.9096 - val_loss: 0.2731 - val_f_2: 0.3886 - val_acc: 0.9080\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2703 - f_2: 0.4441 - acc: 0.9082 - val_loss: 0.2741 - val_f_2: 0.3674 - val_acc: 0.9062\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.001, total=  32.6s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7228 - f_2: 0.0302 - acc: 0.8446 - val_loss: 0.5089 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4496 - f_2: 1.4946e-08 - acc: 0.8753 - val_loss: 0.4009 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3867 - f_2: 1.4400e-08 - acc: 0.8753 - val_loss: 0.3729 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3646 - f_2: 1.3755e-08 - acc: 0.8753 - val_loss: 0.3589 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3559 - f_2: 1.3743e-08 - acc: 0.8753 - val_loss: 0.3497 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3490 - f_2: 1.3669e-08 - acc: 0.8753 - val_loss: 0.3435 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3450 - f_2: 1.3629e-08 - acc: 0.8753 - val_loss: 0.3420 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3420 - f_2: 1.3979e-08 - acc: 0.8753 - val_loss: 0.3380 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3406 - f_2: 1.3680e-08 - acc: 0.8753 - val_loss: 0.3363 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3395 - f_2: 1.3905e-08 - acc: 0.8753 - val_loss: 0.3345 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3372 - f_2: 0.0114 - acc: 0.8764 - val_loss: 0.3321 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3358 - f_2: 0.1023 - acc: 0.8859 - val_loss: 0.3310 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3335 - f_2: 0.1168 - acc: 0.8873 - val_loss: 0.3305 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3350 - f_2: 0.1082 - acc: 0.8867 - val_loss: 0.3301 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3317 - f_2: 0.1279 - acc: 0.8887 - val_loss: 0.3335 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3311 - f_2: 0.1266 - acc: 0.8885 - val_loss: 0.3275 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3301 - f_2: 0.1401 - acc: 0.8897 - val_loss: 0.3263 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.1419 - acc: 0.8895 - val_loss: 0.3261 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.1410 - acc: 0.8888 - val_loss: 0.3251 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.1405 - acc: 0.8898 - val_loss: 0.3271 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.1627 - acc: 0.8898 - val_loss: 0.3228 - val_f_2: 0.0481 - val_acc: 0.8720\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.1664 - acc: 0.8906 - val_loss: 0.3266 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3269 - f_2: 0.1579 - acc: 0.8901 - val_loss: 0.3218 - val_f_2: 0.1586 - val_acc: 0.8844\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.1617 - acc: 0.8899 - val_loss: 0.3222 - val_f_2: 0.0319 - val_acc: 0.8705\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.1640 - acc: 0.8901 - val_loss: 0.3218 - val_f_2: 0.0274 - val_acc: 0.8699\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.1622 - acc: 0.8894 - val_loss: 0.3234 - val_f_2: 0.1107 - val_acc: 0.8788\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.1734 - acc: 0.8899 - val_loss: 0.3208 - val_f_2: 0.1246 - val_acc: 0.8805\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3238 - f_2: 0.1941 - acc: 0.8918 - val_loss: 0.3201 - val_f_2: 0.2377 - val_acc: 0.8941\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.1916 - acc: 0.8910 - val_loss: 0.3208 - val_f_2: 0.1021 - val_acc: 0.8779\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.1667 - acc: 0.8899 - val_loss: 0.3195 - val_f_2: 0.2556 - val_acc: 0.8965\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.1957 - acc: 0.8921 - val_loss: 0.3199 - val_f_2: 0.2492 - val_acc: 0.8956\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3207 - f_2: 0.2035 - acc: 0.8925 - val_loss: 0.3199 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.2342 - acc: 0.8957 - val_loss: 0.3188 - val_f_2: 0.2720 - val_acc: 0.8985\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.2405 - acc: 0.8952 - val_loss: 0.3194 - val_f_2: 0.2072 - val_acc: 0.8906\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.2123 - acc: 0.8930 - val_loss: 0.3199 - val_f_2: 0.2097 - val_acc: 0.8909\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3189 - f_2: 0.2357 - acc: 0.8951 - val_loss: 0.3230 - val_f_2: 0.2652 - val_acc: 0.8976\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2558 - acc: 0.8974 - val_loss: 0.3171 - val_f_2: 0.2944 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.2614 - acc: 0.8974 - val_loss: 0.3175 - val_f_2: 0.2853 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3189 - f_2: 0.2522 - acc: 0.8977 - val_loss: 0.3183 - val_f_2: 0.3051 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3172 - f_2: 0.2498 - acc: 0.8954 - val_loss: 0.3167 - val_f_2: 0.3174 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.2622 - acc: 0.8980 - val_loss: 0.3154 - val_f_2: 0.3411 - val_acc: 0.8997\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.2557 - acc: 0.8965 - val_loss: 0.3155 - val_f_2: 0.3768 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.2567 - acc: 0.8963 - val_loss: 0.3155 - val_f_2: 0.3080 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.2492 - acc: 0.8950 - val_loss: 0.3153 - val_f_2: 0.3059 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.2594 - acc: 0.8972 - val_loss: 0.3146 - val_f_2: 0.3484 - val_acc: 0.8988\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.2387 - acc: 0.8950 - val_loss: 0.3176 - val_f_2: 0.2853 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.2402 - acc: 0.8958 - val_loss: 0.3164 - val_f_2: 0.2877 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.2575 - acc: 0.8971 - val_loss: 0.3143 - val_f_2: 0.3283 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.2351 - acc: 0.8951 - val_loss: 0.3138 - val_f_2: 0.3482 - val_acc: 0.9006\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2585 - acc: 0.8975 - val_loss: 0.3147 - val_f_2: 0.2981 - val_acc: 0.9000\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.01, total=  31.5s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7249 - f_2: 0.0297 - acc: 0.8410 - val_loss: 0.5119 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4514 - f_2: 1.3451e-08 - acc: 0.8722 - val_loss: 0.3997 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3865 - f_2: 1.3395e-08 - acc: 0.8722 - val_loss: 0.3704 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3652 - f_2: 1.3722e-08 - acc: 0.8722 - val_loss: 0.3567 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3557 - f_2: 1.3492e-08 - acc: 0.8722 - val_loss: 0.3473 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3493 - f_2: 1.3527e-08 - acc: 0.8722 - val_loss: 0.3438 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3456 - f_2: 1.4268e-08 - acc: 0.8722 - val_loss: 0.3381 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3421 - f_2: 1.3850e-08 - acc: 0.8722 - val_loss: 0.3362 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3398 - f_2: 1.3386e-08 - acc: 0.8722 - val_loss: 0.3322 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3369 - f_2: 0.0226 - acc: 0.8746 - val_loss: 0.3304 - val_f_2: 0.1911 - val_acc: 0.8885\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3349 - f_2: 0.1408 - acc: 0.8854 - val_loss: 0.3286 - val_f_2: 0.1525 - val_acc: 0.8841\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3331 - f_2: 0.1496 - acc: 0.8867 - val_loss: 0.3300 - val_f_2: 0.2034 - val_acc: 0.8900\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3329 - f_2: 0.1579 - acc: 0.8867 - val_loss: 0.3265 - val_f_2: 0.2559 - val_acc: 0.8965\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3308 - f_2: 0.1824 - acc: 0.8891 - val_loss: 0.3257 - val_f_2: 0.2666 - val_acc: 0.8976\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2092 - acc: 0.8917 - val_loss: 0.3229 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3302 - f_2: 0.1981 - acc: 0.8909 - val_loss: 0.3221 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2067 - acc: 0.8908 - val_loss: 0.3241 - val_f_2: 0.2664 - val_acc: 0.8976\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3287 - f_2: 0.1946 - acc: 0.8908 - val_loss: 0.3205 - val_f_2: 0.2900 - val_acc: 0.8991\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3271 - f_2: 0.2232 - acc: 0.8924 - val_loss: 0.3197 - val_f_2: 0.3019 - val_acc: 0.8994\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.2246 - acc: 0.8928 - val_loss: 0.3190 - val_f_2: 0.2833 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3257 - f_2: 0.2369 - acc: 0.8936 - val_loss: 0.3186 - val_f_2: 0.3086 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.2602 - acc: 0.8970 - val_loss: 0.3174 - val_f_2: 0.3283 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3225 - f_2: 0.2550 - acc: 0.8953 - val_loss: 0.3172 - val_f_2: 0.3760 - val_acc: 0.9032\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.2952 - acc: 0.8995 - val_loss: 0.3164 - val_f_2: 0.3263 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.2775 - acc: 0.8962 - val_loss: 0.3155 - val_f_2: 0.3432 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.2820 - acc: 0.8956 - val_loss: 0.3153 - val_f_2: 0.3352 - val_acc: 0.9012\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2708 - acc: 0.8970 - val_loss: 0.3163 - val_f_2: 0.2981 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.2777 - acc: 0.8964 - val_loss: 0.3134 - val_f_2: 0.3453 - val_acc: 0.9012\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2827 - acc: 0.8969 - val_loss: 0.3146 - val_f_2: 0.3406 - val_acc: 0.9018\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.2846 - acc: 0.8970 - val_loss: 0.3129 - val_f_2: 0.3799 - val_acc: 0.9027\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3189 - f_2: 0.2811 - acc: 0.8970 - val_loss: 0.3150 - val_f_2: 0.3453 - val_acc: 0.9015\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.2933 - acc: 0.8984 - val_loss: 0.3141 - val_f_2: 0.3644 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.2883 - acc: 0.8974 - val_loss: 0.3112 - val_f_2: 0.3527 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3172 - f_2: 0.2880 - acc: 0.8969 - val_loss: 0.3124 - val_f_2: 0.3515 - val_acc: 0.9024\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.2930 - acc: 0.8975 - val_loss: 0.3104 - val_f_2: 0.3733 - val_acc: 0.9029\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.2879 - acc: 0.8980 - val_loss: 0.3102 - val_f_2: 0.3511 - val_acc: 0.9024\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3139 - f_2: 0.2944 - acc: 0.8967 - val_loss: 0.3109 - val_f_2: 0.3508 - val_acc: 0.9021\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.2923 - acc: 0.8978 - val_loss: 0.3093 - val_f_2: 0.3764 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.3034 - acc: 0.8982 - val_loss: 0.3093 - val_f_2: 0.3724 - val_acc: 0.9018\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.2959 - acc: 0.8972 - val_loss: 0.3083 - val_f_2: 0.3660 - val_acc: 0.9024\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3018 - acc: 0.8993 - val_loss: 0.3084 - val_f_2: 0.3800 - val_acc: 0.9012\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.2991 - acc: 0.8982 - val_loss: 0.3101 - val_f_2: 0.3574 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.2987 - acc: 0.8987 - val_loss: 0.3076 - val_f_2: 0.3701 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.2895 - acc: 0.8974 - val_loss: 0.3104 - val_f_2: 0.3581 - val_acc: 0.9024\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.3014 - acc: 0.8982 - val_loss: 0.3065 - val_f_2: 0.3760 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3122 - f_2: 0.2982 - acc: 0.8984 - val_loss: 0.3074 - val_f_2: 0.3595 - val_acc: 0.9024\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3080 - acc: 0.8995 - val_loss: 0.3078 - val_f_2: 0.3564 - val_acc: 0.9018\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.2915 - acc: 0.8965 - val_loss: 0.3061 - val_f_2: 0.3715 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.2912 - acc: 0.8966 - val_loss: 0.3061 - val_f_2: 0.3702 - val_acc: 0.9006\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3244 - acc: 0.9008 - val_loss: 0.3056 - val_f_2: 0.3559 - val_acc: 0.9018\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.01, total=  31.5s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7278 - f_2: 0.0362 - acc: 0.8374 - val_loss: 0.4900 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4569 - f_2: 1.3359e-08 - acc: 0.8691 - val_loss: 0.3819 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3925 - f_2: 1.2859e-08 - acc: 0.8691 - val_loss: 0.3512 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3718 - f_2: 1.3283e-08 - acc: 0.8691 - val_loss: 0.3409 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3612 - f_2: 1.2939e-08 - acc: 0.8691 - val_loss: 0.3300 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3546 - f_2: 1.3303e-08 - acc: 0.8691 - val_loss: 0.3256 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3532 - f_2: 1.3145e-08 - acc: 0.8691 - val_loss: 0.3225 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3500 - f_2: 1.2974e-08 - acc: 0.8691 - val_loss: 0.3205 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3461 - f_2: 1.3110e-08 - acc: 0.8691 - val_loss: 0.3206 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3473 - f_2: 1.3778e-08 - acc: 0.8691 - val_loss: 0.3169 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3439 - f_2: 0.0819 - acc: 0.8782 - val_loss: 0.3145 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3426 - f_2: 0.1117 - acc: 0.8813 - val_loss: 0.3146 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3415 - f_2: 0.1190 - acc: 0.8818 - val_loss: 0.3139 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3396 - f_2: 0.1167 - acc: 0.8814 - val_loss: 0.3123 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3378 - f_2: 0.1332 - acc: 0.8830 - val_loss: 0.3103 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3363 - f_2: 0.1415 - acc: 0.8840 - val_loss: 0.3101 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3361 - f_2: 0.1581 - acc: 0.8863 - val_loss: 0.3084 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3381 - f_2: 0.1603 - acc: 0.8852 - val_loss: 0.3097 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3332 - f_2: 0.1630 - acc: 0.8852 - val_loss: 0.3077 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3368 - f_2: 0.1445 - acc: 0.8827 - val_loss: 0.3076 - val_f_2: 0.0856 - val_acc: 0.8876\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3320 - f_2: 0.1621 - acc: 0.8841 - val_loss: 0.3072 - val_f_2: 0.0119 - val_acc: 0.8808\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3359 - f_2: 0.1604 - acc: 0.8838 - val_loss: 0.3062 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3343 - f_2: 0.1675 - acc: 0.8846 - val_loss: 0.3051 - val_f_2: 0.0073 - val_acc: 0.8802\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3334 - f_2: 0.1663 - acc: 0.8846 - val_loss: 0.3046 - val_f_2: 0.0131 - val_acc: 0.8808\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3311 - f_2: 0.1781 - acc: 0.8855 - val_loss: 0.3034 - val_f_2: 0.2334 - val_acc: 0.9032\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3316 - f_2: 0.1824 - acc: 0.8852 - val_loss: 0.3057 - val_f_2: 0.1955 - val_acc: 0.8988\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3314 - f_2: 0.1763 - acc: 0.8849 - val_loss: 0.3024 - val_f_2: 0.2074 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3305 - f_2: 0.1828 - acc: 0.8858 - val_loss: 0.3025 - val_f_2: 0.1938 - val_acc: 0.8985\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3318 - f_2: 0.1798 - acc: 0.8846 - val_loss: 0.3042 - val_f_2: 0.0365 - val_acc: 0.8829\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3295 - f_2: 0.1885 - acc: 0.8855 - val_loss: 0.3019 - val_f_2: 0.2288 - val_acc: 0.9027\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3284 - f_2: 0.1725 - acc: 0.8842 - val_loss: 0.3005 - val_f_2: 0.3030 - val_acc: 0.9109\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2031 - acc: 0.8860 - val_loss: 0.3004 - val_f_2: 0.3030 - val_acc: 0.9109\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.2204 - acc: 0.8878 - val_loss: 0.3006 - val_f_2: 0.2770 - val_acc: 0.9080\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.2222 - acc: 0.8888 - val_loss: 0.3003 - val_f_2: 0.3189 - val_acc: 0.9106\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.2346 - acc: 0.8894 - val_loss: 0.2995 - val_f_2: 0.3030 - val_acc: 0.9109\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2293 - acc: 0.8891 - val_loss: 0.3006 - val_f_2: 0.3474 - val_acc: 0.9083\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3270 - f_2: 0.2446 - acc: 0.8905 - val_loss: 0.2991 - val_f_2: 0.3101 - val_acc: 0.9115\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3260 - f_2: 0.2284 - acc: 0.8879 - val_loss: 0.3040 - val_f_2: 0.3199 - val_acc: 0.9086\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3259 - f_2: 0.2410 - acc: 0.8889 - val_loss: 0.2992 - val_f_2: 0.3053 - val_acc: 0.9112\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.2418 - acc: 0.8895 - val_loss: 0.2986 - val_f_2: 0.2909 - val_acc: 0.9097\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.2418 - acc: 0.8883 - val_loss: 0.3001 - val_f_2: 0.3165 - val_acc: 0.9088\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2526 - acc: 0.8887 - val_loss: 0.2983 - val_f_2: 0.3053 - val_acc: 0.9112\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3250 - f_2: 0.2504 - acc: 0.8906 - val_loss: 0.3024 - val_f_2: 0.3613 - val_acc: 0.9053\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.2589 - acc: 0.8903 - val_loss: 0.2973 - val_f_2: 0.3077 - val_acc: 0.9112\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.2467 - acc: 0.8902 - val_loss: 0.2975 - val_f_2: 0.3164 - val_acc: 0.9106\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.2481 - acc: 0.8911 - val_loss: 0.2965 - val_f_2: 0.3121 - val_acc: 0.9109\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.2275 - acc: 0.8875 - val_loss: 0.2959 - val_f_2: 0.3077 - val_acc: 0.9112\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2443 - acc: 0.8883 - val_loss: 0.2963 - val_f_2: 0.3053 - val_acc: 0.9112\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.2680 - acc: 0.8912 - val_loss: 0.2960 - val_f_2: 0.3053 - val_acc: 0.9112\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2656 - acc: 0.8919 - val_loss: 0.2971 - val_f_2: 0.3124 - val_acc: 0.9112\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.01, total=  32.1s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4708 - f_2: 0.0302 - acc: 0.8482 - val_loss: 0.3731 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3671 - f_2: 0.0016 - acc: 0.8755 - val_loss: 0.3445 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3470 - f_2: 0.1158 - acc: 0.8876 - val_loss: 0.3291 - val_f_2: 0.2724 - val_acc: 0.8982\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3344 - f_2: 0.1866 - acc: 0.8925 - val_loss: 0.3182 - val_f_2: 0.2955 - val_acc: 0.9003\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2133 - acc: 0.8942 - val_loss: 0.3101 - val_f_2: 0.3286 - val_acc: 0.9006\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.2351 - acc: 0.8947 - val_loss: 0.3071 - val_f_2: 0.3211 - val_acc: 0.9012\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.2603 - acc: 0.8978 - val_loss: 0.3038 - val_f_2: 0.3510 - val_acc: 0.9009\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.2824 - acc: 0.8993 - val_loss: 0.3010 - val_f_2: 0.3632 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.2949 - acc: 0.9008 - val_loss: 0.3001 - val_f_2: 0.3505 - val_acc: 0.9012\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3189 - acc: 0.9022 - val_loss: 0.3001 - val_f_2: 0.3435 - val_acc: 0.9024\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3162 - acc: 0.9019 - val_loss: 0.2987 - val_f_2: 0.3756 - val_acc: 0.9038\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3324 - acc: 0.9049 - val_loss: 0.2980 - val_f_2: 0.3857 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3397 - acc: 0.9043 - val_loss: 0.2976 - val_f_2: 0.3706 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2962 - f_2: 0.3548 - acc: 0.9056 - val_loss: 0.2997 - val_f_2: 0.3494 - val_acc: 0.9003\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3454 - acc: 0.9060 - val_loss: 0.2994 - val_f_2: 0.3626 - val_acc: 0.9015\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2954 - f_2: 0.3405 - acc: 0.9044 - val_loss: 0.2974 - val_f_2: 0.3847 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.3492 - acc: 0.9046 - val_loss: 0.2984 - val_f_2: 0.3884 - val_acc: 0.9015\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2947 - f_2: 0.3596 - acc: 0.9068 - val_loss: 0.2976 - val_f_2: 0.3989 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2941 - f_2: 0.3687 - acc: 0.9057 - val_loss: 0.2972 - val_f_2: 0.3898 - val_acc: 0.9021\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3665 - acc: 0.9062 - val_loss: 0.2990 - val_f_2: 0.4036 - val_acc: 0.8991\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2903 - f_2: 0.3844 - acc: 0.9082 - val_loss: 0.2982 - val_f_2: 0.3834 - val_acc: 0.9015\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.3855 - acc: 0.9086 - val_loss: 0.2986 - val_f_2: 0.3691 - val_acc: 0.9015\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.3901 - acc: 0.9088 - val_loss: 0.2988 - val_f_2: 0.3892 - val_acc: 0.9012\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2879 - f_2: 0.4071 - acc: 0.9099 - val_loss: 0.3002 - val_f_2: 0.3936 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2878 - f_2: 0.4009 - acc: 0.9105 - val_loss: 0.2989 - val_f_2: 0.3740 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.3985 - acc: 0.9102 - val_loss: 0.2989 - val_f_2: 0.3778 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.4129 - acc: 0.9103 - val_loss: 0.2986 - val_f_2: 0.4018 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2857 - f_2: 0.4062 - acc: 0.9099 - val_loss: 0.2976 - val_f_2: 0.4012 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.4089 - acc: 0.9111 - val_loss: 0.2978 - val_f_2: 0.3868 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2841 - f_2: 0.3954 - acc: 0.9105 - val_loss: 0.2997 - val_f_2: 0.3996 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2841 - f_2: 0.4204 - acc: 0.9107 - val_loss: 0.2990 - val_f_2: 0.3948 - val_acc: 0.8988\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2841 - f_2: 0.4042 - acc: 0.9093 - val_loss: 0.2984 - val_f_2: 0.3884 - val_acc: 0.8988\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.4163 - acc: 0.9102 - val_loss: 0.2995 - val_f_2: 0.3963 - val_acc: 0.8994\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4226 - acc: 0.9110 - val_loss: 0.2988 - val_f_2: 0.3720 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2831 - f_2: 0.4153 - acc: 0.9101 - val_loss: 0.3005 - val_f_2: 0.3782 - val_acc: 0.8997\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2825 - f_2: 0.4235 - acc: 0.9116 - val_loss: 0.3004 - val_f_2: 0.4047 - val_acc: 0.8988\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.4247 - acc: 0.9110 - val_loss: 0.2995 - val_f_2: 0.3985 - val_acc: 0.8991\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2827 - f_2: 0.4256 - acc: 0.9110 - val_loss: 0.3010 - val_f_2: 0.3858 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2781 - f_2: 0.4333 - acc: 0.9130 - val_loss: 0.2990 - val_f_2: 0.3778 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.4283 - acc: 0.9118 - val_loss: 0.3049 - val_f_2: 0.3685 - val_acc: 0.8997\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2799 - f_2: 0.4265 - acc: 0.9113 - val_loss: 0.3006 - val_f_2: 0.4264 - val_acc: 0.8962\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2800 - f_2: 0.4298 - acc: 0.9108 - val_loss: 0.3025 - val_f_2: 0.3857 - val_acc: 0.8976\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2802 - f_2: 0.4394 - acc: 0.9124 - val_loss: 0.3008 - val_f_2: 0.4059 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2777 - f_2: 0.4401 - acc: 0.9128 - val_loss: 0.3019 - val_f_2: 0.3796 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2782 - f_2: 0.4444 - acc: 0.9131 - val_loss: 0.3022 - val_f_2: 0.3861 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2782 - f_2: 0.4445 - acc: 0.9131 - val_loss: 0.3022 - val_f_2: 0.3926 - val_acc: 0.8979\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2778 - f_2: 0.4342 - acc: 0.9119 - val_loss: 0.3033 - val_f_2: 0.4460 - val_acc: 0.8944\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2780 - f_2: 0.4477 - acc: 0.9126 - val_loss: 0.3025 - val_f_2: 0.4129 - val_acc: 0.8938\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2751 - f_2: 0.4514 - acc: 0.9142 - val_loss: 0.3022 - val_f_2: 0.4028 - val_acc: 0.8971\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4454 - acc: 0.9122 - val_loss: 0.3021 - val_f_2: 0.3928 - val_acc: 0.8976\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.001, total=  31.7s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4717 - f_2: 0.0311 - acc: 0.8456 - val_loss: 0.3719 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3672 - f_2: 0.0334 - acc: 0.8761 - val_loss: 0.3423 - val_f_2: 0.2342 - val_acc: 0.8932\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3430 - f_2: 0.1899 - acc: 0.8902 - val_loss: 0.3222 - val_f_2: 0.2921 - val_acc: 0.8997\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3295 - f_2: 0.2430 - acc: 0.8924 - val_loss: 0.3140 - val_f_2: 0.3222 - val_acc: 0.9018\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.2764 - acc: 0.8960 - val_loss: 0.3073 - val_f_2: 0.3446 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3084 - acc: 0.9000 - val_loss: 0.3060 - val_f_2: 0.3318 - val_acc: 0.9009\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3353 - acc: 0.9025 - val_loss: 0.3040 - val_f_2: 0.3665 - val_acc: 0.8997\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3382 - acc: 0.9025 - val_loss: 0.3018 - val_f_2: 0.4165 - val_acc: 0.9015\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3572 - acc: 0.9031 - val_loss: 0.3001 - val_f_2: 0.3874 - val_acc: 0.8994\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3019 - f_2: 0.3616 - acc: 0.9043 - val_loss: 0.3006 - val_f_2: 0.3932 - val_acc: 0.9006\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3668 - acc: 0.9046 - val_loss: 0.3014 - val_f_2: 0.3979 - val_acc: 0.9012\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3926 - acc: 0.9067 - val_loss: 0.2980 - val_f_2: 0.4005 - val_acc: 0.9012\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3748 - acc: 0.9050 - val_loss: 0.2997 - val_f_2: 0.4181 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.3901 - acc: 0.9048 - val_loss: 0.2971 - val_f_2: 0.4120 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3827 - acc: 0.9038 - val_loss: 0.2976 - val_f_2: 0.4012 - val_acc: 0.9006\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2942 - f_2: 0.3965 - acc: 0.9054 - val_loss: 0.2982 - val_f_2: 0.3521 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.3880 - acc: 0.9039 - val_loss: 0.2968 - val_f_2: 0.3997 - val_acc: 0.9006\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.4015 - acc: 0.9068 - val_loss: 0.2965 - val_f_2: 0.4360 - val_acc: 0.9018\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2905 - f_2: 0.4003 - acc: 0.9064 - val_loss: 0.2966 - val_f_2: 0.4402 - val_acc: 0.9021\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.4102 - acc: 0.9060 - val_loss: 0.2957 - val_f_2: 0.4066 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.3998 - acc: 0.9067 - val_loss: 0.2956 - val_f_2: 0.4234 - val_acc: 0.9021\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.4180 - acc: 0.9075 - val_loss: 0.2966 - val_f_2: 0.3723 - val_acc: 0.8991\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.4166 - acc: 0.9079 - val_loss: 0.2956 - val_f_2: 0.4358 - val_acc: 0.9018\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.4170 - acc: 0.9057 - val_loss: 0.2955 - val_f_2: 0.3940 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.4236 - acc: 0.9086 - val_loss: 0.2960 - val_f_2: 0.4298 - val_acc: 0.9038\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.4110 - acc: 0.9076 - val_loss: 0.2958 - val_f_2: 0.4025 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2858 - f_2: 0.4166 - acc: 0.9076 - val_loss: 0.2958 - val_f_2: 0.4186 - val_acc: 0.9015\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2825 - f_2: 0.4271 - acc: 0.9082 - val_loss: 0.2976 - val_f_2: 0.4188 - val_acc: 0.8994\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4396 - acc: 0.9104 - val_loss: 0.2991 - val_f_2: 0.3797 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.4271 - acc: 0.9085 - val_loss: 0.2970 - val_f_2: 0.4078 - val_acc: 0.9012\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2836 - f_2: 0.4334 - acc: 0.9095 - val_loss: 0.2962 - val_f_2: 0.4317 - val_acc: 0.9029\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2810 - f_2: 0.4491 - acc: 0.9106 - val_loss: 0.2958 - val_f_2: 0.4097 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2816 - f_2: 0.4448 - acc: 0.9098 - val_loss: 0.2960 - val_f_2: 0.4208 - val_acc: 0.9024\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2806 - f_2: 0.4418 - acc: 0.9107 - val_loss: 0.2968 - val_f_2: 0.4272 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4406 - acc: 0.9105 - val_loss: 0.2969 - val_f_2: 0.4047 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2813 - f_2: 0.4518 - acc: 0.9105 - val_loss: 0.2978 - val_f_2: 0.4542 - val_acc: 0.9003\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4516 - acc: 0.9115 - val_loss: 0.2965 - val_f_2: 0.4166 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2793 - f_2: 0.4574 - acc: 0.9111 - val_loss: 0.2947 - val_f_2: 0.4250 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2791 - f_2: 0.4555 - acc: 0.9107 - val_loss: 0.2957 - val_f_2: 0.4261 - val_acc: 0.8997\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2781 - f_2: 0.4667 - acc: 0.9127 - val_loss: 0.2953 - val_f_2: 0.4464 - val_acc: 0.8997\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2773 - f_2: 0.4485 - acc: 0.9121 - val_loss: 0.2958 - val_f_2: 0.4281 - val_acc: 0.9021\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2769 - f_2: 0.4643 - acc: 0.9124 - val_loss: 0.2968 - val_f_2: 0.4299 - val_acc: 0.9015\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2771 - f_2: 0.4755 - acc: 0.9121 - val_loss: 0.2953 - val_f_2: 0.4228 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2751 - f_2: 0.4710 - acc: 0.9136 - val_loss: 0.2955 - val_f_2: 0.4357 - val_acc: 0.9012\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2782 - f_2: 0.4686 - acc: 0.9124 - val_loss: 0.2961 - val_f_2: 0.4103 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2756 - f_2: 0.4688 - acc: 0.9132 - val_loss: 0.2962 - val_f_2: 0.4375 - val_acc: 0.8997\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2746 - f_2: 0.4812 - acc: 0.9136 - val_loss: 0.2966 - val_f_2: 0.4273 - val_acc: 0.8994\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2759 - f_2: 0.4723 - acc: 0.9119 - val_loss: 0.2960 - val_f_2: 0.4458 - val_acc: 0.8988\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2760 - f_2: 0.4801 - acc: 0.9130 - val_loss: 0.2970 - val_f_2: 0.4426 - val_acc: 0.8997\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4868 - acc: 0.9143 - val_loss: 0.2962 - val_f_2: 0.4378 - val_acc: 0.9015\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.001, total=  32.3s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4797 - f_2: 0.0366 - acc: 0.8431 - val_loss: 0.3533 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3740 - f_2: 0.0216 - acc: 0.8715 - val_loss: 0.3286 - val_f_2: 0.0046 - val_acc: 0.8799\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3526 - f_2: 0.1499 - acc: 0.8852 - val_loss: 0.3104 - val_f_2: 0.3086 - val_acc: 0.9112\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3371 - f_2: 0.2236 - acc: 0.8887 - val_loss: 0.3002 - val_f_2: 0.3396 - val_acc: 0.9115\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3299 - f_2: 0.2484 - acc: 0.8893 - val_loss: 0.2933 - val_f_2: 0.3413 - val_acc: 0.9106\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.2820 - acc: 0.8950 - val_loss: 0.2929 - val_f_2: 0.3772 - val_acc: 0.9106\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.3150 - acc: 0.8972 - val_loss: 0.2897 - val_f_2: 0.3498 - val_acc: 0.9109\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.3148 - acc: 0.8961 - val_loss: 0.2863 - val_f_2: 0.3788 - val_acc: 0.9100\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3318 - acc: 0.8988 - val_loss: 0.2842 - val_f_2: 0.3490 - val_acc: 0.9106\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3365 - acc: 0.8979 - val_loss: 0.2858 - val_f_2: 0.3821 - val_acc: 0.9094\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3419 - acc: 0.8982 - val_loss: 0.2833 - val_f_2: 0.3426 - val_acc: 0.9115\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3472 - acc: 0.8984 - val_loss: 0.2811 - val_f_2: 0.3812 - val_acc: 0.9103\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3490 - acc: 0.8983 - val_loss: 0.2804 - val_f_2: 0.3763 - val_acc: 0.9103\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3527 - acc: 0.8993 - val_loss: 0.2788 - val_f_2: 0.3730 - val_acc: 0.9115\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3533 - acc: 0.8979 - val_loss: 0.2788 - val_f_2: 0.3622 - val_acc: 0.9100\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3014 - f_2: 0.3592 - acc: 0.8991 - val_loss: 0.2787 - val_f_2: 0.3883 - val_acc: 0.9112\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3688 - acc: 0.9009 - val_loss: 0.2789 - val_f_2: 0.3963 - val_acc: 0.9106\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3007 - f_2: 0.3711 - acc: 0.9013 - val_loss: 0.2796 - val_f_2: 0.3983 - val_acc: 0.9103\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2969 - f_2: 0.3786 - acc: 0.9004 - val_loss: 0.2781 - val_f_2: 0.3792 - val_acc: 0.9100\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3876 - acc: 0.9012 - val_loss: 0.2777 - val_f_2: 0.4000 - val_acc: 0.9106\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2960 - f_2: 0.3870 - acc: 0.9021 - val_loss: 0.2809 - val_f_2: 0.4236 - val_acc: 0.9077\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.4076 - acc: 0.9024 - val_loss: 0.2767 - val_f_2: 0.4033 - val_acc: 0.9109\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.3918 - acc: 0.9009 - val_loss: 0.2772 - val_f_2: 0.3975 - val_acc: 0.9118\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.4150 - acc: 0.9033 - val_loss: 0.2763 - val_f_2: 0.4024 - val_acc: 0.9115\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.4096 - acc: 0.9023 - val_loss: 0.2821 - val_f_2: 0.4279 - val_acc: 0.9021\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.4222 - acc: 0.9037 - val_loss: 0.2764 - val_f_2: 0.4111 - val_acc: 0.9100\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.4204 - acc: 0.9031 - val_loss: 0.2762 - val_f_2: 0.4016 - val_acc: 0.9109\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.4134 - acc: 0.9038 - val_loss: 0.2775 - val_f_2: 0.4261 - val_acc: 0.9068\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2908 - f_2: 0.4319 - acc: 0.9046 - val_loss: 0.2771 - val_f_2: 0.4106 - val_acc: 0.9077\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2891 - f_2: 0.4336 - acc: 0.9030 - val_loss: 0.2748 - val_f_2: 0.4159 - val_acc: 0.9083\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2879 - f_2: 0.4253 - acc: 0.9051 - val_loss: 0.2766 - val_f_2: 0.4086 - val_acc: 0.9088\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.4374 - acc: 0.9049 - val_loss: 0.2763 - val_f_2: 0.4094 - val_acc: 0.9074\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2874 - f_2: 0.4467 - acc: 0.9057 - val_loss: 0.2753 - val_f_2: 0.4193 - val_acc: 0.9068\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.4537 - acc: 0.9051 - val_loss: 0.2753 - val_f_2: 0.4037 - val_acc: 0.9109\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.4523 - acc: 0.9057 - val_loss: 0.2749 - val_f_2: 0.4076 - val_acc: 0.9083\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2867 - f_2: 0.4452 - acc: 0.9057 - val_loss: 0.2786 - val_f_2: 0.4372 - val_acc: 0.9041\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.4579 - acc: 0.9052 - val_loss: 0.2747 - val_f_2: 0.4248 - val_acc: 0.9080\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4546 - acc: 0.9068 - val_loss: 0.2762 - val_f_2: 0.4281 - val_acc: 0.9068\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2834 - f_2: 0.4640 - acc: 0.9077 - val_loss: 0.2755 - val_f_2: 0.4199 - val_acc: 0.9062\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.4719 - acc: 0.9076 - val_loss: 0.2746 - val_f_2: 0.4267 - val_acc: 0.9080\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.4688 - acc: 0.9079 - val_loss: 0.2755 - val_f_2: 0.4205 - val_acc: 0.9038\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4793 - acc: 0.9079 - val_loss: 0.2763 - val_f_2: 0.4258 - val_acc: 0.9047\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2836 - f_2: 0.4717 - acc: 0.9067 - val_loss: 0.2750 - val_f_2: 0.4267 - val_acc: 0.9053\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4783 - acc: 0.9091 - val_loss: 0.2749 - val_f_2: 0.4200 - val_acc: 0.9071\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4805 - acc: 0.9077 - val_loss: 0.2752 - val_f_2: 0.4222 - val_acc: 0.9068\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2828 - f_2: 0.4787 - acc: 0.9078 - val_loss: 0.2752 - val_f_2: 0.4117 - val_acc: 0.9077\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2813 - f_2: 0.4837 - acc: 0.9082 - val_loss: 0.2748 - val_f_2: 0.4104 - val_acc: 0.9080\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2813 - f_2: 0.4855 - acc: 0.9083 - val_loss: 0.2756 - val_f_2: 0.4377 - val_acc: 0.9050\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4914 - acc: 0.9086 - val_loss: 0.2739 - val_f_2: 0.4360 - val_acc: 0.9059\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4946 - acc: 0.9085 - val_loss: 0.2759 - val_f_2: 0.4092 - val_acc: 0.9086\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.001, total=  31.3s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7654 - f_2: 0.0219 - acc: 0.8528 - val_loss: 0.5092 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4341 - f_2: 1.3903e-08 - acc: 0.8753 - val_loss: 0.3892 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3755 - f_2: 1.3629e-08 - acc: 0.8753 - val_loss: 0.3653 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3614 - f_2: 1.4314e-08 - acc: 0.8753 - val_loss: 0.3559 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3515 - f_2: 1.3861e-08 - acc: 0.8753 - val_loss: 0.3487 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3471 - f_2: 1.4609e-08 - acc: 0.8753 - val_loss: 0.3433 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3434 - f_2: 1.4032e-08 - acc: 0.8753 - val_loss: 0.3395 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3398 - f_2: 1.3968e-08 - acc: 0.8753 - val_loss: 0.3344 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3359 - f_2: 1.4047e-08 - acc: 0.8753 - val_loss: 0.3314 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3368 - f_2: 1.3914e-08 - acc: 0.8753 - val_loss: 0.3299 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3350 - f_2: 1.3895e-08 - acc: 0.8753 - val_loss: 0.3277 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3315 - f_2: 0.1770 - acc: 0.8929 - val_loss: 0.3295 - val_f_2: 0.2776 - val_acc: 0.8991\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.1817 - acc: 0.8943 - val_loss: 0.3250 - val_f_2: 0.2673 - val_acc: 0.8979\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3323 - f_2: 0.2205 - acc: 0.8966 - val_loss: 0.3242 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.2261 - acc: 0.8980 - val_loss: 0.3244 - val_f_2: 0.2855 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3259 - f_2: 0.2377 - acc: 0.8983 - val_loss: 0.3232 - val_f_2: 0.2855 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.2540 - acc: 0.8993 - val_loss: 0.3233 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3264 - f_2: 0.2378 - acc: 0.8995 - val_loss: 0.3207 - val_f_2: 0.2890 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3299 - f_2: 0.2448 - acc: 0.8992 - val_loss: 0.3209 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.2385 - acc: 0.8989 - val_loss: 0.3194 - val_f_2: 0.2855 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.2501 - acc: 0.9001 - val_loss: 0.3214 - val_f_2: 0.3001 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3229 - f_2: 0.2639 - acc: 0.9006 - val_loss: 0.3181 - val_f_2: 0.2873 - val_acc: 0.8997\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2698 - acc: 0.9007 - val_loss: 0.3179 - val_f_2: 0.2981 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.2630 - acc: 0.9012 - val_loss: 0.3181 - val_f_2: 0.2890 - val_acc: 0.8997\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.2626 - acc: 0.8999 - val_loss: 0.3173 - val_f_2: 0.2853 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.2550 - acc: 0.9002 - val_loss: 0.3213 - val_f_2: 0.2853 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2745 - acc: 0.9010 - val_loss: 0.3161 - val_f_2: 0.3018 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.2730 - acc: 0.9012 - val_loss: 0.3165 - val_f_2: 0.3111 - val_acc: 0.9012\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2668 - acc: 0.8998 - val_loss: 0.3156 - val_f_2: 0.2951 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.2881 - acc: 0.9016 - val_loss: 0.3159 - val_f_2: 0.3036 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.2815 - acc: 0.9022 - val_loss: 0.3176 - val_f_2: 0.2931 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.2895 - acc: 0.9024 - val_loss: 0.3144 - val_f_2: 0.2981 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.2795 - acc: 0.8999 - val_loss: 0.3137 - val_f_2: 0.2914 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.2857 - acc: 0.9024 - val_loss: 0.3166 - val_f_2: 0.2951 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.2853 - acc: 0.9018 - val_loss: 0.3147 - val_f_2: 0.3124 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.2900 - acc: 0.9020 - val_loss: 0.3144 - val_f_2: 0.2981 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3149 - f_2: 0.3037 - acc: 0.9023 - val_loss: 0.3183 - val_f_2: 0.2931 - val_acc: 0.9003\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.2930 - acc: 0.9020 - val_loss: 0.3124 - val_f_2: 0.3075 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3160 - f_2: 0.2880 - acc: 0.9020 - val_loss: 0.3136 - val_f_2: 0.3125 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.2961 - acc: 0.9026 - val_loss: 0.3115 - val_f_2: 0.3154 - val_acc: 0.9009\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.2972 - acc: 0.9027 - val_loss: 0.3170 - val_f_2: 0.2948 - val_acc: 0.8997\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3185 - f_2: 0.2868 - acc: 0.9015 - val_loss: 0.3127 - val_f_2: 0.3124 - val_acc: 0.9006\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.2989 - acc: 0.9022 - val_loss: 0.3134 - val_f_2: 0.3059 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.2987 - acc: 0.9032 - val_loss: 0.3137 - val_f_2: 0.3036 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2953 - acc: 0.9021 - val_loss: 0.3116 - val_f_2: 0.2981 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2916 - acc: 0.9026 - val_loss: 0.3129 - val_f_2: 0.2951 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.3005 - acc: 0.9029 - val_loss: 0.3115 - val_f_2: 0.3123 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3057 - acc: 0.9033 - val_loss: 0.3109 - val_f_2: 0.3109 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.2943 - acc: 0.9022 - val_loss: 0.3120 - val_f_2: 0.3114 - val_acc: 0.9018\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3075 - acc: 0.9030 - val_loss: 0.3142 - val_f_2: 0.2949 - val_acc: 0.9000\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.01, total=  31.7s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7643 - f_2: 0.0176 - acc: 0.8492 - val_loss: 0.5107 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4343 - f_2: 1.3640e-08 - acc: 0.8722 - val_loss: 0.3877 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3756 - f_2: 1.3745e-08 - acc: 0.8722 - val_loss: 0.3658 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3596 - f_2: 1.3433e-08 - acc: 0.8722 - val_loss: 0.3530 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3519 - f_2: 1.3699e-08 - acc: 0.8722 - val_loss: 0.3443 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3435 - f_2: 1.3400e-08 - acc: 0.8722 - val_loss: 0.3394 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3402 - f_2: 0.0027 - acc: 0.8722 - val_loss: 0.3381 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3378 - f_2: 0.0445 - acc: 0.8767 - val_loss: 0.3343 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3350 - f_2: 0.1348 - acc: 0.8868 - val_loss: 0.3309 - val_f_2: 0.0201 - val_acc: 0.8690\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3328 - f_2: 0.1510 - acc: 0.8872 - val_loss: 0.3295 - val_f_2: 0.0500 - val_acc: 0.8723\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.1659 - acc: 0.8891 - val_loss: 0.3267 - val_f_2: 0.2220 - val_acc: 0.8923\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3315 - f_2: 0.1725 - acc: 0.8900 - val_loss: 0.3257 - val_f_2: 0.2245 - val_acc: 0.8926\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3308 - f_2: 0.1714 - acc: 0.8888 - val_loss: 0.3249 - val_f_2: 0.2475 - val_acc: 0.8956\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.2038 - acc: 0.8922 - val_loss: 0.3240 - val_f_2: 0.2645 - val_acc: 0.8973\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3270 - f_2: 0.1863 - acc: 0.8902 - val_loss: 0.3238 - val_f_2: 0.2793 - val_acc: 0.8991\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.1938 - acc: 0.8914 - val_loss: 0.3209 - val_f_2: 0.2855 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3252 - f_2: 0.2112 - acc: 0.8922 - val_loss: 0.3212 - val_f_2: 0.2855 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.2042 - acc: 0.8911 - val_loss: 0.3235 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3258 - f_2: 0.2105 - acc: 0.8924 - val_loss: 0.3181 - val_f_2: 0.2884 - val_acc: 0.8991\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2114 - acc: 0.8922 - val_loss: 0.3180 - val_f_2: 0.3032 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2188 - acc: 0.8933 - val_loss: 0.3179 - val_f_2: 0.2874 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3193 - f_2: 0.2162 - acc: 0.8918 - val_loss: 0.3169 - val_f_2: 0.2912 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.2326 - acc: 0.8941 - val_loss: 0.3183 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.2304 - acc: 0.8939 - val_loss: 0.3169 - val_f_2: 0.2912 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2347 - acc: 0.8950 - val_loss: 0.3149 - val_f_2: 0.2966 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.2113 - acc: 0.8924 - val_loss: 0.3137 - val_f_2: 0.3030 - val_acc: 0.8997\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3180 - f_2: 0.2427 - acc: 0.8939 - val_loss: 0.3132 - val_f_2: 0.2946 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.2341 - acc: 0.8942 - val_loss: 0.3127 - val_f_2: 0.3051 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.2328 - acc: 0.8943 - val_loss: 0.3138 - val_f_2: 0.3018 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2411 - acc: 0.8942 - val_loss: 0.3108 - val_f_2: 0.2983 - val_acc: 0.8997\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.2152 - acc: 0.8913 - val_loss: 0.3119 - val_f_2: 0.2985 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.2450 - acc: 0.8938 - val_loss: 0.3137 - val_f_2: 0.2947 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.2367 - acc: 0.8949 - val_loss: 0.3112 - val_f_2: 0.3037 - val_acc: 0.9003\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.2457 - acc: 0.8949 - val_loss: 0.3100 - val_f_2: 0.2967 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.2415 - acc: 0.8953 - val_loss: 0.3103 - val_f_2: 0.3505 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.2687 - acc: 0.8975 - val_loss: 0.3119 - val_f_2: 0.3332 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.2533 - acc: 0.8961 - val_loss: 0.3081 - val_f_2: 0.3431 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.2354 - acc: 0.8924 - val_loss: 0.3077 - val_f_2: 0.3011 - val_acc: 0.9003\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.2365 - acc: 0.8936 - val_loss: 0.3076 - val_f_2: 0.3144 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3087 - f_2: 0.2630 - acc: 0.8972 - val_loss: 0.3074 - val_f_2: 0.3224 - val_acc: 0.9018\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.2899 - acc: 0.8993 - val_loss: 0.3071 - val_f_2: 0.3296 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.2747 - acc: 0.8977 - val_loss: 0.3095 - val_f_2: 0.3442 - val_acc: 0.9003\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.2737 - acc: 0.8975 - val_loss: 0.3100 - val_f_2: 0.3286 - val_acc: 0.9003\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.2717 - acc: 0.8965 - val_loss: 0.3096 - val_f_2: 0.3044 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.2619 - acc: 0.8947 - val_loss: 0.3049 - val_f_2: 0.3260 - val_acc: 0.9015\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.2850 - acc: 0.8987 - val_loss: 0.3160 - val_f_2: 0.2947 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.2578 - acc: 0.8956 - val_loss: 0.3071 - val_f_2: 0.3276 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.2622 - acc: 0.8962 - val_loss: 0.3046 - val_f_2: 0.3500 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.3126 - acc: 0.9013 - val_loss: 0.3047 - val_f_2: 0.3240 - val_acc: 0.9012\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3090 - f_2: 0.2703 - acc: 0.8967 - val_loss: 0.3048 - val_f_2: 0.3100 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.01, total=  32.1s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7692 - f_2: 0.0269 - acc: 0.8461 - val_loss: 0.4897 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4447 - f_2: 1.3325e-08 - acc: 0.8691 - val_loss: 0.3702 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3828 - f_2: 1.3214e-08 - acc: 0.8691 - val_loss: 0.3461 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3666 - f_2: 1.3338e-08 - acc: 0.8691 - val_loss: 0.3356 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3576 - f_2: 1.3565e-08 - acc: 0.8691 - val_loss: 0.3275 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3529 - f_2: 1.3583e-08 - acc: 0.8691 - val_loss: 0.3246 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3470 - f_2: 1.3028e-08 - acc: 0.8691 - val_loss: 0.3196 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3452 - f_2: 1.2958e-08 - acc: 0.8691 - val_loss: 0.3168 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3440 - f_2: 0.0792 - acc: 0.8778 - val_loss: 0.3151 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3414 - f_2: 0.1297 - acc: 0.8825 - val_loss: 0.3121 - val_f_2: 0.1198 - val_acc: 0.8914\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3402 - f_2: 0.1562 - acc: 0.8854 - val_loss: 0.3113 - val_f_2: 0.0073 - val_acc: 0.8802\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3372 - f_2: 0.1662 - acc: 0.8858 - val_loss: 0.3127 - val_f_2: 0.0819 - val_acc: 0.8879\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3379 - f_2: 0.1713 - acc: 0.8863 - val_loss: 0.3083 - val_f_2: 0.2007 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3372 - f_2: 0.1814 - acc: 0.8872 - val_loss: 0.3067 - val_f_2: 0.2739 - val_acc: 0.9077\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3366 - f_2: 0.1827 - acc: 0.8862 - val_loss: 0.3070 - val_f_2: 0.2771 - val_acc: 0.9080\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3341 - f_2: 0.1934 - acc: 0.8886 - val_loss: 0.3068 - val_f_2: 0.3054 - val_acc: 0.9109\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3328 - f_2: 0.1856 - acc: 0.8873 - val_loss: 0.3039 - val_f_2: 0.3000 - val_acc: 0.9106\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3313 - f_2: 0.2013 - acc: 0.8886 - val_loss: 0.3031 - val_f_2: 0.3000 - val_acc: 0.9106\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3325 - f_2: 0.1938 - acc: 0.8874 - val_loss: 0.3032 - val_f_2: 0.3074 - val_acc: 0.9103\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3327 - f_2: 0.1901 - acc: 0.8856 - val_loss: 0.3009 - val_f_2: 0.3030 - val_acc: 0.9109\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3319 - f_2: 0.1926 - acc: 0.8882 - val_loss: 0.3022 - val_f_2: 0.3081 - val_acc: 0.9112\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.2092 - acc: 0.8889 - val_loss: 0.3066 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2238 - acc: 0.8900 - val_loss: 0.2980 - val_f_2: 0.3078 - val_acc: 0.9109\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3303 - f_2: 0.1944 - acc: 0.8875 - val_loss: 0.2992 - val_f_2: 0.3030 - val_acc: 0.9109\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3271 - f_2: 0.1919 - acc: 0.8871 - val_loss: 0.3011 - val_f_2: 0.3030 - val_acc: 0.9109\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.1971 - acc: 0.8871 - val_loss: 0.2979 - val_f_2: 0.3077 - val_acc: 0.9112\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3264 - f_2: 0.2196 - acc: 0.8904 - val_loss: 0.2970 - val_f_2: 0.3099 - val_acc: 0.9109\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2095 - acc: 0.8878 - val_loss: 0.2965 - val_f_2: 0.3053 - val_acc: 0.9109\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3250 - f_2: 0.2300 - acc: 0.8901 - val_loss: 0.2957 - val_f_2: 0.3053 - val_acc: 0.9109\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.1984 - acc: 0.8874 - val_loss: 0.2950 - val_f_2: 0.3077 - val_acc: 0.9112\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2213 - acc: 0.8900 - val_loss: 0.2943 - val_f_2: 0.3099 - val_acc: 0.9109\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2307 - acc: 0.8891 - val_loss: 0.2942 - val_f_2: 0.3102 - val_acc: 0.9112\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.2064 - acc: 0.8875 - val_loss: 0.2934 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2111 - acc: 0.8873 - val_loss: 0.2931 - val_f_2: 0.3209 - val_acc: 0.9106\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3227 - f_2: 0.2109 - acc: 0.8877 - val_loss: 0.2948 - val_f_2: 0.3053 - val_acc: 0.9112\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.2179 - acc: 0.8884 - val_loss: 0.2918 - val_f_2: 0.3099 - val_acc: 0.9109\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2206 - acc: 0.8898 - val_loss: 0.2924 - val_f_2: 0.3097 - val_acc: 0.9106\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2078 - acc: 0.8873 - val_loss: 0.2950 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2399 - acc: 0.8911 - val_loss: 0.2946 - val_f_2: 0.3053 - val_acc: 0.9112\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2120 - acc: 0.8886 - val_loss: 0.2903 - val_f_2: 0.3099 - val_acc: 0.9109\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.2134 - acc: 0.8874 - val_loss: 0.2909 - val_f_2: 0.3075 - val_acc: 0.9109\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.2235 - acc: 0.8896 - val_loss: 0.2905 - val_f_2: 0.3053 - val_acc: 0.9109\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2172 - acc: 0.8890 - val_loss: 0.2897 - val_f_2: 0.3075 - val_acc: 0.9109\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3245 - f_2: 0.2064 - acc: 0.8866 - val_loss: 0.2898 - val_f_2: 0.3075 - val_acc: 0.9109\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2229 - acc: 0.8887 - val_loss: 0.2892 - val_f_2: 0.3102 - val_acc: 0.9112\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.2190 - acc: 0.8888 - val_loss: 0.2884 - val_f_2: 0.3075 - val_acc: 0.9109\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.2303 - acc: 0.8901 - val_loss: 0.2913 - val_f_2: 0.3097 - val_acc: 0.9106\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.2197 - acc: 0.8887 - val_loss: 0.2893 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.2429 - acc: 0.8900 - val_loss: 0.2910 - val_f_2: 0.3075 - val_acc: 0.9109\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3171 - f_2: 0.2398 - acc: 0.8909 - val_loss: 0.2874 - val_f_2: 0.3121 - val_acc: 0.9109\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.01, total=  31.9s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4725 - f_2: 0.0208 - acc: 0.8555 - val_loss: 0.3836 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3673 - f_2: 1.4173e-08 - acc: 0.8753 - val_loss: 0.3507 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3483 - f_2: 1.4067e-08 - acc: 0.8753 - val_loss: 0.3367 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3357 - f_2: 1.3952e-08 - acc: 0.8753 - val_loss: 0.3288 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3293 - f_2: 1.3689e-08 - acc: 0.8753 - val_loss: 0.3258 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.0853 - acc: 0.8838 - val_loss: 0.3243 - val_f_2: 0.2450 - val_acc: 0.8953\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.1839 - acc: 0.8936 - val_loss: 0.3210 - val_f_2: 0.2843 - val_acc: 0.8991\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.1967 - acc: 0.8940 - val_loss: 0.3207 - val_f_2: 0.3080 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2106 - acc: 0.8957 - val_loss: 0.3181 - val_f_2: 0.3082 - val_acc: 0.9009\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2464 - acc: 0.8973 - val_loss: 0.3166 - val_f_2: 0.3099 - val_acc: 0.9006\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.2296 - acc: 0.8956 - val_loss: 0.3149 - val_f_2: 0.3464 - val_acc: 0.9003\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.2254 - acc: 0.8947 - val_loss: 0.3139 - val_f_2: 0.3356 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.2204 - acc: 0.8943 - val_loss: 0.3129 - val_f_2: 0.3363 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.2873 - acc: 0.8991 - val_loss: 0.3127 - val_f_2: 0.3379 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.3212 - acc: 0.9046 - val_loss: 0.3110 - val_f_2: 0.3591 - val_acc: 0.9009\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.3218 - acc: 0.9046 - val_loss: 0.3112 - val_f_2: 0.3741 - val_acc: 0.9015\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3296 - acc: 0.9042 - val_loss: 0.3122 - val_f_2: 0.3926 - val_acc: 0.9006\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3399 - acc: 0.9053 - val_loss: 0.3100 - val_f_2: 0.3949 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3418 - acc: 0.9058 - val_loss: 0.3095 - val_f_2: 0.3574 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3436 - acc: 0.9063 - val_loss: 0.3083 - val_f_2: 0.3582 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2969 - f_2: 0.3512 - acc: 0.9069 - val_loss: 0.3089 - val_f_2: 0.4031 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3622 - acc: 0.9075 - val_loss: 0.3097 - val_f_2: 0.3491 - val_acc: 0.9009\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3423 - acc: 0.9052 - val_loss: 0.3083 - val_f_2: 0.3976 - val_acc: 0.9012\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.3586 - acc: 0.9062 - val_loss: 0.3089 - val_f_2: 0.3664 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3516 - acc: 0.9052 - val_loss: 0.3071 - val_f_2: 0.3949 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2951 - f_2: 0.3535 - acc: 0.9061 - val_loss: 0.3117 - val_f_2: 0.4054 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2928 - f_2: 0.3556 - acc: 0.9060 - val_loss: 0.3077 - val_f_2: 0.4136 - val_acc: 0.9015\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.3577 - acc: 0.9065 - val_loss: 0.3066 - val_f_2: 0.3988 - val_acc: 0.9021\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.3659 - acc: 0.9067 - val_loss: 0.3088 - val_f_2: 0.4154 - val_acc: 0.9021\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2911 - f_2: 0.3697 - acc: 0.9076 - val_loss: 0.3076 - val_f_2: 0.4108 - val_acc: 0.9012\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2894 - f_2: 0.3816 - acc: 0.9088 - val_loss: 0.3075 - val_f_2: 0.4269 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2909 - f_2: 0.3634 - acc: 0.9061 - val_loss: 0.3071 - val_f_2: 0.3936 - val_acc: 0.9015\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2885 - f_2: 0.3739 - acc: 0.9066 - val_loss: 0.3065 - val_f_2: 0.3925 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.3866 - acc: 0.9082 - val_loss: 0.3061 - val_f_2: 0.4002 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.3663 - acc: 0.9070 - val_loss: 0.3064 - val_f_2: 0.4021 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2874 - f_2: 0.3860 - acc: 0.9075 - val_loss: 0.3112 - val_f_2: 0.4027 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.3801 - acc: 0.9084 - val_loss: 0.3061 - val_f_2: 0.4118 - val_acc: 0.9012\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.3601 - acc: 0.9063 - val_loss: 0.3102 - val_f_2: 0.4398 - val_acc: 0.9003\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2883 - f_2: 0.3711 - acc: 0.9076 - val_loss: 0.3091 - val_f_2: 0.4227 - val_acc: 0.9003\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.3752 - acc: 0.9077 - val_loss: 0.3047 - val_f_2: 0.4213 - val_acc: 0.9003\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.3921 - acc: 0.9080 - val_loss: 0.3082 - val_f_2: 0.4037 - val_acc: 0.8988\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2860 - f_2: 0.3759 - acc: 0.9086 - val_loss: 0.3081 - val_f_2: 0.4368 - val_acc: 0.9006\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2853 - f_2: 0.3947 - acc: 0.9087 - val_loss: 0.3040 - val_f_2: 0.4038 - val_acc: 0.9003\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2820 - f_2: 0.3880 - acc: 0.9085 - val_loss: 0.3053 - val_f_2: 0.4112 - val_acc: 0.9012\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.3889 - acc: 0.9090 - val_loss: 0.3060 - val_f_2: 0.4357 - val_acc: 0.9012\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2841 - f_2: 0.3965 - acc: 0.9095 - val_loss: 0.3136 - val_f_2: 0.4466 - val_acc: 0.8985\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2817 - f_2: 0.3887 - acc: 0.9079 - val_loss: 0.3063 - val_f_2: 0.4362 - val_acc: 0.8994\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2794 - f_2: 0.4006 - acc: 0.9091 - val_loss: 0.3050 - val_f_2: 0.4001 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2814 - f_2: 0.3791 - acc: 0.9082 - val_loss: 0.3070 - val_f_2: 0.4299 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2810 - f_2: 0.3896 - acc: 0.9082 - val_loss: 0.3064 - val_f_2: 0.4369 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.001, total=  32.7s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4729 - f_2: 0.0163 - acc: 0.8523 - val_loss: 0.3846 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3698 - f_2: 1.3940e-08 - acc: 0.8722 - val_loss: 0.3492 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3459 - f_2: 1.3626e-08 - acc: 0.8722 - val_loss: 0.3371 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3395 - f_2: 1.3839e-08 - acc: 0.8722 - val_loss: 0.3303 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3302 - f_2: 1.3509e-08 - acc: 0.8722 - val_loss: 0.3268 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.1727 - acc: 0.8896 - val_loss: 0.3242 - val_f_2: 0.2922 - val_acc: 0.9003\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.2601 - acc: 0.8973 - val_loss: 0.3223 - val_f_2: 0.3076 - val_acc: 0.9012\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.2784 - acc: 0.8980 - val_loss: 0.3199 - val_f_2: 0.3208 - val_acc: 0.9003\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.2922 - acc: 0.8984 - val_loss: 0.3180 - val_f_2: 0.3507 - val_acc: 0.9009\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.3018 - acc: 0.8994 - val_loss: 0.3159 - val_f_2: 0.3668 - val_acc: 0.9021\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3174 - acc: 0.8998 - val_loss: 0.3144 - val_f_2: 0.3612 - val_acc: 0.9015\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3249 - acc: 0.9019 - val_loss: 0.3126 - val_f_2: 0.3749 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3270 - acc: 0.9016 - val_loss: 0.3112 - val_f_2: 0.4009 - val_acc: 0.9012\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3411 - acc: 0.9017 - val_loss: 0.3103 - val_f_2: 0.3621 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3038 - f_2: 0.3413 - acc: 0.9022 - val_loss: 0.3090 - val_f_2: 0.3905 - val_acc: 0.9012\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.3356 - acc: 0.9012 - val_loss: 0.3088 - val_f_2: 0.3987 - val_acc: 0.9015\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3404 - acc: 0.9003 - val_loss: 0.3121 - val_f_2: 0.4264 - val_acc: 0.9015\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.3343 - acc: 0.9012 - val_loss: 0.3083 - val_f_2: 0.3763 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2999 - f_2: 0.3451 - acc: 0.9031 - val_loss: 0.3059 - val_f_2: 0.3940 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2989 - f_2: 0.3216 - acc: 0.9001 - val_loss: 0.3059 - val_f_2: 0.4254 - val_acc: 0.9021\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.3631 - acc: 0.9027 - val_loss: 0.3056 - val_f_2: 0.4270 - val_acc: 0.9027\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.3475 - acc: 0.9020 - val_loss: 0.3050 - val_f_2: 0.4018 - val_acc: 0.9015\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3491 - acc: 0.9016 - val_loss: 0.3042 - val_f_2: 0.3785 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2943 - f_2: 0.3525 - acc: 0.9019 - val_loss: 0.3054 - val_f_2: 0.4276 - val_acc: 0.9029\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2931 - f_2: 0.3672 - acc: 0.9030 - val_loss: 0.3055 - val_f_2: 0.3925 - val_acc: 0.9024\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3583 - acc: 0.9027 - val_loss: 0.3031 - val_f_2: 0.4241 - val_acc: 0.9012\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2906 - f_2: 0.3582 - acc: 0.9025 - val_loss: 0.3034 - val_f_2: 0.4228 - val_acc: 0.9018\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.3750 - acc: 0.9035 - val_loss: 0.3029 - val_f_2: 0.4197 - val_acc: 0.9021\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.3839 - acc: 0.9044 - val_loss: 0.3062 - val_f_2: 0.3866 - val_acc: 0.9029\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2884 - f_2: 0.3825 - acc: 0.9043 - val_loss: 0.3071 - val_f_2: 0.3930 - val_acc: 0.9021\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.3706 - acc: 0.9037 - val_loss: 0.3026 - val_f_2: 0.4242 - val_acc: 0.9018\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2865 - f_2: 0.3753 - acc: 0.9041 - val_loss: 0.3021 - val_f_2: 0.4224 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2866 - f_2: 0.3684 - acc: 0.9030 - val_loss: 0.3026 - val_f_2: 0.4276 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.3707 - acc: 0.9029 - val_loss: 0.3065 - val_f_2: 0.4321 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2869 - f_2: 0.3739 - acc: 0.9039 - val_loss: 0.3029 - val_f_2: 0.4222 - val_acc: 0.8988\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.3773 - acc: 0.9046 - val_loss: 0.3030 - val_f_2: 0.4266 - val_acc: 0.8991\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.3817 - acc: 0.9043 - val_loss: 0.3060 - val_f_2: 0.4286 - val_acc: 0.9018\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.3859 - acc: 0.9052 - val_loss: 0.3005 - val_f_2: 0.4152 - val_acc: 0.9018\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.3822 - acc: 0.9044 - val_loss: 0.3038 - val_f_2: 0.3952 - val_acc: 0.9032\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2828 - f_2: 0.3786 - acc: 0.9046 - val_loss: 0.3046 - val_f_2: 0.3974 - val_acc: 0.9029\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2853 - f_2: 0.3931 - acc: 0.9051 - val_loss: 0.3072 - val_f_2: 0.4074 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2831 - f_2: 0.3887 - acc: 0.9044 - val_loss: 0.3017 - val_f_2: 0.4127 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2823 - f_2: 0.3865 - acc: 0.9034 - val_loss: 0.3039 - val_f_2: 0.4109 - val_acc: 0.9015\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.3749 - acc: 0.9036 - val_loss: 0.3011 - val_f_2: 0.4237 - val_acc: 0.8994\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2800 - f_2: 0.3966 - acc: 0.9048 - val_loss: 0.3048 - val_f_2: 0.4204 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2820 - f_2: 0.3881 - acc: 0.9048 - val_loss: 0.3007 - val_f_2: 0.4009 - val_acc: 0.9021\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.3784 - acc: 0.9043 - val_loss: 0.3011 - val_f_2: 0.4191 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2792 - f_2: 0.3872 - acc: 0.9056 - val_loss: 0.3015 - val_f_2: 0.4285 - val_acc: 0.9009\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2793 - f_2: 0.3981 - acc: 0.9052 - val_loss: 0.3060 - val_f_2: 0.4593 - val_acc: 0.8973\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2772 - f_2: 0.4232 - acc: 0.9079 - val_loss: 0.3029 - val_f_2: 0.4170 - val_acc: 0.8991\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.001, total=  31.4s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4767 - f_2: 0.0178 - acc: 0.8499 - val_loss: 0.3616 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3764 - f_2: 1.3189e-08 - acc: 0.8691 - val_loss: 0.3296 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3531 - f_2: 1.3459e-08 - acc: 0.8691 - val_loss: 0.3200 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3424 - f_2: 1.3043e-08 - acc: 0.8691 - val_loss: 0.3138 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3359 - f_2: 1.3290e-08 - acc: 0.8691 - val_loss: 0.3130 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3329 - f_2: 0.1464 - acc: 0.8841 - val_loss: 0.3053 - val_f_2: 0.2814 - val_acc: 0.9083\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.1950 - acc: 0.8884 - val_loss: 0.3102 - val_f_2: 0.3461 - val_acc: 0.9109\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.2201 - acc: 0.8897 - val_loss: 0.3021 - val_f_2: 0.3338 - val_acc: 0.9103\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.2272 - acc: 0.8907 - val_loss: 0.3044 - val_f_2: 0.3570 - val_acc: 0.9077\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2211 - acc: 0.8886 - val_loss: 0.3077 - val_f_2: 0.3464 - val_acc: 0.9074\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2373 - acc: 0.8898 - val_loss: 0.3009 - val_f_2: 0.3441 - val_acc: 0.9091\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2529 - acc: 0.8923 - val_loss: 0.2975 - val_f_2: 0.3476 - val_acc: 0.9074\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.3257 - acc: 0.8992 - val_loss: 0.2955 - val_f_2: 0.3445 - val_acc: 0.9094\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3239 - acc: 0.8994 - val_loss: 0.2955 - val_f_2: 0.3471 - val_acc: 0.9068\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.3310 - acc: 0.8996 - val_loss: 0.2997 - val_f_2: 0.3596 - val_acc: 0.9065\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.3413 - acc: 0.9002 - val_loss: 0.2955 - val_f_2: 0.3685 - val_acc: 0.9074\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3431 - acc: 0.9000 - val_loss: 0.2942 - val_f_2: 0.3586 - val_acc: 0.9062\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3386 - acc: 0.9002 - val_loss: 0.2905 - val_f_2: 0.3455 - val_acc: 0.9062\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3561 - acc: 0.9009 - val_loss: 0.2987 - val_f_2: 0.3631 - val_acc: 0.9071\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3428 - acc: 0.8997 - val_loss: 0.2989 - val_f_2: 0.3816 - val_acc: 0.9080\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.3509 - acc: 0.8989 - val_loss: 0.2961 - val_f_2: 0.4045 - val_acc: 0.9080\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3734 - acc: 0.9026 - val_loss: 0.2912 - val_f_2: 0.3907 - val_acc: 0.9086\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3587 - acc: 0.9006 - val_loss: 0.2895 - val_f_2: 0.4014 - val_acc: 0.9083\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3704 - acc: 0.8998 - val_loss: 0.3001 - val_f_2: 0.4124 - val_acc: 0.9065\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3005 - f_2: 0.3666 - acc: 0.9005 - val_loss: 0.2901 - val_f_2: 0.3885 - val_acc: 0.9059\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3809 - acc: 0.9023 - val_loss: 0.2878 - val_f_2: 0.3710 - val_acc: 0.9074\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3602 - acc: 0.9010 - val_loss: 0.2895 - val_f_2: 0.3854 - val_acc: 0.9068\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2956 - f_2: 0.3821 - acc: 0.9029 - val_loss: 0.2870 - val_f_2: 0.3729 - val_acc: 0.9077\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.3902 - acc: 0.9035 - val_loss: 0.2960 - val_f_2: 0.4152 - val_acc: 0.9071\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2960 - f_2: 0.3797 - acc: 0.9021 - val_loss: 0.2873 - val_f_2: 0.3954 - val_acc: 0.9065\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2942 - f_2: 0.3836 - acc: 0.9026 - val_loss: 0.2905 - val_f_2: 0.4338 - val_acc: 0.9068\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2937 - f_2: 0.3871 - acc: 0.9015 - val_loss: 0.2882 - val_f_2: 0.4222 - val_acc: 0.9071\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.3928 - acc: 0.9032 - val_loss: 0.2947 - val_f_2: 0.4310 - val_acc: 0.9071\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2940 - f_2: 0.3844 - acc: 0.9018 - val_loss: 0.2872 - val_f_2: 0.4033 - val_acc: 0.9077\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3925 - acc: 0.9024 - val_loss: 0.2885 - val_f_2: 0.3902 - val_acc: 0.9068\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3779 - acc: 0.9027 - val_loss: 0.2913 - val_f_2: 0.4366 - val_acc: 0.9059\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2913 - f_2: 0.4015 - acc: 0.9031 - val_loss: 0.2913 - val_f_2: 0.4298 - val_acc: 0.9062\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2905 - f_2: 0.4002 - acc: 0.9043 - val_loss: 0.2900 - val_f_2: 0.4117 - val_acc: 0.9071\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.4043 - acc: 0.9041 - val_loss: 0.2906 - val_f_2: 0.4336 - val_acc: 0.9047\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2899 - f_2: 0.4080 - acc: 0.9025 - val_loss: 0.2907 - val_f_2: 0.4330 - val_acc: 0.9065\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2910 - f_2: 0.3952 - acc: 0.9032 - val_loss: 0.2953 - val_f_2: 0.4217 - val_acc: 0.9056\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.3992 - acc: 0.9026 - val_loss: 0.2855 - val_f_2: 0.4027 - val_acc: 0.9059\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2900 - f_2: 0.3865 - acc: 0.9015 - val_loss: 0.2929 - val_f_2: 0.4271 - val_acc: 0.9056\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.3980 - acc: 0.9039 - val_loss: 0.2896 - val_f_2: 0.4226 - val_acc: 0.9044\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.3909 - acc: 0.9019 - val_loss: 0.2884 - val_f_2: 0.3972 - val_acc: 0.9053\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.3996 - acc: 0.9040 - val_loss: 0.2864 - val_f_2: 0.4127 - val_acc: 0.9038\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2858 - f_2: 0.3987 - acc: 0.9029 - val_loss: 0.2924 - val_f_2: 0.4497 - val_acc: 0.9035\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2866 - f_2: 0.4048 - acc: 0.9036 - val_loss: 0.2844 - val_f_2: 0.3991 - val_acc: 0.9056\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2862 - f_2: 0.3997 - acc: 0.9034 - val_loss: 0.2857 - val_f_2: 0.4100 - val_acc: 0.9068\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2836 - f_2: 0.4109 - acc: 0.9047 - val_loss: 0.2881 - val_f_2: 0.4351 - val_acc: 0.9035\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.001, total=  31.6s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7916 - f_2: 0.0173 - acc: 0.8590 - val_loss: 0.5119 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4279 - f_2: 1.3974e-08 - acc: 0.8753 - val_loss: 0.3849 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3693 - f_2: 1.3657e-08 - acc: 0.8753 - val_loss: 0.3612 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3546 - f_2: 1.3939e-08 - acc: 0.8753 - val_loss: 0.3488 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3429 - f_2: 1.3868e-08 - acc: 0.8753 - val_loss: 0.3408 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3382 - f_2: 1.3940e-08 - acc: 0.8753 - val_loss: 0.3367 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3327 - f_2: 0.0486 - acc: 0.8798 - val_loss: 0.3328 - val_f_2: 0.2467 - val_acc: 0.8953\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3304 - f_2: 0.1621 - acc: 0.8924 - val_loss: 0.3304 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.1916 - acc: 0.8948 - val_loss: 0.3287 - val_f_2: 0.2898 - val_acc: 0.8988\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3270 - f_2: 0.1870 - acc: 0.8942 - val_loss: 0.3263 - val_f_2: 0.2854 - val_acc: 0.8997\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2304 - acc: 0.8977 - val_loss: 0.3250 - val_f_2: 0.2972 - val_acc: 0.8991\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.2272 - acc: 0.8975 - val_loss: 0.3240 - val_f_2: 0.3104 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2499 - acc: 0.8997 - val_loss: 0.3229 - val_f_2: 0.3006 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.2658 - acc: 0.9002 - val_loss: 0.3220 - val_f_2: 0.2953 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2551 - acc: 0.8989 - val_loss: 0.3201 - val_f_2: 0.3051 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.2549 - acc: 0.8999 - val_loss: 0.3201 - val_f_2: 0.3510 - val_acc: 0.9009\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.2676 - acc: 0.9001 - val_loss: 0.3185 - val_f_2: 0.3344 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3136 - f_2: 0.2901 - acc: 0.9009 - val_loss: 0.3189 - val_f_2: 0.3065 - val_acc: 0.8991\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.2800 - acc: 0.9010 - val_loss: 0.3159 - val_f_2: 0.2958 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.2892 - acc: 0.9023 - val_loss: 0.3166 - val_f_2: 0.3175 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.2852 - acc: 0.9008 - val_loss: 0.3158 - val_f_2: 0.3101 - val_acc: 0.8994\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.2925 - acc: 0.9009 - val_loss: 0.3195 - val_f_2: 0.3470 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.2865 - acc: 0.9014 - val_loss: 0.3149 - val_f_2: 0.3596 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.3018 - acc: 0.9026 - val_loss: 0.3238 - val_f_2: 0.2948 - val_acc: 0.8997\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.3025 - acc: 0.9034 - val_loss: 0.3140 - val_f_2: 0.3982 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3081 - f_2: 0.3005 - acc: 0.9015 - val_loss: 0.3117 - val_f_2: 0.3314 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3183 - acc: 0.9025 - val_loss: 0.3134 - val_f_2: 0.3105 - val_acc: 0.9015\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3036 - acc: 0.9026 - val_loss: 0.3115 - val_f_2: 0.3473 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.3077 - acc: 0.9025 - val_loss: 0.3108 - val_f_2: 0.3104 - val_acc: 0.9009\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.2978 - acc: 0.9026 - val_loss: 0.3103 - val_f_2: 0.3491 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3213 - acc: 0.9046 - val_loss: 0.3125 - val_f_2: 0.3429 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3179 - acc: 0.9046 - val_loss: 0.3107 - val_f_2: 0.3654 - val_acc: 0.8994\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3267 - acc: 0.9031 - val_loss: 0.3185 - val_f_2: 0.3213 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3038 - f_2: 0.3164 - acc: 0.9039 - val_loss: 0.3090 - val_f_2: 0.3231 - val_acc: 0.9003\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3185 - acc: 0.9045 - val_loss: 0.3090 - val_f_2: 0.3780 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3019 - f_2: 0.3324 - acc: 0.9046 - val_loss: 0.3117 - val_f_2: 0.3991 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3303 - acc: 0.9042 - val_loss: 0.3075 - val_f_2: 0.3211 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.3134 - acc: 0.9040 - val_loss: 0.3074 - val_f_2: 0.3403 - val_acc: 0.9021\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3243 - acc: 0.9035 - val_loss: 0.3096 - val_f_2: 0.3497 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3209 - acc: 0.9036 - val_loss: 0.3074 - val_f_2: 0.3722 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3170 - acc: 0.9028 - val_loss: 0.3090 - val_f_2: 0.3366 - val_acc: 0.9003\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2978 - f_2: 0.3362 - acc: 0.9049 - val_loss: 0.3076 - val_f_2: 0.3881 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2978 - f_2: 0.3452 - acc: 0.9051 - val_loss: 0.3065 - val_f_2: 0.3558 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3317 - acc: 0.9042 - val_loss: 0.3072 - val_f_2: 0.3591 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3310 - acc: 0.9054 - val_loss: 0.3057 - val_f_2: 0.3419 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2963 - f_2: 0.3403 - acc: 0.9062 - val_loss: 0.3068 - val_f_2: 0.3688 - val_acc: 0.9012\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.3238 - acc: 0.9046 - val_loss: 0.3069 - val_f_2: 0.4003 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2978 - f_2: 0.3290 - acc: 0.9037 - val_loss: 0.3055 - val_f_2: 0.3623 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3456 - acc: 0.9067 - val_loss: 0.3052 - val_f_2: 0.3332 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.3302 - acc: 0.9047 - val_loss: 0.3118 - val_f_2: 0.3435 - val_acc: 0.8994\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.01, total=  32.8s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7946 - f_2: 0.0126 - acc: 0.8537 - val_loss: 0.5099 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4315 - f_2: 1.3663e-08 - acc: 0.8722 - val_loss: 0.3842 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3701 - f_2: 1.3388e-08 - acc: 0.8722 - val_loss: 0.3594 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3544 - f_2: 1.3413e-08 - acc: 0.8722 - val_loss: 0.3486 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3445 - f_2: 1.3567e-08 - acc: 0.8722 - val_loss: 0.3412 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3393 - f_2: 1.3709e-08 - acc: 0.8722 - val_loss: 0.3363 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3351 - f_2: 0.1261 - acc: 0.8851 - val_loss: 0.3343 - val_f_2: 0.2834 - val_acc: 0.8994\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3326 - f_2: 0.1984 - acc: 0.8930 - val_loss: 0.3341 - val_f_2: 0.2683 - val_acc: 0.8976\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3284 - f_2: 0.2195 - acc: 0.8956 - val_loss: 0.3292 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3261 - f_2: 0.2376 - acc: 0.8972 - val_loss: 0.3259 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.2567 - acc: 0.8974 - val_loss: 0.3242 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.2561 - acc: 0.8984 - val_loss: 0.3231 - val_f_2: 0.2898 - val_acc: 0.8988\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.2651 - acc: 0.8988 - val_loss: 0.3217 - val_f_2: 0.2870 - val_acc: 0.8997\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.2687 - acc: 0.8987 - val_loss: 0.3222 - val_f_2: 0.3152 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2947 - acc: 0.9000 - val_loss: 0.3206 - val_f_2: 0.3270 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3182 - f_2: 0.2924 - acc: 0.8995 - val_loss: 0.3189 - val_f_2: 0.2901 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.2851 - acc: 0.8992 - val_loss: 0.3182 - val_f_2: 0.2871 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2922 - acc: 0.9002 - val_loss: 0.3166 - val_f_2: 0.2917 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3122 - f_2: 0.3087 - acc: 0.9009 - val_loss: 0.3168 - val_f_2: 0.3539 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3073 - acc: 0.9000 - val_loss: 0.3162 - val_f_2: 0.2925 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3109 - acc: 0.9017 - val_loss: 0.3153 - val_f_2: 0.3501 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.3183 - acc: 0.9029 - val_loss: 0.3142 - val_f_2: 0.3284 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3124 - acc: 0.9015 - val_loss: 0.3140 - val_f_2: 0.2960 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3022 - acc: 0.9010 - val_loss: 0.3111 - val_f_2: 0.3157 - val_acc: 0.9003\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3138 - acc: 0.9010 - val_loss: 0.3115 - val_f_2: 0.3157 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3175 - acc: 0.9034 - val_loss: 0.3121 - val_f_2: 0.3507 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.3321 - acc: 0.9028 - val_loss: 0.3112 - val_f_2: 0.3446 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3331 - acc: 0.9023 - val_loss: 0.3084 - val_f_2: 0.3320 - val_acc: 0.9024\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3195 - acc: 0.9039 - val_loss: 0.3132 - val_f_2: 0.3445 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3402 - acc: 0.9026 - val_loss: 0.3099 - val_f_2: 0.3165 - val_acc: 0.9012\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3274 - acc: 0.9026 - val_loss: 0.3141 - val_f_2: 0.3555 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3313 - acc: 0.9036 - val_loss: 0.3071 - val_f_2: 0.3415 - val_acc: 0.9015\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3208 - acc: 0.9017 - val_loss: 0.3066 - val_f_2: 0.3072 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3374 - acc: 0.9028 - val_loss: 0.3102 - val_f_2: 0.3017 - val_acc: 0.9003\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3294 - acc: 0.9020 - val_loss: 0.3072 - val_f_2: 0.3548 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3014 - f_2: 0.3415 - acc: 0.9040 - val_loss: 0.3062 - val_f_2: 0.3541 - val_acc: 0.9015\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.3368 - acc: 0.9029 - val_loss: 0.3076 - val_f_2: 0.3041 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.3374 - acc: 0.9042 - val_loss: 0.3071 - val_f_2: 0.2978 - val_acc: 0.9006\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.3328 - acc: 0.9034 - val_loss: 0.3132 - val_f_2: 0.3138 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3335 - acc: 0.9032 - val_loss: 0.3080 - val_f_2: 0.3954 - val_acc: 0.9018\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3246 - acc: 0.9019 - val_loss: 0.3062 - val_f_2: 0.3569 - val_acc: 0.9012\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2992 - f_2: 0.3344 - acc: 0.9037 - val_loss: 0.3043 - val_f_2: 0.3889 - val_acc: 0.9024\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.3317 - acc: 0.9028 - val_loss: 0.3035 - val_f_2: 0.3446 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3436 - acc: 0.9034 - val_loss: 0.3102 - val_f_2: 0.2999 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2969 - f_2: 0.3468 - acc: 0.9047 - val_loss: 0.3046 - val_f_2: 0.3188 - val_acc: 0.9012\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2962 - f_2: 0.3563 - acc: 0.9054 - val_loss: 0.3034 - val_f_2: 0.3334 - val_acc: 0.9012\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.3474 - acc: 0.9046 - val_loss: 0.3030 - val_f_2: 0.3227 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3449 - acc: 0.9034 - val_loss: 0.3032 - val_f_2: 0.3594 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3535 - acc: 0.9043 - val_loss: 0.3033 - val_f_2: 0.3869 - val_acc: 0.9021\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.3596 - acc: 0.9043 - val_loss: 0.3024 - val_f_2: 0.3147 - val_acc: 0.9012\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.01, total=  32.2s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7975 - f_2: 0.0153 - acc: 0.8515 - val_loss: 0.4903 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4347 - f_2: 1.3106e-08 - acc: 0.8691 - val_loss: 0.3611 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3795 - f_2: 1.3445e-08 - acc: 0.8691 - val_loss: 0.3420 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3594 - f_2: 1.3224e-08 - acc: 0.8691 - val_loss: 0.3289 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3495 - f_2: 1.3487e-08 - acc: 0.8691 - val_loss: 0.3217 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3447 - f_2: 0.0176 - acc: 0.8706 - val_loss: 0.3174 - val_f_2: 0.1184 - val_acc: 0.8912\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3391 - f_2: 0.1676 - acc: 0.8873 - val_loss: 0.3140 - val_f_2: 0.2295 - val_acc: 0.9027\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3377 - f_2: 0.1901 - acc: 0.8897 - val_loss: 0.3143 - val_f_2: 0.3061 - val_acc: 0.9103\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3350 - f_2: 0.2113 - acc: 0.8912 - val_loss: 0.3102 - val_f_2: 0.3039 - val_acc: 0.9109\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3319 - f_2: 0.2297 - acc: 0.8922 - val_loss: 0.3096 - val_f_2: 0.3049 - val_acc: 0.9103\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3320 - f_2: 0.2320 - acc: 0.8931 - val_loss: 0.3071 - val_f_2: 0.3100 - val_acc: 0.9109\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3278 - f_2: 0.2642 - acc: 0.8950 - val_loss: 0.3066 - val_f_2: 0.3100 - val_acc: 0.9109\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3270 - f_2: 0.2574 - acc: 0.8950 - val_loss: 0.3041 - val_f_2: 0.3234 - val_acc: 0.9106\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3254 - f_2: 0.2641 - acc: 0.8955 - val_loss: 0.3039 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2722 - acc: 0.8964 - val_loss: 0.3058 - val_f_2: 0.3271 - val_acc: 0.9097\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.2801 - acc: 0.8959 - val_loss: 0.3009 - val_f_2: 0.3193 - val_acc: 0.9115\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2721 - acc: 0.8961 - val_loss: 0.2991 - val_f_2: 0.3208 - val_acc: 0.9103\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.2807 - acc: 0.8957 - val_loss: 0.2982 - val_f_2: 0.3193 - val_acc: 0.9115\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2877 - acc: 0.8965 - val_loss: 0.2979 - val_f_2: 0.3233 - val_acc: 0.9109\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.2886 - acc: 0.8966 - val_loss: 0.2984 - val_f_2: 0.3325 - val_acc: 0.9086\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2926 - acc: 0.8976 - val_loss: 0.2980 - val_f_2: 0.3411 - val_acc: 0.9083\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.3018 - acc: 0.8972 - val_loss: 0.2970 - val_f_2: 0.3332 - val_acc: 0.9094\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.3111 - acc: 0.8985 - val_loss: 0.2951 - val_f_2: 0.3295 - val_acc: 0.9088\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.2936 - acc: 0.8965 - val_loss: 0.2974 - val_f_2: 0.3392 - val_acc: 0.9077\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.3077 - acc: 0.8970 - val_loss: 0.2939 - val_f_2: 0.3313 - val_acc: 0.9094\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.3057 - acc: 0.8974 - val_loss: 0.2930 - val_f_2: 0.3193 - val_acc: 0.9115\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.3175 - acc: 0.8987 - val_loss: 0.2935 - val_f_2: 0.3215 - val_acc: 0.9112\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3126 - acc: 0.8983 - val_loss: 0.2934 - val_f_2: 0.3298 - val_acc: 0.9100\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.3050 - acc: 0.8967 - val_loss: 0.2916 - val_f_2: 0.3350 - val_acc: 0.9080\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.3244 - acc: 0.8985 - val_loss: 0.2909 - val_f_2: 0.3147 - val_acc: 0.9109\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3020 - acc: 0.8978 - val_loss: 0.2903 - val_f_2: 0.3278 - val_acc: 0.9103\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3122 - acc: 0.8991 - val_loss: 0.2914 - val_f_2: 0.3398 - val_acc: 0.9083\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3131 - acc: 0.8970 - val_loss: 0.2908 - val_f_2: 0.3708 - val_acc: 0.9086\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.3151 - acc: 0.8973 - val_loss: 0.2888 - val_f_2: 0.3182 - val_acc: 0.9097\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3072 - f_2: 0.3316 - acc: 0.8999 - val_loss: 0.2924 - val_f_2: 0.3708 - val_acc: 0.9088\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.3310 - acc: 0.9004 - val_loss: 0.2896 - val_f_2: 0.3353 - val_acc: 0.9088\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3369 - acc: 0.8998 - val_loss: 0.2913 - val_f_2: 0.3279 - val_acc: 0.9106\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3150 - acc: 0.8978 - val_loss: 0.2884 - val_f_2: 0.3434 - val_acc: 0.9088\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3056 - f_2: 0.3293 - acc: 0.8990 - val_loss: 0.2869 - val_f_2: 0.3276 - val_acc: 0.9100\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3282 - acc: 0.8994 - val_loss: 0.2919 - val_f_2: 0.3888 - val_acc: 0.9074\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3255 - acc: 0.8993 - val_loss: 0.2878 - val_f_2: 0.3347 - val_acc: 0.9097\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3226 - acc: 0.8980 - val_loss: 0.2869 - val_f_2: 0.3397 - val_acc: 0.9103\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3203 - acc: 0.8986 - val_loss: 0.2943 - val_f_2: 0.4139 - val_acc: 0.9068\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3333 - acc: 0.8997 - val_loss: 0.2868 - val_f_2: 0.3409 - val_acc: 0.9091\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.3340 - acc: 0.8991 - val_loss: 0.2855 - val_f_2: 0.3222 - val_acc: 0.9109\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3020 - f_2: 0.3239 - acc: 0.8983 - val_loss: 0.2873 - val_f_2: 0.3506 - val_acc: 0.9109\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3341 - acc: 0.8998 - val_loss: 0.2862 - val_f_2: 0.3398 - val_acc: 0.9106\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3436 - acc: 0.8997 - val_loss: 0.2854 - val_f_2: 0.3376 - val_acc: 0.9106\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.3338 - acc: 0.9004 - val_loss: 0.2853 - val_f_2: 0.3340 - val_acc: 0.9115\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3344 - acc: 0.8992 - val_loss: 0.2882 - val_f_2: 0.4043 - val_acc: 0.9077\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.01, total=  32.7s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4651 - f_2: 0.0155 - acc: 0.8610 - val_loss: 0.3882 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3750 - f_2: 1.3820e-08 - acc: 0.8753 - val_loss: 0.3598 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3514 - f_2: 1.3941e-08 - acc: 0.8753 - val_loss: 0.3439 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3398 - f_2: 0.0045 - acc: 0.8759 - val_loss: 0.3346 - val_f_2: 0.1519 - val_acc: 0.8841\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3328 - f_2: 0.2026 - acc: 0.8958 - val_loss: 0.3299 - val_f_2: 0.2767 - val_acc: 0.8985\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3287 - f_2: 0.2301 - acc: 0.8994 - val_loss: 0.3261 - val_f_2: 0.2906 - val_acc: 0.8997\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3252 - f_2: 0.2592 - acc: 0.9012 - val_loss: 0.3231 - val_f_2: 0.3019 - val_acc: 0.9006\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2673 - acc: 0.9022 - val_loss: 0.3236 - val_f_2: 0.3097 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3149 - f_2: 0.3053 - acc: 0.9034 - val_loss: 0.3178 - val_f_2: 0.3150 - val_acc: 0.9003\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.3124 - acc: 0.9045 - val_loss: 0.3171 - val_f_2: 0.3530 - val_acc: 0.9003\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.3332 - acc: 0.9049 - val_loss: 0.3127 - val_f_2: 0.3505 - val_acc: 0.9003\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3437 - acc: 0.9065 - val_loss: 0.3156 - val_f_2: 0.3528 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.3677 - acc: 0.9076 - val_loss: 0.3089 - val_f_2: 0.3779 - val_acc: 0.9021\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2989 - f_2: 0.3823 - acc: 0.9072 - val_loss: 0.3065 - val_f_2: 0.3753 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3703 - acc: 0.9064 - val_loss: 0.3055 - val_f_2: 0.4001 - val_acc: 0.9006\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2956 - f_2: 0.3789 - acc: 0.9072 - val_loss: 0.3046 - val_f_2: 0.4210 - val_acc: 0.9015\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3949 - acc: 0.9086 - val_loss: 0.3031 - val_f_2: 0.3951 - val_acc: 0.9021\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3968 - acc: 0.9085 - val_loss: 0.3164 - val_f_2: 0.3790 - val_acc: 0.9018\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2900 - f_2: 0.4007 - acc: 0.9090 - val_loss: 0.3022 - val_f_2: 0.3877 - val_acc: 0.9021\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.4034 - acc: 0.9090 - val_loss: 0.3018 - val_f_2: 0.4099 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2913 - f_2: 0.4055 - acc: 0.9102 - val_loss: 0.3014 - val_f_2: 0.3998 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.4064 - acc: 0.9091 - val_loss: 0.3003 - val_f_2: 0.4144 - val_acc: 0.9021\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.4150 - acc: 0.9096 - val_loss: 0.3020 - val_f_2: 0.4029 - val_acc: 0.9027\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.4045 - acc: 0.9096 - val_loss: 0.2999 - val_f_2: 0.4036 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.4099 - acc: 0.9091 - val_loss: 0.3030 - val_f_2: 0.3782 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.4121 - acc: 0.9102 - val_loss: 0.3013 - val_f_2: 0.4177 - val_acc: 0.8985\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4275 - acc: 0.9101 - val_loss: 0.3027 - val_f_2: 0.3733 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4217 - acc: 0.9108 - val_loss: 0.3038 - val_f_2: 0.3929 - val_acc: 0.8997\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2823 - f_2: 0.4249 - acc: 0.9113 - val_loss: 0.3145 - val_f_2: 0.3685 - val_acc: 0.9009\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2813 - f_2: 0.4166 - acc: 0.9100 - val_loss: 0.3012 - val_f_2: 0.4240 - val_acc: 0.9003\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2812 - f_2: 0.4290 - acc: 0.9108 - val_loss: 0.2992 - val_f_2: 0.3835 - val_acc: 0.8991\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2796 - f_2: 0.4321 - acc: 0.9119 - val_loss: 0.3045 - val_f_2: 0.4450 - val_acc: 0.8962\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2804 - f_2: 0.4235 - acc: 0.9107 - val_loss: 0.3012 - val_f_2: 0.4195 - val_acc: 0.8988\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2783 - f_2: 0.4245 - acc: 0.9115 - val_loss: 0.3008 - val_f_2: 0.3808 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2772 - f_2: 0.4253 - acc: 0.9113 - val_loss: 0.3001 - val_f_2: 0.4190 - val_acc: 0.8988\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2782 - f_2: 0.4371 - acc: 0.9109 - val_loss: 0.3015 - val_f_2: 0.4196 - val_acc: 0.8973\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2786 - f_2: 0.4447 - acc: 0.9125 - val_loss: 0.3047 - val_f_2: 0.4565 - val_acc: 0.8979\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2765 - f_2: 0.4440 - acc: 0.9132 - val_loss: 0.3031 - val_f_2: 0.3984 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2769 - f_2: 0.4414 - acc: 0.9119 - val_loss: 0.3022 - val_f_2: 0.4024 - val_acc: 0.8988\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4364 - acc: 0.9120 - val_loss: 0.3006 - val_f_2: 0.4119 - val_acc: 0.8962\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2764 - f_2: 0.4403 - acc: 0.9127 - val_loss: 0.3045 - val_f_2: 0.3962 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2742 - f_2: 0.4553 - acc: 0.9135 - val_loss: 0.3014 - val_f_2: 0.4068 - val_acc: 0.8991\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2769 - f_2: 0.4437 - acc: 0.9113 - val_loss: 0.3008 - val_f_2: 0.4194 - val_acc: 0.8976\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2738 - f_2: 0.4439 - acc: 0.9130 - val_loss: 0.3038 - val_f_2: 0.3934 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2742 - f_2: 0.4497 - acc: 0.9135 - val_loss: 0.3065 - val_f_2: 0.3728 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2754 - f_2: 0.4426 - acc: 0.9130 - val_loss: 0.3005 - val_f_2: 0.4097 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2740 - f_2: 0.4358 - acc: 0.9122 - val_loss: 0.3020 - val_f_2: 0.4256 - val_acc: 0.8956\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2729 - f_2: 0.4542 - acc: 0.9129 - val_loss: 0.3043 - val_f_2: 0.4075 - val_acc: 0.8979\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2718 - f_2: 0.4620 - acc: 0.9146 - val_loss: 0.3031 - val_f_2: 0.4195 - val_acc: 0.8971\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2739 - f_2: 0.4614 - acc: 0.9134 - val_loss: 0.3017 - val_f_2: 0.4298 - val_acc: 0.8950\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.001, total=  33.8s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4646 - f_2: 0.0158 - acc: 0.8580 - val_loss: 0.3875 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3730 - f_2: 1.3823e-08 - acc: 0.8722 - val_loss: 0.3588 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3532 - f_2: 1.3467e-08 - acc: 0.8722 - val_loss: 0.3432 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3421 - f_2: 0.0240 - acc: 0.8747 - val_loss: 0.3357 - val_f_2: 0.2465 - val_acc: 0.8950\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3329 - f_2: 0.2220 - acc: 0.8965 - val_loss: 0.3321 - val_f_2: 0.2776 - val_acc: 0.8991\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.2529 - acc: 0.8990 - val_loss: 0.3259 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.2617 - acc: 0.9001 - val_loss: 0.3225 - val_f_2: 0.2869 - val_acc: 0.8994\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3193 - f_2: 0.2869 - acc: 0.9015 - val_loss: 0.3198 - val_f_2: 0.3069 - val_acc: 0.9012\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.2746 - acc: 0.9006 - val_loss: 0.3215 - val_f_2: 0.3238 - val_acc: 0.8988\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3149 - f_2: 0.2945 - acc: 0.9020 - val_loss: 0.3190 - val_f_2: 0.3118 - val_acc: 0.9006\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3099 - acc: 0.9032 - val_loss: 0.3146 - val_f_2: 0.3170 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3089 - f_2: 0.3250 - acc: 0.9037 - val_loss: 0.3135 - val_f_2: 0.3455 - val_acc: 0.9018\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3269 - acc: 0.9029 - val_loss: 0.3147 - val_f_2: 0.3339 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.3497 - acc: 0.9046 - val_loss: 0.3136 - val_f_2: 0.3323 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3519 - acc: 0.9054 - val_loss: 0.3101 - val_f_2: 0.3455 - val_acc: 0.9006\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3678 - acc: 0.9064 - val_loss: 0.3108 - val_f_2: 0.3757 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.3714 - acc: 0.9062 - val_loss: 0.3090 - val_f_2: 0.3711 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3702 - acc: 0.9061 - val_loss: 0.3075 - val_f_2: 0.4061 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.3842 - acc: 0.9065 - val_loss: 0.3070 - val_f_2: 0.3932 - val_acc: 0.9018\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.3837 - acc: 0.9063 - val_loss: 0.3152 - val_f_2: 0.3518 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2964 - f_2: 0.3854 - acc: 0.9060 - val_loss: 0.3067 - val_f_2: 0.4207 - val_acc: 0.9021\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2955 - f_2: 0.4014 - acc: 0.9071 - val_loss: 0.3072 - val_f_2: 0.4306 - val_acc: 0.9041\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3928 - acc: 0.9068 - val_loss: 0.3113 - val_f_2: 0.3855 - val_acc: 0.9012\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.4050 - acc: 0.9074 - val_loss: 0.3036 - val_f_2: 0.4071 - val_acc: 0.9018\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2916 - f_2: 0.4034 - acc: 0.9083 - val_loss: 0.3038 - val_f_2: 0.4264 - val_acc: 0.9035\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2903 - f_2: 0.4122 - acc: 0.9078 - val_loss: 0.3103 - val_f_2: 0.4033 - val_acc: 0.9015\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.4124 - acc: 0.9074 - val_loss: 0.3108 - val_f_2: 0.3944 - val_acc: 0.9024\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2900 - f_2: 0.4267 - acc: 0.9089 - val_loss: 0.3068 - val_f_2: 0.4037 - val_acc: 0.9032\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2894 - f_2: 0.4275 - acc: 0.9088 - val_loss: 0.3032 - val_f_2: 0.4212 - val_acc: 0.9032\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2878 - f_2: 0.4307 - acc: 0.9084 - val_loss: 0.3030 - val_f_2: 0.4347 - val_acc: 0.9032\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2851 - f_2: 0.4275 - acc: 0.9083 - val_loss: 0.3037 - val_f_2: 0.4311 - val_acc: 0.9035\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.4275 - acc: 0.9091 - val_loss: 0.3073 - val_f_2: 0.4227 - val_acc: 0.9032\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2857 - f_2: 0.4356 - acc: 0.9085 - val_loss: 0.3065 - val_f_2: 0.4375 - val_acc: 0.9038\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.4394 - acc: 0.9096 - val_loss: 0.3044 - val_f_2: 0.4329 - val_acc: 0.9024\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.4390 - acc: 0.9099 - val_loss: 0.3053 - val_f_2: 0.4369 - val_acc: 0.9038\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2840 - f_2: 0.4299 - acc: 0.9095 - val_loss: 0.3030 - val_f_2: 0.4393 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.4489 - acc: 0.9111 - val_loss: 0.3038 - val_f_2: 0.4516 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4486 - acc: 0.9099 - val_loss: 0.3034 - val_f_2: 0.4462 - val_acc: 0.9024\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4553 - acc: 0.9098 - val_loss: 0.3035 - val_f_2: 0.4225 - val_acc: 0.9027\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2828 - f_2: 0.4524 - acc: 0.9094 - val_loss: 0.3047 - val_f_2: 0.4475 - val_acc: 0.8991\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.4555 - acc: 0.9103 - val_loss: 0.3037 - val_f_2: 0.4398 - val_acc: 0.9003\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4567 - acc: 0.9108 - val_loss: 0.3053 - val_f_2: 0.4463 - val_acc: 0.9027\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2807 - f_2: 0.4467 - acc: 0.9096 - val_loss: 0.3038 - val_f_2: 0.4424 - val_acc: 0.9027\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2804 - f_2: 0.4615 - acc: 0.9116 - val_loss: 0.3042 - val_f_2: 0.4458 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4506 - acc: 0.9106 - val_loss: 0.3078 - val_f_2: 0.4592 - val_acc: 0.8979\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2810 - f_2: 0.4631 - acc: 0.9113 - val_loss: 0.3029 - val_f_2: 0.4428 - val_acc: 0.9027\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2806 - f_2: 0.4643 - acc: 0.9116 - val_loss: 0.3022 - val_f_2: 0.4359 - val_acc: 0.9015\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4652 - acc: 0.9115 - val_loss: 0.3048 - val_f_2: 0.4537 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2789 - f_2: 0.4729 - acc: 0.9113 - val_loss: 0.3032 - val_f_2: 0.4497 - val_acc: 0.8991\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2766 - f_2: 0.4701 - acc: 0.9114 - val_loss: 0.3057 - val_f_2: 0.4439 - val_acc: 0.8985\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.001, total=  32.1s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4718 - f_2: 0.0106 - acc: 0.8529 - val_loss: 0.3612 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3807 - f_2: 1.3402e-08 - acc: 0.8691 - val_loss: 0.3331 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3594 - f_2: 1.3286e-08 - acc: 0.8691 - val_loss: 0.3250 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3477 - f_2: 0.0178 - acc: 0.8711 - val_loss: 0.3148 - val_f_2: 0.2639 - val_acc: 0.9062\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3385 - f_2: 0.2133 - acc: 0.8924 - val_loss: 0.3110 - val_f_2: 0.3035 - val_acc: 0.9097\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3335 - f_2: 0.2412 - acc: 0.8953 - val_loss: 0.3098 - val_f_2: 0.3264 - val_acc: 0.9103\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.2688 - acc: 0.8970 - val_loss: 0.3048 - val_f_2: 0.3195 - val_acc: 0.9103\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2763 - acc: 0.8981 - val_loss: 0.3056 - val_f_2: 0.3406 - val_acc: 0.9106\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.2915 - acc: 0.8984 - val_loss: 0.3035 - val_f_2: 0.3412 - val_acc: 0.9109\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.2964 - acc: 0.8978 - val_loss: 0.3026 - val_f_2: 0.3484 - val_acc: 0.9109\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.3117 - acc: 0.8981 - val_loss: 0.2965 - val_f_2: 0.3330 - val_acc: 0.9118\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3163 - acc: 0.8995 - val_loss: 0.2945 - val_f_2: 0.3478 - val_acc: 0.9103\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3302 - acc: 0.9001 - val_loss: 0.2944 - val_f_2: 0.3549 - val_acc: 0.9094\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3356 - acc: 0.9000 - val_loss: 0.2938 - val_f_2: 0.3659 - val_acc: 0.9097\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3378 - acc: 0.8992 - val_loss: 0.2938 - val_f_2: 0.3670 - val_acc: 0.9106\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3500 - acc: 0.9007 - val_loss: 0.2933 - val_f_2: 0.3936 - val_acc: 0.9106\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3496 - acc: 0.9006 - val_loss: 0.2910 - val_f_2: 0.3724 - val_acc: 0.9100\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3595 - acc: 0.9003 - val_loss: 0.2906 - val_f_2: 0.3866 - val_acc: 0.9100\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3657 - acc: 0.9006 - val_loss: 0.2929 - val_f_2: 0.4074 - val_acc: 0.9091\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3801 - acc: 0.9017 - val_loss: 0.2894 - val_f_2: 0.3863 - val_acc: 0.9103\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.3776 - acc: 0.9021 - val_loss: 0.2895 - val_f_2: 0.3965 - val_acc: 0.9103\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3815 - acc: 0.9014 - val_loss: 0.2894 - val_f_2: 0.4078 - val_acc: 0.9109\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3906 - acc: 0.9019 - val_loss: 0.2881 - val_f_2: 0.3821 - val_acc: 0.9097\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3965 - acc: 0.9030 - val_loss: 0.2949 - val_f_2: 0.4342 - val_acc: 0.9088\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.3894 - acc: 0.9026 - val_loss: 0.2919 - val_f_2: 0.4305 - val_acc: 0.9083\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2968 - f_2: 0.3999 - acc: 0.9031 - val_loss: 0.2866 - val_f_2: 0.4046 - val_acc: 0.9097\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.4052 - acc: 0.9029 - val_loss: 0.2868 - val_f_2: 0.4227 - val_acc: 0.9077\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2964 - f_2: 0.4098 - acc: 0.9032 - val_loss: 0.2891 - val_f_2: 0.4131 - val_acc: 0.9083\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2942 - f_2: 0.4186 - acc: 0.9049 - val_loss: 0.2873 - val_f_2: 0.4176 - val_acc: 0.9080\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.4167 - acc: 0.9028 - val_loss: 0.2933 - val_f_2: 0.4032 - val_acc: 0.9106\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.4320 - acc: 0.9040 - val_loss: 0.2872 - val_f_2: 0.4338 - val_acc: 0.9080\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.4278 - acc: 0.9043 - val_loss: 0.2854 - val_f_2: 0.4241 - val_acc: 0.9083\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.4311 - acc: 0.9054 - val_loss: 0.2920 - val_f_2: 0.4558 - val_acc: 0.9044\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2911 - f_2: 0.4418 - acc: 0.9048 - val_loss: 0.2864 - val_f_2: 0.4108 - val_acc: 0.9065\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.4269 - acc: 0.9055 - val_loss: 0.2856 - val_f_2: 0.4215 - val_acc: 0.9077\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.4395 - acc: 0.9057 - val_loss: 0.2846 - val_f_2: 0.4153 - val_acc: 0.9086\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2884 - f_2: 0.4437 - acc: 0.9063 - val_loss: 0.2875 - val_f_2: 0.4442 - val_acc: 0.9077\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2880 - f_2: 0.4560 - acc: 0.9065 - val_loss: 0.2864 - val_f_2: 0.4053 - val_acc: 0.9080\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.4417 - acc: 0.9057 - val_loss: 0.2854 - val_f_2: 0.4371 - val_acc: 0.9065\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2872 - f_2: 0.4432 - acc: 0.9040 - val_loss: 0.2877 - val_f_2: 0.4559 - val_acc: 0.9038\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.4501 - acc: 0.9055 - val_loss: 0.2843 - val_f_2: 0.4335 - val_acc: 0.9074\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2884 - f_2: 0.4443 - acc: 0.9053 - val_loss: 0.2850 - val_f_2: 0.4163 - val_acc: 0.9065\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2885 - f_2: 0.4592 - acc: 0.9068 - val_loss: 0.2862 - val_f_2: 0.4494 - val_acc: 0.9065\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2868 - f_2: 0.4664 - acc: 0.9063 - val_loss: 0.2892 - val_f_2: 0.4515 - val_acc: 0.9056\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2863 - f_2: 0.4640 - acc: 0.9071 - val_loss: 0.2843 - val_f_2: 0.4197 - val_acc: 0.9080\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.4649 - acc: 0.9079 - val_loss: 0.2844 - val_f_2: 0.4451 - val_acc: 0.9059\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.4700 - acc: 0.9071 - val_loss: 0.2868 - val_f_2: 0.4489 - val_acc: 0.9056\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.4715 - acc: 0.9071 - val_loss: 0.2869 - val_f_2: 0.4218 - val_acc: 0.9062\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2840 - f_2: 0.4567 - acc: 0.9063 - val_loss: 0.2861 - val_f_2: 0.4421 - val_acc: 0.9056\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.4750 - acc: 0.9088 - val_loss: 0.2847 - val_f_2: 0.4368 - val_acc: 0.9041\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.001, total=  32.1s\n",
      "Train on 20336 samples, validate on 5085 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 58.6min finished\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4099 - f_2: 0.0103 - acc: 0.8658 - val_loss: 0.3441 - val_f_2: 9.8331e-04 - val_acc: 0.8692\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3393 - f_2: 0.0039 - acc: 0.8726 - val_loss: 0.3276 - val_f_2: 9.8331e-04 - val_acc: 0.8692\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3305 - f_2: 0.1540 - acc: 0.8881 - val_loss: 0.3213 - val_f_2: 0.2824 - val_acc: 0.9017\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3228 - f_2: 0.2571 - acc: 0.8970 - val_loss: 0.3191 - val_f_2: 0.3455 - val_acc: 0.9011\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3182 - f_2: 0.2874 - acc: 0.8997 - val_loss: 0.3133 - val_f_2: 0.3319 - val_acc: 0.9013\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3151 - f_2: 0.3042 - acc: 0.8994 - val_loss: 0.3107 - val_f_2: 0.3332 - val_acc: 0.9015\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3111 - f_2: 0.3091 - acc: 0.8990 - val_loss: 0.3086 - val_f_2: 0.3763 - val_acc: 0.9021\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3074 - f_2: 0.3183 - acc: 0.9011 - val_loss: 0.3098 - val_f_2: 0.3402 - val_acc: 0.9009\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3066 - f_2: 0.3132 - acc: 0.9003 - val_loss: 0.3058 - val_f_2: 0.3325 - val_acc: 0.9015\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3032 - f_2: 0.3191 - acc: 0.9010 - val_loss: 0.3042 - val_f_2: 0.3628 - val_acc: 0.9025\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3015 - f_2: 0.3395 - acc: 0.9022 - val_loss: 0.3046 - val_f_2: 0.3890 - val_acc: 0.9025\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.2997 - f_2: 0.3352 - acc: 0.9009 - val_loss: 0.3037 - val_f_2: 0.3687 - val_acc: 0.9019\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.2975 - f_2: 0.3402 - acc: 0.9015 - val_loss: 0.3016 - val_f_2: 0.3764 - val_acc: 0.9030\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.2948 - f_2: 0.3434 - acc: 0.9015 - val_loss: 0.2997 - val_f_2: 0.3610 - val_acc: 0.9017\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.2957 - f_2: 0.3412 - acc: 0.9015 - val_loss: 0.2995 - val_f_2: 0.3725 - val_acc: 0.9027\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.2938 - f_2: 0.3478 - acc: 0.9018 - val_loss: 0.2990 - val_f_2: 0.4044 - val_acc: 0.9056\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.2944 - f_2: 0.3371 - acc: 0.9004 - val_loss: 0.2994 - val_f_2: 0.3713 - val_acc: 0.9015\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.2908 - f_2: 0.3413 - acc: 0.9018 - val_loss: 0.2994 - val_f_2: 0.3974 - val_acc: 0.9036\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.2908 - f_2: 0.3493 - acc: 0.9027 - val_loss: 0.2994 - val_f_2: 0.4066 - val_acc: 0.9046\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.2904 - f_2: 0.3564 - acc: 0.9016 - val_loss: 0.2993 - val_f_2: 0.3778 - val_acc: 0.9025\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.2903 - f_2: 0.3412 - acc: 0.9016 - val_loss: 0.2978 - val_f_2: 0.4043 - val_acc: 0.9030\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.2890 - f_2: 0.3606 - acc: 0.9041 - val_loss: 0.2970 - val_f_2: 0.3663 - val_acc: 0.9032\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.2887 - f_2: 0.3600 - acc: 0.9024 - val_loss: 0.2966 - val_f_2: 0.3679 - val_acc: 0.9015\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.2875 - f_2: 0.3572 - acc: 0.9036 - val_loss: 0.3032 - val_f_2: 0.3942 - val_acc: 0.9023\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.2861 - f_2: 0.3885 - acc: 0.9066 - val_loss: 0.2966 - val_f_2: 0.3675 - val_acc: 0.9027\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.2860 - f_2: 0.3810 - acc: 0.9056 - val_loss: 0.2958 - val_f_2: 0.4048 - val_acc: 0.9021\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2867 - f_2: 0.3775 - acc: 0.9052 - val_loss: 0.2974 - val_f_2: 0.3874 - val_acc: 0.9036\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2846 - f_2: 0.3831 - acc: 0.9047 - val_loss: 0.2998 - val_f_2: 0.3609 - val_acc: 0.9017\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2846 - f_2: 0.3803 - acc: 0.9050 - val_loss: 0.2962 - val_f_2: 0.3534 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2856 - f_2: 0.3728 - acc: 0.9054 - val_loss: 0.2980 - val_f_2: 0.3669 - val_acc: 0.9034\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2852 - f_2: 0.3878 - acc: 0.9063 - val_loss: 0.2990 - val_f_2: 0.4356 - val_acc: 0.9011\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2830 - f_2: 0.3814 - acc: 0.9068 - val_loss: 0.2965 - val_f_2: 0.4369 - val_acc: 0.9009\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2834 - f_2: 0.4019 - acc: 0.9055 - val_loss: 0.2958 - val_f_2: 0.4072 - val_acc: 0.9021\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2831 - f_2: 0.3850 - acc: 0.9059 - val_loss: 0.2958 - val_f_2: 0.4036 - val_acc: 0.9034\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2825 - f_2: 0.3915 - acc: 0.9057 - val_loss: 0.2983 - val_f_2: 0.4394 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2828 - f_2: 0.3988 - acc: 0.9057 - val_loss: 0.2948 - val_f_2: 0.4215 - val_acc: 0.9019\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2825 - f_2: 0.4001 - acc: 0.9068 - val_loss: 0.2996 - val_f_2: 0.4066 - val_acc: 0.9013\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2825 - f_2: 0.3989 - acc: 0.9066 - val_loss: 0.2966 - val_f_2: 0.4129 - val_acc: 0.9027\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2829 - f_2: 0.3972 - acc: 0.9068 - val_loss: 0.2958 - val_f_2: 0.4112 - val_acc: 0.9013\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2829 - f_2: 0.3933 - acc: 0.9052 - val_loss: 0.2953 - val_f_2: 0.4228 - val_acc: 0.9017\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2799 - f_2: 0.4048 - acc: 0.9075 - val_loss: 0.2950 - val_f_2: 0.4132 - val_acc: 0.9029\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2810 - f_2: 0.3938 - acc: 0.9060 - val_loss: 0.2965 - val_f_2: 0.4417 - val_acc: 0.9007\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2813 - f_2: 0.3931 - acc: 0.9057 - val_loss: 0.3034 - val_f_2: 0.3965 - val_acc: 0.9027\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2804 - f_2: 0.4073 - acc: 0.9065 - val_loss: 0.2978 - val_f_2: 0.3814 - val_acc: 0.9025\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2815 - f_2: 0.4043 - acc: 0.9059 - val_loss: 0.2968 - val_f_2: 0.4508 - val_acc: 0.8999\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2808 - f_2: 0.4239 - acc: 0.9076 - val_loss: 0.2946 - val_f_2: 0.4150 - val_acc: 0.9021\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2805 - f_2: 0.4053 - acc: 0.9074 - val_loss: 0.2962 - val_f_2: 0.4523 - val_acc: 0.9013\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2801 - f_2: 0.4175 - acc: 0.9072 - val_loss: 0.2962 - val_f_2: 0.4346 - val_acc: 0.9034\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2797 - f_2: 0.4018 - acc: 0.9060 - val_loss: 0.2968 - val_f_2: 0.4429 - val_acc: 0.9007\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2797 - f_2: 0.4023 - acc: 0.9065 - val_loss: 0.2956 - val_f_2: 0.4332 - val_acc: 0.9023\n"
     ]
    }
   ],
   "source": [
    "# Wrap model in wrapper\n",
    "classifier = KerasClassifier(build_fn=build_model_1)\n",
    "    \n",
    "# Define grid (scores used are the same as the scores used in GSCV for the SVC benchmark model)\n",
    "mlp1_grid = GridSearchCV(estimator=classifier, \n",
    "                    param_grid=parameters, \n",
    "                    scoring=scores, \n",
    "                    refit='F2-Score', verbose=2)\n",
    "\n",
    "# Train grid, passing fit_params of keras.models.Sequential.fit here\n",
    "mlp1_grid_result = mlp1_grid.fit(X_train_red, y_train, epochs=50, \n",
    "                                 verbose=2, shuffle=True, \n",
    "                                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance: F-2 Score of 0.43 with parameters: {'batch_size': 40, 'dropout': 0.3, 'nodes': 15, 'penalty': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Observe best performance\n",
    "print('Best performance: F-2 Score of {:.2f} with parameters: {}'.format(mlp1_grid_result.best_score_, mlp1_grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Grid Search Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 8.8515e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.8min remaining:    0.0s\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 7.3763e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 5.9006e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 4.4258e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 8.8515e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 4.4254e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 5.9010e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.6201 - f_2: 0.0095 - acc: 0.8598 - val_loss: 0.4316 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.4030 - f_2: 2.9321e-08 - acc: 0.8722 - val_loss: 0.3782 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.3734 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3610 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.3585 - f_2: 2.9852e-08 - acc: 0.8722 - val_loss: 0.3512 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3500 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3443 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.3440 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3393 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.3414 - f_2: 2.9549e-08 - acc: 0.8722 - val_loss: 0.3364 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.3379 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3354 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3369 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3317 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3330 - f_2: 3.1295e-08 - acc: 0.8722 - val_loss: 0.3298 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3321 - f_2: 0.0934 - acc: 0.8825 - val_loss: 0.3284 - val_f_2: 0.2493 - val_acc: 0.8976\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3284 - f_2: 0.1962 - acc: 0.8940 - val_loss: 0.3268 - val_f_2: 0.2538 - val_acc: 0.8979\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.3276 - f_2: 0.2176 - acc: 0.8944 - val_loss: 0.3251 - val_f_2: 0.2698 - val_acc: 0.8997\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.3278 - f_2: 0.2186 - acc: 0.8938 - val_loss: 0.3263 - val_f_2: 0.2886 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.3259 - f_2: 0.2074 - acc: 0.8936 - val_loss: 0.3272 - val_f_2: 0.2502 - val_acc: 0.8973\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.3258 - f_2: 0.1986 - acc: 0.8929 - val_loss: 0.3224 - val_f_2: 0.2733 - val_acc: 0.8982\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.3260 - f_2: 0.2271 - acc: 0.8960 - val_loss: 0.3211 - val_f_2: 0.2733 - val_acc: 0.8982\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.3238 - f_2: 0.2545 - acc: 0.8985 - val_loss: 0.3208 - val_f_2: 0.2698 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.3230 - f_2: 0.2679 - acc: 0.8997 - val_loss: 0.3200 - val_f_2: 0.2950 - val_acc: 0.8991\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.3236 - f_2: 0.2637 - acc: 0.8994 - val_loss: 0.3197 - val_f_2: 0.2712 - val_acc: 0.8988\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.3242 - f_2: 0.2620 - acc: 0.8981 - val_loss: 0.3199 - val_f_2: 0.2728 - val_acc: 0.8988\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.3185 - f_2: 0.2789 - acc: 0.9008 - val_loss: 0.3190 - val_f_2: 0.2969 - val_acc: 0.8991\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.3212 - f_2: 0.2961 - acc: 0.9020 - val_loss: 0.3167 - val_f_2: 0.2950 - val_acc: 0.8991\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.3201 - f_2: 0.2829 - acc: 0.9015 - val_loss: 0.3159 - val_f_2: 0.3020 - val_acc: 0.8991\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.3192 - f_2: 0.2917 - acc: 0.9011 - val_loss: 0.3160 - val_f_2: 0.2930 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.3182 - f_2: 0.2933 - acc: 0.9022 - val_loss: 0.3151 - val_f_2: 0.2766 - val_acc: 0.8994\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.3208 - f_2: 0.2849 - acc: 0.9013 - val_loss: 0.3165 - val_f_2: 0.3407 - val_acc: 0.9021\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.3149 - f_2: 0.3026 - acc: 0.9017 - val_loss: 0.3148 - val_f_2: 0.3036 - val_acc: 0.8994\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.3169 - f_2: 0.3168 - acc: 0.9029 - val_loss: 0.3134 - val_f_2: 0.2898 - val_acc: 0.8991\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.3178 - f_2: 0.2900 - acc: 0.9023 - val_loss: 0.3134 - val_f_2: 0.3293 - val_acc: 0.9018\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.3144 - f_2: 0.3083 - acc: 0.9032 - val_loss: 0.3126 - val_f_2: 0.3429 - val_acc: 0.9024\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.3163 - f_2: 0.3114 - acc: 0.9023 - val_loss: 0.3118 - val_f_2: 0.3133 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.3141 - f_2: 0.3105 - acc: 0.9027 - val_loss: 0.3119 - val_f_2: 0.3063 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.3153 - f_2: 0.3199 - acc: 0.9024 - val_loss: 0.3112 - val_f_2: 0.3187 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.3160 - f_2: 0.3087 - acc: 0.9015 - val_loss: 0.3125 - val_f_2: 0.3282 - val_acc: 0.9018\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.3120 - f_2: 0.3268 - acc: 0.9040 - val_loss: 0.3140 - val_f_2: 0.3210 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.3119 - f_2: 0.3262 - acc: 0.9036 - val_loss: 0.3107 - val_f_2: 0.3086 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.3128 - f_2: 0.3055 - acc: 0.9031 - val_loss: 0.3100 - val_f_2: 0.3302 - val_acc: 0.9024\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.3133 - f_2: 0.3387 - acc: 0.9044 - val_loss: 0.3105 - val_f_2: 0.3413 - val_acc: 0.9027\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.3103 - f_2: 0.3238 - acc: 0.9034 - val_loss: 0.3099 - val_f_2: 0.3526 - val_acc: 0.9032\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.3136 - f_2: 0.3455 - acc: 0.9040 - val_loss: 0.3150 - val_f_2: 0.2897 - val_acc: 0.8997\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.3148 - f_2: 0.3327 - acc: 0.9038 - val_loss: 0.3088 - val_f_2: 0.3432 - val_acc: 0.9027\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.3130 - f_2: 0.3078 - acc: 0.9026 - val_loss: 0.3110 - val_f_2: 0.3255 - val_acc: 0.9024\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.3127 - f_2: 0.3370 - acc: 0.9036 - val_loss: 0.3084 - val_f_2: 0.3517 - val_acc: 0.9035\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.3125 - f_2: 0.3445 - acc: 0.9045 - val_loss: 0.3107 - val_f_2: 0.3332 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.3117 - f_2: 0.3371 - acc: 0.9046 - val_loss: 0.3090 - val_f_2: 0.3394 - val_acc: 0.9027\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.3094 - f_2: 0.3461 - acc: 0.9050 - val_loss: 0.3090 - val_f_2: 0.3058 - val_acc: 0.9018\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.3086 - f_2: 0.3402 - acc: 0.9049 - val_loss: 0.3075 - val_f_2: 0.3473 - val_acc: 0.9029\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.3138 - f_2: 0.3470 - acc: 0.9051 - val_loss: 0.3077 - val_f_2: 0.3484 - val_acc: 0.9021\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.3112 - f_2: 0.3454 - acc: 0.9051 - val_loss: 0.3080 - val_f_2: 0.3552 - val_acc: 0.9029\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 6.6381e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 3.6881e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 5.1634e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.4515 - f_2: 0.0149 - acc: 0.8581 - val_loss: 0.3563 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.3786 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3317 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.3559 - f_2: 0.0528 - acc: 0.8748 - val_loss: 0.3183 - val_f_2: 0.2562 - val_acc: 0.9047\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.3432 - f_2: 0.2110 - acc: 0.8903 - val_loss: 0.3048 - val_f_2: 0.3417 - val_acc: 0.9088\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3267 - f_2: 0.3083 - acc: 0.8969 - val_loss: 0.2950 - val_f_2: 0.3481 - val_acc: 0.9088\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.3186 - f_2: 0.3349 - acc: 0.8987 - val_loss: 0.2889 - val_f_2: 0.3312 - val_acc: 0.9094\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.3129 - f_2: 0.3435 - acc: 0.8995 - val_loss: 0.2857 - val_f_2: 0.3331 - val_acc: 0.9083\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.3115 - f_2: 0.3356 - acc: 0.8995 - val_loss: 0.2844 - val_f_2: 0.3749 - val_acc: 0.9083\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3098 - f_2: 0.3275 - acc: 0.8994 - val_loss: 0.2832 - val_f_2: 0.3695 - val_acc: 0.9083\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3063 - f_2: 0.3505 - acc: 0.9004 - val_loss: 0.2824 - val_f_2: 0.3740 - val_acc: 0.9091\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3041 - f_2: 0.3283 - acc: 0.8993 - val_loss: 0.2815 - val_f_2: 0.3561 - val_acc: 0.9094\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3020 - f_2: 0.3329 - acc: 0.9003 - val_loss: 0.2794 - val_f_2: 0.3525 - val_acc: 0.9077\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.2987 - f_2: 0.3352 - acc: 0.9010 - val_loss: 0.2785 - val_f_2: 0.3536 - val_acc: 0.9077\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.2984 - f_2: 0.3336 - acc: 0.8998 - val_loss: 0.2773 - val_f_2: 0.3485 - val_acc: 0.9088\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.2984 - f_2: 0.3482 - acc: 0.9012 - val_loss: 0.2764 - val_f_2: 0.3200 - val_acc: 0.9080\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.2960 - f_2: 0.3346 - acc: 0.9010 - val_loss: 0.2779 - val_f_2: 0.3868 - val_acc: 0.9091\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.2946 - f_2: 0.3433 - acc: 0.9011 - val_loss: 0.2750 - val_f_2: 0.3232 - val_acc: 0.9077\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.2939 - f_2: 0.3514 - acc: 0.9024 - val_loss: 0.2751 - val_f_2: 0.3495 - val_acc: 0.9086\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.2925 - f_2: 0.3437 - acc: 0.9012 - val_loss: 0.2746 - val_f_2: 0.3537 - val_acc: 0.9091\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.2918 - f_2: 0.3460 - acc: 0.9018 - val_loss: 0.2772 - val_f_2: 0.3674 - val_acc: 0.9062\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.2911 - f_2: 0.3501 - acc: 0.9016 - val_loss: 0.2747 - val_f_2: 0.3615 - val_acc: 0.9077\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.2896 - f_2: 0.3539 - acc: 0.9021 - val_loss: 0.2742 - val_f_2: 0.3407 - val_acc: 0.9077\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.2918 - f_2: 0.3653 - acc: 0.9015 - val_loss: 0.2750 - val_f_2: 0.3722 - val_acc: 0.9065\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.2879 - f_2: 0.3755 - acc: 0.9035 - val_loss: 0.2749 - val_f_2: 0.3529 - val_acc: 0.9071\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.2881 - f_2: 0.3510 - acc: 0.9021 - val_loss: 0.2741 - val_f_2: 0.3564 - val_acc: 0.9071\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.2871 - f_2: 0.3721 - acc: 0.9035 - val_loss: 0.2739 - val_f_2: 0.3608 - val_acc: 0.9065\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.2868 - f_2: 0.3800 - acc: 0.9043 - val_loss: 0.2729 - val_f_2: 0.3523 - val_acc: 0.9086\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.2848 - f_2: 0.3826 - acc: 0.9035 - val_loss: 0.2730 - val_f_2: 0.3502 - val_acc: 0.9088\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.2843 - f_2: 0.3665 - acc: 0.9032 - val_loss: 0.2729 - val_f_2: 0.3550 - val_acc: 0.9068\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.2829 - f_2: 0.3665 - acc: 0.9027 - val_loss: 0.2779 - val_f_2: 0.4048 - val_acc: 0.9044\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.2834 - f_2: 0.3614 - acc: 0.9021 - val_loss: 0.2744 - val_f_2: 0.3816 - val_acc: 0.9053\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.2833 - f_2: 0.3928 - acc: 0.9037 - val_loss: 0.2734 - val_f_2: 0.3621 - val_acc: 0.9071\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.2833 - f_2: 0.3688 - acc: 0.9024 - val_loss: 0.2722 - val_f_2: 0.3494 - val_acc: 0.9091\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.2828 - f_2: 0.3742 - acc: 0.9034 - val_loss: 0.2726 - val_f_2: 0.3486 - val_acc: 0.9077\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.2812 - f_2: 0.3644 - acc: 0.9041 - val_loss: 0.2725 - val_f_2: 0.3659 - val_acc: 0.9065\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.2817 - f_2: 0.3866 - acc: 0.9049 - val_loss: 0.2725 - val_f_2: 0.3642 - val_acc: 0.9068\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.2801 - f_2: 0.3956 - acc: 0.9047 - val_loss: 0.2729 - val_f_2: 0.3617 - val_acc: 0.9065\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.2799 - f_2: 0.3728 - acc: 0.9037 - val_loss: 0.2725 - val_f_2: 0.3745 - val_acc: 0.9059\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.2805 - f_2: 0.3843 - acc: 0.9037 - val_loss: 0.2732 - val_f_2: 0.3824 - val_acc: 0.9074\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.2787 - f_2: 0.3872 - acc: 0.9036 - val_loss: 0.2722 - val_f_2: 0.3539 - val_acc: 0.9097\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.2790 - f_2: 0.4030 - acc: 0.9060 - val_loss: 0.2714 - val_f_2: 0.3603 - val_acc: 0.9083\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.2787 - f_2: 0.3860 - acc: 0.9036 - val_loss: 0.2738 - val_f_2: 0.3750 - val_acc: 0.9053\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.2792 - f_2: 0.3835 - acc: 0.9033 - val_loss: 0.2728 - val_f_2: 0.3599 - val_acc: 0.9083\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.2790 - f_2: 0.3891 - acc: 0.9043 - val_loss: 0.2718 - val_f_2: 0.3455 - val_acc: 0.9091\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.2766 - f_2: 0.4054 - acc: 0.9068 - val_loss: 0.2724 - val_f_2: 0.3653 - val_acc: 0.9068\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.2794 - f_2: 0.4000 - acc: 0.9041 - val_loss: 0.2755 - val_f_2: 0.3938 - val_acc: 0.9050\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.2778 - f_2: 0.4024 - acc: 0.9051 - val_loss: 0.2725 - val_f_2: 0.3773 - val_acc: 0.9083\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.2758 - f_2: 0.4040 - acc: 0.9056 - val_loss: 0.2735 - val_f_2: 0.3595 - val_acc: 0.9074\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.2754 - f_2: 0.4023 - acc: 0.9057 - val_loss: 0.2729 - val_f_2: 0.3863 - val_acc: 0.9077\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.2770 - f_2: 0.3925 - acc: 0.9048 - val_loss: 0.2753 - val_f_2: 0.3861 - val_acc: 0.9053\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 5.9010e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.6313 - f_2: 0.0102 - acc: 0.8613 - val_loss: 0.4299 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.3998 - f_2: 2.9853e-08 - acc: 0.8722 - val_loss: 0.3779 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.3672 - f_2: 3.0176e-08 - acc: 0.8722 - val_loss: 0.3588 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.3544 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3497 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3456 - f_2: 0.0177 - acc: 0.8722 - val_loss: 0.3426 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.3411 - f_2: 2.9496e-08 - acc: 0.8722 - val_loss: 0.3390 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.3392 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3345 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.3355 - f_2: 0.0394 - acc: 0.8762 - val_loss: 0.3323 - val_f_2: 0.2250 - val_acc: 0.8944\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3334 - f_2: 0.1788 - acc: 0.8911 - val_loss: 0.3302 - val_f_2: 0.2355 - val_acc: 0.8956\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3294 - f_2: 0.1913 - acc: 0.8940 - val_loss: 0.3292 - val_f_2: 0.2696 - val_acc: 0.8991\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3286 - f_2: 0.2296 - acc: 0.8966 - val_loss: 0.3270 - val_f_2: 0.2537 - val_acc: 0.8979\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3248 - f_2: 0.2412 - acc: 0.8969 - val_loss: 0.3253 - val_f_2: 0.2758 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.3254 - f_2: 0.2478 - acc: 0.8981 - val_loss: 0.3265 - val_f_2: 0.2736 - val_acc: 0.8997\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.3239 - f_2: 0.2711 - acc: 0.8998 - val_loss: 0.3330 - val_f_2: 0.2632 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.3251 - f_2: 0.2534 - acc: 0.8985 - val_loss: 0.3215 - val_f_2: 0.2771 - val_acc: 0.8988\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.3216 - f_2: 0.2591 - acc: 0.8999 - val_loss: 0.3210 - val_f_2: 0.3030 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.3194 - f_2: 0.2880 - acc: 0.9015 - val_loss: 0.3198 - val_f_2: 0.2735 - val_acc: 0.8991\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.3196 - f_2: 0.2763 - acc: 0.9012 - val_loss: 0.3193 - val_f_2: 0.2771 - val_acc: 0.8991\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.3195 - f_2: 0.2659 - acc: 0.8998 - val_loss: 0.3181 - val_f_2: 0.2918 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.3178 - f_2: 0.2898 - acc: 0.9015 - val_loss: 0.3163 - val_f_2: 0.2866 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.3160 - f_2: 0.3017 - acc: 0.9012 - val_loss: 0.3157 - val_f_2: 0.2858 - val_acc: 0.8991\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.3158 - f_2: 0.2883 - acc: 0.9009 - val_loss: 0.3156 - val_f_2: 0.3336 - val_acc: 0.9009\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.3157 - f_2: 0.3016 - acc: 0.9013 - val_loss: 0.3148 - val_f_2: 0.2885 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.3171 - f_2: 0.3055 - acc: 0.9021 - val_loss: 0.3142 - val_f_2: 0.2983 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.3124 - f_2: 0.3172 - acc: 0.9021 - val_loss: 0.3142 - val_f_2: 0.3223 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.3152 - f_2: 0.3164 - acc: 0.9025 - val_loss: 0.3130 - val_f_2: 0.3128 - val_acc: 0.9012\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.3148 - f_2: 0.3134 - acc: 0.9031 - val_loss: 0.3128 - val_f_2: 0.2982 - val_acc: 0.8997\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.3132 - f_2: 0.3169 - acc: 0.9024 - val_loss: 0.3122 - val_f_2: 0.3168 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      " - 4s - loss: 0.3121 - f_2: 0.3076 - acc: 0.9031 - val_loss: 0.3134 - val_f_2: 0.3641 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 4s - loss: 0.3119 - f_2: 0.3274 - acc: 0.9037 - val_loss: 0.3113 - val_f_2: 0.3426 - val_acc: 0.9015\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.3123 - f_2: 0.3141 - acc: 0.9029 - val_loss: 0.3122 - val_f_2: 0.3183 - val_acc: 0.9015\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.3117 - f_2: 0.3206 - acc: 0.9027 - val_loss: 0.3100 - val_f_2: 0.3001 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.3127 - f_2: 0.3246 - acc: 0.9045 - val_loss: 0.3098 - val_f_2: 0.3355 - val_acc: 0.9003\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.3116 - f_2: 0.3345 - acc: 0.9049 - val_loss: 0.3098 - val_f_2: 0.3460 - val_acc: 0.9012\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.3084 - f_2: 0.3375 - acc: 0.9029 - val_loss: 0.3104 - val_f_2: 0.3790 - val_acc: 0.9018\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.3120 - f_2: 0.3214 - acc: 0.9022 - val_loss: 0.3096 - val_f_2: 0.3349 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.3087 - f_2: 0.3285 - acc: 0.9024 - val_loss: 0.3122 - val_f_2: 0.3306 - val_acc: 0.9024\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.3084 - f_2: 0.3293 - acc: 0.9033 - val_loss: 0.3082 - val_f_2: 0.3428 - val_acc: 0.9024\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.3074 - f_2: 0.3332 - acc: 0.9044 - val_loss: 0.3097 - val_f_2: 0.3157 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.3095 - f_2: 0.3368 - acc: 0.9034 - val_loss: 0.3083 - val_f_2: 0.3291 - val_acc: 0.9021\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.3065 - f_2: 0.3462 - acc: 0.9042 - val_loss: 0.3071 - val_f_2: 0.3409 - val_acc: 0.9027\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.3074 - f_2: 0.3405 - acc: 0.9037 - val_loss: 0.3085 - val_f_2: 0.3779 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.3077 - f_2: 0.3304 - acc: 0.9034 - val_loss: 0.3071 - val_f_2: 0.3456 - val_acc: 0.9021\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.3065 - f_2: 0.3422 - acc: 0.9047 - val_loss: 0.3090 - val_f_2: 0.3505 - val_acc: 0.9024\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.3074 - f_2: 0.3614 - acc: 0.9046 - val_loss: 0.3060 - val_f_2: 0.3296 - val_acc: 0.9029\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.3066 - f_2: 0.3171 - acc: 0.9039 - val_loss: 0.3076 - val_f_2: 0.3731 - val_acc: 0.8988\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.3070 - f_2: 0.3456 - acc: 0.9035 - val_loss: 0.3064 - val_f_2: 0.3509 - val_acc: 0.9027\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.3050 - f_2: 0.3540 - acc: 0.9048 - val_loss: 0.3060 - val_f_2: 0.3369 - val_acc: 0.9018\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.3072 - f_2: 0.3318 - acc: 0.9034 - val_loss: 0.3078 - val_f_2: 0.4009 - val_acc: 0.9018\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.3049 - f_2: 0.3534 - acc: 0.9039 - val_loss: 0.3062 - val_f_2: 0.3510 - val_acc: 0.9015\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 6.6381e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 7.3763e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 0.0010 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 6.6381e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: nan - f_2: nan - acc: 7.3763e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 5.9010e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 5.9006e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 6.6386e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 5.9010e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 8.1133e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.6374 - f_2: 0.0124 - acc: 0.8588 - val_loss: 0.4441 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.4162 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3908 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.3861 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3715 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.3716 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3625 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3630 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3555 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.3572 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3491 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.3523 - f_2: 3.0928e-08 - acc: 0.8753 - val_loss: 0.3444 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.3508 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3423 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3461 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3371 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3407 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3350 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3392 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3330 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3394 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3322 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.3376 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3303 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.3363 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3289 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.3347 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3308 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.3356 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3280 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.3344 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3279 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.3347 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3269 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.3321 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3272 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.3351 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3254 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.3289 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3242 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.3273 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3269 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.3288 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3232 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.3305 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3226 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.3298 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3226 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.3277 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3269 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.3308 - f_2: 3.1241e-08 - acc: 0.8753 - val_loss: 0.3218 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.3266 - f_2: 3.2828e-08 - acc: 0.8753 - val_loss: 0.3222 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.3263 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3210 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.3269 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3208 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.3263 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3205 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 32/50\n",
      " - 4s - loss: 0.3286 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3244 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.3242 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3213 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.3238 - f_2: 0.0364 - acc: 0.8784 - val_loss: 0.3196 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.3254 - f_2: 0.1871 - acc: 0.8926 - val_loss: 0.3196 - val_f_2: 0.2684 - val_acc: 0.8994\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.3228 - f_2: 0.2397 - acc: 0.8976 - val_loss: 0.3185 - val_f_2: 0.2800 - val_acc: 0.9003\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.3224 - f_2: 0.2747 - acc: 0.9001 - val_loss: 0.3182 - val_f_2: 0.2913 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.3240 - f_2: 0.2543 - acc: 0.9008 - val_loss: 0.3188 - val_f_2: 0.2815 - val_acc: 0.8997\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.3198 - f_2: 0.2776 - acc: 0.9005 - val_loss: 0.3179 - val_f_2: 0.2817 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.3245 - f_2: 0.2810 - acc: 0.9018 - val_loss: 0.3189 - val_f_2: 0.2754 - val_acc: 0.8997\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.3202 - f_2: 0.2798 - acc: 0.9017 - val_loss: 0.3181 - val_f_2: 0.2955 - val_acc: 0.9015\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.3230 - f_2: 0.2761 - acc: 0.9015 - val_loss: 0.3174 - val_f_2: 0.3004 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.3214 - f_2: 0.2781 - acc: 0.9017 - val_loss: 0.3172 - val_f_2: 0.3041 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.3210 - f_2: 0.2835 - acc: 0.9027 - val_loss: 0.3177 - val_f_2: 0.3275 - val_acc: 0.8994\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.3209 - f_2: 0.2957 - acc: 0.9028 - val_loss: 0.3179 - val_f_2: 0.3226 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.3233 - f_2: 0.2907 - acc: 0.9033 - val_loss: 0.3227 - val_f_2: 0.2737 - val_acc: 0.9003\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.3213 - f_2: 0.2694 - acc: 0.9012 - val_loss: 0.3172 - val_f_2: 0.2948 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.3211 - f_2: 0.2706 - acc: 0.9017 - val_loss: 0.3166 - val_f_2: 0.3021 - val_acc: 0.9015\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.3187 - f_2: 0.2942 - acc: 0.9036 - val_loss: 0.3168 - val_f_2: 0.3086 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.3198 - f_2: 0.2885 - acc: 0.9028 - val_loss: 0.3161 - val_f_2: 0.3145 - val_acc: 0.9012\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.6363 - f_2: 0.0166 - acc: 0.8554 - val_loss: 0.4414 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.4177 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3882 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.3801 - f_2: 3.0759e-08 - acc: 0.8722 - val_loss: 0.3677 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.3692 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3582 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3624 - f_2: 2.9816e-08 - acc: 0.8722 - val_loss: 0.3512 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.3537 - f_2: 0.0118 - acc: 0.8722 - val_loss: 0.3462 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.3505 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3410 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.3449 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3378 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3443 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3357 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3429 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3341 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3393 - f_2: 3.0933e-08 - acc: 0.8722 - val_loss: 0.3318 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3386 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3319 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.3384 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3302 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.3349 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3288 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.3344 - f_2: 3.0226e-08 - acc: 0.8722 - val_loss: 0.3281 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.3354 - f_2: 2.9601e-08 - acc: 0.8722 - val_loss: 0.3278 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.3339 - f_2: 0.0118 - acc: 0.8722 - val_loss: 0.3263 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.3325 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3252 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.3308 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3248 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.3324 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3246 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.3341 - f_2: 3.1106e-08 - acc: 0.8722 - val_loss: 0.3239 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.3313 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3239 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.3290 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3221 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.3294 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3216 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.3288 - f_2: 0.1286 - acc: 0.8854 - val_loss: 0.3209 - val_f_2: 0.2698 - val_acc: 0.8997\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.3276 - f_2: 0.2374 - acc: 0.8969 - val_loss: 0.3206 - val_f_2: 0.2716 - val_acc: 0.8997\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.3267 - f_2: 0.2602 - acc: 0.8987 - val_loss: 0.3200 - val_f_2: 0.2731 - val_acc: 0.8988\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.3295 - f_2: 0.2656 - acc: 0.8987 - val_loss: 0.3203 - val_f_2: 0.2698 - val_acc: 0.8997\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.3263 - f_2: 0.2818 - acc: 0.8994 - val_loss: 0.3191 - val_f_2: 0.2734 - val_acc: 0.8991\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.3277 - f_2: 0.2741 - acc: 0.8989 - val_loss: 0.3187 - val_f_2: 0.2786 - val_acc: 0.8997\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.3256 - f_2: 0.2699 - acc: 0.8996 - val_loss: 0.3190 - val_f_2: 0.2833 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.3247 - f_2: 0.2765 - acc: 0.9002 - val_loss: 0.3189 - val_f_2: 0.2735 - val_acc: 0.8994\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.3258 - f_2: 0.2790 - acc: 0.8997 - val_loss: 0.3183 - val_f_2: 0.2831 - val_acc: 0.8994\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.3251 - f_2: 0.2937 - acc: 0.9003 - val_loss: 0.3176 - val_f_2: 0.2833 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.3237 - f_2: 0.2917 - acc: 0.9003 - val_loss: 0.3179 - val_f_2: 0.2835 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.3231 - f_2: 0.2897 - acc: 0.9020 - val_loss: 0.3172 - val_f_2: 0.2754 - val_acc: 0.8994\n",
      "Epoch 37/50\n",
      " - 4s - loss: 0.3257 - f_2: 0.2881 - acc: 0.9015 - val_loss: 0.3166 - val_f_2: 0.2878 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.3244 - f_2: 0.2934 - acc: 0.9009 - val_loss: 0.3180 - val_f_2: 0.2959 - val_acc: 0.8997\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.3236 - f_2: 0.3054 - acc: 0.9023 - val_loss: 0.3160 - val_f_2: 0.3009 - val_acc: 0.9003\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.3251 - f_2: 0.2901 - acc: 0.9015 - val_loss: 0.3160 - val_f_2: 0.2896 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.3218 - f_2: 0.3068 - acc: 0.9015 - val_loss: 0.3166 - val_f_2: 0.3315 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.3228 - f_2: 0.3110 - acc: 0.9023 - val_loss: 0.3160 - val_f_2: 0.2947 - val_acc: 0.9006\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.3242 - f_2: 0.2981 - acc: 0.9012 - val_loss: 0.3152 - val_f_2: 0.3196 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.3252 - f_2: 0.3163 - acc: 0.9019 - val_loss: 0.3151 - val_f_2: 0.3063 - val_acc: 0.9003\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.3215 - f_2: 0.3285 - acc: 0.9015 - val_loss: 0.3155 - val_f_2: 0.3080 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.3194 - f_2: 0.3089 - acc: 0.9032 - val_loss: 0.3140 - val_f_2: 0.2992 - val_acc: 0.9003\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.3229 - f_2: 0.3252 - acc: 0.9009 - val_loss: 0.3142 - val_f_2: 0.3188 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.3237 - f_2: 0.3193 - acc: 0.9023 - val_loss: 0.3159 - val_f_2: 0.2891 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.3190 - f_2: 0.3295 - acc: 0.9014 - val_loss: 0.3140 - val_f_2: 0.3130 - val_acc: 0.9012\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.3190 - f_2: 0.3268 - acc: 0.9028 - val_loss: 0.3132 - val_f_2: 0.2984 - val_acc: 0.9012\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 8.1133e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.4623 - f_2: 0.0234 - acc: 0.8610 - val_loss: 0.3903 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.3916 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3679 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.3711 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3519 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.3554 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3408 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3460 - f_2: 0.0148 - acc: 0.8753 - val_loss: 0.3334 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.3396 - f_2: 0.0259 - acc: 0.8776 - val_loss: 0.3272 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.3348 - f_2: 0.1304 - acc: 0.8877 - val_loss: 0.3210 - val_f_2: 0.2690 - val_acc: 0.8994\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.3275 - f_2: 0.1906 - acc: 0.8910 - val_loss: 0.3147 - val_f_2: 0.3283 - val_acc: 0.9003\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3195 - f_2: 0.2243 - acc: 0.8936 - val_loss: 0.3107 - val_f_2: 0.3282 - val_acc: 0.9003\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3165 - f_2: 0.2482 - acc: 0.8948 - val_loss: 0.3062 - val_f_2: 0.3499 - val_acc: 0.9018\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3157 - f_2: 0.2441 - acc: 0.8938 - val_loss: 0.3042 - val_f_2: 0.3599 - val_acc: 0.9009\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3116 - f_2: 0.2463 - acc: 0.8944 - val_loss: 0.3026 - val_f_2: 0.3588 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.3065 - f_2: 0.2486 - acc: 0.8958 - val_loss: 0.3034 - val_f_2: 0.3158 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.3090 - f_2: 0.2466 - acc: 0.8948 - val_loss: 0.3008 - val_f_2: 0.3593 - val_acc: 0.9021\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.3045 - f_2: 0.2649 - acc: 0.8964 - val_loss: 0.3007 - val_f_2: 0.3238 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.3058 - f_2: 0.2447 - acc: 0.8951 - val_loss: 0.3004 - val_f_2: 0.3270 - val_acc: 0.8991\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.3036 - f_2: 0.2439 - acc: 0.8941 - val_loss: 0.3001 - val_f_2: 0.3269 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.3046 - f_2: 0.2449 - acc: 0.8944 - val_loss: 0.2982 - val_f_2: 0.3521 - val_acc: 0.9006\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.3033 - f_2: 0.2566 - acc: 0.8964 - val_loss: 0.2976 - val_f_2: 0.3630 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.3020 - f_2: 0.2479 - acc: 0.8954 - val_loss: 0.2978 - val_f_2: 0.3418 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.2994 - f_2: 0.2654 - acc: 0.8952 - val_loss: 0.2962 - val_f_2: 0.3754 - val_acc: 0.8997\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.3001 - f_2: 0.2543 - acc: 0.8953 - val_loss: 0.2974 - val_f_2: 0.3530 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.2999 - f_2: 0.2681 - acc: 0.8961 - val_loss: 0.2959 - val_f_2: 0.3819 - val_acc: 0.9015\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.2971 - f_2: 0.2714 - acc: 0.8970 - val_loss: 0.2963 - val_f_2: 0.3710 - val_acc: 0.9015\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.2955 - f_2: 0.2682 - acc: 0.8977 - val_loss: 0.2974 - val_f_2: 0.3652 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.2972 - f_2: 0.2810 - acc: 0.8982 - val_loss: 0.2953 - val_f_2: 0.3777 - val_acc: 0.9018\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.2979 - f_2: 0.2637 - acc: 0.8958 - val_loss: 0.2951 - val_f_2: 0.3855 - val_acc: 0.9024\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.2956 - f_2: 0.2726 - acc: 0.8970 - val_loss: 0.2961 - val_f_2: 0.3548 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.2921 - f_2: 0.2816 - acc: 0.8984 - val_loss: 0.2943 - val_f_2: 0.3819 - val_acc: 0.9032\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.2944 - f_2: 0.2779 - acc: 0.8987 - val_loss: 0.2949 - val_f_2: 0.3725 - val_acc: 0.9024\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.2924 - f_2: 0.2751 - acc: 0.8981 - val_loss: 0.2942 - val_f_2: 0.3829 - val_acc: 0.9029\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.2922 - f_2: 0.2627 - acc: 0.8974 - val_loss: 0.2952 - val_f_2: 0.3875 - val_acc: 0.9027\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.2936 - f_2: 0.2710 - acc: 0.8980 - val_loss: 0.2948 - val_f_2: 0.3886 - val_acc: 0.9024\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.2951 - f_2: 0.2777 - acc: 0.8979 - val_loss: 0.2952 - val_f_2: 0.3715 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.2929 - f_2: 0.2809 - acc: 0.8972 - val_loss: 0.2948 - val_f_2: 0.3906 - val_acc: 0.9029\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.2918 - f_2: 0.2625 - acc: 0.8957 - val_loss: 0.2946 - val_f_2: 0.3925 - val_acc: 0.9015\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.2908 - f_2: 0.2755 - acc: 0.8970 - val_loss: 0.2950 - val_f_2: 0.3768 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.2916 - f_2: 0.2868 - acc: 0.8980 - val_loss: 0.2951 - val_f_2: 0.3921 - val_acc: 0.9029\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.2893 - f_2: 0.3059 - acc: 0.9005 - val_loss: 0.2955 - val_f_2: 0.3755 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.2923 - f_2: 0.2817 - acc: 0.8984 - val_loss: 0.2952 - val_f_2: 0.3801 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.2892 - f_2: 0.2886 - acc: 0.8967 - val_loss: 0.2967 - val_f_2: 0.3782 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.2886 - f_2: 0.2748 - acc: 0.8984 - val_loss: 0.2998 - val_f_2: 0.3449 - val_acc: 0.8997\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.2881 - f_2: 0.2859 - acc: 0.8981 - val_loss: 0.2960 - val_f_2: 0.4283 - val_acc: 0.8991\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.2878 - f_2: 0.2975 - acc: 0.8994 - val_loss: 0.2954 - val_f_2: 0.4102 - val_acc: 0.9018\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.2888 - f_2: 0.2862 - acc: 0.8978 - val_loss: 0.2965 - val_f_2: 0.3706 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.2899 - f_2: 0.2682 - acc: 0.8974 - val_loss: 0.2949 - val_f_2: 0.4154 - val_acc: 0.9027\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.2873 - f_2: 0.2754 - acc: 0.8979 - val_loss: 0.2957 - val_f_2: 0.4034 - val_acc: 0.9021\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.2878 - f_2: 0.2794 - acc: 0.8975 - val_loss: 0.2964 - val_f_2: 0.4116 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.2875 - f_2: 0.2831 - acc: 0.8969 - val_loss: 0.2967 - val_f_2: 0.4019 - val_acc: 0.8997\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.2876 - f_2: 0.2806 - acc: 0.8977 - val_loss: 0.2970 - val_f_2: 0.3965 - val_acc: 0.9000\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 8.1139e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.4707 - f_2: 0.0170 - acc: 0.8545 - val_loss: 0.3685 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.3978 - f_2: 2.9727e-08 - acc: 0.8691 - val_loss: 0.3481 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.3725 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3294 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3559 - f_2: 0.0506 - acc: 0.8736 - val_loss: 0.3148 - val_f_2: 0.2017 - val_acc: 0.9006\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3399 - f_2: 0.2361 - acc: 0.8918 - val_loss: 0.3014 - val_f_2: 0.3632 - val_acc: 0.9088\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.3321 - f_2: 0.2998 - acc: 0.8947 - val_loss: 0.2944 - val_f_2: 0.3411 - val_acc: 0.9091\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.3249 - f_2: 0.3232 - acc: 0.8964 - val_loss: 0.2895 - val_f_2: 0.3431 - val_acc: 0.9071\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.3218 - f_2: 0.3305 - acc: 0.8992 - val_loss: 0.2865 - val_f_2: 0.3610 - val_acc: 0.9103\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3163 - f_2: 0.3258 - acc: 0.8985 - val_loss: 0.2842 - val_f_2: 0.3466 - val_acc: 0.9077\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3108 - f_2: 0.3396 - acc: 0.9004 - val_loss: 0.2839 - val_f_2: 0.3724 - val_acc: 0.9088\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3131 - f_2: 0.3241 - acc: 0.8980 - val_loss: 0.2815 - val_f_2: 0.3456 - val_acc: 0.9080\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3076 - f_2: 0.3433 - acc: 0.9009 - val_loss: 0.2817 - val_f_2: 0.3513 - val_acc: 0.9088\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.3076 - f_2: 0.3225 - acc: 0.8995 - val_loss: 0.2810 - val_f_2: 0.3362 - val_acc: 0.9071\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.3055 - f_2: 0.3291 - acc: 0.9009 - val_loss: 0.2778 - val_f_2: 0.3226 - val_acc: 0.9068\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.3045 - f_2: 0.3304 - acc: 0.8992 - val_loss: 0.2773 - val_f_2: 0.3165 - val_acc: 0.9068\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.2999 - f_2: 0.3245 - acc: 0.8998 - val_loss: 0.2767 - val_f_2: 0.3119 - val_acc: 0.9065\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.3019 - f_2: 0.3332 - acc: 0.9011 - val_loss: 0.2757 - val_f_2: 0.3385 - val_acc: 0.9074\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.2976 - f_2: 0.3462 - acc: 0.9011 - val_loss: 0.2754 - val_f_2: 0.3418 - val_acc: 0.9074\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.2963 - f_2: 0.3349 - acc: 0.9004 - val_loss: 0.2755 - val_f_2: 0.3276 - val_acc: 0.9083\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.2972 - f_2: 0.3319 - acc: 0.9006 - val_loss: 0.2749 - val_f_2: 0.3668 - val_acc: 0.9086\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.2950 - f_2: 0.3452 - acc: 0.9001 - val_loss: 0.2745 - val_f_2: 0.3509 - val_acc: 0.9074\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.2939 - f_2: 0.3340 - acc: 0.9014 - val_loss: 0.2745 - val_f_2: 0.3301 - val_acc: 0.9062\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.2947 - f_2: 0.3348 - acc: 0.9009 - val_loss: 0.2759 - val_f_2: 0.3719 - val_acc: 0.9062\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.2902 - f_2: 0.3652 - acc: 0.9027 - val_loss: 0.2755 - val_f_2: 0.3778 - val_acc: 0.9068\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.2923 - f_2: 0.3427 - acc: 0.9001 - val_loss: 0.2732 - val_f_2: 0.3266 - val_acc: 0.9068\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.2919 - f_2: 0.3555 - acc: 0.9017 - val_loss: 0.2734 - val_f_2: 0.3450 - val_acc: 0.9056\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.2895 - f_2: 0.3550 - acc: 0.9019 - val_loss: 0.2728 - val_f_2: 0.3384 - val_acc: 0.9059\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.2903 - f_2: 0.3649 - acc: 0.9023 - val_loss: 0.2731 - val_f_2: 0.3571 - val_acc: 0.9059\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.2883 - f_2: 0.3524 - acc: 0.9025 - val_loss: 0.2745 - val_f_2: 0.3584 - val_acc: 0.9062\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.2887 - f_2: 0.3643 - acc: 0.9020 - val_loss: 0.2723 - val_f_2: 0.3464 - val_acc: 0.9080\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.2880 - f_2: 0.3588 - acc: 0.9023 - val_loss: 0.2751 - val_f_2: 0.3613 - val_acc: 0.9047\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.2896 - f_2: 0.3693 - acc: 0.9015 - val_loss: 0.2741 - val_f_2: 0.3509 - val_acc: 0.9077\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.2853 - f_2: 0.3540 - acc: 0.9009 - val_loss: 0.2761 - val_f_2: 0.3929 - val_acc: 0.9032\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.2857 - f_2: 0.3658 - acc: 0.9022 - val_loss: 0.2734 - val_f_2: 0.3467 - val_acc: 0.9056\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.2848 - f_2: 0.3772 - acc: 0.9028 - val_loss: 0.2722 - val_f_2: 0.3336 - val_acc: 0.9077\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.2863 - f_2: 0.3732 - acc: 0.9031 - val_loss: 0.2725 - val_f_2: 0.3500 - val_acc: 0.9062\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.2859 - f_2: 0.3638 - acc: 0.9033 - val_loss: 0.2747 - val_f_2: 0.3597 - val_acc: 0.9044\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.2827 - f_2: 0.3695 - acc: 0.9040 - val_loss: 0.2724 - val_f_2: 0.3556 - val_acc: 0.9044\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.2858 - f_2: 0.3677 - acc: 0.9027 - val_loss: 0.2738 - val_f_2: 0.3606 - val_acc: 0.9038\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.2834 - f_2: 0.3731 - acc: 0.9033 - val_loss: 0.2720 - val_f_2: 0.3442 - val_acc: 0.9065\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.2811 - f_2: 0.3793 - acc: 0.9046 - val_loss: 0.2713 - val_f_2: 0.3435 - val_acc: 0.9050\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.2831 - f_2: 0.3660 - acc: 0.9018 - val_loss: 0.2719 - val_f_2: 0.3493 - val_acc: 0.9053\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.2824 - f_2: 0.3762 - acc: 0.9033 - val_loss: 0.2732 - val_f_2: 0.3691 - val_acc: 0.9056\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.2835 - f_2: 0.3872 - acc: 0.9033 - val_loss: 0.2735 - val_f_2: 0.3639 - val_acc: 0.9047\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.2821 - f_2: 0.3844 - acc: 0.9039 - val_loss: 0.2718 - val_f_2: 0.3610 - val_acc: 0.9059\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.2815 - f_2: 0.3744 - acc: 0.9037 - val_loss: 0.2773 - val_f_2: 0.4019 - val_acc: 0.9012\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.2805 - f_2: 0.3811 - acc: 0.9043 - val_loss: 0.2730 - val_f_2: 0.3785 - val_acc: 0.9032\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.2805 - f_2: 0.3871 - acc: 0.9041 - val_loss: 0.2723 - val_f_2: 0.3641 - val_acc: 0.9053\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.2809 - f_2: 0.3754 - acc: 0.9026 - val_loss: 0.2725 - val_f_2: 0.3655 - val_acc: 0.9050\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.2806 - f_2: 0.3830 - acc: 0.9032 - val_loss: 0.2719 - val_f_2: 0.3677 - val_acc: 0.9053\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 7.3763e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 0.0010 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.6669 - f_2: 0.0210 - acc: 0.8568 - val_loss: 0.4287 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.4252 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3679 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.3876 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3483 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.3735 - f_2: 2.9561e-08 - acc: 0.8691 - val_loss: 0.3392 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3636 - f_2: 2.8830e-08 - acc: 0.8691 - val_loss: 0.3311 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.3576 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3274 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.3543 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3222 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.3488 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3189 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3457 - f_2: 2.9927e-08 - acc: 0.8691 - val_loss: 0.3171 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3444 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3192 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3430 - f_2: 0.0089 - acc: 0.8691 - val_loss: 0.3152 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3417 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3121 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.3381 - f_2: 3.0870e-08 - acc: 0.8691 - val_loss: 0.3133 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.3404 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3114 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.3416 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3094 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.3393 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3084 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.3349 - f_2: 0.1482 - acc: 0.8838 - val_loss: 0.3090 - val_f_2: 0.0066 - val_acc: 0.8802\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.3361 - f_2: 0.1584 - acc: 0.8852 - val_loss: 0.3065 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.3347 - f_2: 0.1662 - acc: 0.8860 - val_loss: 0.3076 - val_f_2: 0.3095 - val_acc: 0.9103\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.3319 - f_2: 0.1863 - acc: 0.8885 - val_loss: 0.3090 - val_f_2: 0.3174 - val_acc: 0.9094\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.3356 - f_2: 0.1910 - acc: 0.8888 - val_loss: 0.3053 - val_f_2: 0.2917 - val_acc: 0.9094\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.3295 - f_2: 0.2110 - acc: 0.8898 - val_loss: 0.3038 - val_f_2: 0.3001 - val_acc: 0.9100\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.3335 - f_2: 0.1996 - acc: 0.8898 - val_loss: 0.3059 - val_f_2: 0.3097 - val_acc: 0.9106\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.3309 - f_2: 0.2508 - acc: 0.8946 - val_loss: 0.3032 - val_f_2: 0.3130 - val_acc: 0.9094\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.3310 - f_2: 0.2675 - acc: 0.8969 - val_loss: 0.3033 - val_f_2: 0.2979 - val_acc: 0.9100\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.3308 - f_2: 0.2525 - acc: 0.8945 - val_loss: 0.3024 - val_f_2: 0.3174 - val_acc: 0.9094\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.3308 - f_2: 0.2426 - acc: 0.8934 - val_loss: 0.3018 - val_f_2: 0.3118 - val_acc: 0.9100\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.3292 - f_2: 0.2554 - acc: 0.8953 - val_loss: 0.3010 - val_f_2: 0.3174 - val_acc: 0.9097\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.3304 - f_2: 0.2695 - acc: 0.8967 - val_loss: 0.3018 - val_f_2: 0.3165 - val_acc: 0.9103\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.3295 - f_2: 0.2655 - acc: 0.8966 - val_loss: 0.3009 - val_f_2: 0.3020 - val_acc: 0.9106\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.3289 - f_2: 0.2886 - acc: 0.8978 - val_loss: 0.2999 - val_f_2: 0.3019 - val_acc: 0.9103\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.3279 - f_2: 0.2893 - acc: 0.8981 - val_loss: 0.2997 - val_f_2: 0.3165 - val_acc: 0.9094\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.3269 - f_2: 0.2884 - acc: 0.8973 - val_loss: 0.2997 - val_f_2: 0.3151 - val_acc: 0.9100\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.3257 - f_2: 0.3071 - acc: 0.8983 - val_loss: 0.3036 - val_f_2: 0.3482 - val_acc: 0.9083\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.3270 - f_2: 0.2852 - acc: 0.8978 - val_loss: 0.2981 - val_f_2: 0.3034 - val_acc: 0.9100\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.3258 - f_2: 0.2932 - acc: 0.8979 - val_loss: 0.2985 - val_f_2: 0.3202 - val_acc: 0.9097\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.3290 - f_2: 0.3198 - acc: 0.8985 - val_loss: 0.2981 - val_f_2: 0.3167 - val_acc: 0.9097\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.3278 - f_2: 0.3106 - acc: 0.8987 - val_loss: 0.2978 - val_f_2: 0.3229 - val_acc: 0.9083\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.3287 - f_2: 0.3020 - acc: 0.8986 - val_loss: 0.2971 - val_f_2: 0.3079 - val_acc: 0.9109\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.3242 - f_2: 0.3109 - acc: 0.8978 - val_loss: 0.2975 - val_f_2: 0.3169 - val_acc: 0.9112\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.3283 - f_2: 0.3207 - acc: 0.8984 - val_loss: 0.2969 - val_f_2: 0.3215 - val_acc: 0.9106\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.3270 - f_2: 0.2974 - acc: 0.8990 - val_loss: 0.2968 - val_f_2: 0.3233 - val_acc: 0.9106\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.3278 - f_2: 0.3116 - acc: 0.8982 - val_loss: 0.2967 - val_f_2: 0.3238 - val_acc: 0.9112\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.3255 - f_2: 0.3101 - acc: 0.8983 - val_loss: 0.2985 - val_f_2: 0.3514 - val_acc: 0.9068\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.3251 - f_2: 0.3076 - acc: 0.8991 - val_loss: 0.2968 - val_f_2: 0.3284 - val_acc: 0.9074\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.3195 - f_2: 0.3069 - acc: 0.8984 - val_loss: 0.2957 - val_f_2: 0.3218 - val_acc: 0.9112\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.3234 - f_2: 0.3152 - acc: 0.8979 - val_loss: 0.2983 - val_f_2: 0.3384 - val_acc: 0.9088\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.3232 - f_2: 0.3152 - acc: 0.8979 - val_loss: 0.2959 - val_f_2: 0.3359 - val_acc: 0.9088\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.3218 - f_2: 0.3277 - acc: 0.8992 - val_loss: 0.2991 - val_f_2: 0.3802 - val_acc: 0.9074\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.3235 - f_2: 0.3126 - acc: 0.8993 - val_loss: 0.2955 - val_f_2: 0.3240 - val_acc: 0.9074\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.01, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 5.1634e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 4s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 6.6386e-04 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: nan - f_2: nan - acc: 0.0010 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 3s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.001, total= 2.7min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0014 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.7040 - f_2: 0.0311 - acc: 0.8404 - val_loss: 0.5099 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4753 - f_2: 1.3552e-08 - acc: 0.8722 - val_loss: 0.4265 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.4144 - f_2: 1.3680e-08 - acc: 0.8722 - val_loss: 0.3915 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3885 - f_2: 1.3649e-08 - acc: 0.8722 - val_loss: 0.3742 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3732 - f_2: 1.3545e-08 - acc: 0.8722 - val_loss: 0.3598 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3644 - f_2: 0.0010 - acc: 0.8724 - val_loss: 0.3508 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3532 - f_2: 0.0738 - acc: 0.8804 - val_loss: 0.3452 - val_f_2: 0.0138 - val_acc: 0.8684\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3464 - f_2: 0.1223 - acc: 0.8854 - val_loss: 0.3390 - val_f_2: 0.1678 - val_acc: 0.8858\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3430 - f_2: 0.1449 - acc: 0.8870 - val_loss: 0.3357 - val_f_2: 0.2265 - val_acc: 0.8929\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3414 - f_2: 0.1542 - acc: 0.8875 - val_loss: 0.3308 - val_f_2: 0.2760 - val_acc: 0.8988\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3363 - f_2: 0.1794 - acc: 0.8894 - val_loss: 0.3282 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3335 - f_2: 0.2219 - acc: 0.8926 - val_loss: 0.3263 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3309 - f_2: 0.2109 - acc: 0.8924 - val_loss: 0.3227 - val_f_2: 0.2982 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3314 - f_2: 0.2119 - acc: 0.8911 - val_loss: 0.3223 - val_f_2: 0.3005 - val_acc: 0.8991\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3275 - f_2: 0.2278 - acc: 0.8932 - val_loss: 0.3191 - val_f_2: 0.2931 - val_acc: 0.8988\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3286 - f_2: 0.2215 - acc: 0.8917 - val_loss: 0.3180 - val_f_2: 0.3103 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3255 - f_2: 0.2375 - acc: 0.8929 - val_loss: 0.3178 - val_f_2: 0.2895 - val_acc: 0.8988\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3260 - f_2: 0.2238 - acc: 0.8917 - val_loss: 0.3153 - val_f_2: 0.3249 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3248 - f_2: 0.2437 - acc: 0.8927 - val_loss: 0.3145 - val_f_2: 0.3050 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3232 - f_2: 0.2201 - acc: 0.8922 - val_loss: 0.3138 - val_f_2: 0.3062 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3222 - f_2: 0.2492 - acc: 0.8934 - val_loss: 0.3139 - val_f_2: 0.2974 - val_acc: 0.8994\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3232 - f_2: 0.2424 - acc: 0.8927 - val_loss: 0.3130 - val_f_2: 0.2978 - val_acc: 0.8991\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3214 - f_2: 0.2466 - acc: 0.8931 - val_loss: 0.3123 - val_f_2: 0.2997 - val_acc: 0.8994\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3224 - f_2: 0.2424 - acc: 0.8930 - val_loss: 0.3109 - val_f_2: 0.3245 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3192 - f_2: 0.2556 - acc: 0.8928 - val_loss: 0.3107 - val_f_2: 0.3323 - val_acc: 0.9018\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3187 - f_2: 0.2501 - acc: 0.8922 - val_loss: 0.3102 - val_f_2: 0.3046 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3178 - f_2: 0.2633 - acc: 0.8963 - val_loss: 0.3093 - val_f_2: 0.3244 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3202 - f_2: 0.2597 - acc: 0.8936 - val_loss: 0.3092 - val_f_2: 0.3178 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3167 - f_2: 0.2582 - acc: 0.8942 - val_loss: 0.3088 - val_f_2: 0.3320 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3176 - f_2: 0.2669 - acc: 0.8952 - val_loss: 0.3085 - val_f_2: 0.3241 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.2680 - acc: 0.8961 - val_loss: 0.3082 - val_f_2: 0.3141 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3160 - f_2: 0.2681 - acc: 0.8949 - val_loss: 0.3082 - val_f_2: 0.3143 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3168 - f_2: 0.2546 - acc: 0.8937 - val_loss: 0.3082 - val_f_2: 0.3067 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3184 - f_2: 0.2649 - acc: 0.8942 - val_loss: 0.3071 - val_f_2: 0.3225 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3136 - f_2: 0.2779 - acc: 0.8961 - val_loss: 0.3072 - val_f_2: 0.3163 - val_acc: 0.9009\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3150 - f_2: 0.2539 - acc: 0.8942 - val_loss: 0.3066 - val_f_2: 0.3428 - val_acc: 0.9015\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3145 - f_2: 0.2795 - acc: 0.8956 - val_loss: 0.3086 - val_f_2: 0.3013 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3141 - f_2: 0.2730 - acc: 0.8959 - val_loss: 0.3065 - val_f_2: 0.3166 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3143 - f_2: 0.2770 - acc: 0.8956 - val_loss: 0.3055 - val_f_2: 0.3425 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3163 - f_2: 0.2662 - acc: 0.8937 - val_loss: 0.3071 - val_f_2: 0.3636 - val_acc: 0.9018\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.2696 - acc: 0.8942 - val_loss: 0.3056 - val_f_2: 0.3528 - val_acc: 0.9012\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3143 - f_2: 0.2795 - acc: 0.8952 - val_loss: 0.3063 - val_f_2: 0.3163 - val_acc: 0.9015\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3129 - f_2: 0.2676 - acc: 0.8958 - val_loss: 0.3048 - val_f_2: 0.3729 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.3127 - f_2: 0.2838 - acc: 0.8954 - val_loss: 0.3046 - val_f_2: 0.3498 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.3121 - f_2: 0.2712 - acc: 0.8948 - val_loss: 0.3056 - val_f_2: 0.3295 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.3115 - f_2: 0.2836 - acc: 0.8952 - val_loss: 0.3051 - val_f_2: 0.3110 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.3120 - f_2: 0.2795 - acc: 0.8967 - val_loss: 0.3051 - val_f_2: 0.3053 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.3125 - f_2: 0.2917 - acc: 0.8965 - val_loss: 0.3047 - val_f_2: 0.3112 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3117 - f_2: 0.2887 - acc: 0.8967 - val_loss: 0.3058 - val_f_2: 0.3013 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.3108 - f_2: 0.2729 - acc: 0.8957 - val_loss: 0.3038 - val_f_2: 0.3663 - val_acc: 0.9009\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0017 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0016 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0013 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0018 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0018 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.7399 - f_2: 0.0164 - acc: 0.8469 - val_loss: 0.5027 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4538 - f_2: 1.4131e-08 - acc: 0.8722 - val_loss: 0.4111 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3991 - f_2: 1.3677e-08 - acc: 0.8722 - val_loss: 0.3805 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3753 - f_2: 1.3319e-08 - acc: 0.8722 - val_loss: 0.3657 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3664 - f_2: 1.3409e-08 - acc: 0.8722 - val_loss: 0.3581 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3570 - f_2: 1.3366e-08 - acc: 0.8722 - val_loss: 0.3514 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3510 - f_2: 1.3585e-08 - acc: 0.8722 - val_loss: 0.3475 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3466 - f_2: 1.3425e-08 - acc: 0.8722 - val_loss: 0.3427 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3434 - f_2: 1.3695e-08 - acc: 0.8722 - val_loss: 0.3395 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3405 - f_2: 1.4041e-08 - acc: 0.8722 - val_loss: 0.3371 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3388 - f_2: 1.3829e-08 - acc: 0.8722 - val_loss: 0.3347 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3350 - f_2: 1.3682e-08 - acc: 0.8722 - val_loss: 0.3334 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3355 - f_2: 0.0027 - acc: 0.8722 - val_loss: 0.3320 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3323 - f_2: 1.3458e-08 - acc: 0.8722 - val_loss: 0.3307 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3329 - f_2: 1.3571e-08 - acc: 0.8722 - val_loss: 0.3294 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3317 - f_2: 1.3382e-08 - acc: 0.8722 - val_loss: 0.3281 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3300 - f_2: 0.1402 - acc: 0.8863 - val_loss: 0.3277 - val_f_2: 0.2779 - val_acc: 0.8991\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3315 - f_2: 0.2254 - acc: 0.8967 - val_loss: 0.3265 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3269 - f_2: 0.2454 - acc: 0.8981 - val_loss: 0.3255 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3266 - f_2: 0.2460 - acc: 0.8986 - val_loss: 0.3248 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3267 - f_2: 0.2544 - acc: 0.8995 - val_loss: 0.3240 - val_f_2: 0.2829 - val_acc: 0.8982\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3250 - f_2: 0.2687 - acc: 0.8998 - val_loss: 0.3235 - val_f_2: 0.2834 - val_acc: 0.8994\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3259 - f_2: 0.2618 - acc: 0.9004 - val_loss: 0.3233 - val_f_2: 0.2846 - val_acc: 0.8985\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3249 - f_2: 0.2753 - acc: 0.9007 - val_loss: 0.3221 - val_f_2: 0.2919 - val_acc: 0.8994\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3204 - f_2: 0.2836 - acc: 0.9017 - val_loss: 0.3222 - val_f_2: 0.2862 - val_acc: 0.8988\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.2705 - acc: 0.8997 - val_loss: 0.3222 - val_f_2: 0.2850 - val_acc: 0.8994\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3202 - f_2: 0.2881 - acc: 0.9024 - val_loss: 0.3214 - val_f_2: 0.2850 - val_acc: 0.8994\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3210 - f_2: 0.2693 - acc: 0.8998 - val_loss: 0.3202 - val_f_2: 0.3057 - val_acc: 0.8991\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3224 - f_2: 0.2999 - acc: 0.9029 - val_loss: 0.3192 - val_f_2: 0.2901 - val_acc: 0.8994\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3211 - f_2: 0.2924 - acc: 0.9024 - val_loss: 0.3191 - val_f_2: 0.3218 - val_acc: 0.9003\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2977 - acc: 0.9022 - val_loss: 0.3181 - val_f_2: 0.2974 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3216 - f_2: 0.2872 - acc: 0.9020 - val_loss: 0.3181 - val_f_2: 0.3118 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3211 - f_2: 0.3056 - acc: 0.9020 - val_loss: 0.3170 - val_f_2: 0.2921 - val_acc: 0.8997\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3199 - f_2: 0.2984 - acc: 0.9023 - val_loss: 0.3170 - val_f_2: 0.3101 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3173 - f_2: 0.3073 - acc: 0.9020 - val_loss: 0.3169 - val_f_2: 0.2988 - val_acc: 0.8991\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3200 - f_2: 0.2942 - acc: 0.9026 - val_loss: 0.3157 - val_f_2: 0.3048 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3185 - f_2: 0.3021 - acc: 0.9026 - val_loss: 0.3155 - val_f_2: 0.2882 - val_acc: 0.8991\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3205 - f_2: 0.3003 - acc: 0.9031 - val_loss: 0.3162 - val_f_2: 0.3459 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3169 - f_2: 0.3052 - acc: 0.9020 - val_loss: 0.3149 - val_f_2: 0.2974 - val_acc: 0.8997\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3187 - f_2: 0.2973 - acc: 0.9023 - val_loss: 0.3153 - val_f_2: 0.3204 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3149 - acc: 0.9030 - val_loss: 0.3147 - val_f_2: 0.3123 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3175 - f_2: 0.3067 - acc: 0.9022 - val_loss: 0.3144 - val_f_2: 0.2886 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3139 - f_2: 0.3241 - acc: 0.9039 - val_loss: 0.3134 - val_f_2: 0.3055 - val_acc: 0.8994\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.3146 - f_2: 0.3231 - acc: 0.9045 - val_loss: 0.3129 - val_f_2: 0.3437 - val_acc: 0.9027\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.3166 - f_2: 0.3373 - acc: 0.9043 - val_loss: 0.3134 - val_f_2: 0.3139 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3180 - f_2: 0.3249 - acc: 0.9038 - val_loss: 0.3129 - val_f_2: 0.3333 - val_acc: 0.9021\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.3147 - f_2: 0.3222 - acc: 0.9034 - val_loss: 0.3117 - val_f_2: 0.3175 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.3128 - f_2: 0.3170 - acc: 0.9041 - val_loss: 0.3122 - val_f_2: 0.3319 - val_acc: 0.9021\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3134 - f_2: 0.3302 - acc: 0.9038 - val_loss: 0.3119 - val_f_2: 0.3253 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.3145 - f_2: 0.3165 - acc: 0.9031 - val_loss: 0.3112 - val_f_2: 0.3523 - val_acc: 0.9029\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0016 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0015 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0018 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0016 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.7627 - f_2: 0.0169 - acc: 0.8570 - val_loss: 0.5112 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4530 - f_2: 0.0027 - acc: 0.8753 - val_loss: 0.4107 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3949 - f_2: 1.3804e-08 - acc: 0.8753 - val_loss: 0.3793 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3697 - f_2: 1.4279e-08 - acc: 0.8753 - val_loss: 0.3634 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3599 - f_2: 1.4104e-08 - acc: 0.8753 - val_loss: 0.3546 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3506 - f_2: 1.3704e-08 - acc: 0.8753 - val_loss: 0.3480 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3471 - f_2: 1.3960e-08 - acc: 0.8753 - val_loss: 0.3430 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3408 - f_2: 1.3846e-08 - acc: 0.8753 - val_loss: 0.3400 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3377 - f_2: 1.3900e-08 - acc: 0.8753 - val_loss: 0.3377 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3360 - f_2: 1.3890e-08 - acc: 0.8753 - val_loss: 0.3349 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3317 - f_2: 1.3891e-08 - acc: 0.8753 - val_loss: 0.3335 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3315 - f_2: 1.4086e-08 - acc: 0.8753 - val_loss: 0.3313 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3275 - f_2: 0.1639 - acc: 0.8926 - val_loss: 0.3292 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3283 - f_2: 0.2039 - acc: 0.8965 - val_loss: 0.3278 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3268 - f_2: 0.2153 - acc: 0.8970 - val_loss: 0.3269 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3253 - f_2: 0.2270 - acc: 0.8977 - val_loss: 0.3256 - val_f_2: 0.2899 - val_acc: 0.8991\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3246 - f_2: 0.2366 - acc: 0.8990 - val_loss: 0.3257 - val_f_2: 0.2868 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3221 - f_2: 0.2367 - acc: 0.8991 - val_loss: 0.3239 - val_f_2: 0.3025 - val_acc: 0.8985\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3220 - f_2: 0.2553 - acc: 0.9003 - val_loss: 0.3227 - val_f_2: 0.2934 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3206 - f_2: 0.2504 - acc: 0.8994 - val_loss: 0.3243 - val_f_2: 0.3277 - val_acc: 0.9003\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2519 - acc: 0.9001 - val_loss: 0.3210 - val_f_2: 0.2934 - val_acc: 0.8997\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3198 - f_2: 0.2492 - acc: 0.9005 - val_loss: 0.3212 - val_f_2: 0.3005 - val_acc: 0.8985\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3180 - f_2: 0.2665 - acc: 0.9016 - val_loss: 0.3200 - val_f_2: 0.3021 - val_acc: 0.8991\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3178 - f_2: 0.2736 - acc: 0.9024 - val_loss: 0.3196 - val_f_2: 0.2934 - val_acc: 0.8997\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3187 - f_2: 0.2760 - acc: 0.9017 - val_loss: 0.3186 - val_f_2: 0.3081 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3165 - f_2: 0.2801 - acc: 0.9020 - val_loss: 0.3183 - val_f_2: 0.3008 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3167 - f_2: 0.2782 - acc: 0.9021 - val_loss: 0.3175 - val_f_2: 0.2953 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3154 - f_2: 0.2741 - acc: 0.9026 - val_loss: 0.3203 - val_f_2: 0.3023 - val_acc: 0.8997\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3138 - f_2: 0.2895 - acc: 0.9032 - val_loss: 0.3166 - val_f_2: 0.3316 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3141 - f_2: 0.3021 - acc: 0.9027 - val_loss: 0.3162 - val_f_2: 0.2953 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3142 - f_2: 0.2916 - acc: 0.9029 - val_loss: 0.3159 - val_f_2: 0.3083 - val_acc: 0.8994\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3153 - f_2: 0.2918 - acc: 0.9030 - val_loss: 0.3161 - val_f_2: 0.3475 - val_acc: 0.9009\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3140 - f_2: 0.2856 - acc: 0.9029 - val_loss: 0.3146 - val_f_2: 0.3177 - val_acc: 0.9003\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3127 - f_2: 0.3047 - acc: 0.9043 - val_loss: 0.3172 - val_f_2: 0.2953 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3136 - f_2: 0.2789 - acc: 0.9024 - val_loss: 0.3142 - val_f_2: 0.3344 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3121 - f_2: 0.3097 - acc: 0.9031 - val_loss: 0.3139 - val_f_2: 0.3229 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3114 - f_2: 0.3147 - acc: 0.9043 - val_loss: 0.3133 - val_f_2: 0.3142 - val_acc: 0.9003\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3126 - f_2: 0.2913 - acc: 0.9033 - val_loss: 0.3136 - val_f_2: 0.3508 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3105 - f_2: 0.3089 - acc: 0.9033 - val_loss: 0.3134 - val_f_2: 0.3163 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3110 - f_2: 0.3041 - acc: 0.9028 - val_loss: 0.3146 - val_f_2: 0.3122 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.3093 - f_2: 0.3164 - acc: 0.9036 - val_loss: 0.3119 - val_f_2: 0.3332 - val_acc: 0.9006\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3080 - f_2: 0.3135 - acc: 0.9032 - val_loss: 0.3129 - val_f_2: 0.3142 - val_acc: 0.9003\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3096 - f_2: 0.3193 - acc: 0.9048 - val_loss: 0.3120 - val_f_2: 0.3234 - val_acc: 0.9003\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.3081 - f_2: 0.3177 - acc: 0.9045 - val_loss: 0.3150 - val_f_2: 0.3123 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.3088 - f_2: 0.3134 - acc: 0.9034 - val_loss: 0.3136 - val_f_2: 0.3064 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.3073 - f_2: 0.3132 - acc: 0.9040 - val_loss: 0.3111 - val_f_2: 0.3438 - val_acc: 0.9012\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.3085 - f_2: 0.3176 - acc: 0.9029 - val_loss: 0.3102 - val_f_2: 0.3333 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.3074 - f_2: 0.3265 - acc: 0.9035 - val_loss: 0.3114 - val_f_2: 0.3246 - val_acc: 0.9009\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3069 - f_2: 0.3123 - acc: 0.9042 - val_loss: 0.3100 - val_f_2: 0.3440 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.3065 - f_2: 0.3246 - acc: 0.9035 - val_loss: 0.3103 - val_f_2: 0.3265 - val_acc: 0.9009\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0018 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0018 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0018 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4694 - f_2: 0.0161 - acc: 0.8557 - val_loss: 0.3948 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3880 - f_2: 1.3816e-08 - acc: 0.8722 - val_loss: 0.3704 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3660 - f_2: 1.3342e-08 - acc: 0.8722 - val_loss: 0.3559 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3508 - f_2: 1.3238e-08 - acc: 0.8722 - val_loss: 0.3446 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3456 - f_2: 1.3416e-08 - acc: 0.8722 - val_loss: 0.3392 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3407 - f_2: 1.3719e-08 - acc: 0.8722 - val_loss: 0.3354 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3325 - f_2: 1.3353e-08 - acc: 0.8722 - val_loss: 0.3323 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3292 - f_2: 0.0103 - acc: 0.8736 - val_loss: 0.3286 - val_f_2: 0.2791 - val_acc: 0.8991\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3270 - f_2: 0.2079 - acc: 0.8937 - val_loss: 0.3259 - val_f_2: 0.2808 - val_acc: 0.8994\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3219 - f_2: 0.2214 - acc: 0.8956 - val_loss: 0.3253 - val_f_2: 0.2852 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3210 - f_2: 0.2453 - acc: 0.8972 - val_loss: 0.3225 - val_f_2: 0.2940 - val_acc: 0.8997\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3177 - f_2: 0.2623 - acc: 0.8997 - val_loss: 0.3202 - val_f_2: 0.3026 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3167 - f_2: 0.2949 - acc: 0.9021 - val_loss: 0.3202 - val_f_2: 0.3084 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3088 - acc: 0.9022 - val_loss: 0.3178 - val_f_2: 0.3131 - val_acc: 0.8991\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3125 - f_2: 0.3092 - acc: 0.9030 - val_loss: 0.3179 - val_f_2: 0.3135 - val_acc: 0.8994\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3099 - f_2: 0.3216 - acc: 0.9044 - val_loss: 0.3207 - val_f_2: 0.3101 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3084 - f_2: 0.3228 - acc: 0.9024 - val_loss: 0.3143 - val_f_2: 0.3524 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3101 - f_2: 0.3406 - acc: 0.9052 - val_loss: 0.3135 - val_f_2: 0.3605 - val_acc: 0.9015\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3087 - f_2: 0.3351 - acc: 0.9038 - val_loss: 0.3152 - val_f_2: 0.3408 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3048 - f_2: 0.3495 - acc: 0.9052 - val_loss: 0.3114 - val_f_2: 0.3386 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3028 - f_2: 0.3444 - acc: 0.9046 - val_loss: 0.3120 - val_f_2: 0.3578 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3044 - f_2: 0.3393 - acc: 0.9041 - val_loss: 0.3107 - val_f_2: 0.3796 - val_acc: 0.9015\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3027 - f_2: 0.3571 - acc: 0.9055 - val_loss: 0.3104 - val_f_2: 0.3445 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3028 - f_2: 0.3465 - acc: 0.9037 - val_loss: 0.3101 - val_f_2: 0.3660 - val_acc: 0.9018\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3018 - f_2: 0.3693 - acc: 0.9060 - val_loss: 0.3088 - val_f_2: 0.3520 - val_acc: 0.9018\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3011 - f_2: 0.3616 - acc: 0.9047 - val_loss: 0.3083 - val_f_2: 0.3669 - val_acc: 0.9021\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2990 - f_2: 0.3507 - acc: 0.9046 - val_loss: 0.3077 - val_f_2: 0.3911 - val_acc: 0.9024\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2995 - f_2: 0.3600 - acc: 0.9040 - val_loss: 0.3073 - val_f_2: 0.3753 - val_acc: 0.9027\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2972 - f_2: 0.3629 - acc: 0.9059 - val_loss: 0.3067 - val_f_2: 0.3908 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2970 - f_2: 0.3715 - acc: 0.9056 - val_loss: 0.3061 - val_f_2: 0.3723 - val_acc: 0.9024\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2971 - f_2: 0.3646 - acc: 0.9057 - val_loss: 0.3061 - val_f_2: 0.3769 - val_acc: 0.9024\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2972 - f_2: 0.3670 - acc: 0.9056 - val_loss: 0.3066 - val_f_2: 0.4004 - val_acc: 0.9024\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2956 - f_2: 0.3816 - acc: 0.9062 - val_loss: 0.3058 - val_f_2: 0.3885 - val_acc: 0.9018\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2932 - f_2: 0.3737 - acc: 0.9054 - val_loss: 0.3072 - val_f_2: 0.4033 - val_acc: 0.9021\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2947 - f_2: 0.3805 - acc: 0.9061 - val_loss: 0.3072 - val_f_2: 0.3936 - val_acc: 0.9021\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2938 - f_2: 0.3843 - acc: 0.9057 - val_loss: 0.3051 - val_f_2: 0.3914 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2928 - f_2: 0.3968 - acc: 0.9082 - val_loss: 0.3056 - val_f_2: 0.3998 - val_acc: 0.9024\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2904 - f_2: 0.3783 - acc: 0.9064 - val_loss: 0.3055 - val_f_2: 0.4129 - val_acc: 0.9029\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2905 - f_2: 0.3927 - acc: 0.9072 - val_loss: 0.3053 - val_f_2: 0.4254 - val_acc: 0.9038\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2910 - f_2: 0.3964 - acc: 0.9062 - val_loss: 0.3049 - val_f_2: 0.3927 - val_acc: 0.9027\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2907 - f_2: 0.3960 - acc: 0.9064 - val_loss: 0.3090 - val_f_2: 0.3955 - val_acc: 0.9024\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2907 - f_2: 0.3907 - acc: 0.9077 - val_loss: 0.3047 - val_f_2: 0.4127 - val_acc: 0.9032\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2885 - f_2: 0.3969 - acc: 0.9067 - val_loss: 0.3080 - val_f_2: 0.3911 - val_acc: 0.9032\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2885 - f_2: 0.4087 - acc: 0.9071 - val_loss: 0.3042 - val_f_2: 0.4045 - val_acc: 0.9032\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2852 - f_2: 0.3977 - acc: 0.9074 - val_loss: 0.3068 - val_f_2: 0.3826 - val_acc: 0.9027\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2884 - f_2: 0.3996 - acc: 0.9074 - val_loss: 0.3057 - val_f_2: 0.4124 - val_acc: 0.9047\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2875 - f_2: 0.4072 - acc: 0.9079 - val_loss: 0.3072 - val_f_2: 0.4272 - val_acc: 0.9041\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2872 - f_2: 0.4060 - acc: 0.9063 - val_loss: 0.3047 - val_f_2: 0.4243 - val_acc: 0.9032\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2884 - f_2: 0.4047 - acc: 0.9077 - val_loss: 0.3104 - val_f_2: 0.4027 - val_acc: 0.9032\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2879 - f_2: 0.4147 - acc: 0.9088 - val_loss: 0.3040 - val_f_2: 0.4023 - val_acc: 0.9029\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4802 - f_2: 0.0092 - acc: 0.8502 - val_loss: 0.3738 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3940 - f_2: 1.3114e-08 - acc: 0.8691 - val_loss: 0.3494 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3744 - f_2: 1.3394e-08 - acc: 0.8691 - val_loss: 0.3327 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3616 - f_2: 1.3064e-08 - acc: 0.8691 - val_loss: 0.3244 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3524 - f_2: 0.0817 - acc: 0.8787 - val_loss: 0.3180 - val_f_2: 0.2817 - val_acc: 0.9080\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3417 - f_2: 0.2396 - acc: 0.8944 - val_loss: 0.3074 - val_f_2: 0.3286 - val_acc: 0.9103\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3308 - f_2: 0.3054 - acc: 0.8995 - val_loss: 0.2990 - val_f_2: 0.3499 - val_acc: 0.9091\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3222 - f_2: 0.3235 - acc: 0.8995 - val_loss: 0.2943 - val_f_2: 0.3705 - val_acc: 0.9103\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3177 - f_2: 0.3439 - acc: 0.8990 - val_loss: 0.2897 - val_f_2: 0.3389 - val_acc: 0.9112\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3386 - acc: 0.9004 - val_loss: 0.2873 - val_f_2: 0.3510 - val_acc: 0.9124\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3114 - f_2: 0.3540 - acc: 0.9021 - val_loss: 0.2853 - val_f_2: 0.3792 - val_acc: 0.9100\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3070 - f_2: 0.3422 - acc: 0.9007 - val_loss: 0.2837 - val_f_2: 0.3573 - val_acc: 0.9097\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3040 - f_2: 0.3486 - acc: 0.9006 - val_loss: 0.2831 - val_f_2: 0.4022 - val_acc: 0.9112\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3033 - f_2: 0.3609 - acc: 0.9016 - val_loss: 0.2812 - val_f_2: 0.3547 - val_acc: 0.9097\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3001 - f_2: 0.3697 - acc: 0.9029 - val_loss: 0.2803 - val_f_2: 0.3800 - val_acc: 0.9103\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.2991 - f_2: 0.3581 - acc: 0.9012 - val_loss: 0.2795 - val_f_2: 0.3574 - val_acc: 0.9103\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.2966 - f_2: 0.3673 - acc: 0.9022 - val_loss: 0.2792 - val_f_2: 0.3801 - val_acc: 0.9103\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.2964 - f_2: 0.3593 - acc: 0.9015 - val_loss: 0.2797 - val_f_2: 0.3771 - val_acc: 0.9112\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.2947 - f_2: 0.3651 - acc: 0.9022 - val_loss: 0.2783 - val_f_2: 0.3617 - val_acc: 0.9112\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2922 - f_2: 0.3705 - acc: 0.9025 - val_loss: 0.2770 - val_f_2: 0.3673 - val_acc: 0.9100\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.2933 - f_2: 0.3770 - acc: 0.9023 - val_loss: 0.2767 - val_f_2: 0.3915 - val_acc: 0.9103\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.2929 - f_2: 0.3623 - acc: 0.9021 - val_loss: 0.2764 - val_f_2: 0.3699 - val_acc: 0.9103\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.3687 - acc: 0.9021 - val_loss: 0.2764 - val_f_2: 0.3703 - val_acc: 0.9115\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.3735 - acc: 0.9018 - val_loss: 0.2757 - val_f_2: 0.3791 - val_acc: 0.9112\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.2892 - f_2: 0.3721 - acc: 0.9028 - val_loss: 0.2753 - val_f_2: 0.3700 - val_acc: 0.9106\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.2886 - f_2: 0.3766 - acc: 0.9026 - val_loss: 0.2766 - val_f_2: 0.4037 - val_acc: 0.9103\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2871 - f_2: 0.3690 - acc: 0.9026 - val_loss: 0.2745 - val_f_2: 0.3550 - val_acc: 0.9086\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2863 - f_2: 0.3907 - acc: 0.9037 - val_loss: 0.2755 - val_f_2: 0.3914 - val_acc: 0.9100\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2867 - f_2: 0.3868 - acc: 0.9038 - val_loss: 0.2751 - val_f_2: 0.3692 - val_acc: 0.9100\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2864 - f_2: 0.3880 - acc: 0.9026 - val_loss: 0.2750 - val_f_2: 0.3647 - val_acc: 0.9088\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2841 - f_2: 0.3839 - acc: 0.9032 - val_loss: 0.2743 - val_f_2: 0.3814 - val_acc: 0.9106\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2836 - f_2: 0.3858 - acc: 0.9037 - val_loss: 0.2747 - val_f_2: 0.3876 - val_acc: 0.9109\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2842 - f_2: 0.3802 - acc: 0.9022 - val_loss: 0.2746 - val_f_2: 0.3828 - val_acc: 0.9086\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2833 - f_2: 0.3909 - acc: 0.9043 - val_loss: 0.2758 - val_f_2: 0.3881 - val_acc: 0.9086\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2808 - f_2: 0.4019 - acc: 0.9053 - val_loss: 0.2747 - val_f_2: 0.3867 - val_acc: 0.9091\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2815 - f_2: 0.3910 - acc: 0.9034 - val_loss: 0.2769 - val_f_2: 0.4161 - val_acc: 0.9077\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2816 - f_2: 0.3858 - acc: 0.9046 - val_loss: 0.2742 - val_f_2: 0.3844 - val_acc: 0.9094\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2811 - f_2: 0.3897 - acc: 0.9040 - val_loss: 0.2744 - val_f_2: 0.4001 - val_acc: 0.9077\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2807 - f_2: 0.4000 - acc: 0.9054 - val_loss: 0.2751 - val_f_2: 0.3836 - val_acc: 0.9103\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2802 - f_2: 0.3953 - acc: 0.9038 - val_loss: 0.2756 - val_f_2: 0.4073 - val_acc: 0.9065\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2804 - f_2: 0.4007 - acc: 0.9033 - val_loss: 0.2753 - val_f_2: 0.4070 - val_acc: 0.9086\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2798 - f_2: 0.3971 - acc: 0.9058 - val_loss: 0.2739 - val_f_2: 0.3912 - val_acc: 0.9088\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2771 - f_2: 0.4143 - acc: 0.9060 - val_loss: 0.2745 - val_f_2: 0.3785 - val_acc: 0.9086\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2786 - f_2: 0.4053 - acc: 0.9048 - val_loss: 0.2742 - val_f_2: 0.3965 - val_acc: 0.9077\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2779 - f_2: 0.4154 - acc: 0.9062 - val_loss: 0.2743 - val_f_2: 0.4013 - val_acc: 0.9077\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2774 - f_2: 0.4229 - acc: 0.9063 - val_loss: 0.2743 - val_f_2: 0.3892 - val_acc: 0.9080\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2768 - f_2: 0.4109 - acc: 0.9060 - val_loss: 0.2754 - val_f_2: 0.3963 - val_acc: 0.9062\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2773 - f_2: 0.4178 - acc: 0.9060 - val_loss: 0.2735 - val_f_2: 0.3968 - val_acc: 0.9088\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2766 - f_2: 0.4011 - acc: 0.9048 - val_loss: 0.2748 - val_f_2: 0.4005 - val_acc: 0.9065\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2762 - f_2: 0.4193 - acc: 0.9068 - val_loss: 0.2743 - val_f_2: 0.4094 - val_acc: 0.9080\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0013 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0013 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0014 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4941 - f_2: 0.0259 - acc: 0.8427 - val_loss: 0.3869 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3970 - f_2: 1.3965e-08 - acc: 0.8753 - val_loss: 0.3641 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3784 - f_2: 0.0327 - acc: 0.8787 - val_loss: 0.3477 - val_f_2: 0.0022 - val_acc: 0.8670\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3639 - f_2: 0.1181 - acc: 0.8874 - val_loss: 0.3339 - val_f_2: 0.2648 - val_acc: 0.8971\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3496 - f_2: 0.1817 - acc: 0.8922 - val_loss: 0.3247 - val_f_2: 0.2786 - val_acc: 0.8988\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3444 - f_2: 0.1994 - acc: 0.8932 - val_loss: 0.3177 - val_f_2: 0.2948 - val_acc: 0.9012\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3362 - f_2: 0.2061 - acc: 0.8928 - val_loss: 0.3112 - val_f_2: 0.3185 - val_acc: 0.9021\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3326 - f_2: 0.2148 - acc: 0.8928 - val_loss: 0.3085 - val_f_2: 0.3059 - val_acc: 0.9012\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3323 - f_2: 0.2139 - acc: 0.8936 - val_loss: 0.3063 - val_f_2: 0.3199 - val_acc: 0.9009\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3225 - f_2: 0.2439 - acc: 0.8953 - val_loss: 0.3040 - val_f_2: 0.3164 - val_acc: 0.9003\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3210 - f_2: 0.2579 - acc: 0.8974 - val_loss: 0.3025 - val_f_2: 0.3229 - val_acc: 0.9009\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3190 - f_2: 0.2511 - acc: 0.8975 - val_loss: 0.3020 - val_f_2: 0.3246 - val_acc: 0.9006\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3144 - f_2: 0.2474 - acc: 0.8970 - val_loss: 0.2996 - val_f_2: 0.3363 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3157 - f_2: 0.2322 - acc: 0.8944 - val_loss: 0.2987 - val_f_2: 0.3302 - val_acc: 0.8991\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3105 - f_2: 0.2505 - acc: 0.8969 - val_loss: 0.2977 - val_f_2: 0.3485 - val_acc: 0.9009\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3098 - f_2: 0.2633 - acc: 0.8972 - val_loss: 0.2987 - val_f_2: 0.3256 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3080 - f_2: 0.2646 - acc: 0.8987 - val_loss: 0.2978 - val_f_2: 0.3469 - val_acc: 0.9018\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3086 - f_2: 0.2770 - acc: 0.8984 - val_loss: 0.2966 - val_f_2: 0.3582 - val_acc: 0.9015\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3071 - f_2: 0.2906 - acc: 0.8989 - val_loss: 0.2990 - val_f_2: 0.3480 - val_acc: 0.9027\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3053 - f_2: 0.2914 - acc: 0.8998 - val_loss: 0.2956 - val_f_2: 0.3560 - val_acc: 0.9012\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.2930 - acc: 0.9003 - val_loss: 0.2956 - val_f_2: 0.3609 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3035 - f_2: 0.2921 - acc: 0.8994 - val_loss: 0.2968 - val_f_2: 0.3576 - val_acc: 0.9015\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3005 - f_2: 0.3100 - acc: 0.9005 - val_loss: 0.2958 - val_f_2: 0.3686 - val_acc: 0.8988\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3030 - f_2: 0.3227 - acc: 0.9020 - val_loss: 0.2956 - val_f_2: 0.3691 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3031 - f_2: 0.3034 - acc: 0.8998 - val_loss: 0.2957 - val_f_2: 0.3458 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3006 - f_2: 0.3109 - acc: 0.9002 - val_loss: 0.2951 - val_f_2: 0.3728 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2980 - f_2: 0.3208 - acc: 0.9017 - val_loss: 0.2953 - val_f_2: 0.3868 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2981 - f_2: 0.3141 - acc: 0.9005 - val_loss: 0.2948 - val_f_2: 0.3745 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2990 - f_2: 0.3317 - acc: 0.9028 - val_loss: 0.2942 - val_f_2: 0.3822 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2993 - f_2: 0.3446 - acc: 0.9027 - val_loss: 0.2948 - val_f_2: 0.3518 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2963 - f_2: 0.3327 - acc: 0.9031 - val_loss: 0.2942 - val_f_2: 0.3646 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2965 - f_2: 0.3404 - acc: 0.9024 - val_loss: 0.2943 - val_f_2: 0.3875 - val_acc: 0.8988\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2950 - f_2: 0.3288 - acc: 0.9030 - val_loss: 0.2950 - val_f_2: 0.3709 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2971 - f_2: 0.3255 - acc: 0.9001 - val_loss: 0.2955 - val_f_2: 0.3656 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2949 - f_2: 0.3431 - acc: 0.9032 - val_loss: 0.2943 - val_f_2: 0.4026 - val_acc: 0.8997\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2951 - f_2: 0.3544 - acc: 0.9028 - val_loss: 0.2938 - val_f_2: 0.4029 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.3520 - acc: 0.9043 - val_loss: 0.2948 - val_f_2: 0.4145 - val_acc: 0.9003\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2914 - f_2: 0.3604 - acc: 0.9047 - val_loss: 0.2939 - val_f_2: 0.3658 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2906 - f_2: 0.3449 - acc: 0.9030 - val_loss: 0.2941 - val_f_2: 0.4014 - val_acc: 0.9003\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2923 - f_2: 0.3605 - acc: 0.9037 - val_loss: 0.2947 - val_f_2: 0.3711 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2904 - f_2: 0.3577 - acc: 0.9034 - val_loss: 0.2939 - val_f_2: 0.3845 - val_acc: 0.9006\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2920 - f_2: 0.3486 - acc: 0.9034 - val_loss: 0.2944 - val_f_2: 0.3690 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2889 - f_2: 0.3579 - acc: 0.9053 - val_loss: 0.2938 - val_f_2: 0.4105 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2896 - f_2: 0.3529 - acc: 0.9054 - val_loss: 0.2950 - val_f_2: 0.3987 - val_acc: 0.9003\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2888 - f_2: 0.3756 - acc: 0.9061 - val_loss: 0.2946 - val_f_2: 0.3871 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2883 - f_2: 0.3668 - acc: 0.9054 - val_loss: 0.2954 - val_f_2: 0.3835 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2897 - f_2: 0.3640 - acc: 0.9039 - val_loss: 0.2944 - val_f_2: 0.3840 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2873 - f_2: 0.3680 - acc: 0.9054 - val_loss: 0.2941 - val_f_2: 0.3934 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2884 - f_2: 0.3794 - acc: 0.9055 - val_loss: 0.2957 - val_f_2: 0.3670 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2861 - f_2: 0.3733 - acc: 0.9071 - val_loss: 0.2935 - val_f_2: 0.4082 - val_acc: 0.9009\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.5051 - f_2: 0.0338 - acc: 0.8388 - val_loss: 0.3869 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3963 - f_2: 1.3959e-08 - acc: 0.8722 - val_loss: 0.3630 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3809 - f_2: 0.0594 - acc: 0.8783 - val_loss: 0.3463 - val_f_2: 0.1030 - val_acc: 0.8788\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3643 - f_2: 0.1484 - acc: 0.8873 - val_loss: 0.3342 - val_f_2: 0.2551 - val_acc: 0.8959\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3523 - f_2: 0.1883 - acc: 0.8891 - val_loss: 0.3246 - val_f_2: 0.2844 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3417 - f_2: 0.2274 - acc: 0.8911 - val_loss: 0.3171 - val_f_2: 0.3010 - val_acc: 0.9006\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3387 - f_2: 0.2380 - acc: 0.8925 - val_loss: 0.3124 - val_f_2: 0.2999 - val_acc: 0.9006\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3346 - f_2: 0.2508 - acc: 0.8933 - val_loss: 0.3080 - val_f_2: 0.3148 - val_acc: 0.9012\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3269 - f_2: 0.2584 - acc: 0.8932 - val_loss: 0.3072 - val_f_2: 0.3042 - val_acc: 0.9009\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3272 - f_2: 0.2301 - acc: 0.8930 - val_loss: 0.3040 - val_f_2: 0.3121 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3197 - f_2: 0.2661 - acc: 0.8952 - val_loss: 0.3019 - val_f_2: 0.3389 - val_acc: 0.9018\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3184 - f_2: 0.2654 - acc: 0.8958 - val_loss: 0.3009 - val_f_2: 0.3355 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3133 - f_2: 0.2769 - acc: 0.8963 - val_loss: 0.2997 - val_f_2: 0.3574 - val_acc: 0.9024\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3139 - f_2: 0.2945 - acc: 0.8969 - val_loss: 0.2996 - val_f_2: 0.3255 - val_acc: 0.9003\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3131 - f_2: 0.2860 - acc: 0.8967 - val_loss: 0.2983 - val_f_2: 0.3439 - val_acc: 0.9012\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3107 - f_2: 0.3077 - acc: 0.8990 - val_loss: 0.2986 - val_f_2: 0.3535 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3139 - f_2: 0.2804 - acc: 0.8957 - val_loss: 0.2973 - val_f_2: 0.3615 - val_acc: 0.8991\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3083 - f_2: 0.3131 - acc: 0.8980 - val_loss: 0.2978 - val_f_2: 0.3578 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3061 - f_2: 0.3190 - acc: 0.8992 - val_loss: 0.2979 - val_f_2: 0.3335 - val_acc: 0.8991\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3061 - f_2: 0.3248 - acc: 0.8992 - val_loss: 0.2966 - val_f_2: 0.3615 - val_acc: 0.9003\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3057 - f_2: 0.3233 - acc: 0.8992 - val_loss: 0.2956 - val_f_2: 0.3753 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3034 - f_2: 0.3149 - acc: 0.8993 - val_loss: 0.2955 - val_f_2: 0.3805 - val_acc: 0.8991\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3007 - f_2: 0.3389 - acc: 0.9020 - val_loss: 0.2951 - val_f_2: 0.3942 - val_acc: 0.8988\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3011 - f_2: 0.3351 - acc: 0.9020 - val_loss: 0.2962 - val_f_2: 0.3710 - val_acc: 0.8991\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3015 - f_2: 0.3504 - acc: 0.9015 - val_loss: 0.2943 - val_f_2: 0.3926 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3012 - f_2: 0.3415 - acc: 0.9020 - val_loss: 0.2962 - val_f_2: 0.3608 - val_acc: 0.8997\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3010 - f_2: 0.3523 - acc: 0.9025 - val_loss: 0.2958 - val_f_2: 0.3595 - val_acc: 0.8988\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2978 - f_2: 0.3718 - acc: 0.9038 - val_loss: 0.2966 - val_f_2: 0.3500 - val_acc: 0.8994\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2967 - f_2: 0.3437 - acc: 0.9020 - val_loss: 0.2963 - val_f_2: 0.3697 - val_acc: 0.8982\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2966 - f_2: 0.3776 - acc: 0.9040 - val_loss: 0.2952 - val_f_2: 0.3969 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2973 - f_2: 0.3725 - acc: 0.9034 - val_loss: 0.2952 - val_f_2: 0.3779 - val_acc: 0.8988\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2971 - f_2: 0.3600 - acc: 0.9027 - val_loss: 0.2960 - val_f_2: 0.3427 - val_acc: 0.8988\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2947 - f_2: 0.3646 - acc: 0.9042 - val_loss: 0.2957 - val_f_2: 0.3747 - val_acc: 0.8982\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2945 - f_2: 0.3820 - acc: 0.9047 - val_loss: 0.2963 - val_f_2: 0.3645 - val_acc: 0.8988\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.3783 - acc: 0.9044 - val_loss: 0.2964 - val_f_2: 0.3650 - val_acc: 0.8985\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2944 - f_2: 0.3700 - acc: 0.9032 - val_loss: 0.2953 - val_f_2: 0.3942 - val_acc: 0.8985\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2926 - f_2: 0.3816 - acc: 0.9051 - val_loss: 0.2961 - val_f_2: 0.3785 - val_acc: 0.8973\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2928 - f_2: 0.3981 - acc: 0.9074 - val_loss: 0.2957 - val_f_2: 0.3905 - val_acc: 0.8973\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2899 - f_2: 0.3939 - acc: 0.9061 - val_loss: 0.2950 - val_f_2: 0.4224 - val_acc: 0.9003\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2910 - f_2: 0.3952 - acc: 0.9075 - val_loss: 0.2954 - val_f_2: 0.3978 - val_acc: 0.8985\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2904 - f_2: 0.3961 - acc: 0.9051 - val_loss: 0.2952 - val_f_2: 0.3916 - val_acc: 0.8991\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2879 - f_2: 0.3958 - acc: 0.9076 - val_loss: 0.2951 - val_f_2: 0.4042 - val_acc: 0.8985\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2870 - f_2: 0.4008 - acc: 0.9076 - val_loss: 0.2964 - val_f_2: 0.3858 - val_acc: 0.8985\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2878 - f_2: 0.3946 - acc: 0.9060 - val_loss: 0.2951 - val_f_2: 0.4150 - val_acc: 0.8994\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2853 - f_2: 0.4202 - acc: 0.9087 - val_loss: 0.2963 - val_f_2: 0.4244 - val_acc: 0.8997\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2876 - f_2: 0.4077 - acc: 0.9079 - val_loss: 0.2955 - val_f_2: 0.4163 - val_acc: 0.8988\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2877 - f_2: 0.4110 - acc: 0.9079 - val_loss: 0.2953 - val_f_2: 0.4022 - val_acc: 0.8994\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2863 - f_2: 0.4220 - acc: 0.9091 - val_loss: 0.2952 - val_f_2: 0.4267 - val_acc: 0.8991\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2860 - f_2: 0.4163 - acc: 0.9076 - val_loss: 0.2959 - val_f_2: 0.3916 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2873 - f_2: 0.4043 - acc: 0.9063 - val_loss: 0.2962 - val_f_2: 0.4038 - val_acc: 0.8997\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0015 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: nan - f_2: nan - acc: 0.0010 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      " - 1s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      " - 2s - loss: nan - f_2: nan - acc: 0.0000e+00 - val_loss: nan - val_f_2: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py:233: RuntimeWarning: invalid value encountered in greater\n",
      "  classes = (proba > 0.5).astype('int32')\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.7576 - f_2: 0.0235 - acc: 0.8440 - val_loss: 0.5117 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4667 - f_2: 1.3550e-08 - acc: 0.8722 - val_loss: 0.4188 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.4122 - f_2: 1.3622e-08 - acc: 0.8722 - val_loss: 0.3884 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3849 - f_2: 1.3567e-08 - acc: 0.8722 - val_loss: 0.3717 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3766 - f_2: 1.3753e-08 - acc: 0.8722 - val_loss: 0.3636 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3681 - f_2: 1.3681e-08 - acc: 0.8722 - val_loss: 0.3569 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3589 - f_2: 1.3660e-08 - acc: 0.8722 - val_loss: 0.3521 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3578 - f_2: 1.3651e-08 - acc: 0.8722 - val_loss: 0.3474 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3543 - f_2: 1.3378e-08 - acc: 0.8722 - val_loss: 0.3446 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3479 - f_2: 1.3569e-08 - acc: 0.8722 - val_loss: 0.3406 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3475 - f_2: 1.3770e-08 - acc: 0.8722 - val_loss: 0.3384 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3440 - f_2: 1.3720e-08 - acc: 0.8722 - val_loss: 0.3363 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3422 - f_2: 1.3408e-08 - acc: 0.8722 - val_loss: 0.3354 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3383 - f_2: 1.4218e-08 - acc: 0.8722 - val_loss: 0.3327 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3385 - f_2: 1.3738e-08 - acc: 0.8722 - val_loss: 0.3316 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3384 - f_2: 1.3928e-08 - acc: 0.8722 - val_loss: 0.3305 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3389 - f_2: 1.3645e-08 - acc: 0.8722 - val_loss: 0.3296 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3369 - f_2: 1.3754e-08 - acc: 0.8722 - val_loss: 0.3294 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3360 - f_2: 1.3568e-08 - acc: 0.8722 - val_loss: 0.3282 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3331 - f_2: 1.3505e-08 - acc: 0.8722 - val_loss: 0.3273 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3332 - f_2: 1.3684e-08 - acc: 0.8722 - val_loss: 0.3266 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3362 - f_2: 1.3429e-08 - acc: 0.8722 - val_loss: 0.3262 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3322 - f_2: 1.3635e-08 - acc: 0.8722 - val_loss: 0.3256 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3335 - f_2: 1.3658e-08 - acc: 0.8722 - val_loss: 0.3257 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3322 - f_2: 1.3949e-08 - acc: 0.8722 - val_loss: 0.3250 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3311 - f_2: 1.3369e-08 - acc: 0.8722 - val_loss: 0.3261 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3314 - f_2: 1.3757e-08 - acc: 0.8722 - val_loss: 0.3237 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3315 - f_2: 1.3929e-08 - acc: 0.8722 - val_loss: 0.3251 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3322 - f_2: 1.3625e-08 - acc: 0.8722 - val_loss: 0.3231 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3309 - f_2: 1.3340e-08 - acc: 0.8722 - val_loss: 0.3225 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.3286 - f_2: 0.0027 - acc: 0.8722 - val_loss: 0.3220 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.1288 - acc: 0.8849 - val_loss: 0.3216 - val_f_2: 0.2834 - val_acc: 0.8994\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3292 - f_2: 0.2472 - acc: 0.8971 - val_loss: 0.3216 - val_f_2: 0.2851 - val_acc: 0.8997\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3272 - f_2: 0.2368 - acc: 0.8971 - val_loss: 0.3210 - val_f_2: 0.2851 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3286 - f_2: 0.2767 - acc: 0.8991 - val_loss: 0.3216 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3272 - f_2: 0.2795 - acc: 0.9002 - val_loss: 0.3199 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3272 - f_2: 0.2751 - acc: 0.8989 - val_loss: 0.3199 - val_f_2: 0.2954 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3276 - f_2: 0.2830 - acc: 0.9003 - val_loss: 0.3192 - val_f_2: 0.2866 - val_acc: 0.8988\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3267 - f_2: 0.2896 - acc: 0.9003 - val_loss: 0.3189 - val_f_2: 0.2868 - val_acc: 0.8991\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3248 - f_2: 0.2832 - acc: 0.9003 - val_loss: 0.3190 - val_f_2: 0.2870 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.3291 - f_2: 0.2886 - acc: 0.9004 - val_loss: 0.3186 - val_f_2: 0.2974 - val_acc: 0.8997\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3255 - f_2: 0.2846 - acc: 0.8992 - val_loss: 0.3184 - val_f_2: 0.3098 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3258 - f_2: 0.2933 - acc: 0.9015 - val_loss: 0.3196 - val_f_2: 0.2954 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.3237 - f_2: 0.2994 - acc: 0.9007 - val_loss: 0.3180 - val_f_2: 0.2921 - val_acc: 0.8997\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.3275 - f_2: 0.2870 - acc: 0.9001 - val_loss: 0.3178 - val_f_2: 0.3121 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.3272 - f_2: 0.2988 - acc: 0.9012 - val_loss: 0.3178 - val_f_2: 0.3121 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.3236 - f_2: 0.3092 - acc: 0.9017 - val_loss: 0.3175 - val_f_2: 0.3106 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.3221 - f_2: 0.3055 - acc: 0.9016 - val_loss: 0.3185 - val_f_2: 0.2900 - val_acc: 0.8994\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3272 - f_2: 0.3003 - acc: 0.9006 - val_loss: 0.3164 - val_f_2: 0.2997 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.3235 - f_2: 0.3057 - acc: 0.9026 - val_loss: 0.3206 - val_f_2: 0.2854 - val_acc: 0.8997\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.7576 - f_2: 0.0265 - acc: 0.8413 - val_loss: 0.4901 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4704 - f_2: 1.2906e-08 - acc: 0.8691 - val_loss: 0.3975 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.4135 - f_2: 1.3189e-08 - acc: 0.8691 - val_loss: 0.3664 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3892 - f_2: 1.2960e-08 - acc: 0.8691 - val_loss: 0.3510 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3810 - f_2: 1.3321e-08 - acc: 0.8691 - val_loss: 0.3431 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3706 - f_2: 1.3289e-08 - acc: 0.8691 - val_loss: 0.3377 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3655 - f_2: 1.3014e-08 - acc: 0.8691 - val_loss: 0.3333 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3614 - f_2: 1.3188e-08 - acc: 0.8691 - val_loss: 0.3295 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3555 - f_2: 1.3282e-08 - acc: 0.8691 - val_loss: 0.3260 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3568 - f_2: 1.3388e-08 - acc: 0.8691 - val_loss: 0.3248 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3518 - f_2: 1.3372e-08 - acc: 0.8691 - val_loss: 0.3211 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3513 - f_2: 1.3258e-08 - acc: 0.8691 - val_loss: 0.3191 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3485 - f_2: 1.3248e-08 - acc: 0.8691 - val_loss: 0.3190 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3468 - f_2: 1.3200e-08 - acc: 0.8691 - val_loss: 0.3168 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3475 - f_2: 1.3080e-08 - acc: 0.8691 - val_loss: 0.3164 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3447 - f_2: 1.2989e-08 - acc: 0.8691 - val_loss: 0.3160 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3438 - f_2: 1.3223e-08 - acc: 0.8691 - val_loss: 0.3136 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3429 - f_2: 1.3419e-08 - acc: 0.8691 - val_loss: 0.3128 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3436 - f_2: 1.3402e-08 - acc: 0.8691 - val_loss: 0.3130 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3417 - f_2: 1.3196e-08 - acc: 0.8691 - val_loss: 0.3121 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3382 - f_2: 1.3032e-08 - acc: 0.8691 - val_loss: 0.3105 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3422 - f_2: 1.2815e-08 - acc: 0.8691 - val_loss: 0.3099 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3397 - f_2: 1.2946e-08 - acc: 0.8691 - val_loss: 0.3094 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3394 - f_2: 1.3076e-08 - acc: 0.8691 - val_loss: 0.3094 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3376 - f_2: 1.3280e-08 - acc: 0.8691 - val_loss: 0.3093 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3378 - f_2: 1.3154e-08 - acc: 0.8691 - val_loss: 0.3093 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3386 - f_2: 1.3438e-08 - acc: 0.8691 - val_loss: 0.3089 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3383 - f_2: 1.3363e-08 - acc: 0.8691 - val_loss: 0.3086 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3366 - f_2: 1.3491e-08 - acc: 0.8691 - val_loss: 0.3082 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3373 - f_2: 1.3242e-08 - acc: 0.8691 - val_loss: 0.3060 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.3363 - f_2: 1.3109e-08 - acc: 0.8691 - val_loss: 0.3072 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3338 - f_2: 1.2842e-08 - acc: 0.8691 - val_loss: 0.3053 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3371 - f_2: 1.2905e-08 - acc: 0.8691 - val_loss: 0.3061 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3346 - f_2: 1.3263e-08 - acc: 0.8691 - val_loss: 0.3056 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3332 - f_2: 1.3274e-08 - acc: 0.8691 - val_loss: 0.3073 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3328 - f_2: 0.0432 - acc: 0.8731 - val_loss: 0.3040 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3340 - f_2: 0.1897 - acc: 0.8869 - val_loss: 0.3035 - val_f_2: 0.3076 - val_acc: 0.9106\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3353 - f_2: 0.2094 - acc: 0.8900 - val_loss: 0.3080 - val_f_2: 0.3184 - val_acc: 0.9100\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3336 - f_2: 0.2592 - acc: 0.8943 - val_loss: 0.3053 - val_f_2: 0.3181 - val_acc: 0.9097\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3328 - f_2: 0.2653 - acc: 0.8945 - val_loss: 0.3038 - val_f_2: 0.3163 - val_acc: 0.9103\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.3312 - f_2: 0.2912 - acc: 0.8980 - val_loss: 0.3021 - val_f_2: 0.3049 - val_acc: 0.9103\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3328 - f_2: 0.2664 - acc: 0.8956 - val_loss: 0.3039 - val_f_2: 0.3126 - val_acc: 0.9109\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3324 - f_2: 0.2500 - acc: 0.8936 - val_loss: 0.3026 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.3311 - f_2: 0.2674 - acc: 0.8956 - val_loss: 0.3040 - val_f_2: 0.3384 - val_acc: 0.9094\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.3291 - f_2: 0.2900 - acc: 0.8964 - val_loss: 0.3019 - val_f_2: 0.3143 - val_acc: 0.9103\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.3303 - f_2: 0.2819 - acc: 0.8966 - val_loss: 0.3021 - val_f_2: 0.3121 - val_acc: 0.9109\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.3335 - f_2: 0.2805 - acc: 0.8958 - val_loss: 0.3015 - val_f_2: 0.3180 - val_acc: 0.9097\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.3296 - f_2: 0.2850 - acc: 0.8970 - val_loss: 0.3020 - val_f_2: 0.3047 - val_acc: 0.9100\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3317 - f_2: 0.2995 - acc: 0.8970 - val_loss: 0.3014 - val_f_2: 0.3121 - val_acc: 0.9109\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.3287 - f_2: 0.2746 - acc: 0.8967 - val_loss: 0.3012 - val_f_2: 0.3166 - val_acc: 0.9106\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.5018 - f_2: 0.0216 - acc: 0.8467 - val_loss: 0.4009 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4012 - f_2: 1.4053e-08 - acc: 0.8753 - val_loss: 0.3773 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3799 - f_2: 1.4103e-08 - acc: 0.8753 - val_loss: 0.3599 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3656 - f_2: 0.0030 - acc: 0.8756 - val_loss: 0.3459 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3530 - f_2: 0.0868 - acc: 0.8833 - val_loss: 0.3335 - val_f_2: 0.2453 - val_acc: 0.8947\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3379 - f_2: 0.2306 - acc: 0.8949 - val_loss: 0.3201 - val_f_2: 0.3236 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3323 - f_2: 0.2950 - acc: 0.8995 - val_loss: 0.3132 - val_f_2: 0.3341 - val_acc: 0.8994\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3230 - f_2: 0.2959 - acc: 0.9003 - val_loss: 0.3083 - val_f_2: 0.3556 - val_acc: 0.9012\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3181 - f_2: 0.3176 - acc: 0.9012 - val_loss: 0.3060 - val_f_2: 0.3489 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3154 - f_2: 0.3280 - acc: 0.9033 - val_loss: 0.3034 - val_f_2: 0.3479 - val_acc: 0.8994\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3290 - acc: 0.9031 - val_loss: 0.3028 - val_f_2: 0.3199 - val_acc: 0.8994\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3115 - f_2: 0.3267 - acc: 0.9040 - val_loss: 0.2997 - val_f_2: 0.3639 - val_acc: 0.9012\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3060 - f_2: 0.3369 - acc: 0.9040 - val_loss: 0.2998 - val_f_2: 0.3572 - val_acc: 0.9015\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3031 - f_2: 0.3473 - acc: 0.9053 - val_loss: 0.2985 - val_f_2: 0.3562 - val_acc: 0.9009\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3043 - f_2: 0.3309 - acc: 0.9031 - val_loss: 0.2973 - val_f_2: 0.3693 - val_acc: 0.9018\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3007 - f_2: 0.3472 - acc: 0.9059 - val_loss: 0.2974 - val_f_2: 0.3509 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.2990 - f_2: 0.3408 - acc: 0.9043 - val_loss: 0.2975 - val_f_2: 0.3489 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.2965 - f_2: 0.3430 - acc: 0.9045 - val_loss: 0.2965 - val_f_2: 0.3447 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.2976 - f_2: 0.3321 - acc: 0.9053 - val_loss: 0.2959 - val_f_2: 0.3941 - val_acc: 0.9018\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.2950 - f_2: 0.3469 - acc: 0.9060 - val_loss: 0.2957 - val_f_2: 0.3769 - val_acc: 0.9018\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.2932 - f_2: 0.3458 - acc: 0.9074 - val_loss: 0.2957 - val_f_2: 0.3696 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.2954 - f_2: 0.3432 - acc: 0.9043 - val_loss: 0.2947 - val_f_2: 0.3624 - val_acc: 0.9012\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.2905 - f_2: 0.3396 - acc: 0.9057 - val_loss: 0.2957 - val_f_2: 0.3466 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2909 - f_2: 0.3596 - acc: 0.9064 - val_loss: 0.2937 - val_f_2: 0.3934 - val_acc: 0.9018\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2881 - f_2: 0.3511 - acc: 0.9057 - val_loss: 0.2947 - val_f_2: 0.3854 - val_acc: 0.9006\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.2894 - f_2: 0.3427 - acc: 0.9050 - val_loss: 0.2943 - val_f_2: 0.3778 - val_acc: 0.9015\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2876 - f_2: 0.3537 - acc: 0.9071 - val_loss: 0.2928 - val_f_2: 0.3728 - val_acc: 0.9024\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2880 - f_2: 0.3637 - acc: 0.9071 - val_loss: 0.2948 - val_f_2: 0.3551 - val_acc: 0.9021\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2870 - f_2: 0.3608 - acc: 0.9068 - val_loss: 0.2933 - val_f_2: 0.3853 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2869 - f_2: 0.3559 - acc: 0.9062 - val_loss: 0.2930 - val_f_2: 0.3720 - val_acc: 0.9021\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2868 - f_2: 0.3528 - acc: 0.9074 - val_loss: 0.2926 - val_f_2: 0.3790 - val_acc: 0.9009\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2863 - f_2: 0.3524 - acc: 0.9066 - val_loss: 0.2972 - val_f_2: 0.3391 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2864 - f_2: 0.3478 - acc: 0.9079 - val_loss: 0.2935 - val_f_2: 0.3951 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.3543 - acc: 0.9060 - val_loss: 0.2937 - val_f_2: 0.3426 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2863 - f_2: 0.3684 - acc: 0.9071 - val_loss: 0.2928 - val_f_2: 0.3855 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2837 - f_2: 0.3511 - acc: 0.9069 - val_loss: 0.2940 - val_f_2: 0.3613 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2839 - f_2: 0.3734 - acc: 0.9085 - val_loss: 0.2930 - val_f_2: 0.4054 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2812 - f_2: 0.3695 - acc: 0.9068 - val_loss: 0.2944 - val_f_2: 0.3660 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2833 - f_2: 0.3632 - acc: 0.9068 - val_loss: 0.2926 - val_f_2: 0.3892 - val_acc: 0.9024\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2800 - f_2: 0.3783 - acc: 0.9090 - val_loss: 0.2940 - val_f_2: 0.3972 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2826 - f_2: 0.3799 - acc: 0.9088 - val_loss: 0.2927 - val_f_2: 0.3944 - val_acc: 0.9012\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2818 - f_2: 0.3765 - acc: 0.9081 - val_loss: 0.2928 - val_f_2: 0.4085 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2775 - f_2: 0.3933 - acc: 0.9096 - val_loss: 0.2925 - val_f_2: 0.4000 - val_acc: 0.9024\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2779 - f_2: 0.3821 - acc: 0.9096 - val_loss: 0.2940 - val_f_2: 0.3870 - val_acc: 0.9024\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2791 - f_2: 0.3826 - acc: 0.9101 - val_loss: 0.2930 - val_f_2: 0.4085 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2771 - f_2: 0.4044 - acc: 0.9107 - val_loss: 0.2945 - val_f_2: 0.3876 - val_acc: 0.9027\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2789 - f_2: 0.3729 - acc: 0.9077 - val_loss: 0.2922 - val_f_2: 0.4146 - val_acc: 0.9015\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2777 - f_2: 0.3819 - acc: 0.9086 - val_loss: 0.2932 - val_f_2: 0.4018 - val_acc: 0.9018\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2767 - f_2: 0.3796 - acc: 0.9090 - val_loss: 0.2937 - val_f_2: 0.4181 - val_acc: 0.9021\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2764 - f_2: 0.3874 - acc: 0.9092 - val_loss: 0.2931 - val_f_2: 0.3875 - val_acc: 0.9021\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.5036 - f_2: 0.0229 - acc: 0.8444 - val_loss: 0.3997 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4010 - f_2: 1.3556e-08 - acc: 0.8722 - val_loss: 0.3756 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3804 - f_2: 1.3617e-08 - acc: 0.8722 - val_loss: 0.3605 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3646 - f_2: 0.0061 - acc: 0.8729 - val_loss: 0.3458 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3544 - f_2: 0.1138 - acc: 0.8835 - val_loss: 0.3316 - val_f_2: 0.2612 - val_acc: 0.8962\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3380 - f_2: 0.2365 - acc: 0.8944 - val_loss: 0.3197 - val_f_2: 0.3245 - val_acc: 0.8997\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3337 - f_2: 0.3089 - acc: 0.8990 - val_loss: 0.3135 - val_f_2: 0.3439 - val_acc: 0.8991\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3267 - f_2: 0.3197 - acc: 0.8996 - val_loss: 0.3079 - val_f_2: 0.3540 - val_acc: 0.8997\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.3277 - acc: 0.9001 - val_loss: 0.3052 - val_f_2: 0.3800 - val_acc: 0.9003\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3156 - f_2: 0.3291 - acc: 0.9001 - val_loss: 0.3037 - val_f_2: 0.3547 - val_acc: 0.8994\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3114 - f_2: 0.3417 - acc: 0.9024 - val_loss: 0.3030 - val_f_2: 0.3289 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3108 - f_2: 0.3404 - acc: 0.9020 - val_loss: 0.3002 - val_f_2: 0.3783 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3082 - f_2: 0.3397 - acc: 0.9029 - val_loss: 0.2992 - val_f_2: 0.3755 - val_acc: 0.9018\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3022 - f_2: 0.3593 - acc: 0.9042 - val_loss: 0.2979 - val_f_2: 0.3718 - val_acc: 0.9006\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3036 - f_2: 0.3290 - acc: 0.9009 - val_loss: 0.2987 - val_f_2: 0.3415 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.2997 - f_2: 0.3450 - acc: 0.9027 - val_loss: 0.2967 - val_f_2: 0.3713 - val_acc: 0.9018\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3014 - f_2: 0.3323 - acc: 0.9023 - val_loss: 0.2966 - val_f_2: 0.3612 - val_acc: 0.8997\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.2993 - f_2: 0.3466 - acc: 0.9037 - val_loss: 0.2960 - val_f_2: 0.3584 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.2965 - f_2: 0.3466 - acc: 0.9031 - val_loss: 0.2959 - val_f_2: 0.3390 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.2945 - f_2: 0.3471 - acc: 0.9029 - val_loss: 0.2947 - val_f_2: 0.3602 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.2930 - f_2: 0.3592 - acc: 0.9054 - val_loss: 0.2938 - val_f_2: 0.3830 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3513 - acc: 0.9034 - val_loss: 0.2927 - val_f_2: 0.3772 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.2934 - f_2: 0.3618 - acc: 0.9037 - val_loss: 0.2935 - val_f_2: 0.4290 - val_acc: 0.9021\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.2888 - f_2: 0.3598 - acc: 0.9044 - val_loss: 0.2939 - val_f_2: 0.3658 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.2929 - f_2: 0.3470 - acc: 0.9024 - val_loss: 0.2922 - val_f_2: 0.3761 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.2876 - f_2: 0.3601 - acc: 0.9052 - val_loss: 0.2935 - val_f_2: 0.3546 - val_acc: 0.9012\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2885 - f_2: 0.3429 - acc: 0.9038 - val_loss: 0.2922 - val_f_2: 0.4245 - val_acc: 0.9027\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.3670 - acc: 0.9049 - val_loss: 0.2914 - val_f_2: 0.4140 - val_acc: 0.9032\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2867 - f_2: 0.3679 - acc: 0.9048 - val_loss: 0.2924 - val_f_2: 0.3828 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2858 - f_2: 0.3724 - acc: 0.9060 - val_loss: 0.2913 - val_f_2: 0.4048 - val_acc: 0.9029\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2839 - f_2: 0.3735 - acc: 0.9061 - val_loss: 0.2913 - val_f_2: 0.4114 - val_acc: 0.9032\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2844 - f_2: 0.3855 - acc: 0.9069 - val_loss: 0.2914 - val_f_2: 0.4093 - val_acc: 0.9024\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2849 - f_2: 0.3667 - acc: 0.9051 - val_loss: 0.2918 - val_f_2: 0.3868 - val_acc: 0.9018\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2819 - f_2: 0.3780 - acc: 0.9062 - val_loss: 0.2919 - val_f_2: 0.3785 - val_acc: 0.9003\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2849 - f_2: 0.3874 - acc: 0.9071 - val_loss: 0.2905 - val_f_2: 0.3784 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2823 - f_2: 0.3876 - acc: 0.9065 - val_loss: 0.2910 - val_f_2: 0.3820 - val_acc: 0.9021\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2828 - f_2: 0.3894 - acc: 0.9068 - val_loss: 0.2907 - val_f_2: 0.3822 - val_acc: 0.9032\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2856 - f_2: 0.3699 - acc: 0.9046 - val_loss: 0.2904 - val_f_2: 0.3719 - val_acc: 0.9021\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2820 - f_2: 0.3775 - acc: 0.9058 - val_loss: 0.2904 - val_f_2: 0.4087 - val_acc: 0.9012\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2815 - f_2: 0.3678 - acc: 0.9049 - val_loss: 0.2906 - val_f_2: 0.4012 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2821 - f_2: 0.3761 - acc: 0.9046 - val_loss: 0.2900 - val_f_2: 0.3945 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2813 - f_2: 0.3742 - acc: 0.9054 - val_loss: 0.2906 - val_f_2: 0.3744 - val_acc: 0.9024\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2817 - f_2: 0.3813 - acc: 0.9055 - val_loss: 0.2896 - val_f_2: 0.4028 - val_acc: 0.9009\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2802 - f_2: 0.3913 - acc: 0.9058 - val_loss: 0.2906 - val_f_2: 0.3821 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2788 - f_2: 0.3813 - acc: 0.9068 - val_loss: 0.2909 - val_f_2: 0.4223 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2788 - f_2: 0.3939 - acc: 0.9063 - val_loss: 0.2897 - val_f_2: 0.4162 - val_acc: 0.9012\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2812 - f_2: 0.3816 - acc: 0.9054 - val_loss: 0.2908 - val_f_2: 0.3843 - val_acc: 0.9012\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2796 - f_2: 0.3855 - acc: 0.9051 - val_loss: 0.2901 - val_f_2: 0.3929 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2800 - f_2: 0.3825 - acc: 0.9053 - val_loss: 0.2910 - val_f_2: 0.3844 - val_acc: 0.8997\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2788 - f_2: 0.3910 - acc: 0.9063 - val_loss: 0.2900 - val_f_2: 0.4013 - val_acc: 0.9000\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.5102 - f_2: 0.0258 - acc: 0.8424 - val_loss: 0.3791 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4107 - f_2: 1.3285e-08 - acc: 0.8690 - val_loss: 0.3584 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3877 - f_2: 1.2751e-08 - acc: 0.8691 - val_loss: 0.3413 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3717 - f_2: 0.0191 - acc: 0.8708 - val_loss: 0.3275 - val_f_2: 0.0188 - val_acc: 0.8814\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3541 - f_2: 0.1571 - acc: 0.8852 - val_loss: 0.3121 - val_f_2: 0.2981 - val_acc: 0.9086\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3439 - f_2: 0.2642 - acc: 0.8925 - val_loss: 0.3020 - val_f_2: 0.3464 - val_acc: 0.9100\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3347 - f_2: 0.2980 - acc: 0.8958 - val_loss: 0.2965 - val_f_2: 0.3539 - val_acc: 0.9100\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3301 - f_2: 0.3099 - acc: 0.8965 - val_loss: 0.2926 - val_f_2: 0.3568 - val_acc: 0.9106\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3237 - f_2: 0.3145 - acc: 0.8967 - val_loss: 0.2890 - val_f_2: 0.3533 - val_acc: 0.9103\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3201 - f_2: 0.3276 - acc: 0.8978 - val_loss: 0.2871 - val_f_2: 0.3545 - val_acc: 0.9080\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3178 - f_2: 0.3288 - acc: 0.8979 - val_loss: 0.2864 - val_f_2: 0.3731 - val_acc: 0.9086\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3171 - f_2: 0.3260 - acc: 0.8984 - val_loss: 0.2861 - val_f_2: 0.3796 - val_acc: 0.9088\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3107 - f_2: 0.3452 - acc: 0.9006 - val_loss: 0.2822 - val_f_2: 0.3405 - val_acc: 0.9071\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3102 - f_2: 0.3306 - acc: 0.8987 - val_loss: 0.2804 - val_f_2: 0.3260 - val_acc: 0.9091\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3087 - f_2: 0.3331 - acc: 0.8996 - val_loss: 0.2801 - val_f_2: 0.3640 - val_acc: 0.9077\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3068 - f_2: 0.3225 - acc: 0.8984 - val_loss: 0.2798 - val_f_2: 0.3501 - val_acc: 0.9077\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3055 - f_2: 0.3378 - acc: 0.8990 - val_loss: 0.2786 - val_f_2: 0.3268 - val_acc: 0.9091\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3026 - f_2: 0.3429 - acc: 0.9010 - val_loss: 0.2781 - val_f_2: 0.3348 - val_acc: 0.9068\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3024 - f_2: 0.3242 - acc: 0.8988 - val_loss: 0.2799 - val_f_2: 0.3581 - val_acc: 0.9074\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.2997 - f_2: 0.3583 - acc: 0.9016 - val_loss: 0.2767 - val_f_2: 0.3391 - val_acc: 0.9077\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.2993 - f_2: 0.3299 - acc: 0.8998 - val_loss: 0.2763 - val_f_2: 0.3277 - val_acc: 0.9068\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.2978 - f_2: 0.3405 - acc: 0.9002 - val_loss: 0.2758 - val_f_2: 0.3355 - val_acc: 0.9074\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.2974 - f_2: 0.3479 - acc: 0.9012 - val_loss: 0.2755 - val_f_2: 0.3548 - val_acc: 0.9071\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.2977 - f_2: 0.3524 - acc: 0.9014 - val_loss: 0.2748 - val_f_2: 0.3486 - val_acc: 0.9083\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.2975 - f_2: 0.3382 - acc: 0.9001 - val_loss: 0.2772 - val_f_2: 0.3587 - val_acc: 0.9062\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.2966 - f_2: 0.3452 - acc: 0.9004 - val_loss: 0.2759 - val_f_2: 0.3724 - val_acc: 0.9083\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2930 - f_2: 0.3429 - acc: 0.9008 - val_loss: 0.2746 - val_f_2: 0.3786 - val_acc: 0.9068\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2930 - f_2: 0.3649 - acc: 0.9030 - val_loss: 0.2735 - val_f_2: 0.3595 - val_acc: 0.9071\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2898 - f_2: 0.3555 - acc: 0.9010 - val_loss: 0.2726 - val_f_2: 0.3680 - val_acc: 0.9077\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2936 - f_2: 0.3480 - acc: 0.9003 - val_loss: 0.2724 - val_f_2: 0.3662 - val_acc: 0.9083\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2917 - f_2: 0.3528 - acc: 0.9018 - val_loss: 0.2736 - val_f_2: 0.3683 - val_acc: 0.9074\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2910 - f_2: 0.3535 - acc: 0.8998 - val_loss: 0.2726 - val_f_2: 0.3600 - val_acc: 0.9068\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2925 - f_2: 0.3618 - acc: 0.9022 - val_loss: 0.2724 - val_f_2: 0.3618 - val_acc: 0.9071\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2897 - f_2: 0.3580 - acc: 0.9020 - val_loss: 0.2734 - val_f_2: 0.3662 - val_acc: 0.9056\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2888 - f_2: 0.3696 - acc: 0.9028 - val_loss: 0.2725 - val_f_2: 0.3615 - val_acc: 0.9059\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2891 - f_2: 0.3720 - acc: 0.9017 - val_loss: 0.2726 - val_f_2: 0.3711 - val_acc: 0.9077\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2868 - f_2: 0.3658 - acc: 0.9032 - val_loss: 0.2739 - val_f_2: 0.3656 - val_acc: 0.9056\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2888 - f_2: 0.3672 - acc: 0.9027 - val_loss: 0.2727 - val_f_2: 0.3607 - val_acc: 0.9083\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2874 - f_2: 0.3630 - acc: 0.9026 - val_loss: 0.2728 - val_f_2: 0.3630 - val_acc: 0.9059\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2855 - f_2: 0.3714 - acc: 0.9029 - val_loss: 0.2730 - val_f_2: 0.3756 - val_acc: 0.9059\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2860 - f_2: 0.3770 - acc: 0.9025 - val_loss: 0.2741 - val_f_2: 0.3755 - val_acc: 0.9062\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2862 - f_2: 0.3731 - acc: 0.9031 - val_loss: 0.2726 - val_f_2: 0.3699 - val_acc: 0.9056\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2853 - f_2: 0.3780 - acc: 0.9028 - val_loss: 0.2724 - val_f_2: 0.3648 - val_acc: 0.9074\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2845 - f_2: 0.3732 - acc: 0.9021 - val_loss: 0.2726 - val_f_2: 0.3775 - val_acc: 0.9062\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2858 - f_2: 0.3640 - acc: 0.9029 - val_loss: 0.2732 - val_f_2: 0.3775 - val_acc: 0.9065\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2852 - f_2: 0.3624 - acc: 0.9018 - val_loss: 0.2717 - val_f_2: 0.3603 - val_acc: 0.9083\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2841 - f_2: 0.3732 - acc: 0.9029 - val_loss: 0.2723 - val_f_2: 0.3613 - val_acc: 0.9065\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2820 - f_2: 0.3697 - acc: 0.9023 - val_loss: 0.2728 - val_f_2: 0.3823 - val_acc: 0.9053\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2815 - f_2: 0.3812 - acc: 0.9035 - val_loss: 0.2718 - val_f_2: 0.3597 - val_acc: 0.9074\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2838 - f_2: 0.3740 - acc: 0.9024 - val_loss: 0.2722 - val_f_2: 0.3718 - val_acc: 0.9068\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.7732 - f_2: 0.0165 - acc: 0.8539 - val_loss: 0.5153 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4588 - f_2: 1.4602e-08 - acc: 0.8753 - val_loss: 0.4164 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.4034 - f_2: 1.3952e-08 - acc: 0.8753 - val_loss: 0.3851 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3804 - f_2: 1.3744e-08 - acc: 0.8753 - val_loss: 0.3709 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3720 - f_2: 1.4209e-08 - acc: 0.8753 - val_loss: 0.3617 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3610 - f_2: 1.3881e-08 - acc: 0.8753 - val_loss: 0.3535 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3534 - f_2: 1.3824e-08 - acc: 0.8753 - val_loss: 0.3477 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3490 - f_2: 1.4300e-08 - acc: 0.8753 - val_loss: 0.3435 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3457 - f_2: 1.4018e-08 - acc: 0.8753 - val_loss: 0.3401 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3435 - f_2: 1.4602e-08 - acc: 0.8753 - val_loss: 0.3379 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3411 - f_2: 1.4066e-08 - acc: 0.8753 - val_loss: 0.3357 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3376 - f_2: 1.4343e-08 - acc: 0.8753 - val_loss: 0.3348 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3382 - f_2: 1.3963e-08 - acc: 0.8753 - val_loss: 0.3326 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3361 - f_2: 1.3977e-08 - acc: 0.8753 - val_loss: 0.3313 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3356 - f_2: 1.4081e-08 - acc: 0.8753 - val_loss: 0.3306 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3332 - f_2: 1.3796e-08 - acc: 0.8753 - val_loss: 0.3287 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3314 - f_2: 1.3870e-08 - acc: 0.8753 - val_loss: 0.3285 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3328 - f_2: 1.3766e-08 - acc: 0.8753 - val_loss: 0.3278 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3315 - f_2: 1.4062e-08 - acc: 0.8753 - val_loss: 0.3264 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3290 - f_2: 0.1250 - acc: 0.8875 - val_loss: 0.3255 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3271 - f_2: 0.1910 - acc: 0.8938 - val_loss: 0.3246 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3278 - f_2: 0.1961 - acc: 0.8944 - val_loss: 0.3236 - val_f_2: 0.2873 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3273 - f_2: 0.2159 - acc: 0.8951 - val_loss: 0.3237 - val_f_2: 0.2854 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3280 - f_2: 0.2184 - acc: 0.8967 - val_loss: 0.3227 - val_f_2: 0.2934 - val_acc: 0.8997\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3272 - f_2: 0.2150 - acc: 0.8961 - val_loss: 0.3222 - val_f_2: 0.2885 - val_acc: 0.8991\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3264 - f_2: 0.2122 - acc: 0.8956 - val_loss: 0.3213 - val_f_2: 0.2888 - val_acc: 0.8994\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3256 - f_2: 0.2343 - acc: 0.8977 - val_loss: 0.3210 - val_f_2: 0.2934 - val_acc: 0.8997\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3270 - f_2: 0.2408 - acc: 0.8981 - val_loss: 0.3208 - val_f_2: 0.3085 - val_acc: 0.8994\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3262 - f_2: 0.2347 - acc: 0.8979 - val_loss: 0.3199 - val_f_2: 0.2976 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3242 - f_2: 0.2398 - acc: 0.8983 - val_loss: 0.3202 - val_f_2: 0.3067 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.3228 - f_2: 0.2427 - acc: 0.8984 - val_loss: 0.3194 - val_f_2: 0.2958 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3230 - f_2: 0.2540 - acc: 0.8992 - val_loss: 0.3192 - val_f_2: 0.3119 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3213 - f_2: 0.2659 - acc: 0.9006 - val_loss: 0.3247 - val_f_2: 0.2854 - val_acc: 0.8997\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3206 - f_2: 0.2655 - acc: 0.9003 - val_loss: 0.3187 - val_f_2: 0.3044 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3215 - f_2: 0.2587 - acc: 0.9002 - val_loss: 0.3178 - val_f_2: 0.3100 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3223 - f_2: 0.2558 - acc: 0.8988 - val_loss: 0.3176 - val_f_2: 0.3082 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3211 - f_2: 0.2663 - acc: 0.8994 - val_loss: 0.3179 - val_f_2: 0.3392 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3193 - f_2: 0.2831 - acc: 0.9011 - val_loss: 0.3182 - val_f_2: 0.3082 - val_acc: 0.9006\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3214 - f_2: 0.2781 - acc: 0.8997 - val_loss: 0.3173 - val_f_2: 0.3016 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3174 - f_2: 0.2783 - acc: 0.9012 - val_loss: 0.3166 - val_f_2: 0.3083 - val_acc: 0.9009\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2757 - acc: 0.9010 - val_loss: 0.3162 - val_f_2: 0.3350 - val_acc: 0.8997\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3155 - f_2: 0.2797 - acc: 0.9007 - val_loss: 0.3183 - val_f_2: 0.3000 - val_acc: 0.9003\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3208 - f_2: 0.2774 - acc: 0.8988 - val_loss: 0.3158 - val_f_2: 0.3392 - val_acc: 0.8997\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.2897 - acc: 0.9017 - val_loss: 0.3155 - val_f_2: 0.3348 - val_acc: 0.8994\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.2734 - acc: 0.9009 - val_loss: 0.3153 - val_f_2: 0.3188 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.3183 - f_2: 0.2821 - acc: 0.9003 - val_loss: 0.3178 - val_f_2: 0.3192 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.3176 - f_2: 0.2663 - acc: 0.8996 - val_loss: 0.3152 - val_f_2: 0.3378 - val_acc: 0.8997\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.3183 - f_2: 0.2864 - acc: 0.9003 - val_loss: 0.3147 - val_f_2: 0.3190 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3141 - f_2: 0.3051 - acc: 0.9032 - val_loss: 0.3178 - val_f_2: 0.3172 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.3166 - f_2: 0.2743 - acc: 0.9001 - val_loss: 0.3155 - val_f_2: 0.3121 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.7817 - f_2: 0.0108 - acc: 0.8507 - val_loss: 0.5214 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4681 - f_2: 1.3849e-08 - acc: 0.8722 - val_loss: 0.4188 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.4038 - f_2: 1.3789e-08 - acc: 0.8722 - val_loss: 0.3848 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3816 - f_2: 1.4278e-08 - acc: 0.8722 - val_loss: 0.3705 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3693 - f_2: 1.3566e-08 - acc: 0.8722 - val_loss: 0.3594 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3613 - f_2: 1.3495e-08 - acc: 0.8722 - val_loss: 0.3518 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3545 - f_2: 1.3588e-08 - acc: 0.8722 - val_loss: 0.3463 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3504 - f_2: 1.3570e-08 - acc: 0.8722 - val_loss: 0.3432 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3431 - f_2: 1.3697e-08 - acc: 0.8722 - val_loss: 0.3394 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3431 - f_2: 1.3780e-08 - acc: 0.8722 - val_loss: 0.3375 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3392 - f_2: 1.3694e-08 - acc: 0.8722 - val_loss: 0.3349 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3354 - f_2: 1.3265e-08 - acc: 0.8722 - val_loss: 0.3337 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3364 - f_2: 1.3541e-08 - acc: 0.8722 - val_loss: 0.3322 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3338 - f_2: 1.3617e-08 - acc: 0.8722 - val_loss: 0.3305 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3313 - f_2: 0.0036 - acc: 0.8726 - val_loss: 0.3290 - val_f_2: 0.0267 - val_acc: 0.8699\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3335 - f_2: 0.1316 - acc: 0.8855 - val_loss: 0.3280 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3316 - f_2: 0.1734 - acc: 0.8904 - val_loss: 0.3279 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3307 - f_2: 0.2144 - acc: 0.8944 - val_loss: 0.3263 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3302 - f_2: 0.2209 - acc: 0.8944 - val_loss: 0.3255 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3292 - f_2: 0.2091 - acc: 0.8936 - val_loss: 0.3250 - val_f_2: 0.2834 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3301 - f_2: 0.2309 - acc: 0.8960 - val_loss: 0.3239 - val_f_2: 0.2834 - val_acc: 0.8994\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3279 - f_2: 0.2467 - acc: 0.8978 - val_loss: 0.3233 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3279 - f_2: 0.2421 - acc: 0.8961 - val_loss: 0.3227 - val_f_2: 0.2918 - val_acc: 0.8994\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3279 - f_2: 0.2484 - acc: 0.8964 - val_loss: 0.3223 - val_f_2: 0.2900 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3265 - f_2: 0.2422 - acc: 0.8967 - val_loss: 0.3226 - val_f_2: 0.2850 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3267 - f_2: 0.2622 - acc: 0.8970 - val_loss: 0.3211 - val_f_2: 0.2900 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3248 - f_2: 0.2699 - acc: 0.8995 - val_loss: 0.3207 - val_f_2: 0.3062 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3254 - f_2: 0.2697 - acc: 0.8989 - val_loss: 0.3202 - val_f_2: 0.2983 - val_acc: 0.8994\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3240 - f_2: 0.2737 - acc: 0.8994 - val_loss: 0.3206 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3212 - f_2: 0.2698 - acc: 0.8997 - val_loss: 0.3200 - val_f_2: 0.2954 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.3226 - f_2: 0.2752 - acc: 0.8992 - val_loss: 0.3184 - val_f_2: 0.3026 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3242 - f_2: 0.2887 - acc: 0.8998 - val_loss: 0.3185 - val_f_2: 0.2917 - val_acc: 0.8994\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3215 - f_2: 0.2933 - acc: 0.9003 - val_loss: 0.3183 - val_f_2: 0.2900 - val_acc: 0.8994\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3215 - f_2: 0.2794 - acc: 0.8999 - val_loss: 0.3175 - val_f_2: 0.3229 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3234 - f_2: 0.2709 - acc: 0.8978 - val_loss: 0.3189 - val_f_2: 0.2954 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3192 - f_2: 0.2866 - acc: 0.8990 - val_loss: 0.3163 - val_f_2: 0.3163 - val_acc: 0.9009\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3218 - f_2: 0.2835 - acc: 0.8998 - val_loss: 0.3171 - val_f_2: 0.3418 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3193 - f_2: 0.3119 - acc: 0.9015 - val_loss: 0.3158 - val_f_2: 0.2997 - val_acc: 0.9006\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3199 - f_2: 0.2904 - acc: 0.8991 - val_loss: 0.3157 - val_f_2: 0.3068 - val_acc: 0.9003\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3188 - f_2: 0.3028 - acc: 0.9013 - val_loss: 0.3160 - val_f_2: 0.2889 - val_acc: 0.9003\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.3203 - f_2: 0.2969 - acc: 0.8999 - val_loss: 0.3146 - val_f_2: 0.2908 - val_acc: 0.9006\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3208 - f_2: 0.2926 - acc: 0.9004 - val_loss: 0.3146 - val_f_2: 0.3242 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3169 - f_2: 0.3121 - acc: 0.9015 - val_loss: 0.3152 - val_f_2: 0.3313 - val_acc: 0.9009\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.3199 - f_2: 0.3044 - acc: 0.9015 - val_loss: 0.3149 - val_f_2: 0.3009 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.3172 - f_2: 0.2984 - acc: 0.9000 - val_loss: 0.3136 - val_f_2: 0.3133 - val_acc: 0.9015\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.3192 - f_2: 0.3024 - acc: 0.9007 - val_loss: 0.3146 - val_f_2: 0.3634 - val_acc: 0.8994\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.3169 - f_2: 0.3129 - acc: 0.9000 - val_loss: 0.3194 - val_f_2: 0.2977 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.3184 - f_2: 0.3137 - acc: 0.9009 - val_loss: 0.3127 - val_f_2: 0.3127 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3154 - f_2: 0.3027 - acc: 0.8993 - val_loss: 0.3172 - val_f_2: 0.3048 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.3163 - f_2: 0.3085 - acc: 0.9010 - val_loss: 0.3141 - val_f_2: 0.3145 - val_acc: 0.9009\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.7867 - f_2: 0.0179 - acc: 0.8474 - val_loss: 0.4961 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4699 - f_2: 1.3073e-08 - acc: 0.8691 - val_loss: 0.3949 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.4081 - f_2: 1.3378e-08 - acc: 0.8691 - val_loss: 0.3629 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3868 - f_2: 1.3217e-08 - acc: 0.8691 - val_loss: 0.3472 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3717 - f_2: 1.3427e-08 - acc: 0.8691 - val_loss: 0.3385 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3658 - f_2: 1.2976e-08 - acc: 0.8691 - val_loss: 0.3340 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3600 - f_2: 1.3122e-08 - acc: 0.8691 - val_loss: 0.3297 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3559 - f_2: 1.3207e-08 - acc: 0.8691 - val_loss: 0.3239 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3495 - f_2: 1.3025e-08 - acc: 0.8691 - val_loss: 0.3208 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3474 - f_2: 1.3061e-08 - acc: 0.8691 - val_loss: 0.3188 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3467 - f_2: 1.3071e-08 - acc: 0.8691 - val_loss: 0.3167 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3435 - f_2: 1.3178e-08 - acc: 0.8691 - val_loss: 0.3149 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3427 - f_2: 1.3263e-08 - acc: 0.8691 - val_loss: 0.3143 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3416 - f_2: 1.3344e-08 - acc: 0.8691 - val_loss: 0.3159 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3383 - f_2: 1.2942e-08 - acc: 0.8691 - val_loss: 0.3124 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3383 - f_2: 0.0234 - acc: 0.8717 - val_loss: 0.3097 - val_f_2: 0.2312 - val_acc: 0.9029\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3356 - f_2: 0.1983 - acc: 0.8896 - val_loss: 0.3089 - val_f_2: 0.2722 - val_acc: 0.9074\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3336 - f_2: 0.2046 - acc: 0.8908 - val_loss: 0.3081 - val_f_2: 0.2988 - val_acc: 0.9100\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3341 - f_2: 0.2168 - acc: 0.8912 - val_loss: 0.3066 - val_f_2: 0.2931 - val_acc: 0.9094\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3354 - f_2: 0.2164 - acc: 0.8916 - val_loss: 0.3060 - val_f_2: 0.3036 - val_acc: 0.9103\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3316 - f_2: 0.2420 - acc: 0.8936 - val_loss: 0.3068 - val_f_2: 0.3098 - val_acc: 0.9106\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3326 - f_2: 0.2446 - acc: 0.8945 - val_loss: 0.3061 - val_f_2: 0.3126 - val_acc: 0.9109\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3335 - f_2: 0.2518 - acc: 0.8939 - val_loss: 0.3041 - val_f_2: 0.3054 - val_acc: 0.9109\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3345 - f_2: 0.2384 - acc: 0.8942 - val_loss: 0.3039 - val_f_2: 0.3187 - val_acc: 0.9103\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3310 - f_2: 0.2567 - acc: 0.8942 - val_loss: 0.3053 - val_f_2: 0.3251 - val_acc: 0.9103\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3312 - f_2: 0.2602 - acc: 0.8941 - val_loss: 0.3026 - val_f_2: 0.3100 - val_acc: 0.9109\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3301 - f_2: 0.2572 - acc: 0.8950 - val_loss: 0.3022 - val_f_2: 0.3189 - val_acc: 0.9109\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3302 - f_2: 0.2695 - acc: 0.8957 - val_loss: 0.3018 - val_f_2: 0.3252 - val_acc: 0.9103\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3303 - f_2: 0.2656 - acc: 0.8942 - val_loss: 0.3021 - val_f_2: 0.3144 - val_acc: 0.9106\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3269 - f_2: 0.2821 - acc: 0.8968 - val_loss: 0.3001 - val_f_2: 0.3167 - val_acc: 0.9109\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.3283 - f_2: 0.2729 - acc: 0.8943 - val_loss: 0.3035 - val_f_2: 0.3396 - val_acc: 0.9080\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3305 - f_2: 0.2727 - acc: 0.8959 - val_loss: 0.3001 - val_f_2: 0.3317 - val_acc: 0.9100\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3282 - f_2: 0.2867 - acc: 0.8956 - val_loss: 0.2998 - val_f_2: 0.3295 - val_acc: 0.9100\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3288 - f_2: 0.2822 - acc: 0.8958 - val_loss: 0.3022 - val_f_2: 0.3295 - val_acc: 0.9100\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3280 - f_2: 0.2793 - acc: 0.8951 - val_loss: 0.2994 - val_f_2: 0.3316 - val_acc: 0.9100\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3262 - f_2: 0.2988 - acc: 0.8968 - val_loss: 0.2977 - val_f_2: 0.3190 - val_acc: 0.9112\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3269 - f_2: 0.2786 - acc: 0.8952 - val_loss: 0.2979 - val_f_2: 0.3298 - val_acc: 0.9103\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3273 - f_2: 0.2866 - acc: 0.8957 - val_loss: 0.2990 - val_f_2: 0.3489 - val_acc: 0.9083\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3250 - f_2: 0.3102 - acc: 0.8964 - val_loss: 0.2973 - val_f_2: 0.3295 - val_acc: 0.9097\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3235 - f_2: 0.2927 - acc: 0.8954 - val_loss: 0.2970 - val_f_2: 0.3351 - val_acc: 0.9097\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.3240 - f_2: 0.2938 - acc: 0.8954 - val_loss: 0.2968 - val_f_2: 0.3339 - val_acc: 0.9097\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3249 - f_2: 0.2950 - acc: 0.8959 - val_loss: 0.2959 - val_f_2: 0.3345 - val_acc: 0.9106\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3217 - f_2: 0.3153 - acc: 0.8984 - val_loss: 0.2964 - val_f_2: 0.3417 - val_acc: 0.9094\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.3254 - f_2: 0.3001 - acc: 0.8964 - val_loss: 0.2966 - val_f_2: 0.3458 - val_acc: 0.9083\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.3213 - f_2: 0.3103 - acc: 0.8977 - val_loss: 0.2964 - val_f_2: 0.3530 - val_acc: 0.9077\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.3238 - f_2: 0.3169 - acc: 0.8969 - val_loss: 0.2949 - val_f_2: 0.3299 - val_acc: 0.9106\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.3238 - f_2: 0.3019 - acc: 0.8962 - val_loss: 0.2957 - val_f_2: 0.3484 - val_acc: 0.9086\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.3211 - f_2: 0.3192 - acc: 0.8965 - val_loss: 0.2947 - val_f_2: 0.3385 - val_acc: 0.9106\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3215 - f_2: 0.3144 - acc: 0.8970 - val_loss: 0.2942 - val_f_2: 0.3302 - val_acc: 0.9109\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.3212 - f_2: 0.3047 - acc: 0.8967 - val_loss: 0.2977 - val_f_2: 0.3590 - val_acc: 0.9071\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.01, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4846 - f_2: 0.0161 - acc: 0.8572 - val_loss: 0.4041 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3999 - f_2: 1.4311e-08 - acc: 0.8753 - val_loss: 0.3807 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3832 - f_2: 1.3764e-08 - acc: 0.8753 - val_loss: 0.3658 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3675 - f_2: 1.4195e-08 - acc: 0.8753 - val_loss: 0.3553 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3602 - f_2: 1.4518e-08 - acc: 0.8753 - val_loss: 0.3487 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3498 - f_2: 1.3862e-08 - acc: 0.8753 - val_loss: 0.3424 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3444 - f_2: 1.4078e-08 - acc: 0.8753 - val_loss: 0.3375 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3412 - f_2: 1.3962e-08 - acc: 0.8753 - val_loss: 0.3341 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3371 - f_2: 1.4077e-08 - acc: 0.8753 - val_loss: 0.3337 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3343 - f_2: 1.3933e-08 - acc: 0.8753 - val_loss: 0.3290 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3315 - f_2: 1.4099e-08 - acc: 0.8753 - val_loss: 0.3271 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3279 - f_2: 1.4041e-08 - acc: 0.8753 - val_loss: 0.3267 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3246 - f_2: 1.4251e-08 - acc: 0.8753 - val_loss: 0.3240 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3257 - f_2: 1.4085e-08 - acc: 0.8753 - val_loss: 0.3224 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3227 - f_2: 1.4151e-08 - acc: 0.8753 - val_loss: 0.3256 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3214 - f_2: 1.3880e-08 - acc: 0.8753 - val_loss: 0.3208 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3165 - f_2: 0.0385 - acc: 0.8795 - val_loss: 0.3217 - val_f_2: 0.2944 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3187 - f_2: 0.2054 - acc: 0.8950 - val_loss: 0.3195 - val_f_2: 0.3075 - val_acc: 0.9003\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3140 - f_2: 0.2425 - acc: 0.8981 - val_loss: 0.3193 - val_f_2: 0.3365 - val_acc: 0.9006\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3121 - f_2: 0.2995 - acc: 0.9031 - val_loss: 0.3181 - val_f_2: 0.3381 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3160 - f_2: 0.3081 - acc: 0.9048 - val_loss: 0.3180 - val_f_2: 0.3383 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3158 - f_2: 0.3205 - acc: 0.9046 - val_loss: 0.3169 - val_f_2: 0.3373 - val_acc: 0.8994\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3129 - f_2: 0.3089 - acc: 0.9042 - val_loss: 0.3181 - val_f_2: 0.3403 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3311 - acc: 0.9054 - val_loss: 0.3186 - val_f_2: 0.3260 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3122 - f_2: 0.3220 - acc: 0.9049 - val_loss: 0.3163 - val_f_2: 0.3519 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3058 - f_2: 0.3415 - acc: 0.9065 - val_loss: 0.3155 - val_f_2: 0.3672 - val_acc: 0.9015\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3075 - f_2: 0.3358 - acc: 0.9057 - val_loss: 0.3160 - val_f_2: 0.3571 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3097 - f_2: 0.3531 - acc: 0.9073 - val_loss: 0.3163 - val_f_2: 0.3489 - val_acc: 0.9012\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3076 - f_2: 0.3455 - acc: 0.9060 - val_loss: 0.3146 - val_f_2: 0.3671 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3062 - f_2: 0.3458 - acc: 0.9049 - val_loss: 0.3177 - val_f_2: 0.3511 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.3073 - f_2: 0.3670 - acc: 0.9069 - val_loss: 0.3152 - val_f_2: 0.3833 - val_acc: 0.9024\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3093 - f_2: 0.3564 - acc: 0.9062 - val_loss: 0.3146 - val_f_2: 0.3674 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3040 - f_2: 0.3548 - acc: 0.9061 - val_loss: 0.3146 - val_f_2: 0.3809 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3029 - f_2: 0.3577 - acc: 0.9069 - val_loss: 0.3152 - val_f_2: 0.3854 - val_acc: 0.9018\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3067 - f_2: 0.3609 - acc: 0.9062 - val_loss: 0.3150 - val_f_2: 0.3670 - val_acc: 0.9018\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.3672 - acc: 0.9071 - val_loss: 0.3150 - val_f_2: 0.3780 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3017 - f_2: 0.3683 - acc: 0.9067 - val_loss: 0.3146 - val_f_2: 0.3949 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3032 - f_2: 0.3626 - acc: 0.9067 - val_loss: 0.3150 - val_f_2: 0.3761 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3015 - f_2: 0.3766 - acc: 0.9080 - val_loss: 0.3156 - val_f_2: 0.3778 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3031 - f_2: 0.3673 - acc: 0.9074 - val_loss: 0.3144 - val_f_2: 0.4036 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.3021 - f_2: 0.3806 - acc: 0.9069 - val_loss: 0.3160 - val_f_2: 0.3752 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2978 - f_2: 0.3930 - acc: 0.9090 - val_loss: 0.3176 - val_f_2: 0.3732 - val_acc: 0.9032\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3000 - f_2: 0.3787 - acc: 0.9080 - val_loss: 0.3155 - val_f_2: 0.4058 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2960 - f_2: 0.3927 - acc: 0.9078 - val_loss: 0.3216 - val_f_2: 0.3884 - val_acc: 0.9024\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2994 - f_2: 0.3949 - acc: 0.9087 - val_loss: 0.3156 - val_f_2: 0.3980 - val_acc: 0.9012\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2982 - f_2: 0.3911 - acc: 0.9079 - val_loss: 0.3141 - val_f_2: 0.4071 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2995 - f_2: 0.3974 - acc: 0.9074 - val_loss: 0.3203 - val_f_2: 0.3888 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.3855 - acc: 0.9078 - val_loss: 0.3165 - val_f_2: 0.3963 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2980 - f_2: 0.3932 - acc: 0.9083 - val_loss: 0.3170 - val_f_2: 0.3970 - val_acc: 0.9024\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2977 - f_2: 0.3962 - acc: 0.9091 - val_loss: 0.3206 - val_f_2: 0.3944 - val_acc: 0.9018\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4885 - f_2: 0.0195 - acc: 0.8528 - val_loss: 0.4031 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3998 - f_2: 1.3704e-08 - acc: 0.8722 - val_loss: 0.3782 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3798 - f_2: 1.3901e-08 - acc: 0.8722 - val_loss: 0.3650 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3716 - f_2: 1.4117e-08 - acc: 0.8722 - val_loss: 0.3553 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3614 - f_2: 1.3792e-08 - acc: 0.8722 - val_loss: 0.3487 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3539 - f_2: 1.3459e-08 - acc: 0.8722 - val_loss: 0.3433 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3466 - f_2: 1.3715e-08 - acc: 0.8722 - val_loss: 0.3383 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3443 - f_2: 1.3529e-08 - acc: 0.8722 - val_loss: 0.3342 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3374 - f_2: 1.3351e-08 - acc: 0.8722 - val_loss: 0.3315 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3354 - f_2: 1.3328e-08 - acc: 0.8722 - val_loss: 0.3306 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3337 - f_2: 1.3341e-08 - acc: 0.8722 - val_loss: 0.3279 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3326 - f_2: 1.3756e-08 - acc: 0.8722 - val_loss: 0.3260 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3300 - f_2: 1.3536e-08 - acc: 0.8722 - val_loss: 0.3242 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3237 - f_2: 1.3401e-08 - acc: 0.8722 - val_loss: 0.3222 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3229 - f_2: 0.2066 - acc: 0.8933 - val_loss: 0.3233 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3215 - f_2: 0.2669 - acc: 0.9004 - val_loss: 0.3203 - val_f_2: 0.2920 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3187 - f_2: 0.2758 - acc: 0.9015 - val_loss: 0.3192 - val_f_2: 0.3007 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3226 - f_2: 0.2755 - acc: 0.9009 - val_loss: 0.3187 - val_f_2: 0.3086 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3170 - f_2: 0.2814 - acc: 0.9009 - val_loss: 0.3173 - val_f_2: 0.2960 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.2865 - acc: 0.9011 - val_loss: 0.3172 - val_f_2: 0.3188 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3149 - f_2: 0.3058 - acc: 0.9029 - val_loss: 0.3172 - val_f_2: 0.3205 - val_acc: 0.8997\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3134 - f_2: 0.3085 - acc: 0.9020 - val_loss: 0.3147 - val_f_2: 0.3385 - val_acc: 0.9012\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3126 - f_2: 0.3169 - acc: 0.9031 - val_loss: 0.3146 - val_f_2: 0.3220 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3115 - f_2: 0.3087 - acc: 0.9013 - val_loss: 0.3136 - val_f_2: 0.3455 - val_acc: 0.9021\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3132 - f_2: 0.3186 - acc: 0.9020 - val_loss: 0.3122 - val_f_2: 0.3550 - val_acc: 0.9018\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3113 - f_2: 0.3039 - acc: 0.9015 - val_loss: 0.3130 - val_f_2: 0.3659 - val_acc: 0.9024\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3094 - f_2: 0.3211 - acc: 0.9026 - val_loss: 0.3121 - val_f_2: 0.3654 - val_acc: 0.9024\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3072 - f_2: 0.3460 - acc: 0.9044 - val_loss: 0.3129 - val_f_2: 0.3845 - val_acc: 0.9032\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3065 - f_2: 0.3352 - acc: 0.9034 - val_loss: 0.3130 - val_f_2: 0.3671 - val_acc: 0.9029\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3103 - f_2: 0.3362 - acc: 0.9023 - val_loss: 0.3113 - val_f_2: 0.3845 - val_acc: 0.9032\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.3067 - f_2: 0.3479 - acc: 0.9033 - val_loss: 0.3130 - val_f_2: 0.3670 - val_acc: 0.9024\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3056 - f_2: 0.3434 - acc: 0.9034 - val_loss: 0.3106 - val_f_2: 0.3825 - val_acc: 0.9024\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3046 - f_2: 0.3403 - acc: 0.9023 - val_loss: 0.3112 - val_f_2: 0.3698 - val_acc: 0.9021\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3046 - f_2: 0.3455 - acc: 0.9037 - val_loss: 0.3118 - val_f_2: 0.3797 - val_acc: 0.9027\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3052 - f_2: 0.3548 - acc: 0.9029 - val_loss: 0.3134 - val_f_2: 0.3753 - val_acc: 0.9027\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3041 - f_2: 0.3592 - acc: 0.9040 - val_loss: 0.3104 - val_f_2: 0.3924 - val_acc: 0.9027\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3031 - f_2: 0.3556 - acc: 0.9041 - val_loss: 0.3144 - val_f_2: 0.3842 - val_acc: 0.9032\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3041 - f_2: 0.3565 - acc: 0.9039 - val_loss: 0.3094 - val_f_2: 0.4062 - val_acc: 0.9032\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3024 - f_2: 0.3705 - acc: 0.9043 - val_loss: 0.3135 - val_f_2: 0.3736 - val_acc: 0.9018\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3085 - f_2: 0.3564 - acc: 0.9038 - val_loss: 0.3110 - val_f_2: 0.3782 - val_acc: 0.9029\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2999 - f_2: 0.3478 - acc: 0.9033 - val_loss: 0.3104 - val_f_2: 0.4102 - val_acc: 0.9035\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3000 - f_2: 0.3640 - acc: 0.9034 - val_loss: 0.3101 - val_f_2: 0.3914 - val_acc: 0.9021\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2973 - f_2: 0.3678 - acc: 0.9038 - val_loss: 0.3095 - val_f_2: 0.4125 - val_acc: 0.9032\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.3000 - f_2: 0.3737 - acc: 0.9049 - val_loss: 0.3094 - val_f_2: 0.4081 - val_acc: 0.9029\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2980 - f_2: 0.3786 - acc: 0.9057 - val_loss: 0.3095 - val_f_2: 0.3881 - val_acc: 0.9021\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2992 - f_2: 0.3747 - acc: 0.9041 - val_loss: 0.3090 - val_f_2: 0.4150 - val_acc: 0.9032\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2958 - f_2: 0.3798 - acc: 0.9049 - val_loss: 0.3086 - val_f_2: 0.4226 - val_acc: 0.9047\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2971 - f_2: 0.3907 - acc: 0.9056 - val_loss: 0.3112 - val_f_2: 0.4072 - val_acc: 0.9041\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3008 - f_2: 0.3839 - acc: 0.9048 - val_loss: 0.3090 - val_f_2: 0.4080 - val_acc: 0.9035\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2967 - f_2: 0.3688 - acc: 0.9045 - val_loss: 0.3098 - val_f_2: 0.4304 - val_acc: 0.9056\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.001, total= 1.4min\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4961 - f_2: 0.0192 - acc: 0.8501 - val_loss: 0.3836 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.4089 - f_2: 1.3466e-08 - acc: 0.8691 - val_loss: 0.3568 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3880 - f_2: 1.3151e-08 - acc: 0.8691 - val_loss: 0.3426 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3731 - f_2: 1.3512e-08 - acc: 0.8691 - val_loss: 0.3322 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3631 - f_2: 1.3164e-08 - acc: 0.8691 - val_loss: 0.3256 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3588 - f_2: 1.3318e-08 - acc: 0.8691 - val_loss: 0.3222 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3526 - f_2: 1.3199e-08 - acc: 0.8691 - val_loss: 0.3175 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3484 - f_2: 1.3366e-08 - acc: 0.8691 - val_loss: 0.3131 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3418 - f_2: 1.3390e-08 - acc: 0.8691 - val_loss: 0.3126 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3396 - f_2: 1.3107e-08 - acc: 0.8691 - val_loss: 0.3099 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3386 - f_2: 1.3322e-08 - acc: 0.8691 - val_loss: 0.3077 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3306 - f_2: 1.3406e-08 - acc: 0.8691 - val_loss: 0.3076 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3310 - f_2: 1.3191e-08 - acc: 0.8691 - val_loss: 0.3041 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3315 - f_2: 1.3204e-08 - acc: 0.8691 - val_loss: 0.3036 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3288 - f_2: 0.1146 - acc: 0.8810 - val_loss: 0.3022 - val_f_2: 0.3177 - val_acc: 0.9103\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3262 - f_2: 0.2686 - acc: 0.8971 - val_loss: 0.3033 - val_f_2: 0.3333 - val_acc: 0.9097\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3234 - f_2: 0.2958 - acc: 0.9001 - val_loss: 0.3001 - val_f_2: 0.3201 - val_acc: 0.9094\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3264 - f_2: 0.2977 - acc: 0.8999 - val_loss: 0.2996 - val_f_2: 0.3358 - val_acc: 0.9097\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3221 - f_2: 0.3077 - acc: 0.9004 - val_loss: 0.3022 - val_f_2: 0.3561 - val_acc: 0.9100\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3205 - f_2: 0.3141 - acc: 0.9015 - val_loss: 0.2987 - val_f_2: 0.3476 - val_acc: 0.9100\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.3228 - acc: 0.9012 - val_loss: 0.2988 - val_f_2: 0.3446 - val_acc: 0.9097\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3185 - f_2: 0.3418 - acc: 0.9029 - val_loss: 0.2970 - val_f_2: 0.3398 - val_acc: 0.9094\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3197 - f_2: 0.3326 - acc: 0.9011 - val_loss: 0.2968 - val_f_2: 0.3353 - val_acc: 0.9097\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3181 - f_2: 0.3353 - acc: 0.9014 - val_loss: 0.2960 - val_f_2: 0.3503 - val_acc: 0.9094\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3164 - f_2: 0.3354 - acc: 0.9014 - val_loss: 0.2949 - val_f_2: 0.3516 - val_acc: 0.9091\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3152 - f_2: 0.3537 - acc: 0.9026 - val_loss: 0.2958 - val_f_2: 0.3519 - val_acc: 0.9088\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3149 - f_2: 0.3491 - acc: 0.9013 - val_loss: 0.2959 - val_f_2: 0.3618 - val_acc: 0.9091\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3137 - f_2: 0.3535 - acc: 0.9018 - val_loss: 0.2968 - val_f_2: 0.3816 - val_acc: 0.9100\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3126 - f_2: 0.3665 - acc: 0.9022 - val_loss: 0.2967 - val_f_2: 0.3554 - val_acc: 0.9091\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3100 - f_2: 0.3621 - acc: 0.9009 - val_loss: 0.2949 - val_f_2: 0.3748 - val_acc: 0.9088\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.3113 - f_2: 0.3778 - acc: 0.9021 - val_loss: 0.2953 - val_f_2: 0.3583 - val_acc: 0.9097\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3126 - f_2: 0.3571 - acc: 0.9020 - val_loss: 0.2947 - val_f_2: 0.3824 - val_acc: 0.9088\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3097 - f_2: 0.3771 - acc: 0.9029 - val_loss: 0.2948 - val_f_2: 0.3943 - val_acc: 0.9091\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.3114 - f_2: 0.3705 - acc: 0.9017 - val_loss: 0.2941 - val_f_2: 0.3757 - val_acc: 0.9088\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3088 - f_2: 0.3808 - acc: 0.9026 - val_loss: 0.2938 - val_f_2: 0.3828 - val_acc: 0.9094\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3087 - f_2: 0.3958 - acc: 0.9038 - val_loss: 0.2933 - val_f_2: 0.3816 - val_acc: 0.9088\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.3067 - f_2: 0.3843 - acc: 0.9022 - val_loss: 0.2941 - val_f_2: 0.3979 - val_acc: 0.9083\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3084 - f_2: 0.3904 - acc: 0.9027 - val_loss: 0.2939 - val_f_2: 0.3882 - val_acc: 0.9088\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3078 - f_2: 0.4022 - acc: 0.9032 - val_loss: 0.2944 - val_f_2: 0.3778 - val_acc: 0.9086\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3093 - f_2: 0.3884 - acc: 0.9033 - val_loss: 0.2933 - val_f_2: 0.3948 - val_acc: 0.9091\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.3043 - f_2: 0.4034 - acc: 0.9027 - val_loss: 0.2924 - val_f_2: 0.3797 - val_acc: 0.9088\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3041 - f_2: 0.3959 - acc: 0.9037 - val_loss: 0.2926 - val_f_2: 0.3952 - val_acc: 0.9100\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.3040 - f_2: 0.3989 - acc: 0.9036 - val_loss: 0.2931 - val_f_2: 0.4009 - val_acc: 0.9097\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.3042 - f_2: 0.4152 - acc: 0.9033 - val_loss: 0.2933 - val_f_2: 0.3974 - val_acc: 0.9059\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.3057 - f_2: 0.4147 - acc: 0.9037 - val_loss: 0.2928 - val_f_2: 0.4004 - val_acc: 0.9094\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.3044 - f_2: 0.4134 - acc: 0.9041 - val_loss: 0.2934 - val_f_2: 0.3945 - val_acc: 0.9080\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.3050 - f_2: 0.4249 - acc: 0.9038 - val_loss: 0.2963 - val_f_2: 0.4275 - val_acc: 0.9071\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.3027 - f_2: 0.4251 - acc: 0.9045 - val_loss: 0.2947 - val_f_2: 0.4153 - val_acc: 0.9056\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.3017 - f_2: 0.4198 - acc: 0.9032 - val_loss: 0.2934 - val_f_2: 0.3953 - val_acc: 0.9083\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.3044 - f_2: 0.4191 - acc: 0.9040 - val_loss: 0.2940 - val_f_2: 0.4022 - val_acc: 0.9059\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.001, total= 1.4min\n",
      "Train on 20336 samples, validate on 5085 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 147.7min finished\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.4632 - f_2: 0.0114 - acc: 0.8598 - val_loss: 0.3872 - val_f_2: 1.3122e-08 - val_acc: 0.8692\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.3861 - f_2: 1.3706e-08 - acc: 0.8726 - val_loss: 0.3649 - val_f_2: 1.3122e-08 - val_acc: 0.8692\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3686 - f_2: 1.3612e-08 - acc: 0.8726 - val_loss: 0.3514 - val_f_2: 1.3122e-08 - val_acc: 0.8692\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.3579 - f_2: 7.8680e-04 - acc: 0.8726 - val_loss: 0.3423 - val_f_2: 1.3122e-08 - val_acc: 0.8692\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3508 - f_2: 1.3518e-08 - acc: 0.8726 - val_loss: 0.3360 - val_f_2: 1.3122e-08 - val_acc: 0.8692\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.3411 - f_2: 1.3338e-08 - acc: 0.8726 - val_loss: 0.3314 - val_f_2: 1.3122e-08 - val_acc: 0.8692\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.3362 - f_2: 1.3335e-08 - acc: 0.8726 - val_loss: 0.3281 - val_f_2: 1.3122e-08 - val_acc: 0.8692\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.3300 - f_2: 1.3852e-08 - acc: 0.8726 - val_loss: 0.3258 - val_f_2: 1.3122e-08 - val_acc: 0.8692\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3279 - f_2: 1.4389e-08 - acc: 0.8726 - val_loss: 0.3254 - val_f_2: 1.3122e-08 - val_acc: 0.8692\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3255 - f_2: 0.1293 - acc: 0.8861 - val_loss: 0.3190 - val_f_2: 0.2964 - val_acc: 0.9021\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3234 - f_2: 0.2596 - acc: 0.8993 - val_loss: 0.3171 - val_f_2: 0.2978 - val_acc: 0.9021\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3217 - f_2: 0.2674 - acc: 0.8990 - val_loss: 0.3157 - val_f_2: 0.3236 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.3194 - f_2: 0.2882 - acc: 0.9008 - val_loss: 0.3138 - val_f_2: 0.3215 - val_acc: 0.9019\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.3181 - f_2: 0.2821 - acc: 0.8993 - val_loss: 0.3128 - val_f_2: 0.3323 - val_acc: 0.9021\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.3161 - f_2: 0.3012 - acc: 0.9007 - val_loss: 0.3138 - val_f_2: 0.3090 - val_acc: 0.9011\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3146 - f_2: 0.2945 - acc: 0.9010 - val_loss: 0.3107 - val_f_2: 0.3321 - val_acc: 0.9023\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.3133 - f_2: 0.3064 - acc: 0.9008 - val_loss: 0.3089 - val_f_2: 0.3389 - val_acc: 0.9023\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.3143 - f_2: 0.3018 - acc: 0.9011 - val_loss: 0.3097 - val_f_2: 0.3383 - val_acc: 0.9027\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3108 - f_2: 0.3074 - acc: 0.9012 - val_loss: 0.3079 - val_f_2: 0.3328 - val_acc: 0.9023\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3115 - f_2: 0.3247 - acc: 0.9015 - val_loss: 0.3071 - val_f_2: 0.3387 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.3082 - f_2: 0.3136 - acc: 0.9008 - val_loss: 0.3069 - val_f_2: 0.3491 - val_acc: 0.9019\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3079 - f_2: 0.3233 - acc: 0.9003 - val_loss: 0.3052 - val_f_2: 0.3521 - val_acc: 0.9017\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.3116 - f_2: 0.3169 - acc: 0.9006 - val_loss: 0.3051 - val_f_2: 0.3686 - val_acc: 0.9023\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3097 - f_2: 0.3396 - acc: 0.9025 - val_loss: 0.3053 - val_f_2: 0.3462 - val_acc: 0.9013\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.3079 - f_2: 0.3109 - acc: 0.9011 - val_loss: 0.3047 - val_f_2: 0.3778 - val_acc: 0.9017\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.3083 - f_2: 0.3346 - acc: 0.9012 - val_loss: 0.3056 - val_f_2: 0.3465 - val_acc: 0.9019\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.3056 - f_2: 0.3337 - acc: 0.9026 - val_loss: 0.3071 - val_f_2: 0.3518 - val_acc: 0.9013\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3061 - f_2: 0.3376 - acc: 0.9017 - val_loss: 0.3037 - val_f_2: 0.3697 - val_acc: 0.9019\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.3066 - f_2: 0.3311 - acc: 0.9009 - val_loss: 0.3053 - val_f_2: 0.3687 - val_acc: 0.9017\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.3046 - f_2: 0.3441 - acc: 0.9017 - val_loss: 0.3053 - val_f_2: 0.4111 - val_acc: 0.9044\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.3026 - f_2: 0.3494 - acc: 0.9020 - val_loss: 0.3035 - val_f_2: 0.3943 - val_acc: 0.9023\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.3046 - f_2: 0.3437 - acc: 0.9021 - val_loss: 0.3033 - val_f_2: 0.3992 - val_acc: 0.9030\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.3040 - f_2: 0.3485 - acc: 0.9010 - val_loss: 0.3026 - val_f_2: 0.3558 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.3023 - f_2: 0.3483 - acc: 0.9025 - val_loss: 0.3032 - val_f_2: 0.3763 - val_acc: 0.9017\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.3014 - f_2: 0.3429 - acc: 0.9033 - val_loss: 0.3027 - val_f_2: 0.3916 - val_acc: 0.9025\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.3015 - f_2: 0.3613 - acc: 0.9024 - val_loss: 0.3038 - val_f_2: 0.4117 - val_acc: 0.9046\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.3020 - f_2: 0.3492 - acc: 0.9021 - val_loss: 0.3027 - val_f_2: 0.3861 - val_acc: 0.9019\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.3021 - f_2: 0.3525 - acc: 0.9021 - val_loss: 0.3031 - val_f_2: 0.3857 - val_acc: 0.9017\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.3024 - f_2: 0.3577 - acc: 0.9015 - val_loss: 0.3022 - val_f_2: 0.3696 - val_acc: 0.9027\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.3027 - f_2: 0.3468 - acc: 0.9023 - val_loss: 0.3030 - val_f_2: 0.4019 - val_acc: 0.9038\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.3030 - f_2: 0.3524 - acc: 0.9011 - val_loss: 0.3025 - val_f_2: 0.3788 - val_acc: 0.9017\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3006 - f_2: 0.3498 - acc: 0.9019 - val_loss: 0.3035 - val_f_2: 0.4047 - val_acc: 0.9038\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.3010 - f_2: 0.3621 - acc: 0.9029 - val_loss: 0.3020 - val_f_2: 0.3871 - val_acc: 0.9023\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.3014 - f_2: 0.3639 - acc: 0.9027 - val_loss: 0.3021 - val_f_2: 0.4025 - val_acc: 0.9036\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.3004 - f_2: 0.3518 - acc: 0.9018 - val_loss: 0.3038 - val_f_2: 0.4065 - val_acc: 0.9030\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.3000 - f_2: 0.3674 - acc: 0.9025 - val_loss: 0.3032 - val_f_2: 0.4061 - val_acc: 0.9036\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.3003 - f_2: 0.3796 - acc: 0.9041 - val_loss: 0.3045 - val_f_2: 0.3861 - val_acc: 0.9021\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.2989 - f_2: 0.3708 - acc: 0.9033 - val_loss: 0.3032 - val_f_2: 0.3967 - val_acc: 0.9038\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2988 - f_2: 0.3622 - acc: 0.9024 - val_loss: 0.3043 - val_f_2: 0.3746 - val_acc: 0.9025\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.2992 - f_2: 0.3659 - acc: 0.9030 - val_loss: 0.3043 - val_f_2: 0.4198 - val_acc: 0.9040\n"
     ]
    }
   ],
   "source": [
    "# Wrap model in wrapper\n",
    "classifier = KerasClassifier(build_fn=build_model_2)\n",
    "    \n",
    "# Define grid (scores used are the same as the scores used in GSCV for the SVC benchmark model)\n",
    "mlp2_grid = GridSearchCV(estimator=classifier, \n",
    "                    param_grid=parameters, \n",
    "                    scoring=scores, \n",
    "                    refit='F2-Score', verbose=2)\n",
    "\n",
    "# Train grid, passing fit_params of keras.models.Sequential.fit here\n",
    "mlp2_grid_result = mlp2_grid.fit(X_train_red, y_train, epochs=50, \n",
    "                                 verbose=2, shuffle=True, \n",
    "                                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance: F-2 Score of 0.39 with parameters: {'batch_size': 80, 'dropout': 0.3, 'nodes': 20, 'penalty': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Observe best performance\n",
    "print('Best performance: F-2 Score of {:.2f} with parameters: {}'.format(mlp2_grid_result.best_score_, mlp2_grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Grid Search CV training above yielded an unsatisfactory performance. Perhaps because a value in the optimizer or a recurrent layer is going to zero and the model attempts to divide by that, it yields a loss that is not a number. This results in many fittings being wasted completely. This means the GCSV process isn't as completely reliable as it is for Model 1. \n",
    "\n",
    "Therefore, although I had found Adamax to be the best optimizer for this model architecture in the Architecture Exploration notebook, I will put this architecture through GSCV again using RMSprop as optimizer, and select that best combination of hyperparameters instead. This will make the GSCV process fairer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 architecture (with RMSprop)\n",
    "def build_model_2_rms(nodes, penalty, dropout):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=X_train_red.shape[1], input_dim=X_train_red.shape[1], activation='relu'))\n",
    "    model.add(Dense(nodes, activation='relu', kernel_regularizer=regularizers.l2(penalty)))\n",
    "    model.add(Dropout(dropout)) \n",
    "    model.add(Dense(nodes + 5, activation='relu', kernel_regularizer=regularizers.l2(penalty)))\n",
    "    model.add(Dropout(dropout)) \n",
    "    model.add(Dense(nodes + 10, activation='relu', kernel_regularizer=regularizers.l2(penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=[f_2, 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.5950 - f_2: 0.0233 - acc: 0.8603 - val_loss: 0.4112 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3899 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3670 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3638 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3521 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3522 - f_2: 0.0148 - acc: 0.8753 - val_loss: 0.3437 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3433 - f_2: 3.1248e-08 - acc: 0.8753 - val_loss: 0.3378 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3398 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3370 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3390 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3322 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3349 - f_2: 3.1993e-08 - acc: 0.8753 - val_loss: 0.3298 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3352 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3292 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3332 - f_2: 3.2238e-08 - acc: 0.8753 - val_loss: 0.3278 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3279 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3259 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3302 - f_2: 0.0577 - acc: 0.8809 - val_loss: 0.3275 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3297 - f_2: 0.1588 - acc: 0.8917 - val_loss: 0.3284 - val_f_2: 0.0060 - val_acc: 0.8676\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.1780 - acc: 0.8931 - val_loss: 0.3222 - val_f_2: 0.2718 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3285 - f_2: 0.1907 - acc: 0.8931 - val_loss: 0.3217 - val_f_2: 0.2208 - val_acc: 0.8938\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2145 - acc: 0.8953 - val_loss: 0.3209 - val_f_2: 0.2653 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.1841 - acc: 0.8931 - val_loss: 0.3243 - val_f_2: 0.2483 - val_acc: 0.8973\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2095 - acc: 0.8961 - val_loss: 0.3243 - val_f_2: 0.2332 - val_acc: 0.8956\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.2210 - acc: 0.8959 - val_loss: 0.3228 - val_f_2: 0.2773 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.2236 - acc: 0.8961 - val_loss: 0.3213 - val_f_2: 0.2733 - val_acc: 0.8988\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2240 - acc: 0.8961 - val_loss: 0.3193 - val_f_2: 0.2904 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3221 - f_2: 0.2191 - acc: 0.8949 - val_loss: 0.3223 - val_f_2: 0.2804 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3250 - f_2: 0.2221 - acc: 0.8949 - val_loss: 0.3175 - val_f_2: 0.3331 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2487 - acc: 0.8960 - val_loss: 0.3175 - val_f_2: 0.3360 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.2690 - acc: 0.8995 - val_loss: 0.3164 - val_f_2: 0.3392 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.2716 - acc: 0.8985 - val_loss: 0.3186 - val_f_2: 0.3300 - val_acc: 0.8997\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.2952 - acc: 0.9006 - val_loss: 0.3193 - val_f_2: 0.2996 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.2710 - acc: 0.9002 - val_loss: 0.3165 - val_f_2: 0.3319 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.2805 - acc: 0.9004 - val_loss: 0.3221 - val_f_2: 0.3067 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2942 - acc: 0.9009 - val_loss: 0.3203 - val_f_2: 0.3155 - val_acc: 0.9015\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.2722 - acc: 0.8995 - val_loss: 0.3163 - val_f_2: 0.3186 - val_acc: 0.9018\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.2817 - acc: 0.8985 - val_loss: 0.3158 - val_f_2: 0.3737 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.2992 - acc: 0.9020 - val_loss: 0.3165 - val_f_2: 0.3683 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.2929 - acc: 0.8993 - val_loss: 0.3151 - val_f_2: 0.3470 - val_acc: 0.9018\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2860 - acc: 0.8998 - val_loss: 0.3148 - val_f_2: 0.3785 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.3045 - acc: 0.9001 - val_loss: 0.3197 - val_f_2: 0.3479 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3120 - acc: 0.9017 - val_loss: 0.3136 - val_f_2: 0.3417 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.2971 - acc: 0.8992 - val_loss: 0.3145 - val_f_2: 0.3794 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3160 - f_2: 0.3065 - acc: 0.9006 - val_loss: 0.3154 - val_f_2: 0.3616 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.3017 - acc: 0.9022 - val_loss: 0.3156 - val_f_2: 0.3455 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.2947 - acc: 0.9003 - val_loss: 0.3146 - val_f_2: 0.3837 - val_acc: 0.9021\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3018 - acc: 0.9008 - val_loss: 0.3130 - val_f_2: 0.3933 - val_acc: 0.9003\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3004 - acc: 0.9009 - val_loss: 0.3142 - val_f_2: 0.3745 - val_acc: 0.9009\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.3103 - acc: 0.9018 - val_loss: 0.3129 - val_f_2: 0.3772 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3201 - acc: 0.9011 - val_loss: 0.3169 - val_f_2: 0.3370 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.3211 - acc: 0.9020 - val_loss: 0.3142 - val_f_2: 0.4245 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3061 - acc: 0.9004 - val_loss: 0.3135 - val_f_2: 0.3877 - val_acc: 0.9018\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.3142 - acc: 0.9024 - val_loss: 0.3129 - val_f_2: 0.4060 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.3111 - acc: 0.9010 - val_loss: 0.3159 - val_f_2: 0.3616 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3188 - acc: 0.9025 - val_loss: 0.3140 - val_f_2: 0.3821 - val_acc: 0.9012\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min remaining:    0.0s\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.5939 - f_2: 0.0184 - acc: 0.8565 - val_loss: 0.4083 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3897 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3659 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3636 - f_2: 3.1905e-08 - acc: 0.8722 - val_loss: 0.3499 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3527 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3422 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3445 - f_2: 0.0118 - acc: 0.8722 - val_loss: 0.3369 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3408 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3358 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3386 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3312 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3387 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3292 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3338 - f_2: 0.0118 - acc: 0.8722 - val_loss: 0.3272 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3352 - f_2: 0.0905 - acc: 0.8815 - val_loss: 0.3262 - val_f_2: 0.1206 - val_acc: 0.8814\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.1565 - acc: 0.8877 - val_loss: 0.3241 - val_f_2: 0.2500 - val_acc: 0.8973\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3301 - f_2: 0.1743 - acc: 0.8894 - val_loss: 0.3262 - val_f_2: 0.2618 - val_acc: 0.8991\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.1790 - acc: 0.8885 - val_loss: 0.3227 - val_f_2: 0.2212 - val_acc: 0.8938\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3264 - f_2: 0.1975 - acc: 0.8910 - val_loss: 0.3225 - val_f_2: 0.3318 - val_acc: 0.9003\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3276 - f_2: 0.2459 - acc: 0.8932 - val_loss: 0.3209 - val_f_2: 0.3100 - val_acc: 0.8994\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3274 - f_2: 0.2514 - acc: 0.8957 - val_loss: 0.3207 - val_f_2: 0.3009 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2731 - acc: 0.8975 - val_loss: 0.3184 - val_f_2: 0.3160 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2839 - acc: 0.8995 - val_loss: 0.3201 - val_f_2: 0.3253 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.2923 - acc: 0.8991 - val_loss: 0.3206 - val_f_2: 0.2846 - val_acc: 0.8991\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3242 - f_2: 0.2852 - acc: 0.8978 - val_loss: 0.3169 - val_f_2: 0.3373 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.3085 - acc: 0.9006 - val_loss: 0.3171 - val_f_2: 0.3768 - val_acc: 0.9015\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.3078 - acc: 0.9009 - val_loss: 0.3178 - val_f_2: 0.3398 - val_acc: 0.9009\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.3048 - acc: 0.9004 - val_loss: 0.3150 - val_f_2: 0.3638 - val_acc: 0.9015\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.3183 - acc: 0.9009 - val_loss: 0.3162 - val_f_2: 0.3342 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.3143 - acc: 0.9006 - val_loss: 0.3140 - val_f_2: 0.3551 - val_acc: 0.9024\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.3110 - acc: 0.9008 - val_loss: 0.3137 - val_f_2: 0.3460 - val_acc: 0.9015\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.3220 - acc: 0.9021 - val_loss: 0.3169 - val_f_2: 0.4201 - val_acc: 0.9015\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.3183 - acc: 0.9023 - val_loss: 0.3142 - val_f_2: 0.3752 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.3303 - acc: 0.9006 - val_loss: 0.3148 - val_f_2: 0.3107 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3175 - acc: 0.9011 - val_loss: 0.3130 - val_f_2: 0.4049 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.3432 - acc: 0.9026 - val_loss: 0.3122 - val_f_2: 0.3806 - val_acc: 0.9021\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.3298 - acc: 0.9010 - val_loss: 0.3113 - val_f_2: 0.3641 - val_acc: 0.9009\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.3397 - acc: 0.9025 - val_loss: 0.3137 - val_f_2: 0.3508 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.3108 - acc: 0.9015 - val_loss: 0.3119 - val_f_2: 0.3742 - val_acc: 0.9018\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.3341 - acc: 0.9015 - val_loss: 0.3147 - val_f_2: 0.3628 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.3496 - acc: 0.9029 - val_loss: 0.3114 - val_f_2: 0.3497 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3353 - acc: 0.9030 - val_loss: 0.3111 - val_f_2: 0.3350 - val_acc: 0.9003\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3135 - f_2: 0.3212 - acc: 0.9018 - val_loss: 0.3106 - val_f_2: 0.3920 - val_acc: 0.9018\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3136 - f_2: 0.3366 - acc: 0.9012 - val_loss: 0.3100 - val_f_2: 0.3874 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.3331 - acc: 0.9022 - val_loss: 0.3087 - val_f_2: 0.3748 - val_acc: 0.9018\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3517 - acc: 0.9032 - val_loss: 0.3110 - val_f_2: 0.3720 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3301 - acc: 0.9015 - val_loss: 0.3107 - val_f_2: 0.3613 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.3468 - acc: 0.9023 - val_loss: 0.3094 - val_f_2: 0.3642 - val_acc: 0.9009\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.3553 - acc: 0.9026 - val_loss: 0.3099 - val_f_2: 0.3829 - val_acc: 0.9024\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3585 - acc: 0.9037 - val_loss: 0.3175 - val_f_2: 0.3607 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.3442 - acc: 0.9018 - val_loss: 0.3094 - val_f_2: 0.4160 - val_acc: 0.9018\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.3408 - acc: 0.9012 - val_loss: 0.3143 - val_f_2: 0.3660 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3525 - acc: 0.9024 - val_loss: 0.3130 - val_f_2: 0.4196 - val_acc: 0.9027\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3491 - acc: 0.9020 - val_loss: 0.3124 - val_f_2: 0.4288 - val_acc: 0.9027\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3090 - f_2: 0.3495 - acc: 0.9029 - val_loss: 0.3106 - val_f_2: 0.4217 - val_acc: 0.9015\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6051 - f_2: 0.0202 - acc: 0.8530 - val_loss: 0.3913 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3965 - f_2: 2.9264e-08 - acc: 0.8691 - val_loss: 0.3483 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3721 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3351 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3605 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3267 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3540 - f_2: 2.8891e-08 - acc: 0.8691 - val_loss: 0.3213 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3484 - f_2: 0.0089 - acc: 0.8691 - val_loss: 0.3198 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3448 - f_2: 3.0312e-08 - acc: 0.8691 - val_loss: 0.3157 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3437 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3140 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3408 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3129 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3408 - f_2: 0.0089 - acc: 0.8691 - val_loss: 0.3117 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3409 - f_2: 0.0081 - acc: 0.8696 - val_loss: 0.3103 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3382 - f_2: 0.1295 - acc: 0.8834 - val_loss: 0.3091 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3367 - f_2: 0.1698 - acc: 0.8864 - val_loss: 0.3074 - val_f_2: 0.1416 - val_acc: 0.8932\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3356 - f_2: 0.1903 - acc: 0.8890 - val_loss: 0.3074 - val_f_2: 0.0949 - val_acc: 0.8882\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3347 - f_2: 0.1900 - acc: 0.8883 - val_loss: 0.3080 - val_f_2: 0.3132 - val_acc: 0.9103\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3346 - f_2: 0.1989 - acc: 0.8902 - val_loss: 0.3073 - val_f_2: 0.2600 - val_acc: 0.9068\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3331 - f_2: 0.1968 - acc: 0.8890 - val_loss: 0.3042 - val_f_2: 0.2600 - val_acc: 0.9068\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3326 - f_2: 0.2027 - acc: 0.8899 - val_loss: 0.3051 - val_f_2: 0.3006 - val_acc: 0.9106\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3301 - f_2: 0.2369 - acc: 0.8925 - val_loss: 0.3046 - val_f_2: 0.3276 - val_acc: 0.9083\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3305 - f_2: 0.2291 - acc: 0.8921 - val_loss: 0.3022 - val_f_2: 0.3045 - val_acc: 0.9112\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3304 - f_2: 0.2350 - acc: 0.8919 - val_loss: 0.3024 - val_f_2: 0.3133 - val_acc: 0.9100\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3298 - f_2: 0.2490 - acc: 0.8942 - val_loss: 0.3027 - val_f_2: 0.3135 - val_acc: 0.9106\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.2627 - acc: 0.8956 - val_loss: 0.3014 - val_f_2: 0.3115 - val_acc: 0.9112\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3279 - f_2: 0.2459 - acc: 0.8922 - val_loss: 0.3030 - val_f_2: 0.3526 - val_acc: 0.9062\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3274 - f_2: 0.2513 - acc: 0.8927 - val_loss: 0.3010 - val_f_2: 0.3202 - val_acc: 0.9094\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.2749 - acc: 0.8936 - val_loss: 0.2995 - val_f_2: 0.3200 - val_acc: 0.9109\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.2702 - acc: 0.8951 - val_loss: 0.2997 - val_f_2: 0.3186 - val_acc: 0.9109\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.2818 - acc: 0.8951 - val_loss: 0.2992 - val_f_2: 0.3206 - val_acc: 0.9106\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3242 - f_2: 0.2755 - acc: 0.8929 - val_loss: 0.2997 - val_f_2: 0.3570 - val_acc: 0.9065\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2801 - acc: 0.8956 - val_loss: 0.2993 - val_f_2: 0.3246 - val_acc: 0.9103\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3270 - f_2: 0.2817 - acc: 0.8942 - val_loss: 0.2975 - val_f_2: 0.3166 - val_acc: 0.9106\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2913 - acc: 0.8959 - val_loss: 0.2977 - val_f_2: 0.3400 - val_acc: 0.9077\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.3037 - acc: 0.8956 - val_loss: 0.2976 - val_f_2: 0.3294 - val_acc: 0.9097\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2791 - acc: 0.8950 - val_loss: 0.2994 - val_f_2: 0.3653 - val_acc: 0.9068\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.2948 - acc: 0.8949 - val_loss: 0.2987 - val_f_2: 0.3229 - val_acc: 0.9106\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.2970 - acc: 0.8939 - val_loss: 0.2980 - val_f_2: 0.3724 - val_acc: 0.9068\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.2976 - acc: 0.8948 - val_loss: 0.2984 - val_f_2: 0.3348 - val_acc: 0.9094\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.2984 - acc: 0.8956 - val_loss: 0.2969 - val_f_2: 0.3244 - val_acc: 0.9068\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3252 - f_2: 0.2996 - acc: 0.8936 - val_loss: 0.2987 - val_f_2: 0.3216 - val_acc: 0.9115\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.3094 - acc: 0.8942 - val_loss: 0.3017 - val_f_2: 0.3208 - val_acc: 0.9112\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2998 - acc: 0.8947 - val_loss: 0.2974 - val_f_2: 0.3555 - val_acc: 0.9074\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.3169 - acc: 0.8957 - val_loss: 0.3028 - val_f_2: 0.4044 - val_acc: 0.9091\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.3133 - acc: 0.8946 - val_loss: 0.2957 - val_f_2: 0.3310 - val_acc: 0.9115\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2994 - acc: 0.8957 - val_loss: 0.2957 - val_f_2: 0.3315 - val_acc: 0.9091\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3217 - f_2: 0.3090 - acc: 0.8961 - val_loss: 0.2956 - val_f_2: 0.3347 - val_acc: 0.9080\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.3127 - acc: 0.8949 - val_loss: 0.2963 - val_f_2: 0.3713 - val_acc: 0.9091\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.3148 - acc: 0.8964 - val_loss: 0.2946 - val_f_2: 0.3400 - val_acc: 0.9077\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.3130 - acc: 0.8950 - val_loss: 0.2989 - val_f_2: 0.3101 - val_acc: 0.9118\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3207 - f_2: 0.2936 - acc: 0.8940 - val_loss: 0.2951 - val_f_2: 0.3636 - val_acc: 0.9103\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.3242 - acc: 0.8952 - val_loss: 0.2969 - val_f_2: 0.3799 - val_acc: 0.9094\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4278 - f_2: 0.0213 - acc: 0.8615 - val_loss: 0.3538 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3530 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3332 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3406 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3272 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3358 - f_2: 3.1622e-08 - acc: 0.8753 - val_loss: 0.3243 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3265 - f_2: 3.2419e-08 - acc: 0.8753 - val_loss: 0.3211 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3217 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.1096 - acc: 0.8862 - val_loss: 0.3189 - val_f_2: 0.2969 - val_acc: 0.9003\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2546 - acc: 0.8989 - val_loss: 0.3166 - val_f_2: 0.3253 - val_acc: 0.9015\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2738 - acc: 0.8995 - val_loss: 0.3162 - val_f_2: 0.3280 - val_acc: 0.9012\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.2981 - acc: 0.9023 - val_loss: 0.3176 - val_f_2: 0.3457 - val_acc: 0.9009\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3139 - f_2: 0.3204 - acc: 0.9032 - val_loss: 0.3132 - val_f_2: 0.3476 - val_acc: 0.8988\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.3277 - acc: 0.9034 - val_loss: 0.3132 - val_f_2: 0.3741 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.3271 - acc: 0.9046 - val_loss: 0.3139 - val_f_2: 0.3684 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3058 - f_2: 0.3283 - acc: 0.9038 - val_loss: 0.3145 - val_f_2: 0.3956 - val_acc: 0.9006\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3388 - acc: 0.9044 - val_loss: 0.3114 - val_f_2: 0.3818 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3509 - acc: 0.9035 - val_loss: 0.3122 - val_f_2: 0.3785 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3428 - acc: 0.9034 - val_loss: 0.3122 - val_f_2: 0.3807 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3604 - acc: 0.9045 - val_loss: 0.3118 - val_f_2: 0.3611 - val_acc: 0.9003\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.3394 - acc: 0.9037 - val_loss: 0.3134 - val_f_2: 0.3831 - val_acc: 0.9009\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.3577 - acc: 0.9061 - val_loss: 0.3175 - val_f_2: 0.3844 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3669 - acc: 0.9054 - val_loss: 0.3128 - val_f_2: 0.4188 - val_acc: 0.8994\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2992 - f_2: 0.3692 - acc: 0.9057 - val_loss: 0.3155 - val_f_2: 0.4067 - val_acc: 0.9012\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3646 - acc: 0.9044 - val_loss: 0.3142 - val_f_2: 0.4045 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2978 - f_2: 0.3792 - acc: 0.9061 - val_loss: 0.3128 - val_f_2: 0.3977 - val_acc: 0.8997\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3774 - acc: 0.9051 - val_loss: 0.3108 - val_f_2: 0.4177 - val_acc: 0.9006\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.3833 - acc: 0.9056 - val_loss: 0.3126 - val_f_2: 0.4210 - val_acc: 0.8962\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2991 - f_2: 0.3924 - acc: 0.9070 - val_loss: 0.3118 - val_f_2: 0.4158 - val_acc: 0.8962\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2988 - f_2: 0.3876 - acc: 0.9064 - val_loss: 0.3125 - val_f_2: 0.4278 - val_acc: 0.8965\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.4012 - acc: 0.9063 - val_loss: 0.3124 - val_f_2: 0.4147 - val_acc: 0.8979\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2958 - f_2: 0.3967 - acc: 0.9074 - val_loss: 0.3129 - val_f_2: 0.4073 - val_acc: 0.8988\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3718 - acc: 0.9053 - val_loss: 0.3158 - val_f_2: 0.4232 - val_acc: 0.8965\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2969 - f_2: 0.3993 - acc: 0.9064 - val_loss: 0.3118 - val_f_2: 0.4152 - val_acc: 0.8985\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2942 - f_2: 0.3977 - acc: 0.9068 - val_loss: 0.3147 - val_f_2: 0.3986 - val_acc: 0.8988\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2944 - f_2: 0.3982 - acc: 0.9070 - val_loss: 0.3157 - val_f_2: 0.4102 - val_acc: 0.8985\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2930 - f_2: 0.4002 - acc: 0.9067 - val_loss: 0.3141 - val_f_2: 0.4253 - val_acc: 0.8994\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2930 - f_2: 0.4050 - acc: 0.9076 - val_loss: 0.3156 - val_f_2: 0.4097 - val_acc: 0.8991\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2935 - f_2: 0.4031 - acc: 0.9071 - val_loss: 0.3209 - val_f_2: 0.3888 - val_acc: 0.8973\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.4003 - acc: 0.9076 - val_loss: 0.3127 - val_f_2: 0.4206 - val_acc: 0.8965\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2923 - f_2: 0.4127 - acc: 0.9084 - val_loss: 0.3155 - val_f_2: 0.4306 - val_acc: 0.8994\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2920 - f_2: 0.4100 - acc: 0.9081 - val_loss: 0.3136 - val_f_2: 0.4357 - val_acc: 0.8971\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2934 - f_2: 0.4146 - acc: 0.9062 - val_loss: 0.3161 - val_f_2: 0.4302 - val_acc: 0.8982\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2899 - f_2: 0.4135 - acc: 0.9076 - val_loss: 0.3174 - val_f_2: 0.4330 - val_acc: 0.8976\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.4248 - acc: 0.9080 - val_loss: 0.3152 - val_f_2: 0.4207 - val_acc: 0.8976\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2911 - f_2: 0.4136 - acc: 0.9085 - val_loss: 0.3134 - val_f_2: 0.4420 - val_acc: 0.8968\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2921 - f_2: 0.4213 - acc: 0.9067 - val_loss: 0.3142 - val_f_2: 0.4352 - val_acc: 0.8965\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2920 - f_2: 0.4210 - acc: 0.9083 - val_loss: 0.3142 - val_f_2: 0.4341 - val_acc: 0.8956\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2918 - f_2: 0.4073 - acc: 0.9097 - val_loss: 0.3138 - val_f_2: 0.4267 - val_acc: 0.8979\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2922 - f_2: 0.4162 - acc: 0.9089 - val_loss: 0.3165 - val_f_2: 0.4265 - val_acc: 0.8988\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.3981 - acc: 0.9069 - val_loss: 0.3139 - val_f_2: 0.4181 - val_acc: 0.8976\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2912 - f_2: 0.4204 - acc: 0.9079 - val_loss: 0.3160 - val_f_2: 0.4340 - val_acc: 0.8959\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4270 - f_2: 0.0345 - acc: 0.8584 - val_loss: 0.3472 - val_f_2: 0.0801 - val_acc: 0.8761\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3486 - f_2: 0.2138 - acc: 0.8920 - val_loss: 0.3215 - val_f_2: 0.2921 - val_acc: 0.9003\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.2580 - acc: 0.8959 - val_loss: 0.3099 - val_f_2: 0.3140 - val_acc: 0.9012\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.3010 - acc: 0.8987 - val_loss: 0.3031 - val_f_2: 0.3560 - val_acc: 0.9015\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3178 - acc: 0.9006 - val_loss: 0.3012 - val_f_2: 0.3905 - val_acc: 0.8994\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3391 - acc: 0.9021 - val_loss: 0.3016 - val_f_2: 0.4138 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3482 - acc: 0.9039 - val_loss: 0.2979 - val_f_2: 0.3788 - val_acc: 0.9009\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3368 - acc: 0.9015 - val_loss: 0.2977 - val_f_2: 0.3637 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3438 - acc: 0.9022 - val_loss: 0.2971 - val_f_2: 0.3445 - val_acc: 0.9027\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.3614 - acc: 0.9042 - val_loss: 0.2954 - val_f_2: 0.3743 - val_acc: 0.9006\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3368 - acc: 0.9027 - val_loss: 0.2953 - val_f_2: 0.4143 - val_acc: 0.9032\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2930 - f_2: 0.3808 - acc: 0.9048 - val_loss: 0.2962 - val_f_2: 0.3741 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2931 - f_2: 0.3758 - acc: 0.9050 - val_loss: 0.2958 - val_f_2: 0.3472 - val_acc: 0.8988\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.3707 - acc: 0.9057 - val_loss: 0.2964 - val_f_2: 0.4445 - val_acc: 0.9021\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.3750 - acc: 0.9049 - val_loss: 0.2940 - val_f_2: 0.3833 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.3753 - acc: 0.9048 - val_loss: 0.2936 - val_f_2: 0.3598 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.3700 - acc: 0.9040 - val_loss: 0.2973 - val_f_2: 0.3717 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2905 - f_2: 0.3842 - acc: 0.9054 - val_loss: 0.2935 - val_f_2: 0.3838 - val_acc: 0.9003\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.3680 - acc: 0.9045 - val_loss: 0.2964 - val_f_2: 0.4289 - val_acc: 0.9018\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2887 - f_2: 0.3739 - acc: 0.9033 - val_loss: 0.2949 - val_f_2: 0.4226 - val_acc: 0.9024\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.3778 - acc: 0.9048 - val_loss: 0.2946 - val_f_2: 0.4158 - val_acc: 0.9018\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.3787 - acc: 0.9045 - val_loss: 0.2965 - val_f_2: 0.4071 - val_acc: 0.9015\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2834 - f_2: 0.3799 - acc: 0.9069 - val_loss: 0.2967 - val_f_2: 0.4129 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.3916 - acc: 0.9050 - val_loss: 0.2969 - val_f_2: 0.3828 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.3979 - acc: 0.9060 - val_loss: 0.3001 - val_f_2: 0.3411 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.3840 - acc: 0.9047 - val_loss: 0.2956 - val_f_2: 0.4232 - val_acc: 0.9018\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.3972 - acc: 0.9061 - val_loss: 0.2975 - val_f_2: 0.4075 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2829 - f_2: 0.4073 - acc: 0.9072 - val_loss: 0.2959 - val_f_2: 0.4305 - val_acc: 0.8991\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.4075 - acc: 0.9065 - val_loss: 0.2962 - val_f_2: 0.4172 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.4025 - acc: 0.9069 - val_loss: 0.2957 - val_f_2: 0.4236 - val_acc: 0.9015\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.3897 - acc: 0.9055 - val_loss: 0.2962 - val_f_2: 0.4334 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.4122 - acc: 0.9076 - val_loss: 0.2982 - val_f_2: 0.4350 - val_acc: 0.8979\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2801 - f_2: 0.4100 - acc: 0.9067 - val_loss: 0.2986 - val_f_2: 0.4186 - val_acc: 0.8985\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2801 - f_2: 0.4296 - acc: 0.9082 - val_loss: 0.2993 - val_f_2: 0.3916 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2800 - f_2: 0.4087 - acc: 0.9060 - val_loss: 0.2972 - val_f_2: 0.4232 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2800 - f_2: 0.4163 - acc: 0.9084 - val_loss: 0.2975 - val_f_2: 0.4115 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2786 - f_2: 0.4024 - acc: 0.9077 - val_loss: 0.2983 - val_f_2: 0.4307 - val_acc: 0.8971\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2803 - f_2: 0.4189 - acc: 0.9073 - val_loss: 0.2970 - val_f_2: 0.4353 - val_acc: 0.8979\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4164 - acc: 0.9081 - val_loss: 0.2970 - val_f_2: 0.4184 - val_acc: 0.8991\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2783 - f_2: 0.4141 - acc: 0.9080 - val_loss: 0.2975 - val_f_2: 0.4442 - val_acc: 0.8988\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2783 - f_2: 0.4176 - acc: 0.9085 - val_loss: 0.2980 - val_f_2: 0.4288 - val_acc: 0.8991\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2783 - f_2: 0.4085 - acc: 0.9074 - val_loss: 0.3009 - val_f_2: 0.4008 - val_acc: 0.9021\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4268 - acc: 0.9098 - val_loss: 0.3017 - val_f_2: 0.4050 - val_acc: 0.8997\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2757 - f_2: 0.4047 - acc: 0.9065 - val_loss: 0.2973 - val_f_2: 0.4230 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2755 - f_2: 0.4122 - acc: 0.9096 - val_loss: 0.2984 - val_f_2: 0.4384 - val_acc: 0.8979\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4260 - acc: 0.9099 - val_loss: 0.2983 - val_f_2: 0.4214 - val_acc: 0.8971\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2771 - f_2: 0.4209 - acc: 0.9088 - val_loss: 0.3003 - val_f_2: 0.4257 - val_acc: 0.8962\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2779 - f_2: 0.4263 - acc: 0.9095 - val_loss: 0.3002 - val_f_2: 0.4334 - val_acc: 0.8950\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4350 - acc: 0.9099 - val_loss: 0.2970 - val_f_2: 0.4181 - val_acc: 0.8982\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2745 - f_2: 0.4318 - acc: 0.9096 - val_loss: 0.3026 - val_f_2: 0.4225 - val_acc: 0.8968\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4326 - f_2: 0.0226 - acc: 0.8555 - val_loss: 0.3287 - val_f_2: 0.0135 - val_acc: 0.8811\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3554 - f_2: 0.1634 - acc: 0.8868 - val_loss: 0.3066 - val_f_2: 0.3227 - val_acc: 0.9112\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3324 - f_2: 0.2604 - acc: 0.8935 - val_loss: 0.2958 - val_f_2: 0.3536 - val_acc: 0.9112\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2912 - acc: 0.8959 - val_loss: 0.2916 - val_f_2: 0.3768 - val_acc: 0.9109\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.3215 - acc: 0.8973 - val_loss: 0.2864 - val_f_2: 0.3570 - val_acc: 0.9109\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.3226 - acc: 0.8974 - val_loss: 0.2825 - val_f_2: 0.3525 - val_acc: 0.9106\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3234 - acc: 0.8987 - val_loss: 0.2809 - val_f_2: 0.3363 - val_acc: 0.9109\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.3175 - acc: 0.8976 - val_loss: 0.2794 - val_f_2: 0.3305 - val_acc: 0.9109\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3396 - acc: 0.8976 - val_loss: 0.2803 - val_f_2: 0.3485 - val_acc: 0.9091\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3325 - acc: 0.8977 - val_loss: 0.2800 - val_f_2: 0.3459 - val_acc: 0.9106\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3366 - acc: 0.8994 - val_loss: 0.2777 - val_f_2: 0.3638 - val_acc: 0.9100\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3672 - acc: 0.8995 - val_loss: 0.2771 - val_f_2: 0.3579 - val_acc: 0.9109\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.3448 - acc: 0.8978 - val_loss: 0.2778 - val_f_2: 0.3748 - val_acc: 0.9103\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2992 - f_2: 0.3582 - acc: 0.9000 - val_loss: 0.2772 - val_f_2: 0.3718 - val_acc: 0.9074\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.3415 - acc: 0.8972 - val_loss: 0.2762 - val_f_2: 0.3783 - val_acc: 0.9080\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2963 - f_2: 0.3569 - acc: 0.8977 - val_loss: 0.2782 - val_f_2: 0.3849 - val_acc: 0.9080\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.3652 - acc: 0.9005 - val_loss: 0.2761 - val_f_2: 0.3890 - val_acc: 0.9112\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.3512 - acc: 0.8994 - val_loss: 0.2756 - val_f_2: 0.4056 - val_acc: 0.9094\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2914 - f_2: 0.3729 - acc: 0.9011 - val_loss: 0.2748 - val_f_2: 0.3950 - val_acc: 0.9115\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3557 - acc: 0.8988 - val_loss: 0.2762 - val_f_2: 0.3794 - val_acc: 0.9091\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2899 - f_2: 0.3747 - acc: 0.9015 - val_loss: 0.2749 - val_f_2: 0.4079 - val_acc: 0.9097\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.3731 - acc: 0.9004 - val_loss: 0.2759 - val_f_2: 0.4178 - val_acc: 0.9094\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2905 - f_2: 0.3735 - acc: 0.9005 - val_loss: 0.2752 - val_f_2: 0.4099 - val_acc: 0.9062\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.3851 - acc: 0.9014 - val_loss: 0.2746 - val_f_2: 0.3929 - val_acc: 0.9100\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.3829 - acc: 0.8993 - val_loss: 0.2760 - val_f_2: 0.4203 - val_acc: 0.9074\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2878 - f_2: 0.3856 - acc: 0.9012 - val_loss: 0.2758 - val_f_2: 0.3970 - val_acc: 0.9086\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.3879 - acc: 0.9037 - val_loss: 0.2755 - val_f_2: 0.3992 - val_acc: 0.9053\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2873 - f_2: 0.3973 - acc: 0.9030 - val_loss: 0.2780 - val_f_2: 0.3859 - val_acc: 0.9086\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.3804 - acc: 0.9009 - val_loss: 0.2745 - val_f_2: 0.4045 - val_acc: 0.9059\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2845 - f_2: 0.4098 - acc: 0.9025 - val_loss: 0.2758 - val_f_2: 0.3854 - val_acc: 0.9083\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2878 - f_2: 0.3872 - acc: 0.9000 - val_loss: 0.2752 - val_f_2: 0.4026 - val_acc: 0.9083\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2882 - f_2: 0.3941 - acc: 0.9026 - val_loss: 0.2750 - val_f_2: 0.4162 - val_acc: 0.9074\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.4055 - acc: 0.9031 - val_loss: 0.2763 - val_f_2: 0.3966 - val_acc: 0.9038\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.3980 - acc: 0.9020 - val_loss: 0.2781 - val_f_2: 0.4113 - val_acc: 0.9035\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2853 - f_2: 0.4065 - acc: 0.9027 - val_loss: 0.2761 - val_f_2: 0.3947 - val_acc: 0.9088\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.4112 - acc: 0.9023 - val_loss: 0.2801 - val_f_2: 0.4327 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2862 - f_2: 0.4123 - acc: 0.9037 - val_loss: 0.2810 - val_f_2: 0.4363 - val_acc: 0.9021\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2840 - f_2: 0.4114 - acc: 0.9036 - val_loss: 0.2779 - val_f_2: 0.4177 - val_acc: 0.9027\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.4101 - acc: 0.9023 - val_loss: 0.2766 - val_f_2: 0.3883 - val_acc: 0.9038\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4192 - acc: 0.9030 - val_loss: 0.2750 - val_f_2: 0.4276 - val_acc: 0.9050\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4125 - acc: 0.9025 - val_loss: 0.2746 - val_f_2: 0.4049 - val_acc: 0.9032\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.4085 - acc: 0.9030 - val_loss: 0.2771 - val_f_2: 0.3835 - val_acc: 0.9053\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4076 - acc: 0.9031 - val_loss: 0.2738 - val_f_2: 0.4065 - val_acc: 0.9074\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.4089 - acc: 0.9049 - val_loss: 0.2754 - val_f_2: 0.4007 - val_acc: 0.9044\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2815 - f_2: 0.4162 - acc: 0.9028 - val_loss: 0.2771 - val_f_2: 0.4080 - val_acc: 0.9029\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.4131 - acc: 0.9030 - val_loss: 0.2759 - val_f_2: 0.4198 - val_acc: 0.9035\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4092 - acc: 0.9022 - val_loss: 0.2781 - val_f_2: 0.4006 - val_acc: 0.9047\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2829 - f_2: 0.4203 - acc: 0.9057 - val_loss: 0.2764 - val_f_2: 0.4080 - val_acc: 0.9047\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4077 - acc: 0.9049 - val_loss: 0.2783 - val_f_2: 0.4262 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4232 - acc: 0.9039 - val_loss: 0.2814 - val_f_2: 0.4421 - val_acc: 0.8982\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=10, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6153 - f_2: 0.0178 - acc: 0.8630 - val_loss: 0.3989 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3777 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3627 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3594 - f_2: 3.1918e-08 - acc: 0.8753 - val_loss: 0.3497 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3463 - f_2: 3.1147e-08 - acc: 0.8753 - val_loss: 0.3406 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3436 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3355 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3374 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3340 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3346 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3307 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3328 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3273 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.0953 - acc: 0.8840 - val_loss: 0.3280 - val_f_2: 0.0913 - val_acc: 0.8779\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3270 - f_2: 0.1808 - acc: 0.8920 - val_loss: 0.3238 - val_f_2: 0.2733 - val_acc: 0.8991\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.2184 - acc: 0.8958 - val_loss: 0.3236 - val_f_2: 0.2577 - val_acc: 0.8985\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2102 - acc: 0.8969 - val_loss: 0.3223 - val_f_2: 0.2852 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.2151 - acc: 0.8960 - val_loss: 0.3218 - val_f_2: 0.2796 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.2198 - acc: 0.8971 - val_loss: 0.3202 - val_f_2: 0.2796 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.2143 - acc: 0.8959 - val_loss: 0.3191 - val_f_2: 0.3237 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.2336 - acc: 0.8978 - val_loss: 0.3214 - val_f_2: 0.3091 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2289 - acc: 0.8971 - val_loss: 0.3181 - val_f_2: 0.2860 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3189 - f_2: 0.2421 - acc: 0.8984 - val_loss: 0.3176 - val_f_2: 0.2967 - val_acc: 0.9015\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3189 - f_2: 0.2598 - acc: 0.8997 - val_loss: 0.3180 - val_f_2: 0.3142 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.2532 - acc: 0.8982 - val_loss: 0.3159 - val_f_2: 0.3211 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2370 - acc: 0.8978 - val_loss: 0.3157 - val_f_2: 0.3433 - val_acc: 0.8991\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.2718 - acc: 0.8998 - val_loss: 0.3156 - val_f_2: 0.3208 - val_acc: 0.8991\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.2642 - acc: 0.8988 - val_loss: 0.3153 - val_f_2: 0.3527 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.2624 - acc: 0.8997 - val_loss: 0.3167 - val_f_2: 0.3223 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.2714 - acc: 0.9004 - val_loss: 0.3136 - val_f_2: 0.3055 - val_acc: 0.9021\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.2942 - acc: 0.9018 - val_loss: 0.3157 - val_f_2: 0.3186 - val_acc: 0.9021\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.2741 - acc: 0.8993 - val_loss: 0.3131 - val_f_2: 0.3179 - val_acc: 0.9018\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.2985 - acc: 0.9024 - val_loss: 0.3122 - val_f_2: 0.3245 - val_acc: 0.8982\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.2883 - acc: 0.9002 - val_loss: 0.3126 - val_f_2: 0.3731 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.2948 - acc: 0.9030 - val_loss: 0.3120 - val_f_2: 0.3306 - val_acc: 0.8991\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3135 - f_2: 0.2836 - acc: 0.9003 - val_loss: 0.3141 - val_f_2: 0.3058 - val_acc: 0.9018\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.2896 - acc: 0.9004 - val_loss: 0.3127 - val_f_2: 0.3263 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.2972 - acc: 0.9005 - val_loss: 0.3245 - val_f_2: 0.2984 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.2798 - acc: 0.8986 - val_loss: 0.3134 - val_f_2: 0.3909 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.3137 - acc: 0.9020 - val_loss: 0.3116 - val_f_2: 0.3759 - val_acc: 0.9009\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.2841 - acc: 0.9013 - val_loss: 0.3116 - val_f_2: 0.3397 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3090 - acc: 0.9023 - val_loss: 0.3138 - val_f_2: 0.3419 - val_acc: 0.8988\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.3092 - f_2: 0.2958 - acc: 0.9009 - val_loss: 0.3126 - val_f_2: 0.3964 - val_acc: 0.9018\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.3117 - f_2: 0.3002 - acc: 0.9018 - val_loss: 0.3114 - val_f_2: 0.2928 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3002 - acc: 0.9027 - val_loss: 0.3132 - val_f_2: 0.3558 - val_acc: 0.9003\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3100 - f_2: 0.3143 - acc: 0.9024 - val_loss: 0.3103 - val_f_2: 0.3539 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3120 - acc: 0.9012 - val_loss: 0.3105 - val_f_2: 0.3087 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3108 - acc: 0.9043 - val_loss: 0.3106 - val_f_2: 0.3199 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3262 - acc: 0.9039 - val_loss: 0.3117 - val_f_2: 0.3146 - val_acc: 0.9012\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3159 - acc: 0.9032 - val_loss: 0.3104 - val_f_2: 0.3936 - val_acc: 0.8997\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3065 - acc: 0.9025 - val_loss: 0.3125 - val_f_2: 0.4006 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3084 - acc: 0.9018 - val_loss: 0.3157 - val_f_2: 0.3642 - val_acc: 0.9029\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3088 - acc: 0.9022 - val_loss: 0.3099 - val_f_2: 0.3752 - val_acc: 0.9018\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3090 - f_2: 0.3216 - acc: 0.9034 - val_loss: 0.3115 - val_f_2: 0.3277 - val_acc: 0.8997\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3114 - acc: 0.9029 - val_loss: 0.3096 - val_f_2: 0.3667 - val_acc: 0.9024\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6100 - f_2: 0.0111 - acc: 0.8600 - val_loss: 0.3963 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3782 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3611 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3578 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3467 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3481 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3404 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3385 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3386 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3370 - f_2: 0.0118 - acc: 0.8722 - val_loss: 0.3312 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3330 - f_2: 0.0665 - acc: 0.8793 - val_loss: 0.3303 - val_f_2: 0.2649 - val_acc: 0.8988\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3319 - f_2: 0.1860 - acc: 0.8909 - val_loss: 0.3276 - val_f_2: 0.2515 - val_acc: 0.8976\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3304 - f_2: 0.2053 - acc: 0.8939 - val_loss: 0.3259 - val_f_2: 0.2717 - val_acc: 0.8991\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3278 - f_2: 0.2375 - acc: 0.8969 - val_loss: 0.3238 - val_f_2: 0.2754 - val_acc: 0.8997\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2541 - acc: 0.8981 - val_loss: 0.3234 - val_f_2: 0.2938 - val_acc: 0.8994\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.2708 - acc: 0.8990 - val_loss: 0.3209 - val_f_2: 0.2789 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.2723 - acc: 0.8997 - val_loss: 0.3214 - val_f_2: 0.2718 - val_acc: 0.8997\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3217 - f_2: 0.2866 - acc: 0.9010 - val_loss: 0.3184 - val_f_2: 0.2754 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.2763 - acc: 0.9002 - val_loss: 0.3175 - val_f_2: 0.3152 - val_acc: 0.9006\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.2863 - acc: 0.9005 - val_loss: 0.3193 - val_f_2: 0.3306 - val_acc: 0.9018\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.2993 - acc: 0.9008 - val_loss: 0.3157 - val_f_2: 0.2718 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2874 - acc: 0.9015 - val_loss: 0.3145 - val_f_2: 0.3235 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3189 - f_2: 0.3247 - acc: 0.9020 - val_loss: 0.3136 - val_f_2: 0.3114 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.3056 - acc: 0.9034 - val_loss: 0.3146 - val_f_2: 0.3141 - val_acc: 0.9003\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.3083 - acc: 0.9015 - val_loss: 0.3134 - val_f_2: 0.3087 - val_acc: 0.9018\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.3204 - acc: 0.9041 - val_loss: 0.3125 - val_f_2: 0.3128 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3279 - acc: 0.9026 - val_loss: 0.3141 - val_f_2: 0.2877 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.3236 - acc: 0.9031 - val_loss: 0.3121 - val_f_2: 0.3007 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.3222 - acc: 0.9028 - val_loss: 0.3104 - val_f_2: 0.3477 - val_acc: 0.9015\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.3251 - acc: 0.9024 - val_loss: 0.3103 - val_f_2: 0.3403 - val_acc: 0.9018\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3376 - acc: 0.9033 - val_loss: 0.3094 - val_f_2: 0.3230 - val_acc: 0.9009\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.3300 - acc: 0.9023 - val_loss: 0.3136 - val_f_2: 0.4329 - val_acc: 0.9024\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.3387 - acc: 0.9034 - val_loss: 0.3098 - val_f_2: 0.4049 - val_acc: 0.9029\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.3278 - acc: 0.9023 - val_loss: 0.3080 - val_f_2: 0.3168 - val_acc: 0.9029\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3086 - f_2: 0.3372 - acc: 0.9044 - val_loss: 0.3097 - val_f_2: 0.4026 - val_acc: 0.9021\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3306 - acc: 0.9037 - val_loss: 0.3087 - val_f_2: 0.3561 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3413 - acc: 0.9040 - val_loss: 0.3118 - val_f_2: 0.4058 - val_acc: 0.9027\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3486 - acc: 0.9031 - val_loss: 0.3077 - val_f_2: 0.3470 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3473 - acc: 0.9049 - val_loss: 0.3075 - val_f_2: 0.3982 - val_acc: 0.9029\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3078 - f_2: 0.3372 - acc: 0.9033 - val_loss: 0.3209 - val_f_2: 0.3860 - val_acc: 0.9024\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3519 - acc: 0.9022 - val_loss: 0.3064 - val_f_2: 0.3269 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3428 - acc: 0.9052 - val_loss: 0.3067 - val_f_2: 0.3647 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3529 - acc: 0.9030 - val_loss: 0.3076 - val_f_2: 0.3994 - val_acc: 0.9021\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3545 - acc: 0.9034 - val_loss: 0.3080 - val_f_2: 0.3245 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3469 - acc: 0.9038 - val_loss: 0.3103 - val_f_2: 0.3289 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3512 - acc: 0.9045 - val_loss: 0.3057 - val_f_2: 0.3860 - val_acc: 0.9029\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3420 - acc: 0.9020 - val_loss: 0.3069 - val_f_2: 0.3450 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3649 - acc: 0.9057 - val_loss: 0.3062 - val_f_2: 0.3405 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3655 - acc: 0.9047 - val_loss: 0.3086 - val_f_2: 0.3298 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.3595 - acc: 0.9037 - val_loss: 0.3081 - val_f_2: 0.3663 - val_acc: 0.9012\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.3624 - acc: 0.9058 - val_loss: 0.3126 - val_f_2: 0.3506 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3629 - acc: 0.9038 - val_loss: 0.3082 - val_f_2: 0.3367 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3574 - acc: 0.9045 - val_loss: 0.3129 - val_f_2: 0.3372 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3663 - acc: 0.9033 - val_loss: 0.3069 - val_f_2: 0.4168 - val_acc: 0.9027\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6185 - f_2: 0.0090 - acc: 0.8573 - val_loss: 0.3772 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3874 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3458 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3670 - f_2: 3.0047e-08 - acc: 0.8691 - val_loss: 0.3326 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3554 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3258 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3496 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3193 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3448 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3160 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3422 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3193 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3411 - f_2: 0.0156 - acc: 0.8709 - val_loss: 0.3136 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3367 - f_2: 0.1574 - acc: 0.8857 - val_loss: 0.3104 - val_f_2: 0.2800 - val_acc: 0.9091\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3380 - f_2: 0.1927 - acc: 0.8896 - val_loss: 0.3092 - val_f_2: 0.2949 - val_acc: 0.9109\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3388 - f_2: 0.1901 - acc: 0.8896 - val_loss: 0.3068 - val_f_2: 0.2962 - val_acc: 0.9106\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3322 - f_2: 0.2123 - acc: 0.8906 - val_loss: 0.3091 - val_f_2: 0.2768 - val_acc: 0.9088\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3328 - f_2: 0.2222 - acc: 0.8912 - val_loss: 0.3074 - val_f_2: 0.3069 - val_acc: 0.9106\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3334 - f_2: 0.2385 - acc: 0.8932 - val_loss: 0.3065 - val_f_2: 0.3069 - val_acc: 0.9097\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3296 - f_2: 0.2347 - acc: 0.8929 - val_loss: 0.3021 - val_f_2: 0.2960 - val_acc: 0.9103\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3286 - f_2: 0.2413 - acc: 0.8922 - val_loss: 0.3012 - val_f_2: 0.2949 - val_acc: 0.9109\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3309 - f_2: 0.2436 - acc: 0.8932 - val_loss: 0.3003 - val_f_2: 0.3064 - val_acc: 0.9112\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3294 - f_2: 0.2533 - acc: 0.8937 - val_loss: 0.3011 - val_f_2: 0.3085 - val_acc: 0.9109\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2582 - acc: 0.8950 - val_loss: 0.2991 - val_f_2: 0.3153 - val_acc: 0.9109\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3284 - f_2: 0.2601 - acc: 0.8950 - val_loss: 0.3005 - val_f_2: 0.3418 - val_acc: 0.9074\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.2663 - acc: 0.8957 - val_loss: 0.2983 - val_f_2: 0.3173 - val_acc: 0.9118\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.3002 - acc: 0.8987 - val_loss: 0.2976 - val_f_2: 0.3187 - val_acc: 0.9103\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.2909 - acc: 0.8965 - val_loss: 0.2972 - val_f_2: 0.3187 - val_acc: 0.9100\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3254 - f_2: 0.2883 - acc: 0.8982 - val_loss: 0.2967 - val_f_2: 0.3085 - val_acc: 0.9103\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.3122 - acc: 0.8990 - val_loss: 0.2955 - val_f_2: 0.3147 - val_acc: 0.9106\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.3019 - acc: 0.8974 - val_loss: 0.2961 - val_f_2: 0.3191 - val_acc: 0.9112\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.3099 - acc: 0.8970 - val_loss: 0.2944 - val_f_2: 0.3015 - val_acc: 0.9109\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.3080 - acc: 0.8990 - val_loss: 0.2953 - val_f_2: 0.3171 - val_acc: 0.9115\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.3141 - acc: 0.8989 - val_loss: 0.2969 - val_f_2: 0.3169 - val_acc: 0.9109\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.3105 - acc: 0.8984 - val_loss: 0.2949 - val_f_2: 0.3220 - val_acc: 0.9103\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.3232 - acc: 0.8986 - val_loss: 0.2941 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2999 - acc: 0.8979 - val_loss: 0.2937 - val_f_2: 0.3251 - val_acc: 0.9100\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.3106 - acc: 0.8990 - val_loss: 0.2935 - val_f_2: 0.3263 - val_acc: 0.9091\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.3052 - acc: 0.8976 - val_loss: 0.2937 - val_f_2: 0.3231 - val_acc: 0.9097\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.3150 - acc: 0.9002 - val_loss: 0.2923 - val_f_2: 0.2992 - val_acc: 0.9106\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.3093 - acc: 0.8969 - val_loss: 0.2935 - val_f_2: 0.3281 - val_acc: 0.9091\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.3316 - acc: 0.8993 - val_loss: 0.2919 - val_f_2: 0.2975 - val_acc: 0.9103\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.3096 - acc: 0.8981 - val_loss: 0.2922 - val_f_2: 0.3245 - val_acc: 0.9106\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.3199 - acc: 0.8987 - val_loss: 0.2910 - val_f_2: 0.2991 - val_acc: 0.9103\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3185 - f_2: 0.2993 - acc: 0.8973 - val_loss: 0.2927 - val_f_2: 0.3528 - val_acc: 0.9094\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.3245 - acc: 0.8995 - val_loss: 0.2909 - val_f_2: 0.3256 - val_acc: 0.9115\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3333 - acc: 0.9002 - val_loss: 0.2922 - val_f_2: 0.3504 - val_acc: 0.9080\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.3306 - acc: 0.8982 - val_loss: 0.2919 - val_f_2: 0.3347 - val_acc: 0.9100\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.3259 - acc: 0.8992 - val_loss: 0.2957 - val_f_2: 0.3970 - val_acc: 0.9077\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.3269 - acc: 0.8980 - val_loss: 0.2914 - val_f_2: 0.3451 - val_acc: 0.9077\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3355 - acc: 0.8987 - val_loss: 0.2949 - val_f_2: 0.2992 - val_acc: 0.9106\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3223 - acc: 0.8992 - val_loss: 0.2902 - val_f_2: 0.3303 - val_acc: 0.9106\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.3269 - acc: 0.8989 - val_loss: 0.2902 - val_f_2: 0.3260 - val_acc: 0.9109\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3160 - f_2: 0.3321 - acc: 0.8989 - val_loss: 0.2918 - val_f_2: 0.3605 - val_acc: 0.9083\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3303 - acc: 0.8988 - val_loss: 0.2912 - val_f_2: 0.3559 - val_acc: 0.9091\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4328 - f_2: 0.0077 - acc: 0.8639 - val_loss: 0.3590 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3492 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3365 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3364 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3272 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3301 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3233 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.0849 - acc: 0.8835 - val_loss: 0.3202 - val_f_2: 0.2580 - val_acc: 0.8985\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2265 - acc: 0.8976 - val_loss: 0.3200 - val_f_2: 0.2806 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3182 - f_2: 0.2634 - acc: 0.8987 - val_loss: 0.3171 - val_f_2: 0.3533 - val_acc: 0.9027\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.2706 - acc: 0.8996 - val_loss: 0.3156 - val_f_2: 0.3078 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.2690 - acc: 0.9003 - val_loss: 0.3143 - val_f_2: 0.3356 - val_acc: 0.9015\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.2813 - acc: 0.8996 - val_loss: 0.3159 - val_f_2: 0.3493 - val_acc: 0.9012\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.2939 - acc: 0.9020 - val_loss: 0.3117 - val_f_2: 0.3595 - val_acc: 0.9015\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.2819 - acc: 0.8987 - val_loss: 0.3109 - val_f_2: 0.3757 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3058 - f_2: 0.3190 - acc: 0.9029 - val_loss: 0.3101 - val_f_2: 0.3854 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3393 - acc: 0.9046 - val_loss: 0.3099 - val_f_2: 0.3684 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3449 - acc: 0.9048 - val_loss: 0.3102 - val_f_2: 0.3529 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3050 - f_2: 0.3329 - acc: 0.9053 - val_loss: 0.3106 - val_f_2: 0.3757 - val_acc: 0.8991\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3528 - acc: 0.9061 - val_loss: 0.3138 - val_f_2: 0.3413 - val_acc: 0.8997\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3655 - acc: 0.9071 - val_loss: 0.3147 - val_f_2: 0.3425 - val_acc: 0.9003\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3705 - acc: 0.9071 - val_loss: 0.3113 - val_f_2: 0.3927 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.3842 - acc: 0.9076 - val_loss: 0.3118 - val_f_2: 0.3799 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.3814 - acc: 0.9085 - val_loss: 0.3141 - val_f_2: 0.3797 - val_acc: 0.8994\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3853 - acc: 0.9079 - val_loss: 0.3129 - val_f_2: 0.3788 - val_acc: 0.8988\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2992 - f_2: 0.3898 - acc: 0.9078 - val_loss: 0.3098 - val_f_2: 0.3870 - val_acc: 0.8988\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2958 - f_2: 0.3990 - acc: 0.9088 - val_loss: 0.3136 - val_f_2: 0.3739 - val_acc: 0.8988\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.3825 - acc: 0.9090 - val_loss: 0.3101 - val_f_2: 0.4079 - val_acc: 0.8991\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.4010 - acc: 0.9093 - val_loss: 0.3102 - val_f_2: 0.3813 - val_acc: 0.8994\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.3943 - acc: 0.9076 - val_loss: 0.3098 - val_f_2: 0.4015 - val_acc: 0.8991\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2950 - f_2: 0.3915 - acc: 0.9076 - val_loss: 0.3118 - val_f_2: 0.3586 - val_acc: 0.8997\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2941 - f_2: 0.3906 - acc: 0.9085 - val_loss: 0.3197 - val_f_2: 0.3807 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2968 - f_2: 0.4064 - acc: 0.9082 - val_loss: 0.3105 - val_f_2: 0.4060 - val_acc: 0.8994\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2942 - f_2: 0.4118 - acc: 0.9091 - val_loss: 0.3099 - val_f_2: 0.3880 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.4153 - acc: 0.9094 - val_loss: 0.3226 - val_f_2: 0.3795 - val_acc: 0.8985\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.4137 - acc: 0.9101 - val_loss: 0.3167 - val_f_2: 0.3617 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.4149 - acc: 0.9099 - val_loss: 0.3158 - val_f_2: 0.3839 - val_acc: 0.8991\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.4006 - acc: 0.9098 - val_loss: 0.3120 - val_f_2: 0.4155 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2911 - f_2: 0.4089 - acc: 0.9107 - val_loss: 0.3153 - val_f_2: 0.3810 - val_acc: 0.8988\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.4210 - acc: 0.9105 - val_loss: 0.3165 - val_f_2: 0.3727 - val_acc: 0.8994\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.4278 - acc: 0.9112 - val_loss: 0.3118 - val_f_2: 0.4168 - val_acc: 0.8985\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2919 - f_2: 0.4235 - acc: 0.9106 - val_loss: 0.3179 - val_f_2: 0.4150 - val_acc: 0.8991\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2936 - f_2: 0.4277 - acc: 0.9104 - val_loss: 0.3099 - val_f_2: 0.4040 - val_acc: 0.9003\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.4185 - acc: 0.9099 - val_loss: 0.3106 - val_f_2: 0.4240 - val_acc: 0.8988\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2911 - f_2: 0.4090 - acc: 0.9105 - val_loss: 0.3107 - val_f_2: 0.4272 - val_acc: 0.8997\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2932 - f_2: 0.4373 - acc: 0.9102 - val_loss: 0.3112 - val_f_2: 0.4273 - val_acc: 0.8988\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2928 - f_2: 0.4501 - acc: 0.9106 - val_loss: 0.3121 - val_f_2: 0.4062 - val_acc: 0.9003\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2906 - f_2: 0.4276 - acc: 0.9102 - val_loss: 0.3113 - val_f_2: 0.4251 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.4370 - acc: 0.9117 - val_loss: 0.3182 - val_f_2: 0.4008 - val_acc: 0.8994\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2923 - f_2: 0.4328 - acc: 0.9104 - val_loss: 0.3193 - val_f_2: 0.4003 - val_acc: 0.8994\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2887 - f_2: 0.4247 - acc: 0.9104 - val_loss: 0.3209 - val_f_2: 0.4038 - val_acc: 0.8982\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.4226 - acc: 0.9108 - val_loss: 0.3242 - val_f_2: 0.4215 - val_acc: 0.9006\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2894 - f_2: 0.4317 - acc: 0.9107 - val_loss: 0.3134 - val_f_2: 0.4363 - val_acc: 0.8953\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4331 - f_2: 0.0089 - acc: 0.8620 - val_loss: 0.3535 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3496 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3327 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3354 - f_2: 3.0604e-08 - acc: 0.8722 - val_loss: 0.3269 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3294 - f_2: 2.9947e-08 - acc: 0.8722 - val_loss: 0.3226 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3241 - f_2: 0.2031 - acc: 0.8930 - val_loss: 0.3211 - val_f_2: 0.3019 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.3058 - acc: 0.9020 - val_loss: 0.3250 - val_f_2: 0.2805 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.2964 - acc: 0.9013 - val_loss: 0.3165 - val_f_2: 0.3314 - val_acc: 0.9015\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.3223 - acc: 0.9028 - val_loss: 0.3137 - val_f_2: 0.3554 - val_acc: 0.9015\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3114 - f_2: 0.3478 - acc: 0.9020 - val_loss: 0.3124 - val_f_2: 0.3814 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3100 - f_2: 0.3531 - acc: 0.9028 - val_loss: 0.3105 - val_f_2: 0.3735 - val_acc: 0.9009\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3501 - acc: 0.9029 - val_loss: 0.3093 - val_f_2: 0.4011 - val_acc: 0.9021\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3625 - acc: 0.9033 - val_loss: 0.3110 - val_f_2: 0.3685 - val_acc: 0.9006\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3759 - acc: 0.9033 - val_loss: 0.3081 - val_f_2: 0.3759 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3019 - f_2: 0.3666 - acc: 0.9050 - val_loss: 0.3077 - val_f_2: 0.4133 - val_acc: 0.9024\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3618 - acc: 0.9047 - val_loss: 0.3075 - val_f_2: 0.4175 - val_acc: 0.9018\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.3797 - acc: 0.9037 - val_loss: 0.3068 - val_f_2: 0.4179 - val_acc: 0.9032\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.3899 - acc: 0.9058 - val_loss: 0.3086 - val_f_2: 0.4043 - val_acc: 0.9035\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3887 - acc: 0.9056 - val_loss: 0.3066 - val_f_2: 0.4148 - val_acc: 0.9024\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.3834 - acc: 0.9054 - val_loss: 0.3155 - val_f_2: 0.4053 - val_acc: 0.9047\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.3938 - acc: 0.9046 - val_loss: 0.3088 - val_f_2: 0.4025 - val_acc: 0.9029\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.3721 - acc: 0.9055 - val_loss: 0.3111 - val_f_2: 0.4344 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.3989 - acc: 0.9051 - val_loss: 0.3109 - val_f_2: 0.4180 - val_acc: 0.9027\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.3987 - acc: 0.9060 - val_loss: 0.3084 - val_f_2: 0.4326 - val_acc: 0.8985\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2939 - f_2: 0.3994 - acc: 0.9065 - val_loss: 0.3052 - val_f_2: 0.4227 - val_acc: 0.9021\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2927 - f_2: 0.3990 - acc: 0.9048 - val_loss: 0.3151 - val_f_2: 0.4167 - val_acc: 0.9038\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.4193 - acc: 0.9068 - val_loss: 0.3057 - val_f_2: 0.4268 - val_acc: 0.9018\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2906 - f_2: 0.4161 - acc: 0.9065 - val_loss: 0.3076 - val_f_2: 0.4228 - val_acc: 0.8997\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.4153 - acc: 0.9072 - val_loss: 0.3065 - val_f_2: 0.4291 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2894 - f_2: 0.4096 - acc: 0.9064 - val_loss: 0.3079 - val_f_2: 0.4432 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.4279 - acc: 0.9082 - val_loss: 0.3072 - val_f_2: 0.4350 - val_acc: 0.9003\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.4206 - acc: 0.9073 - val_loss: 0.3070 - val_f_2: 0.4216 - val_acc: 0.9027\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2894 - f_2: 0.4255 - acc: 0.9086 - val_loss: 0.3116 - val_f_2: 0.4249 - val_acc: 0.9027\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.4293 - acc: 0.9079 - val_loss: 0.3062 - val_f_2: 0.4362 - val_acc: 0.9003\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2900 - f_2: 0.4289 - acc: 0.9086 - val_loss: 0.3126 - val_f_2: 0.4204 - val_acc: 0.9012\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2865 - f_2: 0.4169 - acc: 0.9079 - val_loss: 0.3113 - val_f_2: 0.4343 - val_acc: 0.8997\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.4270 - acc: 0.9085 - val_loss: 0.3053 - val_f_2: 0.4233 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2898 - f_2: 0.4256 - acc: 0.9085 - val_loss: 0.3157 - val_f_2: 0.4184 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.4387 - acc: 0.9075 - val_loss: 0.3065 - val_f_2: 0.4316 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2865 - f_2: 0.4040 - acc: 0.9069 - val_loss: 0.3052 - val_f_2: 0.4300 - val_acc: 0.9038\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2853 - f_2: 0.4123 - acc: 0.9087 - val_loss: 0.3139 - val_f_2: 0.4295 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.4294 - acc: 0.9075 - val_loss: 0.3075 - val_f_2: 0.4374 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.4394 - acc: 0.9100 - val_loss: 0.3185 - val_f_2: 0.4119 - val_acc: 0.9035\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2881 - f_2: 0.4211 - acc: 0.9080 - val_loss: 0.3081 - val_f_2: 0.4372 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2862 - f_2: 0.4282 - acc: 0.9078 - val_loss: 0.3059 - val_f_2: 0.4221 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2868 - f_2: 0.4196 - acc: 0.9096 - val_loss: 0.3067 - val_f_2: 0.4494 - val_acc: 0.8991\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.4325 - acc: 0.9075 - val_loss: 0.3131 - val_f_2: 0.4278 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.4336 - acc: 0.9079 - val_loss: 0.3060 - val_f_2: 0.4227 - val_acc: 0.9024\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4442 - acc: 0.9091 - val_loss: 0.3118 - val_f_2: 0.4345 - val_acc: 0.8994\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2872 - f_2: 0.4315 - acc: 0.9099 - val_loss: 0.3089 - val_f_2: 0.4308 - val_acc: 0.8982\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2842 - f_2: 0.4332 - acc: 0.9086 - val_loss: 0.3068 - val_f_2: 0.4208 - val_acc: 0.9029\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4386 - f_2: 0.0092 - acc: 0.8582 - val_loss: 0.3345 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3548 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3148 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3405 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3082 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3344 - f_2: 0.0104 - acc: 0.8700 - val_loss: 0.3068 - val_f_2: 0.2792 - val_acc: 0.9080\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3295 - f_2: 0.2497 - acc: 0.8936 - val_loss: 0.3019 - val_f_2: 0.3319 - val_acc: 0.9106\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2966 - acc: 0.8977 - val_loss: 0.2999 - val_f_2: 0.3623 - val_acc: 0.9097\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.3061 - acc: 0.8996 - val_loss: 0.2983 - val_f_2: 0.3508 - val_acc: 0.9083\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.3320 - acc: 0.8993 - val_loss: 0.2954 - val_f_2: 0.3367 - val_acc: 0.9094\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3351 - acc: 0.9003 - val_loss: 0.2946 - val_f_2: 0.3652 - val_acc: 0.9097\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.3504 - acc: 0.9001 - val_loss: 0.2941 - val_f_2: 0.3308 - val_acc: 0.9080\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3560 - acc: 0.9006 - val_loss: 0.2935 - val_f_2: 0.3573 - val_acc: 0.9083\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.3681 - acc: 0.9017 - val_loss: 0.2929 - val_f_2: 0.3637 - val_acc: 0.9080\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.3594 - acc: 0.9006 - val_loss: 0.2927 - val_f_2: 0.3736 - val_acc: 0.9086\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3751 - acc: 0.9017 - val_loss: 0.2911 - val_f_2: 0.3685 - val_acc: 0.9091\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3803 - acc: 0.9032 - val_loss: 0.2914 - val_f_2: 0.3907 - val_acc: 0.9086\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3711 - acc: 0.9001 - val_loss: 0.2908 - val_f_2: 0.4189 - val_acc: 0.9088\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.3863 - acc: 0.9004 - val_loss: 0.2902 - val_f_2: 0.3622 - val_acc: 0.9074\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3014 - f_2: 0.3798 - acc: 0.9023 - val_loss: 0.2909 - val_f_2: 0.3912 - val_acc: 0.9080\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.3903 - acc: 0.9032 - val_loss: 0.2907 - val_f_2: 0.4005 - val_acc: 0.9077\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.4080 - acc: 0.9021 - val_loss: 0.2923 - val_f_2: 0.3613 - val_acc: 0.9077\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3957 - acc: 0.9028 - val_loss: 0.2895 - val_f_2: 0.3870 - val_acc: 0.9077\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3977 - acc: 0.9024 - val_loss: 0.2916 - val_f_2: 0.3930 - val_acc: 0.9071\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.4001 - acc: 0.9028 - val_loss: 0.2916 - val_f_2: 0.4140 - val_acc: 0.9077\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2947 - f_2: 0.3967 - acc: 0.9021 - val_loss: 0.2945 - val_f_2: 0.4120 - val_acc: 0.9059\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2969 - f_2: 0.4046 - acc: 0.9029 - val_loss: 0.2904 - val_f_2: 0.3941 - val_acc: 0.9083\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.4051 - acc: 0.9033 - val_loss: 0.2891 - val_f_2: 0.3899 - val_acc: 0.9050\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.4124 - acc: 0.9035 - val_loss: 0.2884 - val_f_2: 0.3889 - val_acc: 0.9065\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.4149 - acc: 0.9032 - val_loss: 0.2908 - val_f_2: 0.4044 - val_acc: 0.9062\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2932 - f_2: 0.4092 - acc: 0.9039 - val_loss: 0.2884 - val_f_2: 0.3947 - val_acc: 0.9080\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2918 - f_2: 0.4054 - acc: 0.9034 - val_loss: 0.2894 - val_f_2: 0.3961 - val_acc: 0.9086\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2932 - f_2: 0.3998 - acc: 0.9034 - val_loss: 0.2933 - val_f_2: 0.4348 - val_acc: 0.9062\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2956 - f_2: 0.4247 - acc: 0.9037 - val_loss: 0.2897 - val_f_2: 0.4258 - val_acc: 0.9050\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.4177 - acc: 0.9046 - val_loss: 0.2897 - val_f_2: 0.4379 - val_acc: 0.9050\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.4175 - acc: 0.9035 - val_loss: 0.2878 - val_f_2: 0.4003 - val_acc: 0.9047\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.4169 - acc: 0.9059 - val_loss: 0.2899 - val_f_2: 0.3997 - val_acc: 0.9050\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2937 - f_2: 0.4106 - acc: 0.9026 - val_loss: 0.2868 - val_f_2: 0.4030 - val_acc: 0.9062\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2911 - f_2: 0.4229 - acc: 0.9036 - val_loss: 0.2886 - val_f_2: 0.3905 - val_acc: 0.9068\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2916 - f_2: 0.4181 - acc: 0.9033 - val_loss: 0.2863 - val_f_2: 0.3880 - val_acc: 0.9044\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2914 - f_2: 0.4192 - acc: 0.9033 - val_loss: 0.2915 - val_f_2: 0.4479 - val_acc: 0.9032\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.4376 - acc: 0.9049 - val_loss: 0.2869 - val_f_2: 0.4276 - val_acc: 0.9047\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.4226 - acc: 0.9023 - val_loss: 0.2870 - val_f_2: 0.3826 - val_acc: 0.9080\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.4278 - acc: 0.9051 - val_loss: 0.2879 - val_f_2: 0.3835 - val_acc: 0.9083\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2899 - f_2: 0.4330 - acc: 0.9043 - val_loss: 0.2856 - val_f_2: 0.3807 - val_acc: 0.9074\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.4248 - acc: 0.9037 - val_loss: 0.2898 - val_f_2: 0.4052 - val_acc: 0.9083\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.4320 - acc: 0.9046 - val_loss: 0.2891 - val_f_2: 0.4286 - val_acc: 0.9038\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2918 - f_2: 0.4296 - acc: 0.9043 - val_loss: 0.2884 - val_f_2: 0.4179 - val_acc: 0.9035\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2879 - f_2: 0.4327 - acc: 0.9041 - val_loss: 0.2924 - val_f_2: 0.4073 - val_acc: 0.9050\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2906 - f_2: 0.4286 - acc: 0.9043 - val_loss: 0.2888 - val_f_2: 0.4153 - val_acc: 0.9059\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2885 - f_2: 0.4421 - acc: 0.9061 - val_loss: 0.2884 - val_f_2: 0.4284 - val_acc: 0.9044\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2882 - f_2: 0.4352 - acc: 0.9045 - val_loss: 0.2904 - val_f_2: 0.4422 - val_acc: 0.9032\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=15, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6255 - f_2: 0.0078 - acc: 0.8658 - val_loss: 0.3943 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3774 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3615 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3543 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3481 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3434 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3390 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3378 - f_2: 0.0148 - acc: 0.8753 - val_loss: 0.3350 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3324 - f_2: 0.0348 - acc: 0.8782 - val_loss: 0.3312 - val_f_2: 0.0199 - val_acc: 0.8690\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.1793 - acc: 0.8935 - val_loss: 0.3285 - val_f_2: 0.2704 - val_acc: 0.8997\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2236 - acc: 0.8979 - val_loss: 0.3314 - val_f_2: 0.2537 - val_acc: 0.8982\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2321 - acc: 0.8984 - val_loss: 0.3239 - val_f_2: 0.2820 - val_acc: 0.8991\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3230 - f_2: 0.2647 - acc: 0.9006 - val_loss: 0.3260 - val_f_2: 0.2767 - val_acc: 0.8988\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3224 - f_2: 0.2561 - acc: 0.9019 - val_loss: 0.3214 - val_f_2: 0.2734 - val_acc: 0.8988\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.2596 - acc: 0.9027 - val_loss: 0.3276 - val_f_2: 0.3369 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.2741 - acc: 0.9010 - val_loss: 0.3214 - val_f_2: 0.3113 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.2879 - acc: 0.9017 - val_loss: 0.3187 - val_f_2: 0.3049 - val_acc: 0.9003\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.2888 - acc: 0.9026 - val_loss: 0.3185 - val_f_2: 0.3048 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.2947 - acc: 0.9032 - val_loss: 0.3175 - val_f_2: 0.3000 - val_acc: 0.9009\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.3001 - acc: 0.9034 - val_loss: 0.3176 - val_f_2: 0.3221 - val_acc: 0.9006\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3142 - f_2: 0.3025 - acc: 0.9040 - val_loss: 0.3143 - val_f_2: 0.3393 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3111 - acc: 0.9049 - val_loss: 0.3158 - val_f_2: 0.3947 - val_acc: 0.9009\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.2993 - acc: 0.9026 - val_loss: 0.3264 - val_f_2: 0.4020 - val_acc: 0.9027\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.3165 - acc: 0.9031 - val_loss: 0.3153 - val_f_2: 0.3284 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3365 - acc: 0.9046 - val_loss: 0.3130 - val_f_2: 0.3453 - val_acc: 0.8997\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3164 - acc: 0.9046 - val_loss: 0.3119 - val_f_2: 0.3397 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3268 - acc: 0.9052 - val_loss: 0.3166 - val_f_2: 0.3853 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.3251 - acc: 0.9044 - val_loss: 0.3125 - val_f_2: 0.3087 - val_acc: 0.9015\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3244 - acc: 0.9057 - val_loss: 0.3113 - val_f_2: 0.3817 - val_acc: 0.9012\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3542 - acc: 0.9050 - val_loss: 0.3120 - val_f_2: 0.3199 - val_acc: 0.9015\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3262 - acc: 0.9041 - val_loss: 0.3145 - val_f_2: 0.3611 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3078 - f_2: 0.3249 - acc: 0.9043 - val_loss: 0.3130 - val_f_2: 0.4045 - val_acc: 0.9009\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3591 - acc: 0.9054 - val_loss: 0.3100 - val_f_2: 0.2976 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3249 - acc: 0.9053 - val_loss: 0.3117 - val_f_2: 0.3556 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3392 - acc: 0.9057 - val_loss: 0.3091 - val_f_2: 0.3409 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3336 - acc: 0.9043 - val_loss: 0.3097 - val_f_2: 0.3847 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3316 - acc: 0.9046 - val_loss: 0.3103 - val_f_2: 0.3683 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3556 - acc: 0.9050 - val_loss: 0.3096 - val_f_2: 0.3748 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3417 - acc: 0.9054 - val_loss: 0.3111 - val_f_2: 0.3899 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3475 - acc: 0.9060 - val_loss: 0.3142 - val_f_2: 0.3051 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3493 - acc: 0.9054 - val_loss: 0.3169 - val_f_2: 0.3155 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3444 - acc: 0.9052 - val_loss: 0.3157 - val_f_2: 0.3742 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3014 - f_2: 0.3503 - acc: 0.9067 - val_loss: 0.3152 - val_f_2: 0.3489 - val_acc: 0.9009\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3473 - acc: 0.9060 - val_loss: 0.3089 - val_f_2: 0.3610 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3392 - acc: 0.9054 - val_loss: 0.3076 - val_f_2: 0.3847 - val_acc: 0.9015\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.3414 - acc: 0.9051 - val_loss: 0.3099 - val_f_2: 0.3427 - val_acc: 0.8994\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2992 - f_2: 0.3615 - acc: 0.9059 - val_loss: 0.3098 - val_f_2: 0.3816 - val_acc: 0.9003\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3592 - acc: 0.9066 - val_loss: 0.3083 - val_f_2: 0.3787 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.3725 - acc: 0.9070 - val_loss: 0.3089 - val_f_2: 0.3899 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.3492 - acc: 0.9065 - val_loss: 0.3119 - val_f_2: 0.4319 - val_acc: 0.9021\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.3577 - acc: 0.9065 - val_loss: 0.3085 - val_f_2: 0.3926 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3597 - acc: 0.9057 - val_loss: 0.3151 - val_f_2: 0.3949 - val_acc: 0.9006\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.3569 - acc: 0.9073 - val_loss: 0.3068 - val_f_2: 0.3813 - val_acc: 0.9009\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6238 - f_2: 0.0114 - acc: 0.8641 - val_loss: 0.3962 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3758 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3582 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3552 - f_2: 3.0821e-08 - acc: 0.8722 - val_loss: 0.3458 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3448 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3383 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3373 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3335 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3334 - f_2: 0.1816 - acc: 0.8902 - val_loss: 0.3309 - val_f_2: 0.2684 - val_acc: 0.8994\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3305 - f_2: 0.2216 - acc: 0.8956 - val_loss: 0.3267 - val_f_2: 0.2737 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.2367 - acc: 0.8960 - val_loss: 0.3252 - val_f_2: 0.2755 - val_acc: 0.8994\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3254 - f_2: 0.2604 - acc: 0.8994 - val_loss: 0.3240 - val_f_2: 0.3151 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2876 - acc: 0.9006 - val_loss: 0.3220 - val_f_2: 0.2884 - val_acc: 0.8991\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.2833 - acc: 0.9018 - val_loss: 0.3216 - val_f_2: 0.2736 - val_acc: 0.8994\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2927 - acc: 0.9006 - val_loss: 0.3247 - val_f_2: 0.2886 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.2909 - acc: 0.9009 - val_loss: 0.3180 - val_f_2: 0.3318 - val_acc: 0.9015\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.3060 - acc: 0.9021 - val_loss: 0.3192 - val_f_2: 0.2736 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.3085 - acc: 0.9030 - val_loss: 0.3154 - val_f_2: 0.3491 - val_acc: 0.9029\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.3126 - acc: 0.9028 - val_loss: 0.3144 - val_f_2: 0.3047 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.2991 - acc: 0.9026 - val_loss: 0.3186 - val_f_2: 0.3014 - val_acc: 0.9006\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3243 - acc: 0.9019 - val_loss: 0.3167 - val_f_2: 0.3407 - val_acc: 0.9018\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3370 - acc: 0.9041 - val_loss: 0.3127 - val_f_2: 0.3423 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3296 - acc: 0.9035 - val_loss: 0.3141 - val_f_2: 0.3873 - val_acc: 0.9018\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3227 - acc: 0.9021 - val_loss: 0.3100 - val_f_2: 0.3399 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3404 - acc: 0.9040 - val_loss: 0.3106 - val_f_2: 0.3487 - val_acc: 0.9018\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3339 - acc: 0.9034 - val_loss: 0.3125 - val_f_2: 0.3230 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3384 - acc: 0.9034 - val_loss: 0.3121 - val_f_2: 0.3267 - val_acc: 0.9003\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3205 - acc: 0.9020 - val_loss: 0.3093 - val_f_2: 0.3366 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3245 - acc: 0.9040 - val_loss: 0.3097 - val_f_2: 0.3463 - val_acc: 0.9015\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3543 - acc: 0.9057 - val_loss: 0.3105 - val_f_2: 0.3912 - val_acc: 0.9021\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3072 - f_2: 0.3365 - acc: 0.9049 - val_loss: 0.3084 - val_f_2: 0.3458 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3501 - acc: 0.9041 - val_loss: 0.3091 - val_f_2: 0.3228 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3290 - acc: 0.9034 - val_loss: 0.3061 - val_f_2: 0.3405 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3465 - acc: 0.9048 - val_loss: 0.3062 - val_f_2: 0.3427 - val_acc: 0.9012\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3677 - acc: 0.9061 - val_loss: 0.3078 - val_f_2: 0.4129 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3597 - acc: 0.9046 - val_loss: 0.3052 - val_f_2: 0.3537 - val_acc: 0.9018\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3397 - acc: 0.9037 - val_loss: 0.3070 - val_f_2: 0.3813 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3573 - acc: 0.9047 - val_loss: 0.3077 - val_f_2: 0.3516 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3670 - acc: 0.9040 - val_loss: 0.3069 - val_f_2: 0.4226 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3032 - f_2: 0.3584 - acc: 0.9047 - val_loss: 0.3075 - val_f_2: 0.4333 - val_acc: 0.9021\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3744 - acc: 0.9057 - val_loss: 0.3045 - val_f_2: 0.3611 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3564 - acc: 0.9051 - val_loss: 0.3067 - val_f_2: 0.4074 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.3632 - acc: 0.9039 - val_loss: 0.3059 - val_f_2: 0.3779 - val_acc: 0.9009\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3659 - acc: 0.9060 - val_loss: 0.3064 - val_f_2: 0.3932 - val_acc: 0.9015\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3871 - acc: 0.9074 - val_loss: 0.3061 - val_f_2: 0.4131 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3014 - f_2: 0.3553 - acc: 0.9048 - val_loss: 0.3053 - val_f_2: 0.3925 - val_acc: 0.9027\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3645 - acc: 0.9054 - val_loss: 0.3036 - val_f_2: 0.3246 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.3832 - acc: 0.9065 - val_loss: 0.3069 - val_f_2: 0.4266 - val_acc: 0.9024\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2977 - f_2: 0.3534 - acc: 0.9045 - val_loss: 0.3036 - val_f_2: 0.4030 - val_acc: 0.9029\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.3568 - acc: 0.9048 - val_loss: 0.3051 - val_f_2: 0.4258 - val_acc: 0.9024\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3928 - acc: 0.9059 - val_loss: 0.3047 - val_f_2: 0.3214 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3728 - acc: 0.9059 - val_loss: 0.3088 - val_f_2: 0.3571 - val_acc: 0.9021\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3570 - acc: 0.9041 - val_loss: 0.3089 - val_f_2: 0.3741 - val_acc: 0.9018\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6305 - f_2: 0.0149 - acc: 0.8606 - val_loss: 0.3744 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3835 - f_2: 3.0107e-08 - acc: 0.8691 - val_loss: 0.3390 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3606 - f_2: 3.0234e-08 - acc: 0.8691 - val_loss: 0.3267 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3494 - f_2: 0.0089 - acc: 0.8691 - val_loss: 0.3214 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3440 - f_2: 2.9784e-08 - acc: 0.8691 - val_loss: 0.3152 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3402 - f_2: 0.1250 - acc: 0.8821 - val_loss: 0.3126 - val_f_2: 0.2185 - val_acc: 0.9021\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3381 - f_2: 0.1929 - acc: 0.8906 - val_loss: 0.3107 - val_f_2: 0.2947 - val_acc: 0.9103\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3352 - f_2: 0.2127 - acc: 0.8928 - val_loss: 0.3102 - val_f_2: 0.3073 - val_acc: 0.9115\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3321 - f_2: 0.2480 - acc: 0.8956 - val_loss: 0.3053 - val_f_2: 0.2960 - val_acc: 0.9103\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3296 - f_2: 0.2645 - acc: 0.8965 - val_loss: 0.3036 - val_f_2: 0.2972 - val_acc: 0.9112\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2678 - acc: 0.8967 - val_loss: 0.3017 - val_f_2: 0.2960 - val_acc: 0.9103\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3259 - f_2: 0.2822 - acc: 0.8980 - val_loss: 0.3020 - val_f_2: 0.3210 - val_acc: 0.9088\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3254 - f_2: 0.2760 - acc: 0.8968 - val_loss: 0.3001 - val_f_2: 0.3111 - val_acc: 0.9118\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.2910 - acc: 0.8994 - val_loss: 0.2994 - val_f_2: 0.3191 - val_acc: 0.9091\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3225 - f_2: 0.2868 - acc: 0.8973 - val_loss: 0.3009 - val_f_2: 0.3129 - val_acc: 0.9112\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.3061 - acc: 0.8987 - val_loss: 0.2970 - val_f_2: 0.3145 - val_acc: 0.9100\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.3126 - acc: 0.8984 - val_loss: 0.2965 - val_f_2: 0.3152 - val_acc: 0.9094\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.3214 - acc: 0.8993 - val_loss: 0.2940 - val_f_2: 0.2992 - val_acc: 0.9106\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.3092 - acc: 0.8987 - val_loss: 0.2981 - val_f_2: 0.3517 - val_acc: 0.9071\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3220 - acc: 0.8995 - val_loss: 0.2937 - val_f_2: 0.3014 - val_acc: 0.9106\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.3083 - acc: 0.8988 - val_loss: 0.2936 - val_f_2: 0.3109 - val_acc: 0.9118\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.3026 - acc: 0.8975 - val_loss: 0.2946 - val_f_2: 0.3381 - val_acc: 0.9080\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.3244 - acc: 0.8994 - val_loss: 0.2933 - val_f_2: 0.3205 - val_acc: 0.9097\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.3205 - acc: 0.8993 - val_loss: 0.2935 - val_f_2: 0.3245 - val_acc: 0.9083\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3265 - acc: 0.8985 - val_loss: 0.2918 - val_f_2: 0.3203 - val_acc: 0.9083\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3135 - f_2: 0.3277 - acc: 0.8987 - val_loss: 0.2925 - val_f_2: 0.3851 - val_acc: 0.9088\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3316 - acc: 0.8992 - val_loss: 0.2952 - val_f_2: 0.3366 - val_acc: 0.9103\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.3304 - acc: 0.8992 - val_loss: 0.3033 - val_f_2: 0.3785 - val_acc: 0.9080\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3490 - acc: 0.8996 - val_loss: 0.2902 - val_f_2: 0.3540 - val_acc: 0.9091\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.3412 - acc: 0.9004 - val_loss: 0.2899 - val_f_2: 0.3083 - val_acc: 0.9115\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3312 - acc: 0.8993 - val_loss: 0.2893 - val_f_2: 0.3441 - val_acc: 0.9080\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3395 - acc: 0.8988 - val_loss: 0.2894 - val_f_2: 0.3726 - val_acc: 0.9086\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.3466 - acc: 0.9007 - val_loss: 0.2943 - val_f_2: 0.4058 - val_acc: 0.9091\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3352 - acc: 0.8995 - val_loss: 0.2898 - val_f_2: 0.3942 - val_acc: 0.9088\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3593 - acc: 0.9012 - val_loss: 0.2896 - val_f_2: 0.3480 - val_acc: 0.9083\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3357 - acc: 0.9006 - val_loss: 0.2899 - val_f_2: 0.3328 - val_acc: 0.9080\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.3513 - acc: 0.9012 - val_loss: 0.2946 - val_f_2: 0.4109 - val_acc: 0.9080\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3436 - acc: 0.8989 - val_loss: 0.2883 - val_f_2: 0.3341 - val_acc: 0.9088\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3421 - acc: 0.8996 - val_loss: 0.2905 - val_f_2: 0.3626 - val_acc: 0.9094\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3081 - f_2: 0.3515 - acc: 0.9009 - val_loss: 0.2880 - val_f_2: 0.3830 - val_acc: 0.9086\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3062 - f_2: 0.3564 - acc: 0.9006 - val_loss: 0.2869 - val_f_2: 0.3376 - val_acc: 0.9086\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3400 - acc: 0.8984 - val_loss: 0.2936 - val_f_2: 0.4289 - val_acc: 0.9065\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.3412 - acc: 0.8999 - val_loss: 0.2910 - val_f_2: 0.3898 - val_acc: 0.9065\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3498 - acc: 0.9015 - val_loss: 0.2902 - val_f_2: 0.4088 - val_acc: 0.9074\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3050 - f_2: 0.3687 - acc: 0.9032 - val_loss: 0.2923 - val_f_2: 0.3183 - val_acc: 0.9112\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3683 - acc: 0.9012 - val_loss: 0.2871 - val_f_2: 0.3852 - val_acc: 0.9086\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3602 - acc: 0.9018 - val_loss: 0.2866 - val_f_2: 0.3513 - val_acc: 0.9106\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3606 - acc: 0.9018 - val_loss: 0.2924 - val_f_2: 0.4489 - val_acc: 0.9068\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3524 - acc: 0.8990 - val_loss: 0.2873 - val_f_2: 0.3226 - val_acc: 0.9106\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3521 - acc: 0.9009 - val_loss: 0.2904 - val_f_2: 0.4119 - val_acc: 0.9062\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4268 - f_2: 0.0121 - acc: 0.8672 - val_loss: 0.3616 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3536 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3394 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3377 - f_2: 3.0842e-08 - acc: 0.8753 - val_loss: 0.3304 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.1917 - acc: 0.8958 - val_loss: 0.3318 - val_f_2: 0.2734 - val_acc: 0.8994\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2780 - acc: 0.9031 - val_loss: 0.3219 - val_f_2: 0.2919 - val_acc: 0.9003\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3190 - f_2: 0.2850 - acc: 0.9037 - val_loss: 0.3192 - val_f_2: 0.3192 - val_acc: 0.9006\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.3109 - acc: 0.9048 - val_loss: 0.3180 - val_f_2: 0.3237 - val_acc: 0.9009\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3130 - f_2: 0.3265 - acc: 0.9056 - val_loss: 0.3158 - val_f_2: 0.3421 - val_acc: 0.9015\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3122 - f_2: 0.3380 - acc: 0.9052 - val_loss: 0.3151 - val_f_2: 0.3297 - val_acc: 0.9015\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3354 - acc: 0.9062 - val_loss: 0.3133 - val_f_2: 0.3663 - val_acc: 0.9021\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.3506 - acc: 0.9062 - val_loss: 0.3116 - val_f_2: 0.3547 - val_acc: 0.9015\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3058 - f_2: 0.3539 - acc: 0.9059 - val_loss: 0.3129 - val_f_2: 0.3525 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3038 - f_2: 0.3585 - acc: 0.9063 - val_loss: 0.3118 - val_f_2: 0.3582 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3674 - acc: 0.9068 - val_loss: 0.3117 - val_f_2: 0.3544 - val_acc: 0.9018\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.3780 - acc: 0.9068 - val_loss: 0.3193 - val_f_2: 0.3603 - val_acc: 0.9021\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3723 - acc: 0.9075 - val_loss: 0.3105 - val_f_2: 0.4002 - val_acc: 0.9021\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2971 - f_2: 0.3719 - acc: 0.9070 - val_loss: 0.3139 - val_f_2: 0.3844 - val_acc: 0.9015\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3005 - f_2: 0.3984 - acc: 0.9082 - val_loss: 0.3104 - val_f_2: 0.3673 - val_acc: 0.9018\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2960 - f_2: 0.3877 - acc: 0.9091 - val_loss: 0.3110 - val_f_2: 0.4017 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.4019 - acc: 0.9094 - val_loss: 0.3159 - val_f_2: 0.3689 - val_acc: 0.9021\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2954 - f_2: 0.3865 - acc: 0.9085 - val_loss: 0.3111 - val_f_2: 0.3889 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.3786 - acc: 0.9090 - val_loss: 0.3113 - val_f_2: 0.3994 - val_acc: 0.9024\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3931 - acc: 0.9081 - val_loss: 0.3113 - val_f_2: 0.3844 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2934 - f_2: 0.3856 - acc: 0.9103 - val_loss: 0.3107 - val_f_2: 0.4124 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2931 - f_2: 0.3924 - acc: 0.9090 - val_loss: 0.3104 - val_f_2: 0.4035 - val_acc: 0.9015\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.3892 - acc: 0.9093 - val_loss: 0.3155 - val_f_2: 0.3898 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3902 - acc: 0.9088 - val_loss: 0.3158 - val_f_2: 0.3900 - val_acc: 0.9015\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2921 - f_2: 0.4066 - acc: 0.9092 - val_loss: 0.3093 - val_f_2: 0.4130 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2916 - f_2: 0.4010 - acc: 0.9092 - val_loss: 0.3129 - val_f_2: 0.3940 - val_acc: 0.9024\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2919 - f_2: 0.3949 - acc: 0.9088 - val_loss: 0.3123 - val_f_2: 0.3939 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2919 - f_2: 0.3945 - acc: 0.9096 - val_loss: 0.3121 - val_f_2: 0.3948 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2906 - f_2: 0.4120 - acc: 0.9100 - val_loss: 0.3159 - val_f_2: 0.3900 - val_acc: 0.9012\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.4039 - acc: 0.9096 - val_loss: 0.3093 - val_f_2: 0.3997 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2891 - f_2: 0.4142 - acc: 0.9105 - val_loss: 0.3110 - val_f_2: 0.3942 - val_acc: 0.9012\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2919 - f_2: 0.4107 - acc: 0.9105 - val_loss: 0.3321 - val_f_2: 0.3639 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2884 - f_2: 0.4016 - acc: 0.9093 - val_loss: 0.3107 - val_f_2: 0.4158 - val_acc: 0.8991\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.4113 - acc: 0.9093 - val_loss: 0.3112 - val_f_2: 0.3928 - val_acc: 0.9012\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2866 - f_2: 0.4028 - acc: 0.9119 - val_loss: 0.3133 - val_f_2: 0.3937 - val_acc: 0.8994\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.4020 - acc: 0.9099 - val_loss: 0.3303 - val_f_2: 0.3868 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.4119 - acc: 0.9092 - val_loss: 0.3096 - val_f_2: 0.4054 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2900 - f_2: 0.4006 - acc: 0.9111 - val_loss: 0.3100 - val_f_2: 0.4066 - val_acc: 0.9003\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2889 - f_2: 0.4095 - acc: 0.9085 - val_loss: 0.3329 - val_f_2: 0.3625 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2891 - f_2: 0.4170 - acc: 0.9104 - val_loss: 0.3131 - val_f_2: 0.3907 - val_acc: 0.9009\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.4133 - acc: 0.9104 - val_loss: 0.3136 - val_f_2: 0.3995 - val_acc: 0.9012\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2879 - f_2: 0.3953 - acc: 0.9089 - val_loss: 0.3325 - val_f_2: 0.3931 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2887 - f_2: 0.4312 - acc: 0.9112 - val_loss: 0.3120 - val_f_2: 0.4223 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.4078 - acc: 0.9096 - val_loss: 0.3327 - val_f_2: 0.3637 - val_acc: 0.9012\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2874 - f_2: 0.4110 - acc: 0.9092 - val_loss: 0.3118 - val_f_2: 0.4007 - val_acc: 0.9009\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2866 - f_2: 0.4191 - acc: 0.9110 - val_loss: 0.3236 - val_f_2: 0.3935 - val_acc: 0.9024\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.4145 - acc: 0.9099 - val_loss: 0.3147 - val_f_2: 0.4023 - val_acc: 0.9029\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4275 - f_2: 0.0149 - acc: 0.8652 - val_loss: 0.3636 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3567 - f_2: 0.0235 - acc: 0.8742 - val_loss: 0.3409 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3372 - f_2: 0.2045 - acc: 0.8946 - val_loss: 0.3297 - val_f_2: 0.2738 - val_acc: 0.8985\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3281 - f_2: 0.2869 - acc: 0.9016 - val_loss: 0.3218 - val_f_2: 0.2941 - val_acc: 0.9003\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.3153 - acc: 0.9029 - val_loss: 0.3157 - val_f_2: 0.3029 - val_acc: 0.9009\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3381 - acc: 0.9035 - val_loss: 0.3127 - val_f_2: 0.3250 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3510 - acc: 0.9048 - val_loss: 0.3082 - val_f_2: 0.3870 - val_acc: 0.9006\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3062 - f_2: 0.3637 - acc: 0.9058 - val_loss: 0.3051 - val_f_2: 0.4076 - val_acc: 0.9015\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.3768 - acc: 0.9062 - val_loss: 0.3053 - val_f_2: 0.3937 - val_acc: 0.9018\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.3664 - acc: 0.9048 - val_loss: 0.3058 - val_f_2: 0.3446 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2978 - f_2: 0.3734 - acc: 0.9050 - val_loss: 0.3000 - val_f_2: 0.4094 - val_acc: 0.9015\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.3685 - acc: 0.9049 - val_loss: 0.3067 - val_f_2: 0.3320 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2958 - f_2: 0.3814 - acc: 0.9055 - val_loss: 0.2997 - val_f_2: 0.3808 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2951 - f_2: 0.3518 - acc: 0.9038 - val_loss: 0.3002 - val_f_2: 0.4074 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2922 - f_2: 0.3898 - acc: 0.9082 - val_loss: 0.2969 - val_f_2: 0.4160 - val_acc: 0.9018\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2942 - f_2: 0.3654 - acc: 0.9050 - val_loss: 0.2983 - val_f_2: 0.4105 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2878 - f_2: 0.3745 - acc: 0.9054 - val_loss: 0.3047 - val_f_2: 0.3743 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2913 - f_2: 0.3791 - acc: 0.9035 - val_loss: 0.2971 - val_f_2: 0.3766 - val_acc: 0.9006\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2899 - f_2: 0.3674 - acc: 0.9051 - val_loss: 0.2993 - val_f_2: 0.4041 - val_acc: 0.9018\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.3506 - acc: 0.9051 - val_loss: 0.2972 - val_f_2: 0.4063 - val_acc: 0.9018\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2873 - f_2: 0.3733 - acc: 0.9059 - val_loss: 0.2993 - val_f_2: 0.4415 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.3916 - acc: 0.9074 - val_loss: 0.2980 - val_f_2: 0.4173 - val_acc: 0.9021\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2878 - f_2: 0.3853 - acc: 0.9053 - val_loss: 0.3095 - val_f_2: 0.3466 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2873 - f_2: 0.3805 - acc: 0.9075 - val_loss: 0.2992 - val_f_2: 0.4060 - val_acc: 0.9015\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.3984 - acc: 0.9078 - val_loss: 0.2979 - val_f_2: 0.4127 - val_acc: 0.9006\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2824 - f_2: 0.3857 - acc: 0.9075 - val_loss: 0.3023 - val_f_2: 0.3647 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.3840 - acc: 0.9068 - val_loss: 0.3004 - val_f_2: 0.4027 - val_acc: 0.9024\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2851 - f_2: 0.3902 - acc: 0.9082 - val_loss: 0.3021 - val_f_2: 0.4145 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.3978 - acc: 0.9074 - val_loss: 0.2991 - val_f_2: 0.4273 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2831 - f_2: 0.3935 - acc: 0.9073 - val_loss: 0.3001 - val_f_2: 0.4220 - val_acc: 0.9018\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2828 - f_2: 0.3992 - acc: 0.9085 - val_loss: 0.2980 - val_f_2: 0.4175 - val_acc: 0.9012\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2820 - f_2: 0.3849 - acc: 0.9076 - val_loss: 0.3017 - val_f_2: 0.4109 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.3816 - acc: 0.9071 - val_loss: 0.2966 - val_f_2: 0.3855 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.3960 - acc: 0.9077 - val_loss: 0.2983 - val_f_2: 0.4002 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.3872 - acc: 0.9073 - val_loss: 0.3002 - val_f_2: 0.4448 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4090 - acc: 0.9085 - val_loss: 0.2990 - val_f_2: 0.3832 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.3929 - acc: 0.9081 - val_loss: 0.3024 - val_f_2: 0.3862 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2819 - f_2: 0.3911 - acc: 0.9088 - val_loss: 0.3002 - val_f_2: 0.4217 - val_acc: 0.8994\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2798 - f_2: 0.4059 - acc: 0.9079 - val_loss: 0.3031 - val_f_2: 0.3872 - val_acc: 0.8982\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2806 - f_2: 0.4173 - acc: 0.9094 - val_loss: 0.3021 - val_f_2: 0.4268 - val_acc: 0.9003\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2823 - f_2: 0.3995 - acc: 0.9071 - val_loss: 0.3039 - val_f_2: 0.3951 - val_acc: 0.9006\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.4002 - acc: 0.9082 - val_loss: 0.3015 - val_f_2: 0.4029 - val_acc: 0.8991\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2800 - f_2: 0.4102 - acc: 0.9101 - val_loss: 0.3076 - val_f_2: 0.4174 - val_acc: 0.9009\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2829 - f_2: 0.4068 - acc: 0.9087 - val_loss: 0.3015 - val_f_2: 0.4236 - val_acc: 0.8988\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2811 - f_2: 0.3975 - acc: 0.9079 - val_loss: 0.3019 - val_f_2: 0.4154 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.3851 - acc: 0.9073 - val_loss: 0.3022 - val_f_2: 0.4086 - val_acc: 0.8997\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2767 - f_2: 0.4053 - acc: 0.9112 - val_loss: 0.3013 - val_f_2: 0.3981 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2812 - f_2: 0.3850 - acc: 0.9076 - val_loss: 0.3006 - val_f_2: 0.4272 - val_acc: 0.9009\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2792 - f_2: 0.4014 - acc: 0.9103 - val_loss: 0.3030 - val_f_2: 0.4054 - val_acc: 0.8997\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2782 - f_2: 0.3981 - acc: 0.9091 - val_loss: 0.3060 - val_f_2: 0.4059 - val_acc: 0.9006\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4346 - f_2: 0.0080 - acc: 0.8618 - val_loss: 0.3395 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3615 - f_2: 3.0318e-08 - acc: 0.8691 - val_loss: 0.3196 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3458 - f_2: 0.0355 - acc: 0.8729 - val_loss: 0.3217 - val_f_2: 0.2905 - val_acc: 0.9083\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3365 - f_2: 0.2414 - acc: 0.8955 - val_loss: 0.3087 - val_f_2: 0.2959 - val_acc: 0.9094\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.2809 - acc: 0.8990 - val_loss: 0.3069 - val_f_2: 0.3047 - val_acc: 0.9100\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.2912 - acc: 0.9002 - val_loss: 0.3034 - val_f_2: 0.3418 - val_acc: 0.9121\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.3161 - acc: 0.9006 - val_loss: 0.2997 - val_f_2: 0.3420 - val_acc: 0.9106\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.3208 - acc: 0.9012 - val_loss: 0.3026 - val_f_2: 0.3533 - val_acc: 0.9091\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.3393 - acc: 0.9021 - val_loss: 0.2961 - val_f_2: 0.3467 - val_acc: 0.9103\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.3433 - acc: 0.9012 - val_loss: 0.2947 - val_f_2: 0.3466 - val_acc: 0.9109\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3499 - acc: 0.9021 - val_loss: 0.2966 - val_f_2: 0.3773 - val_acc: 0.9109\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3625 - acc: 0.9032 - val_loss: 0.2935 - val_f_2: 0.3670 - val_acc: 0.9109\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3090 - f_2: 0.3599 - acc: 0.9013 - val_loss: 0.2943 - val_f_2: 0.3529 - val_acc: 0.9103\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.3589 - acc: 0.9009 - val_loss: 0.3123 - val_f_2: 0.3300 - val_acc: 0.9097\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.3667 - acc: 0.9023 - val_loss: 0.2917 - val_f_2: 0.3627 - val_acc: 0.9115\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3713 - acc: 0.9022 - val_loss: 0.2922 - val_f_2: 0.3598 - val_acc: 0.9103\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3652 - acc: 0.9031 - val_loss: 0.2937 - val_f_2: 0.3511 - val_acc: 0.9097\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3799 - acc: 0.9031 - val_loss: 0.2907 - val_f_2: 0.3956 - val_acc: 0.9115\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3849 - acc: 0.9028 - val_loss: 0.2909 - val_f_2: 0.3698 - val_acc: 0.9097\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3929 - acc: 0.9039 - val_loss: 0.2954 - val_f_2: 0.4143 - val_acc: 0.9091\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3050 - f_2: 0.4007 - acc: 0.9034 - val_loss: 0.2907 - val_f_2: 0.4099 - val_acc: 0.9121\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.4050 - acc: 0.9040 - val_loss: 0.2922 - val_f_2: 0.4329 - val_acc: 0.9097\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.4025 - acc: 0.9035 - val_loss: 0.2909 - val_f_2: 0.3928 - val_acc: 0.9112\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.4032 - acc: 0.9033 - val_loss: 0.2939 - val_f_2: 0.4066 - val_acc: 0.9091\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.3999 - acc: 0.9038 - val_loss: 0.2925 - val_f_2: 0.4059 - val_acc: 0.9088\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.3974 - acc: 0.9046 - val_loss: 0.2909 - val_f_2: 0.3983 - val_acc: 0.9086\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3980 - acc: 0.9026 - val_loss: 0.2917 - val_f_2: 0.4104 - val_acc: 0.9083\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.4131 - acc: 0.9049 - val_loss: 0.2899 - val_f_2: 0.3971 - val_acc: 0.9088\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.4076 - acc: 0.9044 - val_loss: 0.2902 - val_f_2: 0.4034 - val_acc: 0.9086\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.4046 - acc: 0.9053 - val_loss: 0.2946 - val_f_2: 0.3958 - val_acc: 0.9118\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.4165 - acc: 0.9051 - val_loss: 0.2898 - val_f_2: 0.3923 - val_acc: 0.9103\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.4104 - acc: 0.9046 - val_loss: 0.2931 - val_f_2: 0.3869 - val_acc: 0.9106\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.4049 - acc: 0.9036 - val_loss: 0.2909 - val_f_2: 0.3941 - val_acc: 0.9097\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.4207 - acc: 0.9049 - val_loss: 0.2927 - val_f_2: 0.4038 - val_acc: 0.9100\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2996 - f_2: 0.4251 - acc: 0.9050 - val_loss: 0.2972 - val_f_2: 0.3898 - val_acc: 0.9109\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2964 - f_2: 0.4148 - acc: 0.9040 - val_loss: 0.2941 - val_f_2: 0.4192 - val_acc: 0.9065\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2941 - f_2: 0.4257 - acc: 0.9064 - val_loss: 0.3070 - val_f_2: 0.3794 - val_acc: 0.9103\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2947 - f_2: 0.4305 - acc: 0.9046 - val_loss: 0.2960 - val_f_2: 0.4138 - val_acc: 0.9050\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.4218 - acc: 0.9057 - val_loss: 0.2945 - val_f_2: 0.4077 - val_acc: 0.9044\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2942 - f_2: 0.4267 - acc: 0.9051 - val_loss: 0.2938 - val_f_2: 0.3930 - val_acc: 0.9065\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2941 - f_2: 0.4228 - acc: 0.9051 - val_loss: 0.2925 - val_f_2: 0.4001 - val_acc: 0.9065\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.4214 - acc: 0.9047 - val_loss: 0.2925 - val_f_2: 0.4043 - val_acc: 0.9071\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2947 - f_2: 0.4221 - acc: 0.9049 - val_loss: 0.2926 - val_f_2: 0.4018 - val_acc: 0.9077\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2927 - f_2: 0.4247 - acc: 0.9052 - val_loss: 0.2939 - val_f_2: 0.3930 - val_acc: 0.9077\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2954 - f_2: 0.4520 - acc: 0.9063 - val_loss: 0.3121 - val_f_2: 0.3761 - val_acc: 0.9118\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2936 - f_2: 0.4447 - acc: 0.9064 - val_loss: 0.2948 - val_f_2: 0.3805 - val_acc: 0.9059\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2955 - f_2: 0.4297 - acc: 0.9048 - val_loss: 0.2947 - val_f_2: 0.3896 - val_acc: 0.9056\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2935 - f_2: 0.4410 - acc: 0.9053 - val_loss: 0.2925 - val_f_2: 0.4070 - val_acc: 0.9056\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2949 - f_2: 0.4417 - acc: 0.9060 - val_loss: 0.2969 - val_f_2: 0.4087 - val_acc: 0.9050\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2941 - f_2: 0.4361 - acc: 0.9059 - val_loss: 0.2985 - val_f_2: 0.3794 - val_acc: 0.9050\n",
      "[CV]  batch_size=40, dropout=0.2, nodes=20, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6124 - f_2: 0.0221 - acc: 0.8558 - val_loss: 0.4198 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3965 - f_2: 3.1690e-08 - acc: 0.8753 - val_loss: 0.3737 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3719 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3584 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3610 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3485 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3527 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3439 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3512 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3410 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3450 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3374 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3454 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3389 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3437 - f_2: 3.0953e-08 - acc: 0.8753 - val_loss: 0.3342 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3392 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3342 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3414 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3341 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3391 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3320 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3413 - f_2: 3.3216e-08 - acc: 0.8753 - val_loss: 0.3315 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3386 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3314 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3394 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3301 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3340 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3284 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3315 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3283 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3350 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3279 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3360 - f_2: 3.1748e-08 - acc: 0.8753 - val_loss: 0.3287 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3326 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3265 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3360 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3266 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3337 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3282 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3327 - f_2: 0.0057 - acc: 0.8753 - val_loss: 0.3328 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3312 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3254 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3263 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3274 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3285 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3313 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3268 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3252 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3285 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3294 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3243 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3229 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3240 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3281 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3226 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3292 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3272 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3303 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3270 - f_2: 0.0177 - acc: 0.8753 - val_loss: 0.3230 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3241 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3268 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3223 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3276 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3213 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3302 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3246 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3224 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3240 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.1300 - acc: 0.8875 - val_loss: 0.3213 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.1601 - acc: 0.8895 - val_loss: 0.3222 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.1409 - acc: 0.8873 - val_loss: 0.3215 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3245 - f_2: 0.1908 - acc: 0.8913 - val_loss: 0.3219 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.1379 - acc: 0.8891 - val_loss: 0.3244 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.1906 - acc: 0.8919 - val_loss: 0.3232 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.1923 - acc: 0.8923 - val_loss: 0.3220 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6147 - f_2: 0.0173 - acc: 0.8535 - val_loss: 0.4179 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4019 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3722 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3733 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3565 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3592 - f_2: 0.0118 - acc: 0.8722 - val_loss: 0.3462 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3526 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3424 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3502 - f_2: 3.0344e-08 - acc: 0.8722 - val_loss: 0.3405 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3469 - f_2: 2.9678e-08 - acc: 0.8722 - val_loss: 0.3355 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3454 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3340 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3436 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3327 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3396 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3308 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3402 - f_2: 2.9724e-08 - acc: 0.8722 - val_loss: 0.3293 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3408 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3290 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3408 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3278 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3384 - f_2: 3.0664e-08 - acc: 0.8722 - val_loss: 0.3277 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3391 - f_2: 3.0393e-08 - acc: 0.8722 - val_loss: 0.3264 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3373 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3279 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3364 - f_2: 3.1071e-08 - acc: 0.8722 - val_loss: 0.3266 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3335 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3237 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3347 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3242 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3369 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3222 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3352 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3243 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3325 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3227 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3322 - f_2: 3.0459e-08 - acc: 0.8722 - val_loss: 0.3210 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3312 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3235 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3328 - f_2: 3.0755e-08 - acc: 0.8722 - val_loss: 0.3212 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3338 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3207 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3287 - f_2: 0.0882 - acc: 0.8795 - val_loss: 0.3199 - val_f_2: 0.2638 - val_acc: 0.8994\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3315 - f_2: 0.2174 - acc: 0.8910 - val_loss: 0.3199 - val_f_2: 0.2723 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3294 - f_2: 0.2209 - acc: 0.8924 - val_loss: 0.3196 - val_f_2: 0.2823 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3311 - f_2: 0.2305 - acc: 0.8925 - val_loss: 0.3193 - val_f_2: 0.2723 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3287 - f_2: 0.2315 - acc: 0.8926 - val_loss: 0.3197 - val_f_2: 0.2875 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.2434 - acc: 0.8945 - val_loss: 0.3182 - val_f_2: 0.2787 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.2659 - acc: 0.8956 - val_loss: 0.3190 - val_f_2: 0.2704 - val_acc: 0.8997\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.2501 - acc: 0.8952 - val_loss: 0.3189 - val_f_2: 0.3299 - val_acc: 0.9003\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.2772 - acc: 0.8974 - val_loss: 0.3182 - val_f_2: 0.2787 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3262 - f_2: 0.2878 - acc: 0.8966 - val_loss: 0.3200 - val_f_2: 0.2787 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3245 - f_2: 0.2650 - acc: 0.8958 - val_loss: 0.3160 - val_f_2: 0.3203 - val_acc: 0.9018\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3289 - f_2: 0.2744 - acc: 0.8982 - val_loss: 0.3162 - val_f_2: 0.2947 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.3112 - acc: 0.8980 - val_loss: 0.3160 - val_f_2: 0.3229 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.2903 - acc: 0.8975 - val_loss: 0.3153 - val_f_2: 0.2787 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2690 - acc: 0.8952 - val_loss: 0.3197 - val_f_2: 0.4212 - val_acc: 0.9012\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.2838 - acc: 0.8968 - val_loss: 0.3158 - val_f_2: 0.3371 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.2847 - acc: 0.8987 - val_loss: 0.3148 - val_f_2: 0.4035 - val_acc: 0.9018\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3259 - f_2: 0.2924 - acc: 0.8964 - val_loss: 0.3193 - val_f_2: 0.2787 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3261 - f_2: 0.2902 - acc: 0.8961 - val_loss: 0.3145 - val_f_2: 0.3711 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.2988 - acc: 0.8972 - val_loss: 0.3184 - val_f_2: 0.2867 - val_acc: 0.9003\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2956 - acc: 0.8987 - val_loss: 0.3140 - val_f_2: 0.3690 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.3047 - acc: 0.8972 - val_loss: 0.3149 - val_f_2: 0.3170 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3257 - f_2: 0.2972 - acc: 0.8971 - val_loss: 0.3138 - val_f_2: 0.2786 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.2766 - acc: 0.8979 - val_loss: 0.3177 - val_f_2: 0.3904 - val_acc: 0.9012\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6202 - f_2: 0.0259 - acc: 0.8503 - val_loss: 0.3986 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4031 - f_2: 2.9953e-08 - acc: 0.8691 - val_loss: 0.3524 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3780 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3404 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3668 - f_2: 3.0716e-08 - acc: 0.8691 - val_loss: 0.3305 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3632 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3267 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3560 - f_2: 0.0148 - acc: 0.8691 - val_loss: 0.3225 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3515 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3203 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3527 - f_2: 0.0089 - acc: 0.8691 - val_loss: 0.3181 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3488 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3184 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3502 - f_2: 2.9889e-08 - acc: 0.8691 - val_loss: 0.3152 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3455 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3138 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3489 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3156 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3465 - f_2: 3.0662e-08 - acc: 0.8691 - val_loss: 0.3118 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3461 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3123 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3463 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3132 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3450 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3108 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3436 - f_2: 0.0118 - acc: 0.8691 - val_loss: 0.3109 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3437 - f_2: 2.9118e-08 - acc: 0.8691 - val_loss: 0.3092 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3421 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3084 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3418 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3090 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3395 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3072 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3401 - f_2: 2.9171e-08 - acc: 0.8691 - val_loss: 0.3068 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3411 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3063 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3384 - f_2: 2.9212e-08 - acc: 0.8691 - val_loss: 0.3067 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3403 - f_2: 2.9715e-08 - acc: 0.8691 - val_loss: 0.3055 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3379 - f_2: 0.0089 - acc: 0.8691 - val_loss: 0.3055 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3380 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3053 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3384 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3044 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3393 - f_2: 2.9342e-08 - acc: 0.8691 - val_loss: 0.3061 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3391 - f_2: 2.9065e-08 - acc: 0.8691 - val_loss: 0.3042 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3361 - f_2: 2.9702e-08 - acc: 0.8691 - val_loss: 0.3028 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3399 - f_2: 3.0160e-08 - acc: 0.8691 - val_loss: 0.3039 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3346 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3027 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3377 - f_2: 2.9879e-08 - acc: 0.8691 - val_loss: 0.3026 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3353 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3036 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3348 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3044 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3351 - f_2: 0.0378 - acc: 0.8723 - val_loss: 0.3030 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3344 - f_2: 0.0852 - acc: 0.8771 - val_loss: 0.3019 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3354 - f_2: 0.1391 - acc: 0.8824 - val_loss: 0.3045 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3370 - f_2: 0.1730 - acc: 0.8841 - val_loss: 0.3033 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3321 - f_2: 0.1681 - acc: 0.8850 - val_loss: 0.3023 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.3328 - f_2: 0.1569 - acc: 0.8838 - val_loss: 0.3007 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3355 - f_2: 0.2041 - acc: 0.8846 - val_loss: 0.3012 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3350 - f_2: 0.1683 - acc: 0.8843 - val_loss: 0.3017 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3335 - f_2: 0.1735 - acc: 0.8838 - val_loss: 0.3016 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3299 - f_2: 0.1790 - acc: 0.8846 - val_loss: 0.3023 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3324 - f_2: 0.1915 - acc: 0.8863 - val_loss: 0.3023 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3317 - f_2: 0.1928 - acc: 0.8850 - val_loss: 0.3010 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.1842 - acc: 0.8850 - val_loss: 0.3007 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.2105 - acc: 0.8872 - val_loss: 0.3023 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4478 - f_2: 0.0153 - acc: 0.8570 - val_loss: 0.3583 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3620 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3367 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3513 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3319 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3433 - f_2: 3.0528e-08 - acc: 0.8753 - val_loss: 0.3271 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3361 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3236 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3373 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3222 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3287 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3203 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3185 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3182 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3259 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3188 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3189 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3176 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.1126 - acc: 0.8834 - val_loss: 0.3176 - val_f_2: 0.2858 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.2171 - acc: 0.8930 - val_loss: 0.3173 - val_f_2: 0.3545 - val_acc: 0.9012\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.2557 - acc: 0.8972 - val_loss: 0.3167 - val_f_2: 0.3172 - val_acc: 0.8994\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.2860 - acc: 0.8992 - val_loss: 0.3167 - val_f_2: 0.3517 - val_acc: 0.8997\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3136 - f_2: 0.2797 - acc: 0.8988 - val_loss: 0.3148 - val_f_2: 0.3928 - val_acc: 0.9021\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.2676 - acc: 0.8981 - val_loss: 0.3169 - val_f_2: 0.3402 - val_acc: 0.9003\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.2675 - acc: 0.8969 - val_loss: 0.3168 - val_f_2: 0.4172 - val_acc: 0.9018\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.2957 - acc: 0.8984 - val_loss: 0.3182 - val_f_2: 0.3973 - val_acc: 0.9021\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3180 - acc: 0.9012 - val_loss: 0.3165 - val_f_2: 0.3461 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3153 - acc: 0.9012 - val_loss: 0.3150 - val_f_2: 0.3940 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3023 - acc: 0.9015 - val_loss: 0.3144 - val_f_2: 0.3789 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.2920 - acc: 0.8992 - val_loss: 0.3174 - val_f_2: 0.3809 - val_acc: 0.8997\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3053 - acc: 0.9001 - val_loss: 0.3174 - val_f_2: 0.4024 - val_acc: 0.8988\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3027 - acc: 0.8989 - val_loss: 0.3157 - val_f_2: 0.4343 - val_acc: 0.9018\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3080 - f_2: 0.3240 - acc: 0.9012 - val_loss: 0.3199 - val_f_2: 0.3566 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3109 - acc: 0.8992 - val_loss: 0.3184 - val_f_2: 0.4023 - val_acc: 0.8988\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3227 - acc: 0.9007 - val_loss: 0.3166 - val_f_2: 0.3795 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3131 - acc: 0.9003 - val_loss: 0.3171 - val_f_2: 0.4171 - val_acc: 0.8985\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3261 - acc: 0.9019 - val_loss: 0.3156 - val_f_2: 0.4028 - val_acc: 0.8979\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3349 - acc: 0.9004 - val_loss: 0.3148 - val_f_2: 0.4119 - val_acc: 0.8991\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3270 - acc: 0.9015 - val_loss: 0.3148 - val_f_2: 0.4076 - val_acc: 0.8997\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3271 - acc: 0.9011 - val_loss: 0.3153 - val_f_2: 0.4106 - val_acc: 0.8979\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3101 - acc: 0.8999 - val_loss: 0.3153 - val_f_2: 0.4136 - val_acc: 0.8979\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3362 - acc: 0.9019 - val_loss: 0.3213 - val_f_2: 0.4072 - val_acc: 0.8979\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3050 - f_2: 0.3389 - acc: 0.9009 - val_loss: 0.3146 - val_f_2: 0.4236 - val_acc: 0.8988\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3148 - acc: 0.9009 - val_loss: 0.3151 - val_f_2: 0.4250 - val_acc: 0.8973\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3580 - acc: 0.9018 - val_loss: 0.3163 - val_f_2: 0.4152 - val_acc: 0.8976\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3330 - acc: 0.9003 - val_loss: 0.3182 - val_f_2: 0.4027 - val_acc: 0.8985\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3462 - acc: 0.9012 - val_loss: 0.3189 - val_f_2: 0.4286 - val_acc: 0.8982\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3492 - acc: 0.9020 - val_loss: 0.3185 - val_f_2: 0.4449 - val_acc: 0.8988\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3740 - acc: 0.9037 - val_loss: 0.3263 - val_f_2: 0.3982 - val_acc: 0.8988\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3814 - acc: 0.9054 - val_loss: 0.3158 - val_f_2: 0.4186 - val_acc: 0.8982\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3469 - acc: 0.9012 - val_loss: 0.3223 - val_f_2: 0.4293 - val_acc: 0.8982\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3565 - acc: 0.9021 - val_loss: 0.3163 - val_f_2: 0.4282 - val_acc: 0.8962\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.3517 - acc: 0.9020 - val_loss: 0.3212 - val_f_2: 0.4110 - val_acc: 0.8985\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3354 - acc: 0.9011 - val_loss: 0.3157 - val_f_2: 0.4395 - val_acc: 0.8965\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3499 - acc: 0.9006 - val_loss: 0.3182 - val_f_2: 0.4342 - val_acc: 0.8965\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3441 - acc: 0.9005 - val_loss: 0.3194 - val_f_2: 0.4045 - val_acc: 0.8991\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4495 - f_2: 0.0180 - acc: 0.8559 - val_loss: 0.3559 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3613 - f_2: 0.0954 - acc: 0.8814 - val_loss: 0.3296 - val_f_2: 0.2605 - val_acc: 0.8982\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3440 - f_2: 0.1973 - acc: 0.8901 - val_loss: 0.3159 - val_f_2: 0.3084 - val_acc: 0.9015\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3316 - f_2: 0.2353 - acc: 0.8916 - val_loss: 0.3073 - val_f_2: 0.3385 - val_acc: 0.9006\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3245 - f_2: 0.2531 - acc: 0.8919 - val_loss: 0.3036 - val_f_2: 0.3851 - val_acc: 0.9032\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.2929 - acc: 0.8962 - val_loss: 0.3018 - val_f_2: 0.3271 - val_acc: 0.9015\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.2919 - acc: 0.8978 - val_loss: 0.3027 - val_f_2: 0.3350 - val_acc: 0.9006\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.3112 - acc: 0.8978 - val_loss: 0.2994 - val_f_2: 0.3702 - val_acc: 0.8988\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3109 - acc: 0.8973 - val_loss: 0.2992 - val_f_2: 0.3479 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.2998 - acc: 0.8970 - val_loss: 0.2967 - val_f_2: 0.4083 - val_acc: 0.9015\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3213 - acc: 0.8994 - val_loss: 0.2975 - val_f_2: 0.3609 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3295 - acc: 0.8989 - val_loss: 0.2964 - val_f_2: 0.3757 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3400 - acc: 0.9019 - val_loss: 0.2992 - val_f_2: 0.3748 - val_acc: 0.8985\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3191 - acc: 0.8987 - val_loss: 0.2966 - val_f_2: 0.3570 - val_acc: 0.8979\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3440 - acc: 0.9008 - val_loss: 0.2963 - val_f_2: 0.3622 - val_acc: 0.8982\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3282 - acc: 0.8996 - val_loss: 0.2947 - val_f_2: 0.4059 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3465 - acc: 0.9014 - val_loss: 0.2960 - val_f_2: 0.3655 - val_acc: 0.8991\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3620 - acc: 0.9037 - val_loss: 0.2962 - val_f_2: 0.3684 - val_acc: 0.8991\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2958 - f_2: 0.3400 - acc: 0.9031 - val_loss: 0.2968 - val_f_2: 0.4177 - val_acc: 0.9006\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.3691 - acc: 0.9032 - val_loss: 0.2961 - val_f_2: 0.3989 - val_acc: 0.8985\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.3507 - acc: 0.9012 - val_loss: 0.2953 - val_f_2: 0.4305 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2951 - f_2: 0.3522 - acc: 0.9017 - val_loss: 0.2954 - val_f_2: 0.4090 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.3525 - acc: 0.9022 - val_loss: 0.2950 - val_f_2: 0.4279 - val_acc: 0.9018\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3654 - acc: 0.9044 - val_loss: 0.2943 - val_f_2: 0.3671 - val_acc: 0.8988\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2939 - f_2: 0.3564 - acc: 0.9025 - val_loss: 0.3028 - val_f_2: 0.3546 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2903 - f_2: 0.3692 - acc: 0.9028 - val_loss: 0.2963 - val_f_2: 0.4112 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.3700 - acc: 0.9044 - val_loss: 0.2941 - val_f_2: 0.4242 - val_acc: 0.9021\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2933 - f_2: 0.3659 - acc: 0.9034 - val_loss: 0.2945 - val_f_2: 0.4334 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2928 - f_2: 0.3765 - acc: 0.9036 - val_loss: 0.2951 - val_f_2: 0.4217 - val_acc: 0.9021\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2936 - f_2: 0.3615 - acc: 0.9023 - val_loss: 0.2964 - val_f_2: 0.4000 - val_acc: 0.9003\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3666 - acc: 0.9016 - val_loss: 0.2963 - val_f_2: 0.4071 - val_acc: 0.9012\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2903 - f_2: 0.3772 - acc: 0.9043 - val_loss: 0.2999 - val_f_2: 0.4187 - val_acc: 0.9035\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2891 - f_2: 0.3829 - acc: 0.9052 - val_loss: 0.2964 - val_f_2: 0.4234 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.3793 - acc: 0.9055 - val_loss: 0.2941 - val_f_2: 0.4319 - val_acc: 0.9024\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3624 - acc: 0.9036 - val_loss: 0.2977 - val_f_2: 0.4446 - val_acc: 0.8973\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.3786 - acc: 0.9048 - val_loss: 0.2959 - val_f_2: 0.4329 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.3798 - acc: 0.9052 - val_loss: 0.2978 - val_f_2: 0.4253 - val_acc: 0.9003\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.3908 - acc: 0.9055 - val_loss: 0.2965 - val_f_2: 0.4153 - val_acc: 0.8988\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.3943 - acc: 0.9071 - val_loss: 0.2963 - val_f_2: 0.4335 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2857 - f_2: 0.3929 - acc: 0.9069 - val_loss: 0.2998 - val_f_2: 0.4254 - val_acc: 0.9012\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.3843 - acc: 0.9045 - val_loss: 0.3007 - val_f_2: 0.4203 - val_acc: 0.9015\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2905 - f_2: 0.3909 - acc: 0.9052 - val_loss: 0.2983 - val_f_2: 0.3935 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2841 - f_2: 0.4038 - acc: 0.9066 - val_loss: 0.3027 - val_f_2: 0.4114 - val_acc: 0.8997\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.4036 - acc: 0.9065 - val_loss: 0.2993 - val_f_2: 0.4254 - val_acc: 0.8988\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.4000 - acc: 0.9066 - val_loss: 0.2998 - val_f_2: 0.4144 - val_acc: 0.8991\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2860 - f_2: 0.3915 - acc: 0.9053 - val_loss: 0.2991 - val_f_2: 0.4279 - val_acc: 0.8988\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.3815 - acc: 0.9063 - val_loss: 0.3029 - val_f_2: 0.4395 - val_acc: 0.8985\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2862 - f_2: 0.4083 - acc: 0.9073 - val_loss: 0.3029 - val_f_2: 0.4216 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2881 - f_2: 0.3958 - acc: 0.9071 - val_loss: 0.3005 - val_f_2: 0.4494 - val_acc: 0.8938\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.3974 - acc: 0.9066 - val_loss: 0.3008 - val_f_2: 0.4344 - val_acc: 0.8962\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4508 - f_2: 0.0163 - acc: 0.8506 - val_loss: 0.3397 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3673 - f_2: 0.0560 - acc: 0.8752 - val_loss: 0.3146 - val_f_2: 0.1913 - val_acc: 0.8991\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3517 - f_2: 0.1701 - acc: 0.8841 - val_loss: 0.3024 - val_f_2: 0.3096 - val_acc: 0.9091\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3422 - f_2: 0.2016 - acc: 0.8855 - val_loss: 0.2964 - val_f_2: 0.3261 - val_acc: 0.9115\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3345 - f_2: 0.2328 - acc: 0.8897 - val_loss: 0.2916 - val_f_2: 0.3421 - val_acc: 0.9115\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3254 - f_2: 0.2575 - acc: 0.8900 - val_loss: 0.2885 - val_f_2: 0.3352 - val_acc: 0.9124\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.2613 - acc: 0.8913 - val_loss: 0.2844 - val_f_2: 0.3599 - val_acc: 0.9100\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2776 - acc: 0.8917 - val_loss: 0.2829 - val_f_2: 0.3279 - val_acc: 0.9115\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2647 - acc: 0.8910 - val_loss: 0.2822 - val_f_2: 0.3386 - val_acc: 0.9103\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2860 - acc: 0.8938 - val_loss: 0.2800 - val_f_2: 0.3713 - val_acc: 0.9100\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3139 - f_2: 0.3211 - acc: 0.8958 - val_loss: 0.2795 - val_f_2: 0.3420 - val_acc: 0.9086\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.2754 - acc: 0.8929 - val_loss: 0.2791 - val_f_2: 0.4047 - val_acc: 0.9118\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.2966 - acc: 0.8924 - val_loss: 0.2809 - val_f_2: 0.4027 - val_acc: 0.9106\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3110 - f_2: 0.3034 - acc: 0.8923 - val_loss: 0.2787 - val_f_2: 0.3547 - val_acc: 0.9086\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3072 - f_2: 0.3210 - acc: 0.8939 - val_loss: 0.2787 - val_f_2: 0.3783 - val_acc: 0.9100\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3171 - acc: 0.8939 - val_loss: 0.2790 - val_f_2: 0.3717 - val_acc: 0.9083\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3270 - acc: 0.8957 - val_loss: 0.2785 - val_f_2: 0.3973 - val_acc: 0.9088\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3069 - acc: 0.8935 - val_loss: 0.2778 - val_f_2: 0.3807 - val_acc: 0.9097\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3056 - f_2: 0.3318 - acc: 0.8960 - val_loss: 0.2781 - val_f_2: 0.4172 - val_acc: 0.9103\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3182 - acc: 0.8956 - val_loss: 0.2782 - val_f_2: 0.4037 - val_acc: 0.9112\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3174 - acc: 0.8942 - val_loss: 0.2781 - val_f_2: 0.4108 - val_acc: 0.9083\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3312 - acc: 0.8964 - val_loss: 0.2772 - val_f_2: 0.3879 - val_acc: 0.9103\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3569 - acc: 0.8970 - val_loss: 0.2755 - val_f_2: 0.3911 - val_acc: 0.9103\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3421 - acc: 0.8967 - val_loss: 0.2776 - val_f_2: 0.3638 - val_acc: 0.9106\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3014 - f_2: 0.3406 - acc: 0.8970 - val_loss: 0.2768 - val_f_2: 0.4028 - val_acc: 0.9074\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3353 - acc: 0.8958 - val_loss: 0.2781 - val_f_2: 0.3831 - val_acc: 0.9094\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3371 - acc: 0.8945 - val_loss: 0.2779 - val_f_2: 0.4115 - val_acc: 0.9091\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3474 - acc: 0.8978 - val_loss: 0.2785 - val_f_2: 0.4283 - val_acc: 0.9041\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.3353 - acc: 0.8969 - val_loss: 0.2784 - val_f_2: 0.4193 - val_acc: 0.9062\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3422 - acc: 0.8959 - val_loss: 0.2767 - val_f_2: 0.4215 - val_acc: 0.9088\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.3494 - acc: 0.8976 - val_loss: 0.2765 - val_f_2: 0.4398 - val_acc: 0.9080\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3368 - acc: 0.8978 - val_loss: 0.2798 - val_f_2: 0.4438 - val_acc: 0.9050\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2999 - f_2: 0.3410 - acc: 0.8962 - val_loss: 0.2792 - val_f_2: 0.4256 - val_acc: 0.9071\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3670 - acc: 0.8993 - val_loss: 0.2784 - val_f_2: 0.4359 - val_acc: 0.9050\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.3716 - acc: 0.8981 - val_loss: 0.2774 - val_f_2: 0.4288 - val_acc: 0.9068\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2958 - f_2: 0.3578 - acc: 0.8981 - val_loss: 0.2819 - val_f_2: 0.4373 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3730 - acc: 0.8995 - val_loss: 0.2811 - val_f_2: 0.4384 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2932 - f_2: 0.3529 - acc: 0.8979 - val_loss: 0.2784 - val_f_2: 0.4157 - val_acc: 0.9077\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3555 - acc: 0.8984 - val_loss: 0.2800 - val_f_2: 0.4367 - val_acc: 0.9018\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2960 - f_2: 0.3579 - acc: 0.8987 - val_loss: 0.2770 - val_f_2: 0.4043 - val_acc: 0.9091\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3486 - acc: 0.8982 - val_loss: 0.2779 - val_f_2: 0.4347 - val_acc: 0.9056\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2947 - f_2: 0.3637 - acc: 0.8981 - val_loss: 0.2789 - val_f_2: 0.4244 - val_acc: 0.9065\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2937 - f_2: 0.3571 - acc: 0.8984 - val_loss: 0.2779 - val_f_2: 0.4207 - val_acc: 0.9059\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3407 - acc: 0.8966 - val_loss: 0.2776 - val_f_2: 0.4301 - val_acc: 0.9053\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.3710 - acc: 0.9023 - val_loss: 0.2783 - val_f_2: 0.4127 - val_acc: 0.9103\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2928 - f_2: 0.3708 - acc: 0.8981 - val_loss: 0.2778 - val_f_2: 0.4255 - val_acc: 0.9053\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.3741 - acc: 0.8987 - val_loss: 0.2783 - val_f_2: 0.4210 - val_acc: 0.9071\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.3632 - acc: 0.8982 - val_loss: 0.2782 - val_f_2: 0.4176 - val_acc: 0.9065\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3565 - acc: 0.8979 - val_loss: 0.2776 - val_f_2: 0.4182 - val_acc: 0.9056\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2934 - f_2: 0.3454 - acc: 0.8979 - val_loss: 0.2786 - val_f_2: 0.4431 - val_acc: 0.9021\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=10, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6282 - f_2: 0.0160 - acc: 0.8603 - val_loss: 0.4053 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3889 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3686 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3669 - f_2: 3.1842e-08 - acc: 0.8753 - val_loss: 0.3608 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3576 - f_2: 3.2073e-08 - acc: 0.8753 - val_loss: 0.3482 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3528 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3426 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3467 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3380 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3441 - f_2: 3.1333e-08 - acc: 0.8753 - val_loss: 0.3372 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3433 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3341 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3414 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3322 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3398 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3319 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3368 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3288 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3393 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3283 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3336 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3267 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3346 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3262 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3341 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3259 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3336 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3245 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3336 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3249 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3344 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3248 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3323 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3230 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3231 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3331 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3229 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3284 - f_2: 3.2401e-08 - acc: 0.8753 - val_loss: 0.3228 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3308 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3226 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3214 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3214 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3204 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3269 - f_2: 3.0785e-08 - acc: 0.8753 - val_loss: 0.3216 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.0971 - acc: 0.8842 - val_loss: 0.3195 - val_f_2: 0.2704 - val_acc: 0.8997\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3278 - f_2: 0.1461 - acc: 0.8899 - val_loss: 0.3210 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.1665 - acc: 0.8920 - val_loss: 0.3199 - val_f_2: 0.2999 - val_acc: 0.9015\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2282 - acc: 0.8959 - val_loss: 0.3201 - val_f_2: 0.2801 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.2318 - acc: 0.8977 - val_loss: 0.3193 - val_f_2: 0.2818 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.2348 - acc: 0.8972 - val_loss: 0.3191 - val_f_2: 0.2782 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3250 - f_2: 0.2633 - acc: 0.9005 - val_loss: 0.3199 - val_f_2: 0.2842 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3264 - f_2: 0.2575 - acc: 0.8998 - val_loss: 0.3275 - val_f_2: 0.2801 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2687 - acc: 0.9017 - val_loss: 0.3196 - val_f_2: 0.2846 - val_acc: 0.9003\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2793 - acc: 0.9015 - val_loss: 0.3183 - val_f_2: 0.3031 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.2809 - acc: 0.9015 - val_loss: 0.3198 - val_f_2: 0.2861 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.2881 - acc: 0.9026 - val_loss: 0.3193 - val_f_2: 0.2846 - val_acc: 0.9003\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.2762 - acc: 0.9006 - val_loss: 0.3206 - val_f_2: 0.2861 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3221 - f_2: 0.2842 - acc: 0.9016 - val_loss: 0.3258 - val_f_2: 0.2818 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.2826 - acc: 0.9022 - val_loss: 0.3185 - val_f_2: 0.2877 - val_acc: 0.8997\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2853 - acc: 0.9025 - val_loss: 0.3192 - val_f_2: 0.2842 - val_acc: 0.8997\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.2874 - acc: 0.9024 - val_loss: 0.3237 - val_f_2: 0.2863 - val_acc: 0.9003\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.3023 - acc: 0.9026 - val_loss: 0.3197 - val_f_2: 0.3097 - val_acc: 0.9012\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3191 - f_2: 0.2959 - acc: 0.9019 - val_loss: 0.3196 - val_f_2: 0.3400 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3217 - f_2: 0.2919 - acc: 0.9017 - val_loss: 0.3196 - val_f_2: 0.3203 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.3150 - acc: 0.9043 - val_loss: 0.3228 - val_f_2: 0.3248 - val_acc: 0.8988\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2900 - acc: 0.9012 - val_loss: 0.3196 - val_f_2: 0.3059 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.3009 - acc: 0.9041 - val_loss: 0.3211 - val_f_2: 0.3203 - val_acc: 0.9009\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6276 - f_2: 0.0231 - acc: 0.8599 - val_loss: 0.4041 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3904 - f_2: 3.0737e-08 - acc: 0.8722 - val_loss: 0.3679 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3683 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3548 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3560 - f_2: 3.0577e-08 - acc: 0.8722 - val_loss: 0.3459 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3549 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3410 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3446 - f_2: 3.0388e-08 - acc: 0.8722 - val_loss: 0.3363 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3454 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3353 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3396 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3338 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3403 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3297 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3375 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3293 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3376 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3274 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3376 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3276 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3352 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3255 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3357 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3248 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3318 - f_2: 3.0099e-08 - acc: 0.8722 - val_loss: 0.3228 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3327 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3224 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.0207 - acc: 0.8744 - val_loss: 0.3220 - val_f_2: 0.2898 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.1886 - acc: 0.8923 - val_loss: 0.3209 - val_f_2: 0.2818 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3279 - f_2: 0.2694 - acc: 0.8995 - val_loss: 0.3215 - val_f_2: 0.2842 - val_acc: 0.9003\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.2844 - acc: 0.9009 - val_loss: 0.3194 - val_f_2: 0.2818 - val_acc: 0.9003\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.2860 - acc: 0.9003 - val_loss: 0.3224 - val_f_2: 0.2787 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.2823 - acc: 0.9012 - val_loss: 0.3242 - val_f_2: 0.2787 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.3085 - acc: 0.9026 - val_loss: 0.3179 - val_f_2: 0.2768 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3279 - f_2: 0.2936 - acc: 0.9010 - val_loss: 0.3211 - val_f_2: 0.3077 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.3006 - acc: 0.9004 - val_loss: 0.3175 - val_f_2: 0.3183 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3245 - f_2: 0.3047 - acc: 0.9029 - val_loss: 0.3248 - val_f_2: 0.2835 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.3126 - acc: 0.9014 - val_loss: 0.3183 - val_f_2: 0.2858 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.3180 - acc: 0.9020 - val_loss: 0.3265 - val_f_2: 0.2818 - val_acc: 0.9003\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2993 - acc: 0.9019 - val_loss: 0.3160 - val_f_2: 0.3317 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.2949 - acc: 0.9011 - val_loss: 0.3168 - val_f_2: 0.3656 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.3115 - acc: 0.9020 - val_loss: 0.3162 - val_f_2: 0.3285 - val_acc: 0.9009\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.3276 - acc: 0.9030 - val_loss: 0.3166 - val_f_2: 0.2923 - val_acc: 0.9015\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.3272 - acc: 0.9034 - val_loss: 0.3140 - val_f_2: 0.3061 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.3277 - acc: 0.9036 - val_loss: 0.3149 - val_f_2: 0.3288 - val_acc: 0.9012\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.3295 - acc: 0.9039 - val_loss: 0.3137 - val_f_2: 0.3119 - val_acc: 0.8997\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.3521 - acc: 0.9035 - val_loss: 0.3190 - val_f_2: 0.2945 - val_acc: 0.9003\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.3351 - acc: 0.9043 - val_loss: 0.3146 - val_f_2: 0.3604 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3221 - f_2: 0.3222 - acc: 0.9037 - val_loss: 0.3132 - val_f_2: 0.3174 - val_acc: 0.9003\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.3377 - acc: 0.9030 - val_loss: 0.3158 - val_f_2: 0.3121 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.3348 - acc: 0.9040 - val_loss: 0.3140 - val_f_2: 0.3384 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.3456 - acc: 0.9037 - val_loss: 0.3152 - val_f_2: 0.3135 - val_acc: 0.9006\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.3344 - acc: 0.9029 - val_loss: 0.3130 - val_f_2: 0.3504 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.3375 - acc: 0.9045 - val_loss: 0.3148 - val_f_2: 0.3638 - val_acc: 0.9018\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.3442 - acc: 0.9049 - val_loss: 0.3153 - val_f_2: 0.3655 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3180 - f_2: 0.3691 - acc: 0.9049 - val_loss: 0.3139 - val_f_2: 0.3465 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.3508 - acc: 0.9044 - val_loss: 0.3143 - val_f_2: 0.3431 - val_acc: 0.9015\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.3451 - acc: 0.9036 - val_loss: 0.3147 - val_f_2: 0.3333 - val_acc: 0.9024\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3182 - f_2: 0.3470 - acc: 0.9045 - val_loss: 0.3135 - val_f_2: 0.3212 - val_acc: 0.9015\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.3414 - acc: 0.9040 - val_loss: 0.3150 - val_f_2: 0.3962 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.3541 - acc: 0.9037 - val_loss: 0.3172 - val_f_2: 0.3597 - val_acc: 0.9009\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6322 - f_2: 0.0187 - acc: 0.8551 - val_loss: 0.3825 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3989 - f_2: 0.0089 - acc: 0.8691 - val_loss: 0.3490 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3748 - f_2: 2.9883e-08 - acc: 0.8691 - val_loss: 0.3384 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3674 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3305 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3603 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3244 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3543 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3212 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3542 - f_2: 0.0118 - acc: 0.8691 - val_loss: 0.3202 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3477 - f_2: 0.0089 - acc: 0.8691 - val_loss: 0.3161 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3522 - f_2: 2.9630e-08 - acc: 0.8691 - val_loss: 0.3139 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3469 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3133 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3446 - f_2: 2.9387e-08 - acc: 0.8691 - val_loss: 0.3120 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3455 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3110 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3419 - f_2: 2.9844e-08 - acc: 0.8691 - val_loss: 0.3103 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3440 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3085 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3414 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3086 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3369 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3070 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3398 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3072 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3386 - f_2: 2.9211e-08 - acc: 0.8691 - val_loss: 0.3073 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3371 - f_2: 2.8890e-08 - acc: 0.8691 - val_loss: 0.3046 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3382 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3044 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3389 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3053 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3402 - f_2: 0.0068 - acc: 0.8696 - val_loss: 0.3046 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3349 - f_2: 0.1018 - acc: 0.8801 - val_loss: 0.3041 - val_f_2: 0.2949 - val_acc: 0.9109\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3375 - f_2: 0.1938 - acc: 0.8888 - val_loss: 0.3039 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3350 - f_2: 0.2086 - acc: 0.8909 - val_loss: 0.3026 - val_f_2: 0.2949 - val_acc: 0.9109\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3367 - f_2: 0.2443 - acc: 0.8940 - val_loss: 0.3040 - val_f_2: 0.2949 - val_acc: 0.9109\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3347 - f_2: 0.2285 - acc: 0.8939 - val_loss: 0.3027 - val_f_2: 0.2979 - val_acc: 0.9109\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3340 - f_2: 0.2541 - acc: 0.8941 - val_loss: 0.3068 - val_f_2: 0.2975 - val_acc: 0.9103\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3301 - f_2: 0.2702 - acc: 0.8961 - val_loss: 0.3063 - val_f_2: 0.2978 - val_acc: 0.9106\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.3311 - f_2: 0.2675 - acc: 0.8959 - val_loss: 0.3012 - val_f_2: 0.2949 - val_acc: 0.9109\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.2696 - acc: 0.8952 - val_loss: 0.3005 - val_f_2: 0.2975 - val_acc: 0.9103\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3327 - f_2: 0.2566 - acc: 0.8956 - val_loss: 0.3014 - val_f_2: 0.3015 - val_acc: 0.9109\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3335 - f_2: 0.2893 - acc: 0.8972 - val_loss: 0.3005 - val_f_2: 0.2992 - val_acc: 0.9106\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.2858 - acc: 0.8971 - val_loss: 0.3000 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3311 - f_2: 0.2923 - acc: 0.8960 - val_loss: 0.3008 - val_f_2: 0.3064 - val_acc: 0.9112\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3306 - f_2: 0.2676 - acc: 0.8950 - val_loss: 0.3028 - val_f_2: 0.3014 - val_acc: 0.9106\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3343 - f_2: 0.3018 - acc: 0.8973 - val_loss: 0.2998 - val_f_2: 0.2992 - val_acc: 0.9106\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3326 - f_2: 0.2924 - acc: 0.8967 - val_loss: 0.3024 - val_f_2: 0.2975 - val_acc: 0.9103\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3332 - f_2: 0.2894 - acc: 0.8953 - val_loss: 0.2991 - val_f_2: 0.2992 - val_acc: 0.9106\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3278 - f_2: 0.2991 - acc: 0.8978 - val_loss: 0.2973 - val_f_2: 0.2985 - val_acc: 0.9109\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3313 - f_2: 0.2839 - acc: 0.8965 - val_loss: 0.2996 - val_f_2: 0.2975 - val_acc: 0.9103\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3294 - f_2: 0.3006 - acc: 0.8957 - val_loss: 0.2990 - val_f_2: 0.2964 - val_acc: 0.9112\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.2908 - acc: 0.8975 - val_loss: 0.2977 - val_f_2: 0.2985 - val_acc: 0.9109\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3295 - f_2: 0.2919 - acc: 0.8970 - val_loss: 0.3002 - val_f_2: 0.2982 - val_acc: 0.9106\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.3024 - acc: 0.8979 - val_loss: 0.3027 - val_f_2: 0.2987 - val_acc: 0.9112\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.2990 - acc: 0.8963 - val_loss: 0.2979 - val_f_2: 0.2987 - val_acc: 0.9112\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3292 - f_2: 0.3012 - acc: 0.8976 - val_loss: 0.2996 - val_f_2: 0.3000 - val_acc: 0.9100\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.2919 - acc: 0.8972 - val_loss: 0.2998 - val_f_2: 0.3383 - val_acc: 0.9083\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3312 - f_2: 0.2995 - acc: 0.8967 - val_loss: 0.3062 - val_f_2: 0.3456 - val_acc: 0.9074\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3259 - f_2: 0.3074 - acc: 0.8977 - val_loss: 0.3032 - val_f_2: 0.3133 - val_acc: 0.9103\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.01, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4447 - f_2: 0.0163 - acc: 0.8622 - val_loss: 0.3657 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3620 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3404 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3472 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3322 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3388 - f_2: 3.2558e-08 - acc: 0.8753 - val_loss: 0.3273 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3349 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3248 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3321 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3233 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3264 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3226 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3203 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3252 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3193 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.0083 - acc: 0.8757 - val_loss: 0.3181 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.2612 - acc: 0.9005 - val_loss: 0.3196 - val_f_2: 0.3068 - val_acc: 0.9009\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3041 - acc: 0.9033 - val_loss: 0.3190 - val_f_2: 0.3380 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.3209 - acc: 0.9040 - val_loss: 0.3191 - val_f_2: 0.3217 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.3088 - acc: 0.9046 - val_loss: 0.3168 - val_f_2: 0.3117 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.3191 - acc: 0.9054 - val_loss: 0.3169 - val_f_2: 0.3698 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.3482 - acc: 0.9055 - val_loss: 0.3159 - val_f_2: 0.3746 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3611 - acc: 0.9065 - val_loss: 0.3165 - val_f_2: 0.3507 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3522 - acc: 0.9057 - val_loss: 0.3150 - val_f_2: 0.3734 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3566 - acc: 0.9061 - val_loss: 0.3189 - val_f_2: 0.3502 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3594 - acc: 0.9060 - val_loss: 0.3168 - val_f_2: 0.3798 - val_acc: 0.9003\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3087 - f_2: 0.3559 - acc: 0.9057 - val_loss: 0.3160 - val_f_2: 0.4029 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3536 - acc: 0.9059 - val_loss: 0.3238 - val_f_2: 0.3510 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3611 - acc: 0.9060 - val_loss: 0.3172 - val_f_2: 0.3667 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3664 - acc: 0.9067 - val_loss: 0.3262 - val_f_2: 0.3760 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3653 - acc: 0.9060 - val_loss: 0.3178 - val_f_2: 0.4019 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3817 - acc: 0.9068 - val_loss: 0.3166 - val_f_2: 0.3793 - val_acc: 0.9012\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3903 - acc: 0.9076 - val_loss: 0.3165 - val_f_2: 0.3784 - val_acc: 0.9012\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3860 - acc: 0.9075 - val_loss: 0.3175 - val_f_2: 0.4125 - val_acc: 0.8994\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3037 - f_2: 0.3766 - acc: 0.9067 - val_loss: 0.3167 - val_f_2: 0.3762 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3781 - acc: 0.9062 - val_loss: 0.3317 - val_f_2: 0.3778 - val_acc: 0.9015\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3817 - acc: 0.9068 - val_loss: 0.3202 - val_f_2: 0.4015 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3020 - f_2: 0.3733 - acc: 0.9073 - val_loss: 0.3215 - val_f_2: 0.3808 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3949 - acc: 0.9067 - val_loss: 0.3220 - val_f_2: 0.4012 - val_acc: 0.9024\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3790 - acc: 0.9066 - val_loss: 0.3187 - val_f_2: 0.3977 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3763 - acc: 0.9060 - val_loss: 0.3209 - val_f_2: 0.4008 - val_acc: 0.8994\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3014 - f_2: 0.3868 - acc: 0.9084 - val_loss: 0.3216 - val_f_2: 0.3998 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3820 - acc: 0.9068 - val_loss: 0.3200 - val_f_2: 0.4027 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3886 - acc: 0.9079 - val_loss: 0.3196 - val_f_2: 0.3912 - val_acc: 0.9006\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3859 - acc: 0.9078 - val_loss: 0.3194 - val_f_2: 0.4076 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3923 - acc: 0.9074 - val_loss: 0.3206 - val_f_2: 0.3814 - val_acc: 0.9003\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.4059 - acc: 0.9085 - val_loss: 0.3203 - val_f_2: 0.3829 - val_acc: 0.9015\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3815 - acc: 0.9068 - val_loss: 0.3202 - val_f_2: 0.4240 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.3834 - acc: 0.9079 - val_loss: 0.3197 - val_f_2: 0.4164 - val_acc: 0.9009\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.3858 - acc: 0.9087 - val_loss: 0.3199 - val_f_2: 0.4128 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3816 - acc: 0.9075 - val_loss: 0.3220 - val_f_2: 0.4018 - val_acc: 0.9015\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.4004 - acc: 0.9077 - val_loss: 0.3184 - val_f_2: 0.4125 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3884 - acc: 0.9065 - val_loss: 0.3209 - val_f_2: 0.4281 - val_acc: 0.9012\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.4037 - acc: 0.9083 - val_loss: 0.3355 - val_f_2: 0.3977 - val_acc: 0.9021\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.4020 - acc: 0.9077 - val_loss: 0.3236 - val_f_2: 0.4044 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2999 - f_2: 0.3970 - acc: 0.9091 - val_loss: 0.3174 - val_f_2: 0.4035 - val_acc: 0.9021\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4447 - f_2: 0.0194 - acc: 0.8607 - val_loss: 0.3632 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3630 - f_2: 0.0118 - acc: 0.8722 - val_loss: 0.3383 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3451 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3350 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3389 - f_2: 0.0089 - acc: 0.8722 - val_loss: 0.3278 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3340 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3245 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3299 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3228 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.0198 - acc: 0.8742 - val_loss: 0.3196 - val_f_2: 0.1402 - val_acc: 0.8844\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3242 - f_2: 0.2091 - acc: 0.8927 - val_loss: 0.3178 - val_f_2: 0.2721 - val_acc: 0.8997\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.2461 - acc: 0.8928 - val_loss: 0.3163 - val_f_2: 0.3067 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.2560 - acc: 0.8945 - val_loss: 0.3149 - val_f_2: 0.3112 - val_acc: 0.9006\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.2685 - acc: 0.8950 - val_loss: 0.3138 - val_f_2: 0.3538 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.2638 - acc: 0.8964 - val_loss: 0.3147 - val_f_2: 0.3415 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.2696 - acc: 0.8965 - val_loss: 0.3143 - val_f_2: 0.3894 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3122 - f_2: 0.3356 - acc: 0.9012 - val_loss: 0.3126 - val_f_2: 0.4050 - val_acc: 0.9027\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.3497 - acc: 0.9043 - val_loss: 0.3144 - val_f_2: 0.3705 - val_acc: 0.9012\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3573 - acc: 0.9046 - val_loss: 0.3134 - val_f_2: 0.4144 - val_acc: 0.9024\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3686 - acc: 0.9048 - val_loss: 0.3155 - val_f_2: 0.3954 - val_acc: 0.9024\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3780 - acc: 0.9048 - val_loss: 0.3155 - val_f_2: 0.3946 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3081 - f_2: 0.3717 - acc: 0.9054 - val_loss: 0.3150 - val_f_2: 0.3876 - val_acc: 0.9027\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3888 - acc: 0.9060 - val_loss: 0.3116 - val_f_2: 0.4154 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.4007 - acc: 0.9065 - val_loss: 0.3135 - val_f_2: 0.4185 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3867 - acc: 0.9065 - val_loss: 0.3158 - val_f_2: 0.4204 - val_acc: 0.8991\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.4041 - acc: 0.9067 - val_loss: 0.3128 - val_f_2: 0.4139 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.4022 - acc: 0.9068 - val_loss: 0.3147 - val_f_2: 0.4160 - val_acc: 0.8988\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.4077 - acc: 0.9056 - val_loss: 0.3138 - val_f_2: 0.4104 - val_acc: 0.8991\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3969 - acc: 0.9082 - val_loss: 0.3228 - val_f_2: 0.3999 - val_acc: 0.9035\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.4166 - acc: 0.9071 - val_loss: 0.3132 - val_f_2: 0.4105 - val_acc: 0.9021\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3005 - f_2: 0.4144 - acc: 0.9072 - val_loss: 0.3153 - val_f_2: 0.4132 - val_acc: 0.8994\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.4199 - acc: 0.9076 - val_loss: 0.3209 - val_f_2: 0.4138 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2999 - f_2: 0.4092 - acc: 0.9063 - val_loss: 0.3160 - val_f_2: 0.4211 - val_acc: 0.8973\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.4331 - acc: 0.9082 - val_loss: 0.3214 - val_f_2: 0.3999 - val_acc: 0.9012\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.4104 - acc: 0.9071 - val_loss: 0.3186 - val_f_2: 0.4045 - val_acc: 0.9012\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.4174 - acc: 0.9074 - val_loss: 0.3130 - val_f_2: 0.4033 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2988 - f_2: 0.4128 - acc: 0.9088 - val_loss: 0.3203 - val_f_2: 0.4190 - val_acc: 0.8976\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2989 - f_2: 0.4331 - acc: 0.9091 - val_loss: 0.3182 - val_f_2: 0.4119 - val_acc: 0.8988\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.4337 - acc: 0.9083 - val_loss: 0.3185 - val_f_2: 0.4085 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.4264 - acc: 0.9074 - val_loss: 0.3221 - val_f_2: 0.4067 - val_acc: 0.8988\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.4278 - acc: 0.9081 - val_loss: 0.3177 - val_f_2: 0.4074 - val_acc: 0.8991\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.4233 - acc: 0.9088 - val_loss: 0.3233 - val_f_2: 0.4139 - val_acc: 0.8973\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.4293 - acc: 0.9080 - val_loss: 0.3193 - val_f_2: 0.4150 - val_acc: 0.8976\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.4377 - acc: 0.9090 - val_loss: 0.3186 - val_f_2: 0.4318 - val_acc: 0.8965\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.4407 - acc: 0.9096 - val_loss: 0.3283 - val_f_2: 0.4295 - val_acc: 0.8973\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2989 - f_2: 0.4445 - acc: 0.9089 - val_loss: 0.3194 - val_f_2: 0.4343 - val_acc: 0.8982\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.4428 - acc: 0.9094 - val_loss: 0.3193 - val_f_2: 0.4321 - val_acc: 0.8985\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2971 - f_2: 0.4540 - acc: 0.9097 - val_loss: 0.3189 - val_f_2: 0.4360 - val_acc: 0.8976\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2954 - f_2: 0.4342 - acc: 0.9066 - val_loss: 0.3175 - val_f_2: 0.4239 - val_acc: 0.8973\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.4438 - acc: 0.9093 - val_loss: 0.3235 - val_f_2: 0.4320 - val_acc: 0.8979\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.4445 - acc: 0.9090 - val_loss: 0.3238 - val_f_2: 0.4093 - val_acc: 0.8979\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.4447 - acc: 0.9094 - val_loss: 0.3189 - val_f_2: 0.4158 - val_acc: 0.8982\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.4481 - acc: 0.9096 - val_loss: 0.3246 - val_f_2: 0.4124 - val_acc: 0.8971\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4496 - f_2: 0.0185 - acc: 0.8568 - val_loss: 0.3433 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3671 - f_2: 2.9252e-08 - acc: 0.8691 - val_loss: 0.3207 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3507 - f_2: 3.0067e-08 - acc: 0.8691 - val_loss: 0.3118 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3447 - f_2: 2.9344e-08 - acc: 0.8691 - val_loss: 0.3082 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3371 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3053 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3353 - f_2: 2.9762e-08 - acc: 0.8691 - val_loss: 0.3050 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3293 - f_2: 0.0701 - acc: 0.8752 - val_loss: 0.3007 - val_f_2: 0.2888 - val_acc: 0.9094\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3285 - f_2: 0.2575 - acc: 0.8930 - val_loss: 0.2989 - val_f_2: 0.3087 - val_acc: 0.9103\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2815 - acc: 0.8945 - val_loss: 0.2983 - val_f_2: 0.3351 - val_acc: 0.9068\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3225 - f_2: 0.3025 - acc: 0.8974 - val_loss: 0.2981 - val_f_2: 0.3379 - val_acc: 0.9094\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.2966 - acc: 0.8961 - val_loss: 0.2967 - val_f_2: 0.3278 - val_acc: 0.9074\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3185 - f_2: 0.3035 - acc: 0.8953 - val_loss: 0.2971 - val_f_2: 0.3441 - val_acc: 0.9083\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.3383 - acc: 0.8988 - val_loss: 0.2961 - val_f_2: 0.3695 - val_acc: 0.9086\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.3280 - acc: 0.8973 - val_loss: 0.2937 - val_f_2: 0.3562 - val_acc: 0.9086\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3359 - acc: 0.8984 - val_loss: 0.2933 - val_f_2: 0.3909 - val_acc: 0.9094\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3419 - acc: 0.8970 - val_loss: 0.2961 - val_f_2: 0.4468 - val_acc: 0.9080\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.3553 - acc: 0.8970 - val_loss: 0.2946 - val_f_2: 0.3956 - val_acc: 0.9080\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3135 - f_2: 0.3520 - acc: 0.8982 - val_loss: 0.2932 - val_f_2: 0.3714 - val_acc: 0.9091\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.3496 - acc: 0.8986 - val_loss: 0.2936 - val_f_2: 0.3844 - val_acc: 0.9088\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3581 - acc: 0.8977 - val_loss: 0.2935 - val_f_2: 0.3800 - val_acc: 0.9080\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3625 - acc: 0.8995 - val_loss: 0.2925 - val_f_2: 0.3953 - val_acc: 0.9086\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3351 - acc: 0.8987 - val_loss: 0.2944 - val_f_2: 0.4309 - val_acc: 0.9074\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3090 - f_2: 0.3795 - acc: 0.8980 - val_loss: 0.2928 - val_f_2: 0.3919 - val_acc: 0.9071\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.3109 - f_2: 0.3594 - acc: 0.8995 - val_loss: 0.3020 - val_f_2: 0.4040 - val_acc: 0.9059\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3771 - acc: 0.8995 - val_loss: 0.2978 - val_f_2: 0.4048 - val_acc: 0.9068\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.3844 - acc: 0.9000 - val_loss: 0.2935 - val_f_2: 0.4137 - val_acc: 0.9059\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3804 - acc: 0.8998 - val_loss: 0.2965 - val_f_2: 0.4114 - val_acc: 0.9065\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3938 - acc: 0.8994 - val_loss: 0.2930 - val_f_2: 0.4089 - val_acc: 0.9080\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3752 - acc: 0.9008 - val_loss: 0.2954 - val_f_2: 0.3994 - val_acc: 0.9071\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3916 - acc: 0.8994 - val_loss: 0.2958 - val_f_2: 0.4231 - val_acc: 0.9053\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3931 - acc: 0.8997 - val_loss: 0.2948 - val_f_2: 0.4245 - val_acc: 0.9053\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3843 - acc: 0.9004 - val_loss: 0.2962 - val_f_2: 0.3968 - val_acc: 0.9074\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.4027 - acc: 0.9017 - val_loss: 0.2930 - val_f_2: 0.4027 - val_acc: 0.9083\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3959 - acc: 0.9009 - val_loss: 0.2985 - val_f_2: 0.4292 - val_acc: 0.9050\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3020 - f_2: 0.4022 - acc: 0.9012 - val_loss: 0.2985 - val_f_2: 0.4016 - val_acc: 0.9047\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3038 - f_2: 0.3892 - acc: 0.8998 - val_loss: 0.3015 - val_f_2: 0.4647 - val_acc: 0.9032\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3007 - f_2: 0.3950 - acc: 0.8992 - val_loss: 0.2986 - val_f_2: 0.4470 - val_acc: 0.9015\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.4137 - acc: 0.9006 - val_loss: 0.2970 - val_f_2: 0.4368 - val_acc: 0.9038\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3903 - acc: 0.8982 - val_loss: 0.2953 - val_f_2: 0.4046 - val_acc: 0.9068\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3814 - acc: 0.8978 - val_loss: 0.2968 - val_f_2: 0.4104 - val_acc: 0.9053\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3854 - acc: 0.8995 - val_loss: 0.2934 - val_f_2: 0.3880 - val_acc: 0.9097\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3058 - f_2: 0.3908 - acc: 0.8998 - val_loss: 0.2964 - val_f_2: 0.4080 - val_acc: 0.9074\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3038 - f_2: 0.4060 - acc: 0.9012 - val_loss: 0.2981 - val_f_2: 0.4289 - val_acc: 0.9062\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3911 - acc: 0.8996 - val_loss: 0.2986 - val_f_2: 0.4172 - val_acc: 0.9065\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3039 - f_2: 0.3955 - acc: 0.9000 - val_loss: 0.2946 - val_f_2: 0.4148 - val_acc: 0.9044\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.4047 - acc: 0.9020 - val_loss: 0.2959 - val_f_2: 0.4128 - val_acc: 0.9065\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3901 - acc: 0.9003 - val_loss: 0.2954 - val_f_2: 0.4127 - val_acc: 0.9056\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3936 - acc: 0.8978 - val_loss: 0.2955 - val_f_2: 0.4342 - val_acc: 0.9044\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.4060 - acc: 0.9012 - val_loss: 0.2973 - val_f_2: 0.3943 - val_acc: 0.9074\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3998 - acc: 0.9006 - val_loss: 0.2965 - val_f_2: 0.4215 - val_acc: 0.9050\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=15, penalty=0.001, total= 1.0min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6425 - f_2: 0.0142 - acc: 0.8644 - val_loss: 0.4019 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3851 - f_2: 0.0089 - acc: 0.8753 - val_loss: 0.3647 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3623 - f_2: 3.2484e-08 - acc: 0.8753 - val_loss: 0.3535 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3506 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3418 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3427 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3370 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3396 - f_2: 3.1912e-08 - acc: 0.8753 - val_loss: 0.3336 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3372 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3319 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3343 - f_2: 0.0118 - acc: 0.8753 - val_loss: 0.3309 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3329 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3275 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.1009 - acc: 0.8855 - val_loss: 0.3269 - val_f_2: 0.2332 - val_acc: 0.8956\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3289 - f_2: 0.1948 - acc: 0.8929 - val_loss: 0.3247 - val_f_2: 0.2941 - val_acc: 0.8994\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.2111 - acc: 0.8946 - val_loss: 0.3241 - val_f_2: 0.2735 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3268 - f_2: 0.2346 - acc: 0.8981 - val_loss: 0.3230 - val_f_2: 0.2718 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.2172 - acc: 0.8967 - val_loss: 0.3259 - val_f_2: 0.2704 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3264 - f_2: 0.2360 - acc: 0.8984 - val_loss: 0.3202 - val_f_2: 0.2929 - val_acc: 0.9006\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3245 - f_2: 0.2523 - acc: 0.8987 - val_loss: 0.3202 - val_f_2: 0.3036 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2636 - acc: 0.8991 - val_loss: 0.3195 - val_f_2: 0.3048 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2520 - acc: 0.8990 - val_loss: 0.3199 - val_f_2: 0.2878 - val_acc: 0.9006\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2606 - acc: 0.9001 - val_loss: 0.3249 - val_f_2: 0.3113 - val_acc: 0.8991\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.2660 - acc: 0.8995 - val_loss: 0.3177 - val_f_2: 0.3096 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2896 - acc: 0.9011 - val_loss: 0.3184 - val_f_2: 0.2814 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.2638 - acc: 0.8998 - val_loss: 0.3215 - val_f_2: 0.3232 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2726 - acc: 0.9003 - val_loss: 0.3170 - val_f_2: 0.3251 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2905 - acc: 0.9012 - val_loss: 0.3168 - val_f_2: 0.3173 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3171 - f_2: 0.2887 - acc: 0.9029 - val_loss: 0.3195 - val_f_2: 0.3127 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.3099 - acc: 0.9024 - val_loss: 0.3169 - val_f_2: 0.3080 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.2914 - acc: 0.9013 - val_loss: 0.3215 - val_f_2: 0.3434 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.3194 - acc: 0.9017 - val_loss: 0.3144 - val_f_2: 0.3155 - val_acc: 0.8991\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.2883 - acc: 0.9008 - val_loss: 0.3155 - val_f_2: 0.3232 - val_acc: 0.8991\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.2997 - acc: 0.9023 - val_loss: 0.3161 - val_f_2: 0.2816 - val_acc: 0.8997\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.2973 - acc: 0.9017 - val_loss: 0.3188 - val_f_2: 0.3441 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3103 - acc: 0.9021 - val_loss: 0.3180 - val_f_2: 0.3604 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.3129 - acc: 0.9029 - val_loss: 0.3147 - val_f_2: 0.3657 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3004 - acc: 0.9015 - val_loss: 0.3162 - val_f_2: 0.3657 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.3127 - acc: 0.9028 - val_loss: 0.3213 - val_f_2: 0.3637 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.2945 - acc: 0.9024 - val_loss: 0.3147 - val_f_2: 0.4118 - val_acc: 0.9027\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.3254 - acc: 0.9017 - val_loss: 0.3158 - val_f_2: 0.3861 - val_acc: 0.9021\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.3114 - acc: 0.9017 - val_loss: 0.3174 - val_f_2: 0.4249 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3183 - acc: 0.9024 - val_loss: 0.3144 - val_f_2: 0.3426 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.2995 - acc: 0.9017 - val_loss: 0.3128 - val_f_2: 0.3029 - val_acc: 0.9009\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.3057 - acc: 0.9020 - val_loss: 0.3205 - val_f_2: 0.3751 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.3251 - acc: 0.9023 - val_loss: 0.3148 - val_f_2: 0.4030 - val_acc: 0.9027\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3247 - acc: 0.9026 - val_loss: 0.3141 - val_f_2: 0.3546 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.3198 - acc: 0.9028 - val_loss: 0.3159 - val_f_2: 0.4201 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3165 - acc: 0.9024 - val_loss: 0.3196 - val_f_2: 0.3633 - val_acc: 0.9012\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3399 - acc: 0.9038 - val_loss: 0.3135 - val_f_2: 0.3900 - val_acc: 0.9027\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3225 - acc: 0.9040 - val_loss: 0.3155 - val_f_2: 0.4054 - val_acc: 0.9012\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.3289 - acc: 0.9036 - val_loss: 0.3146 - val_f_2: 0.3637 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3183 - acc: 0.9031 - val_loss: 0.3167 - val_f_2: 0.3896 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3354 - acc: 0.9040 - val_loss: 0.3119 - val_f_2: 0.3215 - val_acc: 0.8985\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6408 - f_2: 0.0115 - acc: 0.8623 - val_loss: 0.4015 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3874 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3644 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3608 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3485 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3501 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3411 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3447 - f_2: 3.1104e-08 - acc: 0.8722 - val_loss: 0.3365 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3410 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3337 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3337 - f_2: 0.0059 - acc: 0.8722 - val_loss: 0.3332 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3359 - f_2: 0.0341 - acc: 0.8745 - val_loss: 0.3291 - val_f_2: 0.1606 - val_acc: 0.8864\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3329 - f_2: 0.1812 - acc: 0.8912 - val_loss: 0.3300 - val_f_2: 0.2684 - val_acc: 0.8994\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3309 - f_2: 0.2181 - acc: 0.8940 - val_loss: 0.3258 - val_f_2: 0.2736 - val_acc: 0.8994\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3281 - f_2: 0.2272 - acc: 0.8950 - val_loss: 0.3240 - val_f_2: 0.2736 - val_acc: 0.8994\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.2289 - acc: 0.8939 - val_loss: 0.3232 - val_f_2: 0.2936 - val_acc: 0.8991\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3294 - f_2: 0.2301 - acc: 0.8958 - val_loss: 0.3265 - val_f_2: 0.2736 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2571 - acc: 0.8989 - val_loss: 0.3207 - val_f_2: 0.3150 - val_acc: 0.9003\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2728 - acc: 0.8976 - val_loss: 0.3219 - val_f_2: 0.2768 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2711 - acc: 0.8981 - val_loss: 0.3205 - val_f_2: 0.2736 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.2824 - acc: 0.8995 - val_loss: 0.3186 - val_f_2: 0.3248 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.2908 - acc: 0.9001 - val_loss: 0.3296 - val_f_2: 0.2737 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.3093 - acc: 0.9025 - val_loss: 0.3199 - val_f_2: 0.2918 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.3127 - acc: 0.9015 - val_loss: 0.3162 - val_f_2: 0.3127 - val_acc: 0.9012\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.3042 - acc: 0.9007 - val_loss: 0.3274 - val_f_2: 0.2964 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.2916 - acc: 0.8998 - val_loss: 0.3146 - val_f_2: 0.3297 - val_acc: 0.9021\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.3220 - acc: 0.9017 - val_loss: 0.3140 - val_f_2: 0.3326 - val_acc: 0.9021\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.3197 - acc: 0.9012 - val_loss: 0.3159 - val_f_2: 0.3542 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.3110 - acc: 0.9009 - val_loss: 0.3143 - val_f_2: 0.3080 - val_acc: 0.9006\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.3307 - acc: 0.9003 - val_loss: 0.3144 - val_f_2: 0.3399 - val_acc: 0.9012\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.3171 - acc: 0.9014 - val_loss: 0.3141 - val_f_2: 0.3356 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.3267 - acc: 0.9009 - val_loss: 0.3137 - val_f_2: 0.3567 - val_acc: 0.9018\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3191 - acc: 0.9003 - val_loss: 0.3136 - val_f_2: 0.3899 - val_acc: 0.9021\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.3334 - acc: 0.9021 - val_loss: 0.3124 - val_f_2: 0.3466 - val_acc: 0.9012\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.3552 - acc: 0.9028 - val_loss: 0.3113 - val_f_2: 0.3226 - val_acc: 0.9009\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3378 - acc: 0.9031 - val_loss: 0.3143 - val_f_2: 0.3630 - val_acc: 0.9009\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3169 - f_2: 0.3321 - acc: 0.9024 - val_loss: 0.3127 - val_f_2: 0.3936 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.3256 - acc: 0.9021 - val_loss: 0.3120 - val_f_2: 0.3609 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3122 - f_2: 0.3470 - acc: 0.9031 - val_loss: 0.3134 - val_f_2: 0.3432 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.3444 - acc: 0.9009 - val_loss: 0.3121 - val_f_2: 0.3571 - val_acc: 0.9015\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.3239 - acc: 0.9009 - val_loss: 0.3175 - val_f_2: 0.2941 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.3335 - acc: 0.9012 - val_loss: 0.3121 - val_f_2: 0.4211 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3460 - acc: 0.9032 - val_loss: 0.3112 - val_f_2: 0.3525 - val_acc: 0.9012\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3364 - acc: 0.9028 - val_loss: 0.3121 - val_f_2: 0.4090 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.3471 - acc: 0.9018 - val_loss: 0.3125 - val_f_2: 0.3163 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3546 - acc: 0.9032 - val_loss: 0.3104 - val_f_2: 0.3358 - val_acc: 0.9006\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3601 - acc: 0.9038 - val_loss: 0.3119 - val_f_2: 0.4221 - val_acc: 0.9027\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3554 - acc: 0.9034 - val_loss: 0.3116 - val_f_2: 0.4067 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3518 - acc: 0.9012 - val_loss: 0.3105 - val_f_2: 0.3653 - val_acc: 0.9021\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3114 - f_2: 0.3645 - acc: 0.9037 - val_loss: 0.3145 - val_f_2: 0.3904 - val_acc: 0.9024\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3482 - acc: 0.9031 - val_loss: 0.3099 - val_f_2: 0.3030 - val_acc: 0.9012\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3496 - acc: 0.9033 - val_loss: 0.3120 - val_f_2: 0.3288 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3707 - acc: 0.9043 - val_loss: 0.3125 - val_f_2: 0.3843 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3123 - f_2: 0.3429 - acc: 0.9018 - val_loss: 0.3088 - val_f_2: 0.4041 - val_acc: 0.9024\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.6421 - f_2: 0.0101 - acc: 0.8582 - val_loss: 0.3794 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3894 - f_2: 3.0344e-08 - acc: 0.8691 - val_loss: 0.3459 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3690 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3296 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3527 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3228 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3503 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3193 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3445 - f_2: 0.0030 - acc: 0.8691 - val_loss: 0.3151 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3445 - f_2: 2.9867e-08 - acc: 0.8691 - val_loss: 0.3135 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3403 - f_2: 0.0315 - acc: 0.8723 - val_loss: 0.3105 - val_f_2: 0.0277 - val_acc: 0.8820\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3377 - f_2: 0.1849 - acc: 0.8888 - val_loss: 0.3087 - val_f_2: 0.2318 - val_acc: 0.9035\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3363 - f_2: 0.2009 - acc: 0.8900 - val_loss: 0.3069 - val_f_2: 0.2952 - val_acc: 0.9109\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3364 - f_2: 0.2276 - acc: 0.8917 - val_loss: 0.3051 - val_f_2: 0.2972 - val_acc: 0.9112\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3360 - f_2: 0.2305 - acc: 0.8925 - val_loss: 0.3086 - val_f_2: 0.3211 - val_acc: 0.9088\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3345 - f_2: 0.2410 - acc: 0.8917 - val_loss: 0.3037 - val_f_2: 0.3111 - val_acc: 0.9121\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3341 - f_2: 0.2483 - acc: 0.8940 - val_loss: 0.3020 - val_f_2: 0.3003 - val_acc: 0.9115\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3312 - f_2: 0.2574 - acc: 0.8932 - val_loss: 0.3018 - val_f_2: 0.3133 - val_acc: 0.9121\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3286 - f_2: 0.2588 - acc: 0.8954 - val_loss: 0.3012 - val_f_2: 0.3132 - val_acc: 0.9109\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3303 - f_2: 0.2674 - acc: 0.8955 - val_loss: 0.3003 - val_f_2: 0.2988 - val_acc: 0.9115\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3308 - f_2: 0.2731 - acc: 0.8944 - val_loss: 0.3003 - val_f_2: 0.3133 - val_acc: 0.9121\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3278 - f_2: 0.2765 - acc: 0.8966 - val_loss: 0.2988 - val_f_2: 0.2976 - val_acc: 0.9106\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3269 - f_2: 0.2862 - acc: 0.8954 - val_loss: 0.2986 - val_f_2: 0.3109 - val_acc: 0.9118\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.2776 - acc: 0.8964 - val_loss: 0.3010 - val_f_2: 0.3636 - val_acc: 0.9077\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2863 - acc: 0.8963 - val_loss: 0.2990 - val_f_2: 0.3404 - val_acc: 0.9074\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3268 - f_2: 0.2967 - acc: 0.8953 - val_loss: 0.2966 - val_f_2: 0.3244 - val_acc: 0.9086\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3252 - f_2: 0.2976 - acc: 0.8972 - val_loss: 0.2957 - val_f_2: 0.3207 - val_acc: 0.9097\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3254 - f_2: 0.3097 - acc: 0.8969 - val_loss: 0.2971 - val_f_2: 0.3205 - val_acc: 0.9109\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.3054 - acc: 0.8971 - val_loss: 0.2955 - val_f_2: 0.3229 - val_acc: 0.9086\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.2991 - acc: 0.8964 - val_loss: 0.2951 - val_f_2: 0.3341 - val_acc: 0.9077\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3221 - f_2: 0.3080 - acc: 0.8960 - val_loss: 0.2940 - val_f_2: 0.3229 - val_acc: 0.9091\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.3129 - acc: 0.8975 - val_loss: 0.2939 - val_f_2: 0.3169 - val_acc: 0.9115\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.2967 - acc: 0.8970 - val_loss: 0.2942 - val_f_2: 0.3722 - val_acc: 0.9074\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.3273 - acc: 0.8976 - val_loss: 0.2945 - val_f_2: 0.3370 - val_acc: 0.9091\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.3294 - acc: 0.8981 - val_loss: 0.2929 - val_f_2: 0.3311 - val_acc: 0.9086\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3354 - acc: 0.8981 - val_loss: 0.2932 - val_f_2: 0.3459 - val_acc: 0.9091\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.3344 - acc: 0.8979 - val_loss: 0.2918 - val_f_2: 0.3155 - val_acc: 0.9106\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3191 - f_2: 0.3139 - acc: 0.8973 - val_loss: 0.2940 - val_f_2: 0.4033 - val_acc: 0.9068\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3207 - f_2: 0.3238 - acc: 0.8976 - val_loss: 0.2943 - val_f_2: 0.3435 - val_acc: 0.9088\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3280 - acc: 0.8977 - val_loss: 0.2919 - val_f_2: 0.3491 - val_acc: 0.9088\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3180 - f_2: 0.3268 - acc: 0.8972 - val_loss: 0.2912 - val_f_2: 0.3282 - val_acc: 0.9106\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.3370 - acc: 0.8981 - val_loss: 0.2964 - val_f_2: 0.3132 - val_acc: 0.9106\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3182 - f_2: 0.3218 - acc: 0.8981 - val_loss: 0.2934 - val_f_2: 0.3721 - val_acc: 0.9071\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.3144 - acc: 0.8969 - val_loss: 0.2937 - val_f_2: 0.3821 - val_acc: 0.9068\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3160 - f_2: 0.3543 - acc: 0.9009 - val_loss: 0.2923 - val_f_2: 0.3103 - val_acc: 0.9100\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3343 - acc: 0.8978 - val_loss: 0.2919 - val_f_2: 0.3378 - val_acc: 0.9109\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.3471 - acc: 0.8998 - val_loss: 0.2904 - val_f_2: 0.3335 - val_acc: 0.9097\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.3378 - acc: 0.8984 - val_loss: 0.2936 - val_f_2: 0.4162 - val_acc: 0.9077\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.3534 - acc: 0.8995 - val_loss: 0.2901 - val_f_2: 0.3212 - val_acc: 0.9109\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3136 - f_2: 0.3389 - acc: 0.8988 - val_loss: 0.2938 - val_f_2: 0.3608 - val_acc: 0.9103\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3446 - acc: 0.8981 - val_loss: 0.2917 - val_f_2: 0.3655 - val_acc: 0.9088\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.3387 - acc: 0.8990 - val_loss: 0.2897 - val_f_2: 0.3484 - val_acc: 0.9103\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3164 - f_2: 0.3354 - acc: 0.8981 - val_loss: 0.2933 - val_f_2: 0.4135 - val_acc: 0.9071\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.01, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4392 - f_2: 0.0069 - acc: 0.8662 - val_loss: 0.3679 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3672 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3449 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3515 - f_2: 0.0030 - acc: 0.8753 - val_loss: 0.3359 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3393 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3304 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3338 - f_2: 0.0059 - acc: 0.8753 - val_loss: 0.3266 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3254 - f_2: 3.0805e-08 - acc: 0.8753 - val_loss: 0.3230 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2371 - acc: 0.8983 - val_loss: 0.3245 - val_f_2: 0.2716 - val_acc: 0.8991\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3207 - f_2: 0.2964 - acc: 0.9037 - val_loss: 0.3191 - val_f_2: 0.3245 - val_acc: 0.8997\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.3033 - acc: 0.9051 - val_loss: 0.3270 - val_f_2: 0.3193 - val_acc: 0.8985\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.3280 - acc: 0.9071 - val_loss: 0.3177 - val_f_2: 0.3278 - val_acc: 0.9003\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.3091 - acc: 0.9051 - val_loss: 0.3175 - val_f_2: 0.3551 - val_acc: 0.9018\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3100 - f_2: 0.3496 - acc: 0.9067 - val_loss: 0.3165 - val_f_2: 0.3547 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3565 - acc: 0.9061 - val_loss: 0.3220 - val_f_2: 0.3418 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3623 - acc: 0.9073 - val_loss: 0.3199 - val_f_2: 0.3532 - val_acc: 0.9018\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3587 - acc: 0.9067 - val_loss: 0.3195 - val_f_2: 0.3644 - val_acc: 0.9015\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3785 - acc: 0.9076 - val_loss: 0.3278 - val_f_2: 0.3494 - val_acc: 0.9009\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3699 - acc: 0.9063 - val_loss: 0.3192 - val_f_2: 0.3898 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3701 - acc: 0.9070 - val_loss: 0.3202 - val_f_2: 0.3737 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.3662 - acc: 0.9074 - val_loss: 0.3183 - val_f_2: 0.3864 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3861 - acc: 0.9079 - val_loss: 0.3182 - val_f_2: 0.3988 - val_acc: 0.9021\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3841 - acc: 0.9073 - val_loss: 0.3246 - val_f_2: 0.4008 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.3968 - acc: 0.9086 - val_loss: 0.3182 - val_f_2: 0.3959 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3946 - acc: 0.9086 - val_loss: 0.3221 - val_f_2: 0.3858 - val_acc: 0.8994\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3947 - acc: 0.9083 - val_loss: 0.3182 - val_f_2: 0.3983 - val_acc: 0.8991\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3051 - f_2: 0.3868 - acc: 0.9088 - val_loss: 0.3196 - val_f_2: 0.3896 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3062 - f_2: 0.4029 - acc: 0.9082 - val_loss: 0.3282 - val_f_2: 0.3962 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.3954 - acc: 0.9085 - val_loss: 0.3345 - val_f_2: 0.3760 - val_acc: 0.9009\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.4031 - acc: 0.9085 - val_loss: 0.3200 - val_f_2: 0.3990 - val_acc: 0.8997\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.4110 - acc: 0.9086 - val_loss: 0.3265 - val_f_2: 0.3980 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3032 - f_2: 0.4135 - acc: 0.9088 - val_loss: 0.3214 - val_f_2: 0.4049 - val_acc: 0.8997\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3954 - acc: 0.9093 - val_loss: 0.3248 - val_f_2: 0.3877 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.4102 - acc: 0.9099 - val_loss: 0.3204 - val_f_2: 0.4048 - val_acc: 0.8991\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.4168 - acc: 0.9093 - val_loss: 0.3248 - val_f_2: 0.4044 - val_acc: 0.9003\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3019 - f_2: 0.4014 - acc: 0.9101 - val_loss: 0.3261 - val_f_2: 0.3941 - val_acc: 0.8991\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.4145 - acc: 0.9091 - val_loss: 0.3260 - val_f_2: 0.4049 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.4030 - acc: 0.9093 - val_loss: 0.3279 - val_f_2: 0.4042 - val_acc: 0.8979\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.4158 - acc: 0.9104 - val_loss: 0.3275 - val_f_2: 0.4110 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.4133 - acc: 0.9102 - val_loss: 0.3216 - val_f_2: 0.4175 - val_acc: 0.8979\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2963 - f_2: 0.4159 - acc: 0.9097 - val_loss: 0.3232 - val_f_2: 0.4100 - val_acc: 0.8965\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.4226 - acc: 0.9090 - val_loss: 0.3214 - val_f_2: 0.4075 - val_acc: 0.9003\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.4121 - acc: 0.9092 - val_loss: 0.3270 - val_f_2: 0.4176 - val_acc: 0.8991\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.4129 - acc: 0.9094 - val_loss: 0.3211 - val_f_2: 0.4155 - val_acc: 0.8994\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.4168 - acc: 0.9097 - val_loss: 0.3228 - val_f_2: 0.4210 - val_acc: 0.8979\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.4234 - acc: 0.9105 - val_loss: 0.3238 - val_f_2: 0.4031 - val_acc: 0.9003\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.4186 - acc: 0.9091 - val_loss: 0.3239 - val_f_2: 0.4144 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.4208 - acc: 0.9099 - val_loss: 0.3244 - val_f_2: 0.4110 - val_acc: 0.8997\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.4194 - acc: 0.9102 - val_loss: 0.3220 - val_f_2: 0.4119 - val_acc: 0.8997\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.4183 - acc: 0.9100 - val_loss: 0.3272 - val_f_2: 0.4092 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.4186 - acc: 0.9110 - val_loss: 0.3306 - val_f_2: 0.4000 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.4252 - acc: 0.9093 - val_loss: 0.3235 - val_f_2: 0.4177 - val_acc: 0.8982\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4406 - f_2: 0.0113 - acc: 0.8633 - val_loss: 0.3706 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3639 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3458 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3453 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3356 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3373 - f_2: 0.0030 - acc: 0.8722 - val_loss: 0.3287 - val_f_2: 2.9563e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3322 - f_2: 0.0339 - acc: 0.8756 - val_loss: 0.3255 - val_f_2: 0.2755 - val_acc: 0.8997\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3279 - f_2: 0.2534 - acc: 0.8995 - val_loss: 0.3234 - val_f_2: 0.2769 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2924 - acc: 0.9020 - val_loss: 0.3222 - val_f_2: 0.2827 - val_acc: 0.8997\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.3066 - acc: 0.9034 - val_loss: 0.3183 - val_f_2: 0.3354 - val_acc: 0.9015\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.3067 - acc: 0.9040 - val_loss: 0.3167 - val_f_2: 0.3212 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3190 - f_2: 0.3173 - acc: 0.9034 - val_loss: 0.3210 - val_f_2: 0.3130 - val_acc: 0.9012\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.3322 - acc: 0.9032 - val_loss: 0.3157 - val_f_2: 0.3417 - val_acc: 0.9009\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3388 - acc: 0.9051 - val_loss: 0.3183 - val_f_2: 0.3423 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.3470 - acc: 0.9049 - val_loss: 0.3128 - val_f_2: 0.3841 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.3674 - acc: 0.9065 - val_loss: 0.3139 - val_f_2: 0.4041 - val_acc: 0.9018\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3830 - acc: 0.9074 - val_loss: 0.3131 - val_f_2: 0.3657 - val_acc: 0.9006\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3739 - acc: 0.9068 - val_loss: 0.3211 - val_f_2: 0.3707 - val_acc: 0.9009\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.3820 - acc: 0.9064 - val_loss: 0.3206 - val_f_2: 0.3706 - val_acc: 0.9015\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3766 - acc: 0.9064 - val_loss: 0.3128 - val_f_2: 0.3946 - val_acc: 0.9021\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3798 - acc: 0.9060 - val_loss: 0.3146 - val_f_2: 0.4028 - val_acc: 0.9024\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3888 - acc: 0.9071 - val_loss: 0.3202 - val_f_2: 0.4005 - val_acc: 0.9024\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3965 - acc: 0.9076 - val_loss: 0.3126 - val_f_2: 0.4337 - val_acc: 0.9032\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3946 - acc: 0.9067 - val_loss: 0.3124 - val_f_2: 0.4052 - val_acc: 0.9027\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.4128 - acc: 0.9068 - val_loss: 0.3185 - val_f_2: 0.4010 - val_acc: 0.9029\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3057 - f_2: 0.4077 - acc: 0.9076 - val_loss: 0.3127 - val_f_2: 0.4151 - val_acc: 0.9027\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3953 - acc: 0.9070 - val_loss: 0.3135 - val_f_2: 0.4069 - val_acc: 0.9021\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3039 - f_2: 0.3867 - acc: 0.9062 - val_loss: 0.3245 - val_f_2: 0.3890 - val_acc: 0.9024\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.4160 - acc: 0.9076 - val_loss: 0.3155 - val_f_2: 0.4022 - val_acc: 0.9018\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.4040 - acc: 0.9062 - val_loss: 0.3160 - val_f_2: 0.4037 - val_acc: 0.9012\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.4223 - acc: 0.9088 - val_loss: 0.3179 - val_f_2: 0.4109 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.4157 - acc: 0.9073 - val_loss: 0.3157 - val_f_2: 0.4035 - val_acc: 0.9027\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.4168 - acc: 0.9074 - val_loss: 0.3221 - val_f_2: 0.4075 - val_acc: 0.9029\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.4093 - acc: 0.9062 - val_loss: 0.3153 - val_f_2: 0.4211 - val_acc: 0.9021\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.4069 - acc: 0.9070 - val_loss: 0.3176 - val_f_2: 0.4239 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.4341 - acc: 0.9092 - val_loss: 0.3177 - val_f_2: 0.4338 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.4216 - acc: 0.9088 - val_loss: 0.3161 - val_f_2: 0.4328 - val_acc: 0.9018\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.4214 - acc: 0.9079 - val_loss: 0.3233 - val_f_2: 0.4150 - val_acc: 0.9021\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.4192 - acc: 0.9062 - val_loss: 0.3221 - val_f_2: 0.4139 - val_acc: 0.9029\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.4058 - acc: 0.9071 - val_loss: 0.3164 - val_f_2: 0.4137 - val_acc: 0.9015\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2988 - f_2: 0.4283 - acc: 0.9085 - val_loss: 0.3239 - val_f_2: 0.4170 - val_acc: 0.9024\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.4347 - acc: 0.9091 - val_loss: 0.3158 - val_f_2: 0.4287 - val_acc: 0.9032\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2977 - f_2: 0.4261 - acc: 0.9083 - val_loss: 0.3220 - val_f_2: 0.4198 - val_acc: 0.9024\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.4383 - acc: 0.9079 - val_loss: 0.3173 - val_f_2: 0.4364 - val_acc: 0.8994\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.4450 - acc: 0.9079 - val_loss: 0.3207 - val_f_2: 0.4301 - val_acc: 0.9029\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.4304 - acc: 0.9083 - val_loss: 0.3281 - val_f_2: 0.4099 - val_acc: 0.9018\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.4275 - acc: 0.9076 - val_loss: 0.3198 - val_f_2: 0.4268 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.4416 - acc: 0.9094 - val_loss: 0.3174 - val_f_2: 0.4419 - val_acc: 0.9018\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.4348 - acc: 0.9088 - val_loss: 0.3191 - val_f_2: 0.4324 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.4373 - acc: 0.9091 - val_loss: 0.3207 - val_f_2: 0.4238 - val_acc: 0.9029\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.4232 - acc: 0.9083 - val_loss: 0.3291 - val_f_2: 0.4187 - val_acc: 0.9012\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.4329 - acc: 0.9091 - val_loss: 0.3226 - val_f_2: 0.4242 - val_acc: 0.9006\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=40, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4470 - f_2: 0.0069 - acc: 0.8591 - val_loss: 0.3466 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3708 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3237 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3554 - f_2: 0.0059 - acc: 0.8691 - val_loss: 0.3161 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3420 - f_2: 2.9209e-08 - acc: 0.8691 - val_loss: 0.3117 - val_f_2: 3.3673e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3386 - f_2: 0.0242 - acc: 0.8716 - val_loss: 0.3115 - val_f_2: 0.3083 - val_acc: 0.9100\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3331 - f_2: 0.2624 - acc: 0.8977 - val_loss: 0.3107 - val_f_2: 0.2937 - val_acc: 0.9100\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.2936 - acc: 0.8990 - val_loss: 0.3059 - val_f_2: 0.3273 - val_acc: 0.9106\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.3102 - acc: 0.9004 - val_loss: 0.3026 - val_f_2: 0.3417 - val_acc: 0.9100\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.3214 - acc: 0.9009 - val_loss: 0.3009 - val_f_2: 0.3421 - val_acc: 0.9094\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.3272 - acc: 0.9001 - val_loss: 0.2984 - val_f_2: 0.3355 - val_acc: 0.9097\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.3319 - acc: 0.9003 - val_loss: 0.2976 - val_f_2: 0.3417 - val_acc: 0.9088\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3191 - f_2: 0.3427 - acc: 0.9004 - val_loss: 0.2974 - val_f_2: 0.3552 - val_acc: 0.9091\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.3577 - acc: 0.9009 - val_loss: 0.2970 - val_f_2: 0.3939 - val_acc: 0.9100\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.3618 - acc: 0.9012 - val_loss: 0.2978 - val_f_2: 0.3882 - val_acc: 0.9103\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3555 - acc: 0.9017 - val_loss: 0.2967 - val_f_2: 0.3891 - val_acc: 0.9094\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3116 - f_2: 0.3675 - acc: 0.9017 - val_loss: 0.2954 - val_f_2: 0.3914 - val_acc: 0.9100\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3680 - acc: 0.9023 - val_loss: 0.2945 - val_f_2: 0.3871 - val_acc: 0.9097\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3699 - acc: 0.9014 - val_loss: 0.2938 - val_f_2: 0.3918 - val_acc: 0.9109\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3738 - acc: 0.9004 - val_loss: 0.2955 - val_f_2: 0.4005 - val_acc: 0.9088\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3731 - acc: 0.9006 - val_loss: 0.2965 - val_f_2: 0.4293 - val_acc: 0.9100\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3086 - f_2: 0.3858 - acc: 0.9007 - val_loss: 0.2956 - val_f_2: 0.3989 - val_acc: 0.9086\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3722 - acc: 0.9011 - val_loss: 0.2998 - val_f_2: 0.3837 - val_acc: 0.9100\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3602 - acc: 0.9006 - val_loss: 0.2962 - val_f_2: 0.4026 - val_acc: 0.9086\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3074 - f_2: 0.3954 - acc: 0.9021 - val_loss: 0.2975 - val_f_2: 0.3942 - val_acc: 0.9083\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3677 - acc: 0.9000 - val_loss: 0.2981 - val_f_2: 0.4235 - val_acc: 0.9071\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3075 - f_2: 0.3661 - acc: 0.9004 - val_loss: 0.2971 - val_f_2: 0.4240 - val_acc: 0.9077\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.4056 - acc: 0.9016 - val_loss: 0.2953 - val_f_2: 0.3645 - val_acc: 0.9097\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3975 - acc: 0.9028 - val_loss: 0.2979 - val_f_2: 0.3975 - val_acc: 0.9080\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3812 - acc: 0.9018 - val_loss: 0.2998 - val_f_2: 0.4373 - val_acc: 0.9083\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3966 - acc: 0.9020 - val_loss: 0.3014 - val_f_2: 0.4239 - val_acc: 0.9027\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.4156 - acc: 0.9045 - val_loss: 0.2964 - val_f_2: 0.4054 - val_acc: 0.9088\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.3937 - acc: 0.9018 - val_loss: 0.2997 - val_f_2: 0.3918 - val_acc: 0.9047\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3879 - acc: 0.9026 - val_loss: 0.2984 - val_f_2: 0.4092 - val_acc: 0.9083\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3999 - acc: 0.9009 - val_loss: 0.3013 - val_f_2: 0.4092 - val_acc: 0.9086\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.4072 - acc: 0.9032 - val_loss: 0.2986 - val_f_2: 0.3762 - val_acc: 0.9080\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.4129 - acc: 0.9029 - val_loss: 0.2991 - val_f_2: 0.3742 - val_acc: 0.9118\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.4113 - acc: 0.9042 - val_loss: 0.2964 - val_f_2: 0.4126 - val_acc: 0.9062\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.4016 - acc: 0.9013 - val_loss: 0.3039 - val_f_2: 0.4027 - val_acc: 0.9077\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.4028 - acc: 0.9024 - val_loss: 0.2958 - val_f_2: 0.3879 - val_acc: 0.9086\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.4014 - acc: 0.9023 - val_loss: 0.2986 - val_f_2: 0.4066 - val_acc: 0.9083\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3956 - acc: 0.9037 - val_loss: 0.2988 - val_f_2: 0.4056 - val_acc: 0.9047\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3933 - acc: 0.9023 - val_loss: 0.3002 - val_f_2: 0.4095 - val_acc: 0.9068\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.4293 - acc: 0.9036 - val_loss: 0.3035 - val_f_2: 0.4055 - val_acc: 0.9053\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.4142 - acc: 0.9021 - val_loss: 0.2985 - val_f_2: 0.4144 - val_acc: 0.9059\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3005 - f_2: 0.4114 - acc: 0.9032 - val_loss: 0.3075 - val_f_2: 0.3883 - val_acc: 0.9053\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.4145 - acc: 0.9026 - val_loss: 0.3012 - val_f_2: 0.4066 - val_acc: 0.9053\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.4245 - acc: 0.9033 - val_loss: 0.2996 - val_f_2: 0.4081 - val_acc: 0.9044\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.4168 - acc: 0.9032 - val_loss: 0.2983 - val_f_2: 0.4126 - val_acc: 0.9041\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3056 - f_2: 0.4141 - acc: 0.9026 - val_loss: 0.3000 - val_f_2: 0.3854 - val_acc: 0.9088\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3944 - acc: 0.9019 - val_loss: 0.2987 - val_f_2: 0.3979 - val_acc: 0.9053\n",
      "[CV]  batch_size=40, dropout=0.3, nodes=20, penalty=0.001, total= 1.1min\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7191 - f_2: 0.0325 - acc: 0.8411 - val_loss: 0.5038 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4440 - f_2: 1.3887e-08 - acc: 0.8753 - val_loss: 0.3942 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3851 - f_2: 1.3834e-08 - acc: 0.8753 - val_loss: 0.3674 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3658 - f_2: 1.3780e-08 - acc: 0.8753 - val_loss: 0.3557 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3547 - f_2: 1.4032e-08 - acc: 0.8753 - val_loss: 0.3481 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3479 - f_2: 1.3599e-08 - acc: 0.8753 - val_loss: 0.3411 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3434 - f_2: 1.3634e-08 - acc: 0.8753 - val_loss: 0.3371 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3386 - f_2: 1.3930e-08 - acc: 0.8753 - val_loss: 0.3357 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3372 - f_2: 1.3823e-08 - acc: 0.8753 - val_loss: 0.3329 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3348 - f_2: 1.3886e-08 - acc: 0.8753 - val_loss: 0.3302 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3347 - f_2: 1.3830e-08 - acc: 0.8753 - val_loss: 0.3294 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3326 - f_2: 1.4048e-08 - acc: 0.8753 - val_loss: 0.3272 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3322 - f_2: 1.3930e-08 - acc: 0.8753 - val_loss: 0.3283 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3332 - f_2: 1.3887e-08 - acc: 0.8753 - val_loss: 0.3257 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.0448 - acc: 0.8799 - val_loss: 0.3262 - val_f_2: 0.0095 - val_acc: 0.8678\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.1577 - acc: 0.8917 - val_loss: 0.3240 - val_f_2: 0.2408 - val_acc: 0.8947\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3259 - f_2: 0.1834 - acc: 0.8934 - val_loss: 0.3243 - val_f_2: 0.2473 - val_acc: 0.8953\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2084 - acc: 0.8958 - val_loss: 0.3217 - val_f_2: 0.2652 - val_acc: 0.8976\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.2031 - acc: 0.8956 - val_loss: 0.3227 - val_f_2: 0.2492 - val_acc: 0.8956\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2114 - acc: 0.8960 - val_loss: 0.3206 - val_f_2: 0.2740 - val_acc: 0.8988\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2244 - acc: 0.8975 - val_loss: 0.3211 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3239 - f_2: 0.2250 - acc: 0.8965 - val_loss: 0.3207 - val_f_2: 0.2855 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2324 - acc: 0.8970 - val_loss: 0.3207 - val_f_2: 0.2444 - val_acc: 0.8950\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.2269 - acc: 0.8965 - val_loss: 0.3188 - val_f_2: 0.2853 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.2330 - acc: 0.8973 - val_loss: 0.3178 - val_f_2: 0.2914 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.2389 - acc: 0.8985 - val_loss: 0.3178 - val_f_2: 0.3064 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3224 - f_2: 0.2482 - acc: 0.8986 - val_loss: 0.3169 - val_f_2: 0.2958 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3230 - f_2: 0.2318 - acc: 0.8963 - val_loss: 0.3173 - val_f_2: 0.2917 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.2511 - acc: 0.8995 - val_loss: 0.3217 - val_f_2: 0.2916 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.2574 - acc: 0.8979 - val_loss: 0.3164 - val_f_2: 0.2958 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.2498 - acc: 0.8976 - val_loss: 0.3159 - val_f_2: 0.2981 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.2577 - acc: 0.9001 - val_loss: 0.3181 - val_f_2: 0.3104 - val_acc: 0.9012\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.2543 - acc: 0.8987 - val_loss: 0.3159 - val_f_2: 0.3053 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.2383 - acc: 0.8968 - val_loss: 0.3144 - val_f_2: 0.3124 - val_acc: 0.9012\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.2478 - acc: 0.8978 - val_loss: 0.3140 - val_f_2: 0.3252 - val_acc: 0.9006\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2687 - acc: 0.9001 - val_loss: 0.3136 - val_f_2: 0.3055 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2636 - acc: 0.8988 - val_loss: 0.3159 - val_f_2: 0.3305 - val_acc: 0.8982\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.2752 - acc: 0.8987 - val_loss: 0.3133 - val_f_2: 0.3034 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3182 - f_2: 0.2543 - acc: 0.8981 - val_loss: 0.3124 - val_f_2: 0.3210 - val_acc: 0.9021\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.2480 - acc: 0.8973 - val_loss: 0.3132 - val_f_2: 0.3563 - val_acc: 0.8991\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3166 - f_2: 0.2699 - acc: 0.8995 - val_loss: 0.3123 - val_f_2: 0.3206 - val_acc: 0.9012\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.2560 - acc: 0.8976 - val_loss: 0.3125 - val_f_2: 0.3507 - val_acc: 0.8988\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.2569 - acc: 0.8975 - val_loss: 0.3149 - val_f_2: 0.2981 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.2613 - acc: 0.8979 - val_loss: 0.3123 - val_f_2: 0.3498 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.2642 - acc: 0.8996 - val_loss: 0.3126 - val_f_2: 0.3254 - val_acc: 0.9012\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.2806 - acc: 0.8992 - val_loss: 0.3122 - val_f_2: 0.3184 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.2686 - acc: 0.8979 - val_loss: 0.3205 - val_f_2: 0.2997 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.2690 - acc: 0.8982 - val_loss: 0.3111 - val_f_2: 0.3375 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.2823 - acc: 0.9004 - val_loss: 0.3130 - val_f_2: 0.3413 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.2666 - acc: 0.8988 - val_loss: 0.3124 - val_f_2: 0.3822 - val_acc: 0.9000\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.01, total=  32.6s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7214 - f_2: 0.0394 - acc: 0.8398 - val_loss: 0.5044 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4484 - f_2: 1.3459e-08 - acc: 0.8722 - val_loss: 0.3919 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3839 - f_2: 1.3672e-08 - acc: 0.8722 - val_loss: 0.3651 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3636 - f_2: 1.3785e-08 - acc: 0.8722 - val_loss: 0.3532 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3538 - f_2: 1.3515e-08 - acc: 0.8722 - val_loss: 0.3449 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3474 - f_2: 1.3960e-08 - acc: 0.8722 - val_loss: 0.3408 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3422 - f_2: 1.3422e-08 - acc: 0.8722 - val_loss: 0.3373 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3389 - f_2: 1.3914e-08 - acc: 0.8722 - val_loss: 0.3344 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3374 - f_2: 1.3636e-08 - acc: 0.8722 - val_loss: 0.3324 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3373 - f_2: 1.3937e-08 - acc: 0.8722 - val_loss: 0.3315 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3333 - f_2: 1.3468e-08 - acc: 0.8722 - val_loss: 0.3285 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3339 - f_2: 1.3325e-08 - acc: 0.8722 - val_loss: 0.3273 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.1337 - acc: 0.8859 - val_loss: 0.3259 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.2184 - acc: 0.8936 - val_loss: 0.3251 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.2283 - acc: 0.8944 - val_loss: 0.3233 - val_f_2: 0.2854 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3278 - f_2: 0.2344 - acc: 0.8955 - val_loss: 0.3238 - val_f_2: 0.3099 - val_acc: 0.8997\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3260 - f_2: 0.2404 - acc: 0.8961 - val_loss: 0.3217 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3281 - f_2: 0.2444 - acc: 0.8960 - val_loss: 0.3216 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3225 - f_2: 0.2500 - acc: 0.8967 - val_loss: 0.3199 - val_f_2: 0.2873 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3257 - f_2: 0.2649 - acc: 0.8978 - val_loss: 0.3189 - val_f_2: 0.2960 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.2768 - acc: 0.8984 - val_loss: 0.3191 - val_f_2: 0.3069 - val_acc: 0.8997\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3227 - f_2: 0.2682 - acc: 0.8975 - val_loss: 0.3183 - val_f_2: 0.3069 - val_acc: 0.8997\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.2705 - acc: 0.8974 - val_loss: 0.3180 - val_f_2: 0.3189 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2683 - acc: 0.8981 - val_loss: 0.3175 - val_f_2: 0.3587 - val_acc: 0.9018\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3207 - f_2: 0.2788 - acc: 0.8995 - val_loss: 0.3193 - val_f_2: 0.2852 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3227 - f_2: 0.2854 - acc: 0.8995 - val_loss: 0.3160 - val_f_2: 0.3070 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.2927 - acc: 0.8998 - val_loss: 0.3207 - val_f_2: 0.2931 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.2955 - acc: 0.9001 - val_loss: 0.3155 - val_f_2: 0.3701 - val_acc: 0.9018\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.2867 - acc: 0.8993 - val_loss: 0.3153 - val_f_2: 0.3840 - val_acc: 0.9029\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.2985 - acc: 0.8998 - val_loss: 0.3145 - val_f_2: 0.2950 - val_acc: 0.9003\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.2920 - acc: 0.8987 - val_loss: 0.3141 - val_f_2: 0.3554 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2862 - acc: 0.8997 - val_loss: 0.3134 - val_f_2: 0.3684 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.2980 - acc: 0.8998 - val_loss: 0.3135 - val_f_2: 0.3449 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3190 - f_2: 0.2947 - acc: 0.8992 - val_loss: 0.3118 - val_f_2: 0.3043 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.3006 - acc: 0.9001 - val_loss: 0.3132 - val_f_2: 0.2931 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.3052 - acc: 0.9013 - val_loss: 0.3119 - val_f_2: 0.3560 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3134 - acc: 0.9015 - val_loss: 0.3132 - val_f_2: 0.3393 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.3240 - acc: 0.9027 - val_loss: 0.3110 - val_f_2: 0.3122 - val_acc: 0.9009\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3100 - acc: 0.9005 - val_loss: 0.3112 - val_f_2: 0.3636 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3221 - acc: 0.9011 - val_loss: 0.3118 - val_f_2: 0.3414 - val_acc: 0.9018\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.3166 - acc: 0.9011 - val_loss: 0.3102 - val_f_2: 0.3611 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3159 - f_2: 0.3270 - acc: 0.9014 - val_loss: 0.3099 - val_f_2: 0.3014 - val_acc: 0.9006\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3242 - acc: 0.9017 - val_loss: 0.3156 - val_f_2: 0.3377 - val_acc: 0.9003\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.3321 - acc: 0.9035 - val_loss: 0.3108 - val_f_2: 0.3983 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.3287 - acc: 0.9006 - val_loss: 0.3103 - val_f_2: 0.3393 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3251 - acc: 0.9010 - val_loss: 0.3088 - val_f_2: 0.3637 - val_acc: 0.9015\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3320 - acc: 0.9017 - val_loss: 0.3113 - val_f_2: 0.3179 - val_acc: 0.9012\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3288 - acc: 0.9017 - val_loss: 0.3097 - val_f_2: 0.3089 - val_acc: 0.9009\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3114 - f_2: 0.3262 - acc: 0.9015 - val_loss: 0.3090 - val_f_2: 0.3385 - val_acc: 0.9024\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.3271 - acc: 0.9009 - val_loss: 0.3082 - val_f_2: 0.3581 - val_acc: 0.9012\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.01, total=  32.5s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7236 - f_2: 0.0272 - acc: 0.8376 - val_loss: 0.4847 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4564 - f_2: 1.3181e-08 - acc: 0.8691 - val_loss: 0.3784 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3916 - f_2: 1.3286e-08 - acc: 0.8691 - val_loss: 0.3483 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3737 - f_2: 1.3190e-08 - acc: 0.8691 - val_loss: 0.3361 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3616 - f_2: 1.3039e-08 - acc: 0.8691 - val_loss: 0.3285 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3546 - f_2: 1.3337e-08 - acc: 0.8691 - val_loss: 0.3259 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3499 - f_2: 1.3865e-08 - acc: 0.8691 - val_loss: 0.3204 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3492 - f_2: 1.3519e-08 - acc: 0.8691 - val_loss: 0.3175 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3464 - f_2: 1.3452e-08 - acc: 0.8691 - val_loss: 0.3159 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3443 - f_2: 1.3768e-08 - acc: 0.8691 - val_loss: 0.3140 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3418 - f_2: 1.3292e-08 - acc: 0.8691 - val_loss: 0.3151 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3412 - f_2: 1.3179e-08 - acc: 0.8691 - val_loss: 0.3108 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3376 - f_2: 1.3151e-08 - acc: 0.8691 - val_loss: 0.3099 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3366 - f_2: 0.0750 - acc: 0.8767 - val_loss: 0.3086 - val_f_2: 0.2618 - val_acc: 0.9065\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3380 - f_2: 0.1867 - acc: 0.8880 - val_loss: 0.3076 - val_f_2: 0.2725 - val_acc: 0.9074\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3352 - f_2: 0.2064 - acc: 0.8891 - val_loss: 0.3098 - val_f_2: 0.3128 - val_acc: 0.9112\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3371 - f_2: 0.1949 - acc: 0.8891 - val_loss: 0.3070 - val_f_2: 0.3007 - val_acc: 0.9106\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3332 - f_2: 0.2277 - acc: 0.8919 - val_loss: 0.3051 - val_f_2: 0.3039 - val_acc: 0.9109\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3338 - f_2: 0.2116 - acc: 0.8904 - val_loss: 0.3044 - val_f_2: 0.2988 - val_acc: 0.9103\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3324 - f_2: 0.2202 - acc: 0.8909 - val_loss: 0.3032 - val_f_2: 0.3054 - val_acc: 0.9112\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3313 - f_2: 0.2240 - acc: 0.8918 - val_loss: 0.3034 - val_f_2: 0.3200 - val_acc: 0.9106\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3296 - f_2: 0.2298 - acc: 0.8918 - val_loss: 0.3062 - val_f_2: 0.3279 - val_acc: 0.9088\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.2413 - acc: 0.8913 - val_loss: 0.3031 - val_f_2: 0.3197 - val_acc: 0.9115\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3275 - f_2: 0.2504 - acc: 0.8927 - val_loss: 0.3005 - val_f_2: 0.3213 - val_acc: 0.9106\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3311 - f_2: 0.2507 - acc: 0.8923 - val_loss: 0.3004 - val_f_2: 0.3292 - val_acc: 0.9091\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2500 - acc: 0.8919 - val_loss: 0.3022 - val_f_2: 0.3153 - val_acc: 0.9115\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3298 - f_2: 0.2606 - acc: 0.8939 - val_loss: 0.2999 - val_f_2: 0.3335 - val_acc: 0.9088\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3262 - f_2: 0.2638 - acc: 0.8935 - val_loss: 0.2985 - val_f_2: 0.3316 - val_acc: 0.9091\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.2827 - acc: 0.8954 - val_loss: 0.2973 - val_f_2: 0.3260 - val_acc: 0.9100\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.2731 - acc: 0.8941 - val_loss: 0.2983 - val_f_2: 0.3373 - val_acc: 0.9065\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.2838 - acc: 0.8956 - val_loss: 0.2974 - val_f_2: 0.3311 - val_acc: 0.9091\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2856 - acc: 0.8939 - val_loss: 0.2962 - val_f_2: 0.3225 - val_acc: 0.9112\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3229 - f_2: 0.2834 - acc: 0.8944 - val_loss: 0.2961 - val_f_2: 0.3288 - val_acc: 0.9106\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3238 - f_2: 0.3030 - acc: 0.8949 - val_loss: 0.2955 - val_f_2: 0.3303 - val_acc: 0.9100\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.2978 - acc: 0.8962 - val_loss: 0.2971 - val_f_2: 0.3444 - val_acc: 0.9083\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.2901 - acc: 0.8947 - val_loss: 0.2980 - val_f_2: 0.3694 - val_acc: 0.9068\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.2810 - acc: 0.8936 - val_loss: 0.2954 - val_f_2: 0.3376 - val_acc: 0.9091\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2918 - acc: 0.8944 - val_loss: 0.2983 - val_f_2: 0.4061 - val_acc: 0.9088\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.2885 - acc: 0.8947 - val_loss: 0.2967 - val_f_2: 0.3855 - val_acc: 0.9077\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3230 - f_2: 0.3019 - acc: 0.8950 - val_loss: 0.2989 - val_f_2: 0.3704 - val_acc: 0.9062\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3230 - f_2: 0.2865 - acc: 0.8950 - val_loss: 0.2942 - val_f_2: 0.3480 - val_acc: 0.9047\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.2998 - acc: 0.8937 - val_loss: 0.2934 - val_f_2: 0.3352 - val_acc: 0.9106\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2949 - acc: 0.8941 - val_loss: 0.2930 - val_f_2: 0.3335 - val_acc: 0.9097\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3218 - f_2: 0.3002 - acc: 0.8958 - val_loss: 0.2952 - val_f_2: 0.3378 - val_acc: 0.9071\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.3133 - acc: 0.8962 - val_loss: 0.2925 - val_f_2: 0.3323 - val_acc: 0.9083\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.3090 - acc: 0.8955 - val_loss: 0.2936 - val_f_2: 0.3475 - val_acc: 0.9068\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3208 - f_2: 0.3036 - acc: 0.8950 - val_loss: 0.2930 - val_f_2: 0.3503 - val_acc: 0.9065\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.3090 - acc: 0.8953 - val_loss: 0.2923 - val_f_2: 0.3510 - val_acc: 0.9080\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3198 - f_2: 0.3294 - acc: 0.8959 - val_loss: 0.2935 - val_f_2: 0.3237 - val_acc: 0.9109\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.3050 - acc: 0.8953 - val_loss: 0.2928 - val_f_2: 0.3852 - val_acc: 0.9080\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.01, total=  32.6s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4751 - f_2: 0.0221 - acc: 0.8460 - val_loss: 0.3700 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3678 - f_2: 0.0349 - acc: 0.8790 - val_loss: 0.3384 - val_f_2: 0.1944 - val_acc: 0.8888\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3464 - f_2: 0.1809 - acc: 0.8933 - val_loss: 0.3237 - val_f_2: 0.2906 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3295 - f_2: 0.2354 - acc: 0.8963 - val_loss: 0.3121 - val_f_2: 0.3298 - val_acc: 0.9015\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.2646 - acc: 0.8978 - val_loss: 0.3082 - val_f_2: 0.3236 - val_acc: 0.9009\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.2736 - acc: 0.8982 - val_loss: 0.3024 - val_f_2: 0.3432 - val_acc: 0.8997\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.2929 - acc: 0.9001 - val_loss: 0.3013 - val_f_2: 0.3428 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.2999 - acc: 0.9005 - val_loss: 0.2991 - val_f_2: 0.3640 - val_acc: 0.9018\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3224 - acc: 0.9028 - val_loss: 0.2982 - val_f_2: 0.3504 - val_acc: 0.9009\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3172 - acc: 0.9031 - val_loss: 0.2973 - val_f_2: 0.3673 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3018 - f_2: 0.3166 - acc: 0.9029 - val_loss: 0.2961 - val_f_2: 0.3847 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3245 - acc: 0.9022 - val_loss: 0.2960 - val_f_2: 0.3906 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2971 - f_2: 0.3387 - acc: 0.9041 - val_loss: 0.2967 - val_f_2: 0.3564 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2978 - f_2: 0.3306 - acc: 0.9031 - val_loss: 0.2946 - val_f_2: 0.3555 - val_acc: 0.9018\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2958 - f_2: 0.3183 - acc: 0.9033 - val_loss: 0.2949 - val_f_2: 0.3664 - val_acc: 0.8991\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3253 - acc: 0.9022 - val_loss: 0.2961 - val_f_2: 0.3635 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.3426 - acc: 0.9048 - val_loss: 0.2957 - val_f_2: 0.4087 - val_acc: 0.8985\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2930 - f_2: 0.3464 - acc: 0.9042 - val_loss: 0.2961 - val_f_2: 0.3992 - val_acc: 0.8985\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2921 - f_2: 0.3484 - acc: 0.9032 - val_loss: 0.2990 - val_f_2: 0.3469 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2918 - f_2: 0.3313 - acc: 0.9036 - val_loss: 0.2952 - val_f_2: 0.3686 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2910 - f_2: 0.3553 - acc: 0.9045 - val_loss: 0.2960 - val_f_2: 0.3829 - val_acc: 0.8991\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2916 - f_2: 0.3549 - acc: 0.9060 - val_loss: 0.2948 - val_f_2: 0.3799 - val_acc: 0.8991\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.3466 - acc: 0.9039 - val_loss: 0.2969 - val_f_2: 0.3990 - val_acc: 0.8988\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3708 - acc: 0.9054 - val_loss: 0.2963 - val_f_2: 0.3728 - val_acc: 0.8991\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.3683 - acc: 0.9065 - val_loss: 0.2970 - val_f_2: 0.3691 - val_acc: 0.8979\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2867 - f_2: 0.3680 - acc: 0.9062 - val_loss: 0.2965 - val_f_2: 0.3964 - val_acc: 0.8991\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.3630 - acc: 0.9067 - val_loss: 0.2971 - val_f_2: 0.3965 - val_acc: 0.8985\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2873 - f_2: 0.3598 - acc: 0.9046 - val_loss: 0.2971 - val_f_2: 0.3774 - val_acc: 0.8988\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.3720 - acc: 0.9068 - val_loss: 0.2966 - val_f_2: 0.3732 - val_acc: 0.8985\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.3812 - acc: 0.9073 - val_loss: 0.2969 - val_f_2: 0.3877 - val_acc: 0.8991\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2828 - f_2: 0.3836 - acc: 0.9074 - val_loss: 0.2975 - val_f_2: 0.4043 - val_acc: 0.8985\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.3700 - acc: 0.9070 - val_loss: 0.2976 - val_f_2: 0.3991 - val_acc: 0.8979\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.3797 - acc: 0.9065 - val_loss: 0.2997 - val_f_2: 0.4138 - val_acc: 0.8976\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.3745 - acc: 0.9065 - val_loss: 0.2981 - val_f_2: 0.4052 - val_acc: 0.8979\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2821 - f_2: 0.3768 - acc: 0.9068 - val_loss: 0.3005 - val_f_2: 0.3838 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2847 - f_2: 0.3792 - acc: 0.9056 - val_loss: 0.2986 - val_f_2: 0.3879 - val_acc: 0.8976\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2798 - f_2: 0.3965 - acc: 0.9091 - val_loss: 0.3030 - val_f_2: 0.3837 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4065 - acc: 0.9102 - val_loss: 0.2980 - val_f_2: 0.4080 - val_acc: 0.8971\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.3998 - acc: 0.9085 - val_loss: 0.2978 - val_f_2: 0.3931 - val_acc: 0.8982\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2802 - f_2: 0.3877 - acc: 0.9081 - val_loss: 0.2980 - val_f_2: 0.4130 - val_acc: 0.8971\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2801 - f_2: 0.3773 - acc: 0.9060 - val_loss: 0.3025 - val_f_2: 0.3769 - val_acc: 0.9003\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2806 - f_2: 0.3999 - acc: 0.9088 - val_loss: 0.3017 - val_f_2: 0.3989 - val_acc: 0.8997\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2791 - f_2: 0.3917 - acc: 0.9088 - val_loss: 0.3021 - val_f_2: 0.3846 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2810 - f_2: 0.3983 - acc: 0.9077 - val_loss: 0.3008 - val_f_2: 0.3717 - val_acc: 0.8997\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2774 - f_2: 0.4042 - acc: 0.9091 - val_loss: 0.3009 - val_f_2: 0.4021 - val_acc: 0.8976\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2796 - f_2: 0.4007 - acc: 0.9085 - val_loss: 0.3012 - val_f_2: 0.3860 - val_acc: 0.9003\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2773 - f_2: 0.4143 - acc: 0.9095 - val_loss: 0.3007 - val_f_2: 0.3975 - val_acc: 0.8950\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2779 - f_2: 0.4108 - acc: 0.9104 - val_loss: 0.2979 - val_f_2: 0.3929 - val_acc: 0.8997\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2771 - f_2: 0.4151 - acc: 0.9103 - val_loss: 0.2986 - val_f_2: 0.3955 - val_acc: 0.8953\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2785 - f_2: 0.4044 - acc: 0.9092 - val_loss: 0.3026 - val_f_2: 0.3853 - val_acc: 0.8991\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.001, total=  32.9s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4722 - f_2: 0.0281 - acc: 0.8450 - val_loss: 0.3685 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3656 - f_2: 0.0791 - acc: 0.8807 - val_loss: 0.3383 - val_f_2: 0.2384 - val_acc: 0.8938\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3463 - f_2: 0.2173 - acc: 0.8933 - val_loss: 0.3200 - val_f_2: 0.2991 - val_acc: 0.9006\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.2655 - acc: 0.8956 - val_loss: 0.3106 - val_f_2: 0.3438 - val_acc: 0.9021\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.2906 - acc: 0.8969 - val_loss: 0.3056 - val_f_2: 0.3339 - val_acc: 0.9015\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.3012 - acc: 0.8998 - val_loss: 0.3018 - val_f_2: 0.3637 - val_acc: 0.9009\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.3301 - acc: 0.9009 - val_loss: 0.3010 - val_f_2: 0.3601 - val_acc: 0.9006\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3241 - acc: 0.9001 - val_loss: 0.2996 - val_f_2: 0.3466 - val_acc: 0.9018\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3317 - acc: 0.9012 - val_loss: 0.2986 - val_f_2: 0.3696 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3037 - f_2: 0.3518 - acc: 0.9031 - val_loss: 0.2973 - val_f_2: 0.3726 - val_acc: 0.8988\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.3559 - acc: 0.9033 - val_loss: 0.2971 - val_f_2: 0.3615 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3689 - acc: 0.9053 - val_loss: 0.2964 - val_f_2: 0.3503 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.3494 - acc: 0.9023 - val_loss: 0.2962 - val_f_2: 0.4109 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.3446 - acc: 0.9020 - val_loss: 0.2964 - val_f_2: 0.3570 - val_acc: 0.9009\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2935 - f_2: 0.3649 - acc: 0.9035 - val_loss: 0.2952 - val_f_2: 0.3954 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2927 - f_2: 0.3692 - acc: 0.9043 - val_loss: 0.2949 - val_f_2: 0.4017 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3879 - acc: 0.9070 - val_loss: 0.2948 - val_f_2: 0.3863 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2928 - f_2: 0.3796 - acc: 0.9062 - val_loss: 0.2953 - val_f_2: 0.3634 - val_acc: 0.8982\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.3762 - acc: 0.9057 - val_loss: 0.2954 - val_f_2: 0.3826 - val_acc: 0.8985\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2895 - f_2: 0.3858 - acc: 0.9059 - val_loss: 0.2946 - val_f_2: 0.3688 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2910 - f_2: 0.3812 - acc: 0.9057 - val_loss: 0.2946 - val_f_2: 0.3469 - val_acc: 0.8979\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2882 - f_2: 0.3834 - acc: 0.9071 - val_loss: 0.2949 - val_f_2: 0.3908 - val_acc: 0.8982\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2858 - f_2: 0.3978 - acc: 0.9059 - val_loss: 0.2939 - val_f_2: 0.3770 - val_acc: 0.8988\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.3945 - acc: 0.9079 - val_loss: 0.2958 - val_f_2: 0.4166 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2858 - f_2: 0.4029 - acc: 0.9076 - val_loss: 0.2951 - val_f_2: 0.3655 - val_acc: 0.8991\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.3989 - acc: 0.9074 - val_loss: 0.2956 - val_f_2: 0.3559 - val_acc: 0.8985\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.4090 - acc: 0.9068 - val_loss: 0.2952 - val_f_2: 0.4093 - val_acc: 0.8997\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2844 - f_2: 0.4020 - acc: 0.9068 - val_loss: 0.2975 - val_f_2: 0.3610 - val_acc: 0.8979\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.4036 - acc: 0.9081 - val_loss: 0.2947 - val_f_2: 0.3957 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.3940 - acc: 0.9074 - val_loss: 0.2975 - val_f_2: 0.4202 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2835 - f_2: 0.4013 - acc: 0.9073 - val_loss: 0.2952 - val_f_2: 0.4356 - val_acc: 0.9015\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2829 - f_2: 0.4084 - acc: 0.9076 - val_loss: 0.2952 - val_f_2: 0.4294 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2823 - f_2: 0.4062 - acc: 0.9074 - val_loss: 0.2953 - val_f_2: 0.3790 - val_acc: 0.8979\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2820 - f_2: 0.4211 - acc: 0.9097 - val_loss: 0.2946 - val_f_2: 0.4351 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2814 - f_2: 0.3915 - acc: 0.9057 - val_loss: 0.2942 - val_f_2: 0.4107 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2802 - f_2: 0.4165 - acc: 0.9096 - val_loss: 0.2947 - val_f_2: 0.4048 - val_acc: 0.8982\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2795 - f_2: 0.4289 - acc: 0.9113 - val_loss: 0.2954 - val_f_2: 0.3700 - val_acc: 0.8985\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2801 - f_2: 0.4249 - acc: 0.9096 - val_loss: 0.2952 - val_f_2: 0.4337 - val_acc: 0.9018\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2788 - f_2: 0.4176 - acc: 0.9088 - val_loss: 0.2941 - val_f_2: 0.4179 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2815 - f_2: 0.4189 - acc: 0.9083 - val_loss: 0.2953 - val_f_2: 0.4380 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2804 - f_2: 0.4224 - acc: 0.9097 - val_loss: 0.2946 - val_f_2: 0.4051 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2798 - f_2: 0.4237 - acc: 0.9101 - val_loss: 0.2943 - val_f_2: 0.4088 - val_acc: 0.8997\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2786 - f_2: 0.4293 - acc: 0.9085 - val_loss: 0.2964 - val_f_2: 0.4232 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2784 - f_2: 0.4337 - acc: 0.9105 - val_loss: 0.2947 - val_f_2: 0.4004 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2781 - f_2: 0.4262 - acc: 0.9096 - val_loss: 0.2952 - val_f_2: 0.4216 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2786 - f_2: 0.4270 - acc: 0.9088 - val_loss: 0.2955 - val_f_2: 0.4272 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2781 - f_2: 0.4351 - acc: 0.9098 - val_loss: 0.2959 - val_f_2: 0.4395 - val_acc: 0.9024\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2760 - f_2: 0.4368 - acc: 0.9100 - val_loss: 0.2946 - val_f_2: 0.4103 - val_acc: 0.8991\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2781 - f_2: 0.4269 - acc: 0.9103 - val_loss: 0.2975 - val_f_2: 0.4568 - val_acc: 0.8973\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2765 - f_2: 0.4461 - acc: 0.9102 - val_loss: 0.2966 - val_f_2: 0.3927 - val_acc: 0.8997\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.001, total=  32.5s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4767 - f_2: 0.0243 - acc: 0.8390 - val_loss: 0.3524 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3726 - f_2: 0.0951 - acc: 0.8793 - val_loss: 0.3166 - val_f_2: 0.2605 - val_acc: 0.9065\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3477 - f_2: 0.2321 - acc: 0.8918 - val_loss: 0.3016 - val_f_2: 0.3269 - val_acc: 0.9106\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3349 - f_2: 0.2716 - acc: 0.8933 - val_loss: 0.2949 - val_f_2: 0.3531 - val_acc: 0.9121\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3274 - f_2: 0.2781 - acc: 0.8925 - val_loss: 0.2903 - val_f_2: 0.3664 - val_acc: 0.9115\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.2900 - acc: 0.8962 - val_loss: 0.2886 - val_f_2: 0.3907 - val_acc: 0.9091\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.3025 - acc: 0.8964 - val_loss: 0.2851 - val_f_2: 0.3667 - val_acc: 0.9103\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3193 - acc: 0.8968 - val_loss: 0.2836 - val_f_2: 0.3574 - val_acc: 0.9103\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3120 - acc: 0.8978 - val_loss: 0.2830 - val_f_2: 0.3977 - val_acc: 0.9106\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3118 - f_2: 0.3256 - acc: 0.8974 - val_loss: 0.2807 - val_f_2: 0.3683 - val_acc: 0.9106\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3339 - acc: 0.8987 - val_loss: 0.2824 - val_f_2: 0.4164 - val_acc: 0.9121\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3469 - acc: 0.8994 - val_loss: 0.2782 - val_f_2: 0.3647 - val_acc: 0.9115\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3498 - acc: 0.8990 - val_loss: 0.2786 - val_f_2: 0.3655 - val_acc: 0.9109\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3367 - acc: 0.8988 - val_loss: 0.2767 - val_f_2: 0.3375 - val_acc: 0.9109\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3019 - f_2: 0.3352 - acc: 0.8988 - val_loss: 0.2767 - val_f_2: 0.3734 - val_acc: 0.9100\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3380 - acc: 0.8975 - val_loss: 0.2758 - val_f_2: 0.3640 - val_acc: 0.9106\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3536 - acc: 0.9012 - val_loss: 0.2764 - val_f_2: 0.3719 - val_acc: 0.9103\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.3441 - acc: 0.8995 - val_loss: 0.2764 - val_f_2: 0.3900 - val_acc: 0.9097\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.3772 - acc: 0.9015 - val_loss: 0.2753 - val_f_2: 0.3771 - val_acc: 0.9086\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2962 - f_2: 0.3587 - acc: 0.8996 - val_loss: 0.2750 - val_f_2: 0.3668 - val_acc: 0.9086\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.3669 - acc: 0.8984 - val_loss: 0.2752 - val_f_2: 0.4024 - val_acc: 0.9103\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3687 - acc: 0.8994 - val_loss: 0.2733 - val_f_2: 0.3710 - val_acc: 0.9086\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2915 - f_2: 0.3864 - acc: 0.9015 - val_loss: 0.2744 - val_f_2: 0.4015 - val_acc: 0.9094\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2922 - f_2: 0.3813 - acc: 0.9021 - val_loss: 0.2742 - val_f_2: 0.3765 - val_acc: 0.9100\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2941 - f_2: 0.3636 - acc: 0.8998 - val_loss: 0.2762 - val_f_2: 0.3996 - val_acc: 0.9088\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2910 - f_2: 0.3707 - acc: 0.9001 - val_loss: 0.2751 - val_f_2: 0.4012 - val_acc: 0.9091\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2909 - f_2: 0.3767 - acc: 0.9012 - val_loss: 0.2736 - val_f_2: 0.3904 - val_acc: 0.9100\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2893 - f_2: 0.3838 - acc: 0.9026 - val_loss: 0.2761 - val_f_2: 0.4264 - val_acc: 0.9086\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2897 - f_2: 0.3985 - acc: 0.9020 - val_loss: 0.2734 - val_f_2: 0.3983 - val_acc: 0.9097\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.3946 - acc: 0.9023 - val_loss: 0.2742 - val_f_2: 0.4030 - val_acc: 0.9074\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.3945 - acc: 0.9024 - val_loss: 0.2771 - val_f_2: 0.4232 - val_acc: 0.9080\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2862 - f_2: 0.4088 - acc: 0.9015 - val_loss: 0.2777 - val_f_2: 0.4328 - val_acc: 0.9056\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2881 - f_2: 0.4035 - acc: 0.9032 - val_loss: 0.2780 - val_f_2: 0.4337 - val_acc: 0.9068\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2860 - f_2: 0.4028 - acc: 0.9027 - val_loss: 0.2734 - val_f_2: 0.4072 - val_acc: 0.9077\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.4149 - acc: 0.9043 - val_loss: 0.2758 - val_f_2: 0.3826 - val_acc: 0.9088\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2865 - f_2: 0.3961 - acc: 0.9034 - val_loss: 0.2767 - val_f_2: 0.4298 - val_acc: 0.9041\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.4073 - acc: 0.9019 - val_loss: 0.2746 - val_f_2: 0.3893 - val_acc: 0.9065\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2845 - f_2: 0.4126 - acc: 0.9042 - val_loss: 0.2748 - val_f_2: 0.3800 - val_acc: 0.9103\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.4027 - acc: 0.9027 - val_loss: 0.2755 - val_f_2: 0.4199 - val_acc: 0.9053\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2841 - f_2: 0.4134 - acc: 0.9029 - val_loss: 0.2739 - val_f_2: 0.4106 - val_acc: 0.9050\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2840 - f_2: 0.4147 - acc: 0.9043 - val_loss: 0.2732 - val_f_2: 0.4241 - val_acc: 0.9059\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2817 - f_2: 0.4240 - acc: 0.9057 - val_loss: 0.2722 - val_f_2: 0.4015 - val_acc: 0.9071\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2837 - f_2: 0.4084 - acc: 0.9021 - val_loss: 0.2742 - val_f_2: 0.4189 - val_acc: 0.9074\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2817 - f_2: 0.4372 - acc: 0.9061 - val_loss: 0.2713 - val_f_2: 0.3960 - val_acc: 0.9068\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2817 - f_2: 0.4264 - acc: 0.9060 - val_loss: 0.2729 - val_f_2: 0.4136 - val_acc: 0.9074\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2822 - f_2: 0.4292 - acc: 0.9056 - val_loss: 0.2751 - val_f_2: 0.4303 - val_acc: 0.9044\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2816 - f_2: 0.4172 - acc: 0.9030 - val_loss: 0.2741 - val_f_2: 0.4075 - val_acc: 0.9035\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2823 - f_2: 0.4258 - acc: 0.9047 - val_loss: 0.2728 - val_f_2: 0.4018 - val_acc: 0.9059\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2805 - f_2: 0.4127 - acc: 0.9032 - val_loss: 0.2738 - val_f_2: 0.4242 - val_acc: 0.9047\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2826 - f_2: 0.4415 - acc: 0.9059 - val_loss: 0.2738 - val_f_2: 0.4142 - val_acc: 0.9056\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=10, penalty=0.001, total=  33.2s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7647 - f_2: 0.0206 - acc: 0.8542 - val_loss: 0.5055 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4345 - f_2: 1.3974e-08 - acc: 0.8753 - val_loss: 0.3848 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3770 - f_2: 1.3987e-08 - acc: 0.8753 - val_loss: 0.3636 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3602 - f_2: 1.3957e-08 - acc: 0.8753 - val_loss: 0.3535 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3521 - f_2: 1.4077e-08 - acc: 0.8753 - val_loss: 0.3449 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3461 - f_2: 1.3980e-08 - acc: 0.8753 - val_loss: 0.3395 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3421 - f_2: 1.4131e-08 - acc: 0.8753 - val_loss: 0.3377 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3375 - f_2: 1.3983e-08 - acc: 0.8753 - val_loss: 0.3333 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3356 - f_2: 1.3821e-08 - acc: 0.8753 - val_loss: 0.3316 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3323 - f_2: 1.3679e-08 - acc: 0.8753 - val_loss: 0.3308 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3326 - f_2: 1.4063e-08 - acc: 0.8753 - val_loss: 0.3280 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3279 - f_2: 0.0211 - acc: 0.8775 - val_loss: 0.3269 - val_f_2: 0.0376 - val_acc: 0.8708\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3292 - f_2: 0.1743 - acc: 0.8930 - val_loss: 0.3255 - val_f_2: 0.2060 - val_acc: 0.8903\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.1797 - acc: 0.8932 - val_loss: 0.3261 - val_f_2: 0.1457 - val_acc: 0.8832\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.2010 - acc: 0.8944 - val_loss: 0.3242 - val_f_2: 0.2663 - val_acc: 0.8976\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.2062 - acc: 0.8953 - val_loss: 0.3234 - val_f_2: 0.2853 - val_acc: 0.8994\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2202 - acc: 0.8967 - val_loss: 0.3214 - val_f_2: 0.2883 - val_acc: 0.8991\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3246 - f_2: 0.2150 - acc: 0.8963 - val_loss: 0.3207 - val_f_2: 0.2938 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2175 - acc: 0.8973 - val_loss: 0.3216 - val_f_2: 0.3050 - val_acc: 0.9006\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2366 - acc: 0.8988 - val_loss: 0.3197 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3248 - f_2: 0.2117 - acc: 0.8957 - val_loss: 0.3207 - val_f_2: 0.3447 - val_acc: 0.9015\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2397 - acc: 0.8980 - val_loss: 0.3182 - val_f_2: 0.2996 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.2252 - acc: 0.8963 - val_loss: 0.3185 - val_f_2: 0.3322 - val_acc: 0.8991\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.2551 - acc: 0.8990 - val_loss: 0.3175 - val_f_2: 0.2977 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.2414 - acc: 0.8984 - val_loss: 0.3166 - val_f_2: 0.3285 - val_acc: 0.8985\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3172 - f_2: 0.2431 - acc: 0.8971 - val_loss: 0.3164 - val_f_2: 0.3273 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3205 - f_2: 0.2418 - acc: 0.8977 - val_loss: 0.3159 - val_f_2: 0.3258 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3185 - f_2: 0.2582 - acc: 0.8998 - val_loss: 0.3156 - val_f_2: 0.3186 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.2661 - acc: 0.9005 - val_loss: 0.3173 - val_f_2: 0.3244 - val_acc: 0.9009\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2775 - acc: 0.9024 - val_loss: 0.3163 - val_f_2: 0.3256 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.2764 - acc: 0.9020 - val_loss: 0.3165 - val_f_2: 0.3345 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.2964 - acc: 0.9026 - val_loss: 0.3149 - val_f_2: 0.3147 - val_acc: 0.9021\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.2973 - acc: 0.9025 - val_loss: 0.3132 - val_f_2: 0.3141 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3164 - f_2: 0.2924 - acc: 0.9030 - val_loss: 0.3131 - val_f_2: 0.3206 - val_acc: 0.9012\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3033 - acc: 0.9038 - val_loss: 0.3124 - val_f_2: 0.3316 - val_acc: 0.9009\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3094 - acc: 0.9036 - val_loss: 0.3127 - val_f_2: 0.3189 - val_acc: 0.9015\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.3138 - acc: 0.9048 - val_loss: 0.3132 - val_f_2: 0.3659 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3052 - acc: 0.9043 - val_loss: 0.3144 - val_f_2: 0.3176 - val_acc: 0.9018\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.3150 - acc: 0.9043 - val_loss: 0.3114 - val_f_2: 0.3113 - val_acc: 0.9015\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3133 - f_2: 0.2954 - acc: 0.9030 - val_loss: 0.3124 - val_f_2: 0.3688 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3096 - acc: 0.9037 - val_loss: 0.3124 - val_f_2: 0.3173 - val_acc: 0.9012\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3079 - acc: 0.9033 - val_loss: 0.3171 - val_f_2: 0.2981 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3122 - f_2: 0.3086 - acc: 0.9032 - val_loss: 0.3114 - val_f_2: 0.3267 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3114 - f_2: 0.3033 - acc: 0.9023 - val_loss: 0.3155 - val_f_2: 0.3444 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3113 - f_2: 0.3112 - acc: 0.9032 - val_loss: 0.3167 - val_f_2: 0.3203 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3142 - acc: 0.9031 - val_loss: 0.3111 - val_f_2: 0.3479 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3209 - acc: 0.9037 - val_loss: 0.3109 - val_f_2: 0.3222 - val_acc: 0.9015\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.3134 - acc: 0.9029 - val_loss: 0.3108 - val_f_2: 0.3600 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3044 - acc: 0.9030 - val_loss: 0.3113 - val_f_2: 0.3755 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3114 - f_2: 0.3145 - acc: 0.9031 - val_loss: 0.3103 - val_f_2: 0.3582 - val_acc: 0.9000\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.01, total=  33.7s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7650 - f_2: 0.0225 - acc: 0.8506 - val_loss: 0.5037 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4326 - f_2: 1.3615e-08 - acc: 0.8722 - val_loss: 0.3820 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3744 - f_2: 1.3682e-08 - acc: 0.8722 - val_loss: 0.3624 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3603 - f_2: 1.3366e-08 - acc: 0.8722 - val_loss: 0.3515 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3500 - f_2: 1.3370e-08 - acc: 0.8722 - val_loss: 0.3438 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3454 - f_2: 1.3608e-08 - acc: 0.8722 - val_loss: 0.3385 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3429 - f_2: 1.3363e-08 - acc: 0.8722 - val_loss: 0.3362 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3386 - f_2: 1.3509e-08 - acc: 0.8722 - val_loss: 0.3337 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3353 - f_2: 1.3607e-08 - acc: 0.8722 - val_loss: 0.3336 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3332 - f_2: 1.3738e-08 - acc: 0.8722 - val_loss: 0.3296 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3330 - f_2: 0.0879 - acc: 0.8811 - val_loss: 0.3299 - val_f_2: 0.2758 - val_acc: 0.8988\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3319 - f_2: 0.1993 - acc: 0.8923 - val_loss: 0.3265 - val_f_2: 0.2680 - val_acc: 0.8976\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3284 - f_2: 0.2217 - acc: 0.8944 - val_loss: 0.3249 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3271 - f_2: 0.2319 - acc: 0.8942 - val_loss: 0.3238 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3276 - f_2: 0.2401 - acc: 0.8963 - val_loss: 0.3245 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3255 - f_2: 0.2325 - acc: 0.8953 - val_loss: 0.3222 - val_f_2: 0.2868 - val_acc: 0.8997\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.2645 - acc: 0.8976 - val_loss: 0.3228 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3238 - f_2: 0.2551 - acc: 0.8976 - val_loss: 0.3196 - val_f_2: 0.2918 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3227 - f_2: 0.2723 - acc: 0.8994 - val_loss: 0.3185 - val_f_2: 0.2866 - val_acc: 0.8991\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2786 - acc: 0.8985 - val_loss: 0.3216 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3239 - f_2: 0.2674 - acc: 0.8981 - val_loss: 0.3181 - val_f_2: 0.2852 - val_acc: 0.8991\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2741 - acc: 0.8993 - val_loss: 0.3169 - val_f_2: 0.3076 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3207 - f_2: 0.2818 - acc: 0.8993 - val_loss: 0.3168 - val_f_2: 0.2931 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.2908 - acc: 0.9004 - val_loss: 0.3173 - val_f_2: 0.3036 - val_acc: 0.9003\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3172 - f_2: 0.3095 - acc: 0.9024 - val_loss: 0.3149 - val_f_2: 0.2995 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3185 - f_2: 0.2957 - acc: 0.9011 - val_loss: 0.3147 - val_f_2: 0.3015 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.3041 - acc: 0.9012 - val_loss: 0.3143 - val_f_2: 0.2947 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3182 - f_2: 0.2997 - acc: 0.8999 - val_loss: 0.3140 - val_f_2: 0.3095 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.3033 - acc: 0.9003 - val_loss: 0.3125 - val_f_2: 0.3494 - val_acc: 0.9018\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3167 - acc: 0.9017 - val_loss: 0.3120 - val_f_2: 0.3082 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.3160 - acc: 0.9025 - val_loss: 0.3159 - val_f_2: 0.3484 - val_acc: 0.9021\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.3079 - acc: 0.9012 - val_loss: 0.3112 - val_f_2: 0.3059 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.3276 - acc: 0.9025 - val_loss: 0.3109 - val_f_2: 0.3554 - val_acc: 0.9024\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3160 - f_2: 0.3201 - acc: 0.9020 - val_loss: 0.3102 - val_f_2: 0.3099 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.3162 - acc: 0.9020 - val_loss: 0.3100 - val_f_2: 0.3080 - val_acc: 0.9009\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3083 - acc: 0.9010 - val_loss: 0.3113 - val_f_2: 0.3034 - val_acc: 0.9009\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3215 - acc: 0.9038 - val_loss: 0.3091 - val_f_2: 0.3065 - val_acc: 0.9012\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3227 - acc: 0.9007 - val_loss: 0.3105 - val_f_2: 0.3012 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3156 - acc: 0.9020 - val_loss: 0.3089 - val_f_2: 0.3735 - val_acc: 0.9012\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3295 - acc: 0.9020 - val_loss: 0.3086 - val_f_2: 0.3692 - val_acc: 0.9024\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3098 - f_2: 0.3414 - acc: 0.9039 - val_loss: 0.3098 - val_f_2: 0.3548 - val_acc: 0.9021\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.3163 - acc: 0.9022 - val_loss: 0.3093 - val_f_2: 0.3209 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3320 - acc: 0.9034 - val_loss: 0.3114 - val_f_2: 0.3044 - val_acc: 0.9015\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.3240 - acc: 0.9031 - val_loss: 0.3085 - val_f_2: 0.3221 - val_acc: 0.9018\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3096 - f_2: 0.3197 - acc: 0.9023 - val_loss: 0.3076 - val_f_2: 0.3535 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3103 - f_2: 0.3310 - acc: 0.9031 - val_loss: 0.3094 - val_f_2: 0.3681 - val_acc: 0.9015\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3208 - acc: 0.9015 - val_loss: 0.3067 - val_f_2: 0.3504 - val_acc: 0.9021\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.3298 - acc: 0.9022 - val_loss: 0.3070 - val_f_2: 0.3282 - val_acc: 0.9009\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3466 - acc: 0.9028 - val_loss: 0.3159 - val_f_2: 0.3155 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3100 - f_2: 0.3245 - acc: 0.9025 - val_loss: 0.3071 - val_f_2: 0.3234 - val_acc: 0.9009\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.01, total=  34.0s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7677 - f_2: 0.0276 - acc: 0.8496 - val_loss: 0.4846 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4419 - f_2: 1.2905e-08 - acc: 0.8691 - val_loss: 0.3646 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3845 - f_2: 1.3137e-08 - acc: 0.8691 - val_loss: 0.3446 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3697 - f_2: 1.3104e-08 - acc: 0.8691 - val_loss: 0.3335 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3576 - f_2: 1.3138e-08 - acc: 0.8691 - val_loss: 0.3263 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3518 - f_2: 1.3465e-08 - acc: 0.8691 - val_loss: 0.3231 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3488 - f_2: 1.3697e-08 - acc: 0.8691 - val_loss: 0.3185 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3440 - f_2: 1.3191e-08 - acc: 0.8691 - val_loss: 0.3179 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3407 - f_2: 1.3239e-08 - acc: 0.8691 - val_loss: 0.3161 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3384 - f_2: 1.3212e-08 - acc: 0.8691 - val_loss: 0.3133 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3368 - f_2: 0.0854 - acc: 0.8784 - val_loss: 0.3105 - val_f_2: 0.2824 - val_acc: 0.9086\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3388 - f_2: 0.1778 - acc: 0.8876 - val_loss: 0.3091 - val_f_2: 0.2977 - val_acc: 0.9103\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3345 - f_2: 0.1988 - acc: 0.8897 - val_loss: 0.3088 - val_f_2: 0.3049 - val_acc: 0.9103\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3325 - f_2: 0.2149 - acc: 0.8917 - val_loss: 0.3069 - val_f_2: 0.3126 - val_acc: 0.9109\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3340 - f_2: 0.2238 - acc: 0.8919 - val_loss: 0.3059 - val_f_2: 0.3000 - val_acc: 0.9106\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3312 - f_2: 0.2334 - acc: 0.8921 - val_loss: 0.3046 - val_f_2: 0.3076 - val_acc: 0.9106\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.2569 - acc: 0.8940 - val_loss: 0.3035 - val_f_2: 0.3126 - val_acc: 0.9109\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3303 - f_2: 0.2516 - acc: 0.8949 - val_loss: 0.3022 - val_f_2: 0.3126 - val_acc: 0.9109\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3293 - f_2: 0.2717 - acc: 0.8957 - val_loss: 0.3093 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3286 - f_2: 0.2595 - acc: 0.8941 - val_loss: 0.3024 - val_f_2: 0.3331 - val_acc: 0.9097\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.2781 - acc: 0.8956 - val_loss: 0.3020 - val_f_2: 0.3248 - val_acc: 0.9097\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3267 - f_2: 0.2925 - acc: 0.8964 - val_loss: 0.2991 - val_f_2: 0.3070 - val_acc: 0.9103\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2804 - acc: 0.8965 - val_loss: 0.2992 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.2899 - acc: 0.8972 - val_loss: 0.2979 - val_f_2: 0.3070 - val_acc: 0.9103\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.2760 - acc: 0.8965 - val_loss: 0.2986 - val_f_2: 0.3166 - val_acc: 0.9106\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3230 - f_2: 0.2995 - acc: 0.8970 - val_loss: 0.2974 - val_f_2: 0.3209 - val_acc: 0.9109\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2902 - acc: 0.8961 - val_loss: 0.2986 - val_f_2: 0.3181 - val_acc: 0.9097\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3225 - f_2: 0.3057 - acc: 0.8976 - val_loss: 0.2971 - val_f_2: 0.3142 - val_acc: 0.9103\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2985 - acc: 0.8975 - val_loss: 0.2969 - val_f_2: 0.3292 - val_acc: 0.9091\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.3041 - acc: 0.8980 - val_loss: 0.2965 - val_f_2: 0.3296 - val_acc: 0.9097\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.2904 - acc: 0.8966 - val_loss: 0.3008 - val_f_2: 0.3446 - val_acc: 0.9083\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.3064 - acc: 0.8970 - val_loss: 0.2950 - val_f_2: 0.3256 - val_acc: 0.9103\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.3054 - acc: 0.8982 - val_loss: 0.2963 - val_f_2: 0.3364 - val_acc: 0.9086\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2944 - acc: 0.8964 - val_loss: 0.2942 - val_f_2: 0.3161 - val_acc: 0.9097\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.3142 - acc: 0.8979 - val_loss: 0.2940 - val_f_2: 0.3147 - val_acc: 0.9109\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3177 - f_2: 0.3136 - acc: 0.8980 - val_loss: 0.2935 - val_f_2: 0.3183 - val_acc: 0.9100\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3173 - f_2: 0.3264 - acc: 0.8995 - val_loss: 0.2927 - val_f_2: 0.3207 - val_acc: 0.9100\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.3200 - acc: 0.8976 - val_loss: 0.2927 - val_f_2: 0.3316 - val_acc: 0.9100\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.3189 - acc: 0.8976 - val_loss: 0.2959 - val_f_2: 0.3688 - val_acc: 0.9083\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.3281 - acc: 0.8993 - val_loss: 0.2918 - val_f_2: 0.3203 - val_acc: 0.9094\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.3094 - acc: 0.8989 - val_loss: 0.2920 - val_f_2: 0.3308 - val_acc: 0.9088\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.3197 - acc: 0.8968 - val_loss: 0.3000 - val_f_2: 0.3927 - val_acc: 0.9077\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.3283 - acc: 0.8988 - val_loss: 0.2923 - val_f_2: 0.3603 - val_acc: 0.9077\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3259 - acc: 0.8986 - val_loss: 0.2923 - val_f_2: 0.3400 - val_acc: 0.9088\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3139 - f_2: 0.3182 - acc: 0.8977 - val_loss: 0.2907 - val_f_2: 0.3388 - val_acc: 0.9097\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.3278 - acc: 0.8978 - val_loss: 0.2913 - val_f_2: 0.3312 - val_acc: 0.9094\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.3243 - acc: 0.8971 - val_loss: 0.2908 - val_f_2: 0.3416 - val_acc: 0.9097\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.3194 - acc: 0.8979 - val_loss: 0.2898 - val_f_2: 0.3300 - val_acc: 0.9094\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.3336 - acc: 0.8990 - val_loss: 0.2897 - val_f_2: 0.3483 - val_acc: 0.9091\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.3297 - acc: 0.8977 - val_loss: 0.2902 - val_f_2: 0.3561 - val_acc: 0.9100\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.01, total=  34.9s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4700 - f_2: 0.0172 - acc: 0.8573 - val_loss: 0.3784 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3668 - f_2: 1.3826e-08 - acc: 0.8753 - val_loss: 0.3443 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3440 - f_2: 1.3842e-08 - acc: 0.8753 - val_loss: 0.3320 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3353 - f_2: 1.4055e-08 - acc: 0.8753 - val_loss: 0.3285 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3305 - f_2: 1.4121e-08 - acc: 0.8753 - val_loss: 0.3241 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3249 - f_2: 1.4239e-08 - acc: 0.8753 - val_loss: 0.3236 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.0850 - acc: 0.8840 - val_loss: 0.3210 - val_f_2: 0.2933 - val_acc: 0.8994\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.2590 - acc: 0.8991 - val_loss: 0.3185 - val_f_2: 0.3298 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.2970 - acc: 0.9017 - val_loss: 0.3165 - val_f_2: 0.3260 - val_acc: 0.8991\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2832 - acc: 0.9015 - val_loss: 0.3158 - val_f_2: 0.3485 - val_acc: 0.9003\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.2992 - acc: 0.9026 - val_loss: 0.3145 - val_f_2: 0.3638 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3204 - acc: 0.9043 - val_loss: 0.3138 - val_f_2: 0.3541 - val_acc: 0.9012\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3239 - acc: 0.9039 - val_loss: 0.3138 - val_f_2: 0.3334 - val_acc: 0.8994\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.3195 - acc: 0.9040 - val_loss: 0.3119 - val_f_2: 0.3772 - val_acc: 0.9009\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3056 - f_2: 0.3301 - acc: 0.9034 - val_loss: 0.3127 - val_f_2: 0.3588 - val_acc: 0.9015\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3276 - acc: 0.9035 - val_loss: 0.3116 - val_f_2: 0.4109 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3550 - acc: 0.9043 - val_loss: 0.3121 - val_f_2: 0.3622 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3037 - f_2: 0.3442 - acc: 0.9041 - val_loss: 0.3094 - val_f_2: 0.3875 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3537 - acc: 0.9046 - val_loss: 0.3096 - val_f_2: 0.3849 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3623 - acc: 0.9057 - val_loss: 0.3110 - val_f_2: 0.3846 - val_acc: 0.8997\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2997 - f_2: 0.3590 - acc: 0.9060 - val_loss: 0.3096 - val_f_2: 0.4062 - val_acc: 0.9018\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2958 - f_2: 0.3572 - acc: 0.9062 - val_loss: 0.3099 - val_f_2: 0.4078 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3633 - acc: 0.9056 - val_loss: 0.3103 - val_f_2: 0.3840 - val_acc: 0.9003\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.3716 - acc: 0.9065 - val_loss: 0.3086 - val_f_2: 0.3866 - val_acc: 0.9015\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2952 - f_2: 0.3735 - acc: 0.9069 - val_loss: 0.3100 - val_f_2: 0.4169 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.3779 - acc: 0.9083 - val_loss: 0.3085 - val_f_2: 0.3971 - val_acc: 0.9003\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2941 - f_2: 0.3707 - acc: 0.9065 - val_loss: 0.3098 - val_f_2: 0.3990 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.3806 - acc: 0.9066 - val_loss: 0.3097 - val_f_2: 0.3946 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.3767 - acc: 0.9065 - val_loss: 0.3089 - val_f_2: 0.4201 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3788 - acc: 0.9071 - val_loss: 0.3094 - val_f_2: 0.3948 - val_acc: 0.8997\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.3854 - acc: 0.9066 - val_loss: 0.3089 - val_f_2: 0.4004 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3876 - acc: 0.9083 - val_loss: 0.3092 - val_f_2: 0.3968 - val_acc: 0.9015\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3900 - acc: 0.9087 - val_loss: 0.3120 - val_f_2: 0.3975 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.3922 - acc: 0.9084 - val_loss: 0.3148 - val_f_2: 0.3917 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2906 - f_2: 0.3860 - acc: 0.9066 - val_loss: 0.3120 - val_f_2: 0.3891 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2923 - f_2: 0.3870 - acc: 0.9063 - val_loss: 0.3088 - val_f_2: 0.3975 - val_acc: 0.8988\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2918 - f_2: 0.3797 - acc: 0.9069 - val_loss: 0.3100 - val_f_2: 0.3961 - val_acc: 0.8985\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2867 - f_2: 0.3960 - acc: 0.9069 - val_loss: 0.3134 - val_f_2: 0.4030 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2886 - f_2: 0.3916 - acc: 0.9077 - val_loss: 0.3115 - val_f_2: 0.4061 - val_acc: 0.8994\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.3980 - acc: 0.9081 - val_loss: 0.3123 - val_f_2: 0.4011 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2875 - f_2: 0.4019 - acc: 0.9084 - val_loss: 0.3086 - val_f_2: 0.4125 - val_acc: 0.8976\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2901 - f_2: 0.3865 - acc: 0.9068 - val_loss: 0.3183 - val_f_2: 0.4143 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2870 - f_2: 0.4107 - acc: 0.9083 - val_loss: 0.3082 - val_f_2: 0.4259 - val_acc: 0.8991\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.4137 - acc: 0.9086 - val_loss: 0.3081 - val_f_2: 0.4188 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2887 - f_2: 0.3994 - acc: 0.9074 - val_loss: 0.3117 - val_f_2: 0.4103 - val_acc: 0.8991\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.4114 - acc: 0.9094 - val_loss: 0.3087 - val_f_2: 0.4439 - val_acc: 0.8994\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2868 - f_2: 0.4085 - acc: 0.9089 - val_loss: 0.3121 - val_f_2: 0.4040 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2848 - f_2: 0.4018 - acc: 0.9088 - val_loss: 0.3094 - val_f_2: 0.3941 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2857 - f_2: 0.4058 - acc: 0.9094 - val_loss: 0.3132 - val_f_2: 0.3906 - val_acc: 0.8988\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.3986 - acc: 0.9091 - val_loss: 0.3164 - val_f_2: 0.4117 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.001, total=  33.5s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4722 - f_2: 0.0207 - acc: 0.8544 - val_loss: 0.3772 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3660 - f_2: 1.3464e-08 - acc: 0.8722 - val_loss: 0.3448 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3439 - f_2: 1.3618e-08 - acc: 0.8722 - val_loss: 0.3326 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3331 - f_2: 1.3792e-08 - acc: 0.8722 - val_loss: 0.3280 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3287 - f_2: 1.3463e-08 - acc: 0.8722 - val_loss: 0.3243 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.0977 - acc: 0.8823 - val_loss: 0.3214 - val_f_2: 0.3180 - val_acc: 0.9003\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.2700 - acc: 0.8992 - val_loss: 0.3189 - val_f_2: 0.3286 - val_acc: 0.9015\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.2845 - acc: 0.8995 - val_loss: 0.3177 - val_f_2: 0.3586 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.3121 - acc: 0.9020 - val_loss: 0.3153 - val_f_2: 0.3408 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.3094 - acc: 0.9014 - val_loss: 0.3135 - val_f_2: 0.3577 - val_acc: 0.9009\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3103 - f_2: 0.3454 - acc: 0.9023 - val_loss: 0.3127 - val_f_2: 0.3769 - val_acc: 0.9024\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3580 - acc: 0.9036 - val_loss: 0.3116 - val_f_2: 0.3598 - val_acc: 0.9012\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3529 - acc: 0.9034 - val_loss: 0.3122 - val_f_2: 0.3908 - val_acc: 0.9032\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3539 - acc: 0.9026 - val_loss: 0.3101 - val_f_2: 0.3777 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3626 - acc: 0.9026 - val_loss: 0.3160 - val_f_2: 0.3607 - val_acc: 0.9012\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3742 - acc: 0.9057 - val_loss: 0.3074 - val_f_2: 0.4018 - val_acc: 0.9021\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3823 - acc: 0.9043 - val_loss: 0.3083 - val_f_2: 0.3783 - val_acc: 0.9003\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3797 - acc: 0.9050 - val_loss: 0.3078 - val_f_2: 0.4125 - val_acc: 0.9024\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.3861 - acc: 0.9050 - val_loss: 0.3083 - val_f_2: 0.4203 - val_acc: 0.9029\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3946 - acc: 0.9046 - val_loss: 0.3064 - val_f_2: 0.4207 - val_acc: 0.9038\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3943 - acc: 0.9059 - val_loss: 0.3069 - val_f_2: 0.4349 - val_acc: 0.9041\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.3909 - acc: 0.9061 - val_loss: 0.3063 - val_f_2: 0.4249 - val_acc: 0.9044\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3850 - acc: 0.9048 - val_loss: 0.3062 - val_f_2: 0.4348 - val_acc: 0.9024\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.4009 - acc: 0.9058 - val_loss: 0.3061 - val_f_2: 0.4098 - val_acc: 0.9029\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.3976 - acc: 0.9058 - val_loss: 0.3075 - val_f_2: 0.4084 - val_acc: 0.9029\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2940 - f_2: 0.3958 - acc: 0.9059 - val_loss: 0.3056 - val_f_2: 0.4192 - val_acc: 0.9021\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2960 - f_2: 0.4039 - acc: 0.9052 - val_loss: 0.3052 - val_f_2: 0.4200 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2940 - f_2: 0.4119 - acc: 0.9058 - val_loss: 0.3069 - val_f_2: 0.4286 - val_acc: 0.9003\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.4113 - acc: 0.9060 - val_loss: 0.3055 - val_f_2: 0.4221 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2916 - f_2: 0.4157 - acc: 0.9065 - val_loss: 0.3108 - val_f_2: 0.4045 - val_acc: 0.9032\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2908 - f_2: 0.4276 - acc: 0.9084 - val_loss: 0.3057 - val_f_2: 0.4198 - val_acc: 0.9024\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.4208 - acc: 0.9067 - val_loss: 0.3047 - val_f_2: 0.4183 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2918 - f_2: 0.4194 - acc: 0.9071 - val_loss: 0.3051 - val_f_2: 0.4189 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.4097 - acc: 0.9060 - val_loss: 0.3046 - val_f_2: 0.4173 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2863 - f_2: 0.4110 - acc: 0.9079 - val_loss: 0.3059 - val_f_2: 0.4295 - val_acc: 0.8985\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.4159 - acc: 0.9068 - val_loss: 0.3066 - val_f_2: 0.3937 - val_acc: 0.9027\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2894 - f_2: 0.4221 - acc: 0.9088 - val_loss: 0.3055 - val_f_2: 0.4268 - val_acc: 0.8988\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.4255 - acc: 0.9080 - val_loss: 0.3061 - val_f_2: 0.4065 - val_acc: 0.8997\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2889 - f_2: 0.4331 - acc: 0.9076 - val_loss: 0.3071 - val_f_2: 0.4301 - val_acc: 0.8994\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2872 - f_2: 0.4296 - acc: 0.9085 - val_loss: 0.3084 - val_f_2: 0.4231 - val_acc: 0.9009\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2861 - f_2: 0.4404 - acc: 0.9084 - val_loss: 0.3046 - val_f_2: 0.4040 - val_acc: 0.9029\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.4277 - acc: 0.9078 - val_loss: 0.3106 - val_f_2: 0.4069 - val_acc: 0.9021\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2864 - f_2: 0.4327 - acc: 0.9083 - val_loss: 0.3062 - val_f_2: 0.4219 - val_acc: 0.8994\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.4338 - acc: 0.9076 - val_loss: 0.3073 - val_f_2: 0.4208 - val_acc: 0.8997\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2856 - f_2: 0.4363 - acc: 0.9088 - val_loss: 0.3084 - val_f_2: 0.4320 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.4411 - acc: 0.9096 - val_loss: 0.3075 - val_f_2: 0.4272 - val_acc: 0.8991\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.4548 - acc: 0.9113 - val_loss: 0.3074 - val_f_2: 0.4327 - val_acc: 0.8968\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2829 - f_2: 0.4612 - acc: 0.9109 - val_loss: 0.3075 - val_f_2: 0.4220 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2838 - f_2: 0.4414 - acc: 0.9096 - val_loss: 0.3066 - val_f_2: 0.4266 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2814 - f_2: 0.4377 - acc: 0.9097 - val_loss: 0.3060 - val_f_2: 0.4309 - val_acc: 0.8997\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.001, total=  33.4s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4765 - f_2: 0.0193 - acc: 0.8509 - val_loss: 0.3556 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3736 - f_2: 1.3012e-08 - acc: 0.8691 - val_loss: 0.3234 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3474 - f_2: 1.3427e-08 - acc: 0.8691 - val_loss: 0.3140 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3418 - f_2: 1.2830e-08 - acc: 0.8691 - val_loss: 0.3095 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3362 - f_2: 1.3216e-08 - acc: 0.8691 - val_loss: 0.3065 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3328 - f_2: 0.0437 - acc: 0.8731 - val_loss: 0.3038 - val_f_2: 0.2689 - val_acc: 0.9071\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3274 - f_2: 0.2652 - acc: 0.8953 - val_loss: 0.3011 - val_f_2: 0.3092 - val_acc: 0.9097\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.2777 - acc: 0.8952 - val_loss: 0.3001 - val_f_2: 0.3486 - val_acc: 0.9100\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2907 - acc: 0.8962 - val_loss: 0.2993 - val_f_2: 0.3428 - val_acc: 0.9106\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.3165 - acc: 0.8981 - val_loss: 0.2968 - val_f_2: 0.3452 - val_acc: 0.9094\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3164 - f_2: 0.3281 - acc: 0.8976 - val_loss: 0.2989 - val_f_2: 0.3867 - val_acc: 0.9091\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.3328 - acc: 0.8978 - val_loss: 0.2953 - val_f_2: 0.3573 - val_acc: 0.9074\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.3436 - acc: 0.9001 - val_loss: 0.2933 - val_f_2: 0.3473 - val_acc: 0.9068\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3456 - acc: 0.8996 - val_loss: 0.2929 - val_f_2: 0.3698 - val_acc: 0.9068\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.3588 - acc: 0.9001 - val_loss: 0.2910 - val_f_2: 0.3543 - val_acc: 0.9071\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3458 - acc: 0.8987 - val_loss: 0.2920 - val_f_2: 0.3736 - val_acc: 0.9080\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3576 - acc: 0.9002 - val_loss: 0.2907 - val_f_2: 0.3547 - val_acc: 0.9080\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3754 - acc: 0.9004 - val_loss: 0.2898 - val_f_2: 0.3358 - val_acc: 0.9080\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3686 - acc: 0.9020 - val_loss: 0.2925 - val_f_2: 0.3987 - val_acc: 0.9077\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3881 - acc: 0.9017 - val_loss: 0.2889 - val_f_2: 0.3617 - val_acc: 0.9080\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3610 - acc: 0.8999 - val_loss: 0.2948 - val_f_2: 0.4398 - val_acc: 0.9094\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3711 - acc: 0.8996 - val_loss: 0.2895 - val_f_2: 0.3928 - val_acc: 0.9083\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3811 - acc: 0.9008 - val_loss: 0.2890 - val_f_2: 0.3832 - val_acc: 0.9077\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.3959 - acc: 0.9021 - val_loss: 0.2878 - val_f_2: 0.3805 - val_acc: 0.9074\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.4038 - acc: 0.9014 - val_loss: 0.2874 - val_f_2: 0.3969 - val_acc: 0.9077\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2991 - f_2: 0.3892 - acc: 0.9018 - val_loss: 0.2888 - val_f_2: 0.4261 - val_acc: 0.9077\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.3929 - acc: 0.9006 - val_loss: 0.2905 - val_f_2: 0.4281 - val_acc: 0.9062\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.4011 - acc: 0.9012 - val_loss: 0.2928 - val_f_2: 0.3535 - val_acc: 0.9077\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2964 - f_2: 0.3986 - acc: 0.9013 - val_loss: 0.2969 - val_f_2: 0.4290 - val_acc: 0.9062\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.4077 - acc: 0.9023 - val_loss: 0.2912 - val_f_2: 0.4133 - val_acc: 0.9065\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2964 - f_2: 0.4077 - acc: 0.9029 - val_loss: 0.2881 - val_f_2: 0.3997 - val_acc: 0.9074\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.4201 - acc: 0.9048 - val_loss: 0.2887 - val_f_2: 0.4232 - val_acc: 0.9068\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.4019 - acc: 0.9027 - val_loss: 0.2900 - val_f_2: 0.4391 - val_acc: 0.9053\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2944 - f_2: 0.4115 - acc: 0.9025 - val_loss: 0.2899 - val_f_2: 0.4310 - val_acc: 0.9056\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2939 - f_2: 0.4248 - acc: 0.9028 - val_loss: 0.2919 - val_f_2: 0.4027 - val_acc: 0.9077\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2942 - f_2: 0.4214 - acc: 0.9035 - val_loss: 0.2909 - val_f_2: 0.4221 - val_acc: 0.9050\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.4163 - acc: 0.9037 - val_loss: 0.2890 - val_f_2: 0.4363 - val_acc: 0.9044\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2914 - f_2: 0.4276 - acc: 0.9039 - val_loss: 0.2860 - val_f_2: 0.3910 - val_acc: 0.9083\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2923 - f_2: 0.4189 - acc: 0.9036 - val_loss: 0.2897 - val_f_2: 0.4289 - val_acc: 0.9053\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.4231 - acc: 0.9036 - val_loss: 0.2870 - val_f_2: 0.3927 - val_acc: 0.9086\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2923 - f_2: 0.4270 - acc: 0.9031 - val_loss: 0.2855 - val_f_2: 0.3922 - val_acc: 0.9077\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.4192 - acc: 0.9032 - val_loss: 0.2859 - val_f_2: 0.4074 - val_acc: 0.9091\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2924 - f_2: 0.4295 - acc: 0.9041 - val_loss: 0.2871 - val_f_2: 0.4141 - val_acc: 0.9068\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2903 - f_2: 0.4274 - acc: 0.9033 - val_loss: 0.2882 - val_f_2: 0.4136 - val_acc: 0.9056\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2914 - f_2: 0.4288 - acc: 0.9040 - val_loss: 0.2882 - val_f_2: 0.4442 - val_acc: 0.9047\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2889 - f_2: 0.4398 - acc: 0.9048 - val_loss: 0.2881 - val_f_2: 0.4497 - val_acc: 0.9035\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.4361 - acc: 0.9041 - val_loss: 0.2848 - val_f_2: 0.4037 - val_acc: 0.9068\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2893 - f_2: 0.4169 - acc: 0.9027 - val_loss: 0.2887 - val_f_2: 0.4528 - val_acc: 0.9029\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2887 - f_2: 0.4418 - acc: 0.9035 - val_loss: 0.2861 - val_f_2: 0.4076 - val_acc: 0.9091\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.4210 - acc: 0.9036 - val_loss: 0.2909 - val_f_2: 0.3986 - val_acc: 0.9077\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=15, penalty=0.001, total=  33.2s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7905 - f_2: 0.0157 - acc: 0.8590 - val_loss: 0.5044 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4251 - f_2: 1.4110e-08 - acc: 0.8753 - val_loss: 0.3821 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3696 - f_2: 1.3910e-08 - acc: 0.8753 - val_loss: 0.3591 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3543 - f_2: 1.3740e-08 - acc: 0.8753 - val_loss: 0.3480 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3473 - f_2: 1.4101e-08 - acc: 0.8753 - val_loss: 0.3415 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3388 - f_2: 1.3943e-08 - acc: 0.8753 - val_loss: 0.3383 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3349 - f_2: 1.3984e-08 - acc: 0.8753 - val_loss: 0.3341 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3327 - f_2: 1.3782e-08 - acc: 0.8753 - val_loss: 0.3341 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3297 - f_2: 0.0845 - acc: 0.8838 - val_loss: 0.3303 - val_f_2: 0.2528 - val_acc: 0.8959\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3274 - f_2: 0.1954 - acc: 0.8958 - val_loss: 0.3274 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2282 - acc: 0.8982 - val_loss: 0.3269 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.2463 - acc: 0.8995 - val_loss: 0.3271 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3225 - f_2: 0.2475 - acc: 0.9003 - val_loss: 0.3236 - val_f_2: 0.2866 - val_acc: 0.8988\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.2492 - acc: 0.8993 - val_loss: 0.3233 - val_f_2: 0.2916 - val_acc: 0.8991\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.2541 - acc: 0.9007 - val_loss: 0.3254 - val_f_2: 0.2870 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3170 - f_2: 0.2690 - acc: 0.9025 - val_loss: 0.3202 - val_f_2: 0.3475 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3189 - f_2: 0.2753 - acc: 0.9006 - val_loss: 0.3210 - val_f_2: 0.3084 - val_acc: 0.8997\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2766 - acc: 0.9013 - val_loss: 0.3192 - val_f_2: 0.3030 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.2864 - acc: 0.9016 - val_loss: 0.3179 - val_f_2: 0.2982 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3160 - f_2: 0.2714 - acc: 0.9015 - val_loss: 0.3179 - val_f_2: 0.3178 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.2936 - acc: 0.9021 - val_loss: 0.3177 - val_f_2: 0.3243 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3136 - f_2: 0.2920 - acc: 0.9025 - val_loss: 0.3160 - val_f_2: 0.3207 - val_acc: 0.8997\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3142 - f_2: 0.3132 - acc: 0.9051 - val_loss: 0.3228 - val_f_2: 0.2909 - val_acc: 0.8994\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.2921 - acc: 0.9035 - val_loss: 0.3154 - val_f_2: 0.3428 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3159 - acc: 0.9034 - val_loss: 0.3142 - val_f_2: 0.3124 - val_acc: 0.9006\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3140 - acc: 0.9055 - val_loss: 0.3173 - val_f_2: 0.3633 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3104 - f_2: 0.3097 - acc: 0.9049 - val_loss: 0.3149 - val_f_2: 0.2977 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3137 - acc: 0.9033 - val_loss: 0.3137 - val_f_2: 0.3358 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3060 - acc: 0.9029 - val_loss: 0.3123 - val_f_2: 0.3500 - val_acc: 0.9012\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3233 - acc: 0.9052 - val_loss: 0.3135 - val_f_2: 0.3195 - val_acc: 0.9003\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3217 - acc: 0.9037 - val_loss: 0.3168 - val_f_2: 0.3231 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3140 - acc: 0.9025 - val_loss: 0.3121 - val_f_2: 0.3377 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3284 - acc: 0.9032 - val_loss: 0.3111 - val_f_2: 0.3645 - val_acc: 0.9003\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3371 - acc: 0.9046 - val_loss: 0.3116 - val_f_2: 0.3669 - val_acc: 0.9003\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3323 - acc: 0.9051 - val_loss: 0.3119 - val_f_2: 0.3866 - val_acc: 0.9018\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3423 - acc: 0.9036 - val_loss: 0.3100 - val_f_2: 0.3364 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.3316 - acc: 0.9050 - val_loss: 0.3136 - val_f_2: 0.3140 - val_acc: 0.9003\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.3211 - acc: 0.9039 - val_loss: 0.3102 - val_f_2: 0.3497 - val_acc: 0.9006\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3039 - f_2: 0.3337 - acc: 0.9058 - val_loss: 0.3122 - val_f_2: 0.3267 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3387 - acc: 0.9043 - val_loss: 0.3106 - val_f_2: 0.3611 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3511 - acc: 0.9055 - val_loss: 0.3091 - val_f_2: 0.3447 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3346 - acc: 0.9042 - val_loss: 0.3080 - val_f_2: 0.3178 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3364 - acc: 0.9053 - val_loss: 0.3124 - val_f_2: 0.3586 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3378 - acc: 0.9046 - val_loss: 0.3092 - val_f_2: 0.3341 - val_acc: 0.8994\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3460 - acc: 0.9049 - val_loss: 0.3088 - val_f_2: 0.3141 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.3364 - acc: 0.9052 - val_loss: 0.3100 - val_f_2: 0.3991 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3019 - f_2: 0.3443 - acc: 0.9052 - val_loss: 0.3075 - val_f_2: 0.3196 - val_acc: 0.9012\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.3425 - acc: 0.9050 - val_loss: 0.3077 - val_f_2: 0.3776 - val_acc: 0.9021\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3449 - acc: 0.9051 - val_loss: 0.3150 - val_f_2: 0.3375 - val_acc: 0.9012\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.3472 - acc: 0.9057 - val_loss: 0.3080 - val_f_2: 0.3917 - val_acc: 0.9003\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.01, total=  34.0s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7884 - f_2: 0.0152 - acc: 0.8557 - val_loss: 0.5033 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4253 - f_2: 1.3357e-08 - acc: 0.8722 - val_loss: 0.3797 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3709 - f_2: 1.4317e-08 - acc: 0.8722 - val_loss: 0.3610 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3545 - f_2: 1.3444e-08 - acc: 0.8722 - val_loss: 0.3471 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3492 - f_2: 1.3457e-08 - acc: 0.8722 - val_loss: 0.3416 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3419 - f_2: 1.3572e-08 - acc: 0.8722 - val_loss: 0.3369 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3372 - f_2: 1.3626e-08 - acc: 0.8722 - val_loss: 0.3341 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3327 - f_2: 0.0288 - acc: 0.8756 - val_loss: 0.3313 - val_f_2: 0.2348 - val_acc: 0.8941\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3304 - f_2: 0.2094 - acc: 0.8942 - val_loss: 0.3327 - val_f_2: 0.2793 - val_acc: 0.8991\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.2170 - acc: 0.8953 - val_loss: 0.3271 - val_f_2: 0.2834 - val_acc: 0.8994\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.2498 - acc: 0.8971 - val_loss: 0.3265 - val_f_2: 0.2902 - val_acc: 0.8997\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2466 - acc: 0.8978 - val_loss: 0.3242 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3242 - f_2: 0.2692 - acc: 0.9001 - val_loss: 0.3230 - val_f_2: 0.2934 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2712 - acc: 0.9002 - val_loss: 0.3226 - val_f_2: 0.3220 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3206 - f_2: 0.2792 - acc: 0.8999 - val_loss: 0.3253 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2797 - acc: 0.8993 - val_loss: 0.3212 - val_f_2: 0.2873 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3183 - f_2: 0.2997 - acc: 0.9022 - val_loss: 0.3183 - val_f_2: 0.2855 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.2947 - acc: 0.9015 - val_loss: 0.3251 - val_f_2: 0.3399 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.3124 - acc: 0.9016 - val_loss: 0.3167 - val_f_2: 0.2887 - val_acc: 0.8994\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3076 - acc: 0.9025 - val_loss: 0.3161 - val_f_2: 0.2964 - val_acc: 0.8994\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3050 - acc: 0.9006 - val_loss: 0.3145 - val_f_2: 0.3225 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3236 - acc: 0.9030 - val_loss: 0.3187 - val_f_2: 0.2873 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3138 - f_2: 0.3195 - acc: 0.9033 - val_loss: 0.3211 - val_f_2: 0.3025 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.3201 - acc: 0.9029 - val_loss: 0.3138 - val_f_2: 0.3635 - val_acc: 0.9018\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3282 - acc: 0.9033 - val_loss: 0.3188 - val_f_2: 0.3147 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.3302 - acc: 0.9027 - val_loss: 0.3111 - val_f_2: 0.3326 - val_acc: 0.9015\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3302 - acc: 0.9018 - val_loss: 0.3152 - val_f_2: 0.3605 - val_acc: 0.9015\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.3268 - acc: 0.9020 - val_loss: 0.3108 - val_f_2: 0.3606 - val_acc: 0.9018\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3357 - acc: 0.9034 - val_loss: 0.3108 - val_f_2: 0.3465 - val_acc: 0.9027\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3081 - f_2: 0.3351 - acc: 0.9032 - val_loss: 0.3131 - val_f_2: 0.3544 - val_acc: 0.9021\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3418 - acc: 0.9039 - val_loss: 0.3096 - val_f_2: 0.3761 - val_acc: 0.9024\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3371 - acc: 0.9043 - val_loss: 0.3109 - val_f_2: 0.3790 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3561 - acc: 0.9050 - val_loss: 0.3116 - val_f_2: 0.3341 - val_acc: 0.9027\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3068 - f_2: 0.3425 - acc: 0.9034 - val_loss: 0.3108 - val_f_2: 0.3581 - val_acc: 0.9024\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3075 - f_2: 0.3532 - acc: 0.9054 - val_loss: 0.3075 - val_f_2: 0.3600 - val_acc: 0.9024\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3057 - f_2: 0.3445 - acc: 0.9048 - val_loss: 0.3109 - val_f_2: 0.3020 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3482 - acc: 0.9051 - val_loss: 0.3086 - val_f_2: 0.3780 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3524 - acc: 0.9053 - val_loss: 0.3101 - val_f_2: 0.3020 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3058 - f_2: 0.3561 - acc: 0.9036 - val_loss: 0.3102 - val_f_2: 0.4095 - val_acc: 0.9018\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3557 - acc: 0.9058 - val_loss: 0.3088 - val_f_2: 0.4115 - val_acc: 0.9018\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3050 - f_2: 0.3555 - acc: 0.9047 - val_loss: 0.3077 - val_f_2: 0.3822 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3628 - acc: 0.9045 - val_loss: 0.3067 - val_f_2: 0.3646 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.3473 - acc: 0.9037 - val_loss: 0.3098 - val_f_2: 0.3357 - val_acc: 0.9024\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3519 - acc: 0.9040 - val_loss: 0.3055 - val_f_2: 0.3599 - val_acc: 0.9021\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3588 - acc: 0.9044 - val_loss: 0.3055 - val_f_2: 0.3484 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3039 - f_2: 0.3530 - acc: 0.9040 - val_loss: 0.3064 - val_f_2: 0.3683 - val_acc: 0.9012\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3700 - acc: 0.9043 - val_loss: 0.3054 - val_f_2: 0.3594 - val_acc: 0.9018\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3545 - acc: 0.9038 - val_loss: 0.3042 - val_f_2: 0.3399 - val_acc: 0.9027\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3525 - acc: 0.9055 - val_loss: 0.3055 - val_f_2: 0.3940 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.3640 - acc: 0.9054 - val_loss: 0.3118 - val_f_2: 0.4360 - val_acc: 0.8997\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.01, total=  33.8s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7948 - f_2: 0.0144 - acc: 0.8515 - val_loss: 0.4818 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4348 - f_2: 1.3395e-08 - acc: 0.8691 - val_loss: 0.3610 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3795 - f_2: 1.3270e-08 - acc: 0.8691 - val_loss: 0.3407 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3618 - f_2: 1.3082e-08 - acc: 0.8691 - val_loss: 0.3292 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3547 - f_2: 1.3499e-08 - acc: 0.8691 - val_loss: 0.3239 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3462 - f_2: 1.3105e-08 - acc: 0.8691 - val_loss: 0.3194 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3426 - f_2: 1.3180e-08 - acc: 0.8691 - val_loss: 0.3175 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3410 - f_2: 1.3428e-08 - acc: 0.8691 - val_loss: 0.3157 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3386 - f_2: 0.1578 - acc: 0.8863 - val_loss: 0.3133 - val_f_2: 0.2929 - val_acc: 0.9097\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3351 - f_2: 0.2111 - acc: 0.8913 - val_loss: 0.3129 - val_f_2: 0.3154 - val_acc: 0.9109\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.2311 - acc: 0.8936 - val_loss: 0.3110 - val_f_2: 0.3212 - val_acc: 0.9103\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3327 - f_2: 0.2405 - acc: 0.8947 - val_loss: 0.3086 - val_f_2: 0.3226 - val_acc: 0.9091\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3296 - f_2: 0.2689 - acc: 0.8973 - val_loss: 0.3062 - val_f_2: 0.3189 - val_acc: 0.9109\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2675 - acc: 0.8967 - val_loss: 0.3041 - val_f_2: 0.3049 - val_acc: 0.9103\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2640 - acc: 0.8969 - val_loss: 0.3033 - val_f_2: 0.3120 - val_acc: 0.9103\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3271 - f_2: 0.2782 - acc: 0.8976 - val_loss: 0.3013 - val_f_2: 0.3054 - val_acc: 0.9112\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3257 - f_2: 0.2795 - acc: 0.8972 - val_loss: 0.3037 - val_f_2: 0.3271 - val_acc: 0.9097\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.2877 - acc: 0.8981 - val_loss: 0.3002 - val_f_2: 0.3185 - val_acc: 0.9103\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3263 - f_2: 0.2840 - acc: 0.8983 - val_loss: 0.2997 - val_f_2: 0.3236 - val_acc: 0.9109\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.2981 - acc: 0.8976 - val_loss: 0.3012 - val_f_2: 0.3147 - val_acc: 0.9109\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3217 - f_2: 0.2925 - acc: 0.8988 - val_loss: 0.2990 - val_f_2: 0.3431 - val_acc: 0.9088\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.3094 - acc: 0.8978 - val_loss: 0.2979 - val_f_2: 0.3274 - val_acc: 0.9103\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3199 - f_2: 0.3069 - acc: 0.8995 - val_loss: 0.2980 - val_f_2: 0.3297 - val_acc: 0.9100\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.3044 - acc: 0.8981 - val_loss: 0.2980 - val_f_2: 0.3320 - val_acc: 0.9097\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.3181 - acc: 0.8987 - val_loss: 0.2962 - val_f_2: 0.3318 - val_acc: 0.9094\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.3330 - acc: 0.9007 - val_loss: 0.2947 - val_f_2: 0.3190 - val_acc: 0.9112\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3146 - acc: 0.8995 - val_loss: 0.2941 - val_f_2: 0.3192 - val_acc: 0.9115\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.2999 - acc: 0.8974 - val_loss: 0.2996 - val_f_2: 0.4038 - val_acc: 0.9086\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3149 - f_2: 0.3334 - acc: 0.9002 - val_loss: 0.2950 - val_f_2: 0.3675 - val_acc: 0.9071\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.3318 - acc: 0.8998 - val_loss: 0.2923 - val_f_2: 0.3146 - val_acc: 0.9109\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.3189 - acc: 0.8998 - val_loss: 0.2923 - val_f_2: 0.3344 - val_acc: 0.9097\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.3321 - acc: 0.9000 - val_loss: 0.2923 - val_f_2: 0.3347 - val_acc: 0.9100\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.3258 - acc: 0.9007 - val_loss: 0.2959 - val_f_2: 0.3941 - val_acc: 0.9086\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3321 - acc: 0.8985 - val_loss: 0.2908 - val_f_2: 0.3326 - val_acc: 0.9106\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3283 - acc: 0.8985 - val_loss: 0.2943 - val_f_2: 0.3815 - val_acc: 0.9074\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.3608 - acc: 0.9004 - val_loss: 0.2903 - val_f_2: 0.3330 - val_acc: 0.9115\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3134 - f_2: 0.3436 - acc: 0.9008 - val_loss: 0.2916 - val_f_2: 0.3287 - val_acc: 0.9121\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3356 - acc: 0.8992 - val_loss: 0.2911 - val_f_2: 0.3634 - val_acc: 0.9077\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.3425 - acc: 0.8995 - val_loss: 0.2897 - val_f_2: 0.3160 - val_acc: 0.9106\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3398 - acc: 0.9006 - val_loss: 0.2912 - val_f_2: 0.3967 - val_acc: 0.9097\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3314 - acc: 0.8976 - val_loss: 0.2897 - val_f_2: 0.3327 - val_acc: 0.9109\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.3436 - acc: 0.9004 - val_loss: 0.2900 - val_f_2: 0.3805 - val_acc: 0.9080\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3399 - acc: 0.9000 - val_loss: 0.2894 - val_f_2: 0.3558 - val_acc: 0.9080\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3343 - acc: 0.8997 - val_loss: 0.2890 - val_f_2: 0.3610 - val_acc: 0.9077\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3103 - f_2: 0.3454 - acc: 0.8992 - val_loss: 0.2879 - val_f_2: 0.3381 - val_acc: 0.9094\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3088 - f_2: 0.3461 - acc: 0.9010 - val_loss: 0.2902 - val_f_2: 0.3925 - val_acc: 0.9083\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3391 - acc: 0.8987 - val_loss: 0.2871 - val_f_2: 0.3376 - val_acc: 0.9094\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3410 - acc: 0.8996 - val_loss: 0.2876 - val_f_2: 0.3617 - val_acc: 0.9077\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3529 - acc: 0.9015 - val_loss: 0.2868 - val_f_2: 0.3337 - val_acc: 0.9115\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3365 - acc: 0.9011 - val_loss: 0.3098 - val_f_2: 0.4710 - val_acc: 0.9032\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.01, total=  34.9s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4619 - f_2: 0.0108 - acc: 0.8604 - val_loss: 0.3825 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3730 - f_2: 1.3773e-08 - acc: 0.8753 - val_loss: 0.3525 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3514 - f_2: 1.3976e-08 - acc: 0.8753 - val_loss: 0.3389 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3363 - f_2: 0.1540 - acc: 0.8911 - val_loss: 0.3327 - val_f_2: 0.2674 - val_acc: 0.8976\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3286 - f_2: 0.2177 - acc: 0.8966 - val_loss: 0.3249 - val_f_2: 0.3054 - val_acc: 0.8997\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3239 - f_2: 0.2669 - acc: 0.9015 - val_loss: 0.3227 - val_f_2: 0.3052 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3119 - acc: 0.9041 - val_loss: 0.3163 - val_f_2: 0.3391 - val_acc: 0.9009\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3215 - acc: 0.9056 - val_loss: 0.3155 - val_f_2: 0.3540 - val_acc: 0.9003\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3413 - acc: 0.9052 - val_loss: 0.3114 - val_f_2: 0.3496 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3065 - f_2: 0.3380 - acc: 0.9057 - val_loss: 0.3085 - val_f_2: 0.4000 - val_acc: 0.9024\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3456 - acc: 0.9047 - val_loss: 0.3062 - val_f_2: 0.3898 - val_acc: 0.9018\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3677 - acc: 0.9067 - val_loss: 0.3061 - val_f_2: 0.3500 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3565 - acc: 0.9063 - val_loss: 0.3043 - val_f_2: 0.3847 - val_acc: 0.9024\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3633 - acc: 0.9080 - val_loss: 0.3030 - val_f_2: 0.3919 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.3589 - acc: 0.9055 - val_loss: 0.3019 - val_f_2: 0.3924 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2964 - f_2: 0.3614 - acc: 0.9062 - val_loss: 0.3010 - val_f_2: 0.3905 - val_acc: 0.9009\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.3672 - acc: 0.9054 - val_loss: 0.3048 - val_f_2: 0.3484 - val_acc: 0.9021\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.3680 - acc: 0.9063 - val_loss: 0.2993 - val_f_2: 0.3932 - val_acc: 0.9024\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2917 - f_2: 0.3687 - acc: 0.9060 - val_loss: 0.2992 - val_f_2: 0.3841 - val_acc: 0.9029\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2904 - f_2: 0.3773 - acc: 0.9068 - val_loss: 0.2998 - val_f_2: 0.4016 - val_acc: 0.9012\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2893 - f_2: 0.3830 - acc: 0.9075 - val_loss: 0.3017 - val_f_2: 0.3698 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2883 - f_2: 0.3695 - acc: 0.9068 - val_loss: 0.2970 - val_f_2: 0.4256 - val_acc: 0.9038\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2879 - f_2: 0.3788 - acc: 0.9068 - val_loss: 0.2989 - val_f_2: 0.3874 - val_acc: 0.9018\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2874 - f_2: 0.3775 - acc: 0.9073 - val_loss: 0.2981 - val_f_2: 0.4154 - val_acc: 0.9024\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2868 - f_2: 0.3784 - acc: 0.9080 - val_loss: 0.2998 - val_f_2: 0.4216 - val_acc: 0.9024\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.3842 - acc: 0.9082 - val_loss: 0.2984 - val_f_2: 0.3916 - val_acc: 0.9027\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2857 - f_2: 0.3877 - acc: 0.9091 - val_loss: 0.3032 - val_f_2: 0.3896 - val_acc: 0.9024\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2852 - f_2: 0.3857 - acc: 0.9078 - val_loss: 0.2982 - val_f_2: 0.4064 - val_acc: 0.9032\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2878 - f_2: 0.3861 - acc: 0.9076 - val_loss: 0.2980 - val_f_2: 0.4130 - val_acc: 0.9003\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2841 - f_2: 0.3962 - acc: 0.9079 - val_loss: 0.2989 - val_f_2: 0.3993 - val_acc: 0.9018\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2869 - f_2: 0.3796 - acc: 0.9070 - val_loss: 0.2975 - val_f_2: 0.4221 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2840 - f_2: 0.4017 - acc: 0.9090 - val_loss: 0.2982 - val_f_2: 0.4142 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2830 - f_2: 0.3951 - acc: 0.9095 - val_loss: 0.2986 - val_f_2: 0.3963 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2840 - f_2: 0.3834 - acc: 0.9077 - val_loss: 0.3016 - val_f_2: 0.3974 - val_acc: 0.9021\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2840 - f_2: 0.3899 - acc: 0.9085 - val_loss: 0.3024 - val_f_2: 0.3846 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2839 - f_2: 0.3884 - acc: 0.9074 - val_loss: 0.3099 - val_f_2: 0.3467 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4062 - acc: 0.9103 - val_loss: 0.3019 - val_f_2: 0.4025 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2817 - f_2: 0.4051 - acc: 0.9092 - val_loss: 0.3022 - val_f_2: 0.3980 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4005 - acc: 0.9099 - val_loss: 0.3045 - val_f_2: 0.3843 - val_acc: 0.9018\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2798 - f_2: 0.4093 - acc: 0.9108 - val_loss: 0.3004 - val_f_2: 0.4066 - val_acc: 0.9018\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2792 - f_2: 0.3987 - acc: 0.9100 - val_loss: 0.3010 - val_f_2: 0.3907 - val_acc: 0.9003\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2812 - f_2: 0.4012 - acc: 0.9091 - val_loss: 0.3039 - val_f_2: 0.3885 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2790 - f_2: 0.3938 - acc: 0.9087 - val_loss: 0.2998 - val_f_2: 0.4040 - val_acc: 0.8979\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2821 - f_2: 0.4076 - acc: 0.9100 - val_loss: 0.3026 - val_f_2: 0.4480 - val_acc: 0.8982\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2816 - f_2: 0.3936 - acc: 0.9089 - val_loss: 0.3014 - val_f_2: 0.3814 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2814 - f_2: 0.4064 - acc: 0.9097 - val_loss: 0.2996 - val_f_2: 0.3879 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2812 - f_2: 0.3965 - acc: 0.9086 - val_loss: 0.3120 - val_f_2: 0.3866 - val_acc: 0.9015\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2814 - f_2: 0.3973 - acc: 0.9079 - val_loss: 0.3024 - val_f_2: 0.4217 - val_acc: 0.8971\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2808 - f_2: 0.4067 - acc: 0.9093 - val_loss: 0.3009 - val_f_2: 0.4202 - val_acc: 0.8997\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2806 - f_2: 0.4018 - acc: 0.9079 - val_loss: 0.3003 - val_f_2: 0.4322 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.001, total=  34.9s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4639 - f_2: 0.0116 - acc: 0.8601 - val_loss: 0.3816 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3684 - f_2: 1.3710e-08 - acc: 0.8722 - val_loss: 0.3514 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3483 - f_2: 0.0523 - acc: 0.8777 - val_loss: 0.3395 - val_f_2: 0.2637 - val_acc: 0.8971\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3383 - f_2: 0.2025 - acc: 0.8940 - val_loss: 0.3296 - val_f_2: 0.2811 - val_acc: 0.8997\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3314 - f_2: 0.2594 - acc: 0.9001 - val_loss: 0.3234 - val_f_2: 0.3086 - val_acc: 0.9003\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.3074 - acc: 0.9034 - val_loss: 0.3214 - val_f_2: 0.3056 - val_acc: 0.9006\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3193 - f_2: 0.3194 - acc: 0.9043 - val_loss: 0.3152 - val_f_2: 0.3244 - val_acc: 0.8985\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.3354 - acc: 0.9042 - val_loss: 0.3141 - val_f_2: 0.3130 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3103 - f_2: 0.3350 - acc: 0.9032 - val_loss: 0.3084 - val_f_2: 0.3508 - val_acc: 0.9012\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3071 - f_2: 0.3540 - acc: 0.9052 - val_loss: 0.3067 - val_f_2: 0.3779 - val_acc: 0.9003\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3662 - acc: 0.9043 - val_loss: 0.3056 - val_f_2: 0.4116 - val_acc: 0.9029\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3720 - acc: 0.9056 - val_loss: 0.3056 - val_f_2: 0.3868 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3781 - acc: 0.9051 - val_loss: 0.3029 - val_f_2: 0.3942 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.3799 - acc: 0.9057 - val_loss: 0.3009 - val_f_2: 0.3658 - val_acc: 0.9012\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.2951 - f_2: 0.3851 - acc: 0.9062 - val_loss: 0.3017 - val_f_2: 0.3713 - val_acc: 0.9003\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.3760 - acc: 0.9058 - val_loss: 0.2988 - val_f_2: 0.3819 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.3991 - acc: 0.9070 - val_loss: 0.2978 - val_f_2: 0.4154 - val_acc: 0.9009\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2919 - f_2: 0.3942 - acc: 0.9063 - val_loss: 0.2991 - val_f_2: 0.4074 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2913 - f_2: 0.3752 - acc: 0.9046 - val_loss: 0.2976 - val_f_2: 0.4099 - val_acc: 0.9021\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2890 - f_2: 0.4064 - acc: 0.9089 - val_loss: 0.2973 - val_f_2: 0.3863 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2910 - f_2: 0.4008 - acc: 0.9070 - val_loss: 0.2969 - val_f_2: 0.4366 - val_acc: 0.9024\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.4036 - acc: 0.9074 - val_loss: 0.2963 - val_f_2: 0.4106 - val_acc: 0.9029\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2881 - f_2: 0.4026 - acc: 0.9062 - val_loss: 0.2975 - val_f_2: 0.3952 - val_acc: 0.9015\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2872 - f_2: 0.4104 - acc: 0.9082 - val_loss: 0.2942 - val_f_2: 0.4169 - val_acc: 0.9029\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2877 - f_2: 0.4032 - acc: 0.9076 - val_loss: 0.2954 - val_f_2: 0.4350 - val_acc: 0.9024\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2869 - f_2: 0.4057 - acc: 0.9068 - val_loss: 0.2984 - val_f_2: 0.3806 - val_acc: 0.9012\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2871 - f_2: 0.4101 - acc: 0.9070 - val_loss: 0.2960 - val_f_2: 0.4211 - val_acc: 0.9038\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2853 - f_2: 0.3963 - acc: 0.9072 - val_loss: 0.2961 - val_f_2: 0.4253 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2859 - f_2: 0.3984 - acc: 0.9070 - val_loss: 0.2954 - val_f_2: 0.4197 - val_acc: 0.9018\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2824 - f_2: 0.4085 - acc: 0.9079 - val_loss: 0.2957 - val_f_2: 0.4013 - val_acc: 0.9021\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2850 - f_2: 0.4094 - acc: 0.9071 - val_loss: 0.2956 - val_f_2: 0.4228 - val_acc: 0.9018\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2823 - f_2: 0.4038 - acc: 0.9068 - val_loss: 0.2964 - val_f_2: 0.4188 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2824 - f_2: 0.4241 - acc: 0.9087 - val_loss: 0.2956 - val_f_2: 0.4035 - val_acc: 0.9027\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2818 - f_2: 0.4257 - acc: 0.9099 - val_loss: 0.2955 - val_f_2: 0.4235 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2833 - f_2: 0.4183 - acc: 0.9100 - val_loss: 0.2975 - val_f_2: 0.4305 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2831 - f_2: 0.4207 - acc: 0.9095 - val_loss: 0.2964 - val_f_2: 0.4226 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2806 - f_2: 0.4264 - acc: 0.9090 - val_loss: 0.3096 - val_f_2: 0.3619 - val_acc: 0.9021\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2832 - f_2: 0.4144 - acc: 0.9085 - val_loss: 0.2974 - val_f_2: 0.3961 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4284 - acc: 0.9105 - val_loss: 0.3032 - val_f_2: 0.3982 - val_acc: 0.9024\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2800 - f_2: 0.4290 - acc: 0.9114 - val_loss: 0.2990 - val_f_2: 0.4402 - val_acc: 0.9018\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2789 - f_2: 0.4343 - acc: 0.9103 - val_loss: 0.2969 - val_f_2: 0.4337 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2785 - f_2: 0.4376 - acc: 0.9105 - val_loss: 0.2990 - val_f_2: 0.4051 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2802 - f_2: 0.4186 - acc: 0.9082 - val_loss: 0.2990 - val_f_2: 0.4309 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2769 - f_2: 0.4306 - acc: 0.9094 - val_loss: 0.3040 - val_f_2: 0.4182 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2784 - f_2: 0.4406 - acc: 0.9115 - val_loss: 0.3037 - val_f_2: 0.3748 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2770 - f_2: 0.4250 - acc: 0.9098 - val_loss: 0.2969 - val_f_2: 0.4375 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2771 - f_2: 0.4440 - acc: 0.9108 - val_loss: 0.3006 - val_f_2: 0.3803 - val_acc: 0.9021\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2809 - f_2: 0.4253 - acc: 0.9102 - val_loss: 0.2967 - val_f_2: 0.4194 - val_acc: 0.9009\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2761 - f_2: 0.4285 - acc: 0.9109 - val_loss: 0.2986 - val_f_2: 0.4109 - val_acc: 0.9012\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2797 - f_2: 0.4280 - acc: 0.9095 - val_loss: 0.2968 - val_f_2: 0.4380 - val_acc: 0.9009\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.001, total=  33.7s\n",
      "[CV] batch_size=80, dropout=0.2, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4717 - f_2: 0.0138 - acc: 0.8541 - val_loss: 0.3582 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3759 - f_2: 1.3880e-08 - acc: 0.8691 - val_loss: 0.3286 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3546 - f_2: 1.3230e-08 - acc: 0.8691 - val_loss: 0.3191 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3466 - f_2: 0.0042 - acc: 0.8694 - val_loss: 0.3131 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3387 - f_2: 0.1962 - acc: 0.8905 - val_loss: 0.3086 - val_f_2: 0.2930 - val_acc: 0.9091\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3327 - f_2: 0.2622 - acc: 0.8981 - val_loss: 0.3136 - val_f_2: 0.3308 - val_acc: 0.9112\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.2796 - acc: 0.8995 - val_loss: 0.3045 - val_f_2: 0.3279 - val_acc: 0.9106\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.2966 - acc: 0.8997 - val_loss: 0.3020 - val_f_2: 0.3338 - val_acc: 0.9106\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3221 - f_2: 0.3120 - acc: 0.9009 - val_loss: 0.2997 - val_f_2: 0.3379 - val_acc: 0.9097\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.3241 - acc: 0.9010 - val_loss: 0.2975 - val_f_2: 0.3289 - val_acc: 0.9100\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3180 - f_2: 0.3218 - acc: 0.9006 - val_loss: 0.2980 - val_f_2: 0.3459 - val_acc: 0.9088\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3157 - f_2: 0.3351 - acc: 0.9021 - val_loss: 0.2961 - val_f_2: 0.3459 - val_acc: 0.9091\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3454 - acc: 0.9021 - val_loss: 0.2970 - val_f_2: 0.3768 - val_acc: 0.9094\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3582 - acc: 0.9017 - val_loss: 0.2954 - val_f_2: 0.3617 - val_acc: 0.9100\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3087 - f_2: 0.3487 - acc: 0.9009 - val_loss: 0.2944 - val_f_2: 0.3540 - val_acc: 0.9094\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3081 - f_2: 0.3536 - acc: 0.9024 - val_loss: 0.2923 - val_f_2: 0.3630 - val_acc: 0.9091\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3086 - f_2: 0.3637 - acc: 0.9023 - val_loss: 0.2914 - val_f_2: 0.3833 - val_acc: 0.9094\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.3724 - acc: 0.9021 - val_loss: 0.2927 - val_f_2: 0.3895 - val_acc: 0.9094\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3055 - f_2: 0.3808 - acc: 0.9037 - val_loss: 0.2952 - val_f_2: 0.4012 - val_acc: 0.9094\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3886 - acc: 0.9042 - val_loss: 0.2916 - val_f_2: 0.3919 - val_acc: 0.9094\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3846 - acc: 0.9023 - val_loss: 0.2924 - val_f_2: 0.3748 - val_acc: 0.9103\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3025 - f_2: 0.3984 - acc: 0.9044 - val_loss: 0.2905 - val_f_2: 0.3857 - val_acc: 0.9097\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.3815 - acc: 0.9025 - val_loss: 0.2901 - val_f_2: 0.4033 - val_acc: 0.9097\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3862 - acc: 0.9027 - val_loss: 0.2948 - val_f_2: 0.4248 - val_acc: 0.9083\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2992 - f_2: 0.3885 - acc: 0.9012 - val_loss: 0.2893 - val_f_2: 0.3753 - val_acc: 0.9100\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.3926 - acc: 0.9029 - val_loss: 0.2936 - val_f_2: 0.3862 - val_acc: 0.9100\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3981 - acc: 0.9029 - val_loss: 0.2891 - val_f_2: 0.4146 - val_acc: 0.9088\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.4026 - acc: 0.9032 - val_loss: 0.2907 - val_f_2: 0.4078 - val_acc: 0.9086\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2949 - f_2: 0.4071 - acc: 0.9034 - val_loss: 0.2893 - val_f_2: 0.4004 - val_acc: 0.9080\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3926 - acc: 0.9017 - val_loss: 0.2897 - val_f_2: 0.4283 - val_acc: 0.9077\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.4087 - acc: 0.9035 - val_loss: 0.2881 - val_f_2: 0.4142 - val_acc: 0.9080\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.4026 - acc: 0.9042 - val_loss: 0.2907 - val_f_2: 0.4278 - val_acc: 0.9071\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2937 - f_2: 0.4142 - acc: 0.9054 - val_loss: 0.2912 - val_f_2: 0.4420 - val_acc: 0.9074\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.4117 - acc: 0.9040 - val_loss: 0.2907 - val_f_2: 0.4097 - val_acc: 0.9068\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.4279 - acc: 0.9053 - val_loss: 0.2884 - val_f_2: 0.4071 - val_acc: 0.9083\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.4011 - acc: 0.9038 - val_loss: 0.2917 - val_f_2: 0.4361 - val_acc: 0.9077\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2909 - f_2: 0.4170 - acc: 0.9037 - val_loss: 0.2904 - val_f_2: 0.3928 - val_acc: 0.9083\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2921 - f_2: 0.4201 - acc: 0.9040 - val_loss: 0.2878 - val_f_2: 0.4179 - val_acc: 0.9091\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.4173 - acc: 0.9040 - val_loss: 0.2943 - val_f_2: 0.3970 - val_acc: 0.9088\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2939 - f_2: 0.4384 - acc: 0.9057 - val_loss: 0.2914 - val_f_2: 0.4229 - val_acc: 0.9103\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2913 - f_2: 0.4292 - acc: 0.9049 - val_loss: 0.2931 - val_f_2: 0.4074 - val_acc: 0.9088\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2921 - f_2: 0.4274 - acc: 0.9046 - val_loss: 0.2882 - val_f_2: 0.4123 - val_acc: 0.9056\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2907 - f_2: 0.4192 - acc: 0.9043 - val_loss: 0.2915 - val_f_2: 0.4202 - val_acc: 0.9056\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2899 - f_2: 0.4255 - acc: 0.9051 - val_loss: 0.2846 - val_f_2: 0.3744 - val_acc: 0.9097\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2885 - f_2: 0.4174 - acc: 0.9059 - val_loss: 0.2926 - val_f_2: 0.3972 - val_acc: 0.9068\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.4228 - acc: 0.9049 - val_loss: 0.2886 - val_f_2: 0.4328 - val_acc: 0.9047\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2901 - f_2: 0.4284 - acc: 0.9051 - val_loss: 0.2902 - val_f_2: 0.4081 - val_acc: 0.9086\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2889 - f_2: 0.4326 - acc: 0.9068 - val_loss: 0.2859 - val_f_2: 0.4104 - val_acc: 0.9074\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.4283 - acc: 0.9049 - val_loss: 0.2882 - val_f_2: 0.4086 - val_acc: 0.9074\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2910 - f_2: 0.4277 - acc: 0.9035 - val_loss: 0.2927 - val_f_2: 0.4308 - val_acc: 0.9021\n",
      "[CV]  batch_size=80, dropout=0.2, nodes=20, penalty=0.001, total=  33.9s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7294 - f_2: 0.0396 - acc: 0.8418 - val_loss: 0.5087 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4585 - f_2: 1.4092e-08 - acc: 0.8753 - val_loss: 0.4018 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3943 - f_2: 1.4588e-08 - acc: 0.8753 - val_loss: 0.3744 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3749 - f_2: 1.3950e-08 - acc: 0.8753 - val_loss: 0.3621 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3647 - f_2: 1.3954e-08 - acc: 0.8753 - val_loss: 0.3531 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3563 - f_2: 1.4129e-08 - acc: 0.8753 - val_loss: 0.3469 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3500 - f_2: 1.3740e-08 - acc: 0.8753 - val_loss: 0.3427 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3505 - f_2: 1.3944e-08 - acc: 0.8753 - val_loss: 0.3412 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3487 - f_2: 1.3912e-08 - acc: 0.8753 - val_loss: 0.3395 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3438 - f_2: 1.4141e-08 - acc: 0.8753 - val_loss: 0.3375 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3442 - f_2: 1.4188e-08 - acc: 0.8753 - val_loss: 0.3357 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3446 - f_2: 1.4010e-08 - acc: 0.8753 - val_loss: 0.3340 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3406 - f_2: 1.3500e-08 - acc: 0.8753 - val_loss: 0.3342 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3401 - f_2: 1.3810e-08 - acc: 0.8753 - val_loss: 0.3333 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3410 - f_2: 1.4140e-08 - acc: 0.8753 - val_loss: 0.3317 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3386 - f_2: 1.3858e-08 - acc: 0.8753 - val_loss: 0.3318 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3388 - f_2: 1.3797e-08 - acc: 0.8753 - val_loss: 0.3336 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3346 - f_2: 1.3847e-08 - acc: 0.8753 - val_loss: 0.3296 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3358 - f_2: 1.3862e-08 - acc: 0.8753 - val_loss: 0.3305 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3348 - f_2: 1.3972e-08 - acc: 0.8753 - val_loss: 0.3291 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3335 - f_2: 1.3627e-08 - acc: 0.8753 - val_loss: 0.3331 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3364 - f_2: 1.4061e-08 - acc: 0.8753 - val_loss: 0.3317 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3380 - f_2: 1.3513e-08 - acc: 0.8753 - val_loss: 0.3322 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3333 - f_2: 1.3741e-08 - acc: 0.8753 - val_loss: 0.3280 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3319 - f_2: 1.4472e-08 - acc: 0.8753 - val_loss: 0.3271 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3319 - f_2: 1.3866e-08 - acc: 0.8753 - val_loss: 0.3276 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3372 - f_2: 1.3859e-08 - acc: 0.8753 - val_loss: 0.3273 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3317 - f_2: 1.3962e-08 - acc: 0.8753 - val_loss: 0.3285 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3313 - f_2: 1.3837e-08 - acc: 0.8753 - val_loss: 0.3262 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3279 - f_2: 1.3818e-08 - acc: 0.8753 - val_loss: 0.3250 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3298 - f_2: 1.3814e-08 - acc: 0.8753 - val_loss: 0.3251 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3330 - f_2: 1.3966e-08 - acc: 0.8753 - val_loss: 0.3250 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3311 - f_2: 1.3887e-08 - acc: 0.8753 - val_loss: 0.3266 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3290 - f_2: 1.4161e-08 - acc: 0.8753 - val_loss: 0.3251 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3300 - f_2: 1.3924e-08 - acc: 0.8753 - val_loss: 0.3263 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3306 - f_2: 1.3827e-08 - acc: 0.8753 - val_loss: 0.3243 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3293 - f_2: 1.4219e-08 - acc: 0.8753 - val_loss: 0.3237 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3271 - f_2: 1.3895e-08 - acc: 0.8753 - val_loss: 0.3233 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3295 - f_2: 1.4139e-08 - acc: 0.8753 - val_loss: 0.3242 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3280 - f_2: 1.4331e-08 - acc: 0.8753 - val_loss: 0.3294 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3297 - f_2: 1.4070e-08 - acc: 0.8753 - val_loss: 0.3249 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3274 - f_2: 1.4263e-08 - acc: 0.8753 - val_loss: 0.3246 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3262 - f_2: 1.3926e-08 - acc: 0.8753 - val_loss: 0.3233 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3279 - f_2: 1.4492e-08 - acc: 0.8753 - val_loss: 0.3230 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3256 - f_2: 1.4262e-08 - acc: 0.8753 - val_loss: 0.3237 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3254 - f_2: 1.4066e-08 - acc: 0.8753 - val_loss: 0.3223 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3293 - f_2: 1.4462e-08 - acc: 0.8753 - val_loss: 0.3248 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3258 - f_2: 1.4105e-08 - acc: 0.8753 - val_loss: 0.3248 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3263 - f_2: 1.4085e-08 - acc: 0.8753 - val_loss: 0.3219 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3265 - f_2: 1.3722e-08 - acc: 0.8753 - val_loss: 0.3231 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.01, total=  32.2s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7344 - f_2: 0.0341 - acc: 0.8353 - val_loss: 0.5119 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4587 - f_2: 1.3624e-08 - acc: 0.8722 - val_loss: 0.4003 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3963 - f_2: 1.3409e-08 - acc: 0.8722 - val_loss: 0.3714 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3751 - f_2: 1.3592e-08 - acc: 0.8722 - val_loss: 0.3592 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3654 - f_2: 1.3604e-08 - acc: 0.8722 - val_loss: 0.3511 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3574 - f_2: 1.3519e-08 - acc: 0.8722 - val_loss: 0.3455 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3523 - f_2: 1.3859e-08 - acc: 0.8722 - val_loss: 0.3424 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3490 - f_2: 1.3410e-08 - acc: 0.8722 - val_loss: 0.3412 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3474 - f_2: 1.3318e-08 - acc: 0.8722 - val_loss: 0.3391 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3444 - f_2: 1.3824e-08 - acc: 0.8722 - val_loss: 0.3349 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3439 - f_2: 1.3435e-08 - acc: 0.8722 - val_loss: 0.3354 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3416 - f_2: 1.3327e-08 - acc: 0.8722 - val_loss: 0.3319 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3400 - f_2: 1.3539e-08 - acc: 0.8722 - val_loss: 0.3325 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3405 - f_2: 1.3808e-08 - acc: 0.8722 - val_loss: 0.3294 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3394 - f_2: 1.3371e-08 - acc: 0.8722 - val_loss: 0.3294 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3437 - f_2: 1.3614e-08 - acc: 0.8722 - val_loss: 0.3285 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3388 - f_2: 1.3818e-08 - acc: 0.8722 - val_loss: 0.3271 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3359 - f_2: 1.3405e-08 - acc: 0.8722 - val_loss: 0.3259 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3350 - f_2: 1.3457e-08 - acc: 0.8722 - val_loss: 0.3248 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3351 - f_2: 1.3475e-08 - acc: 0.8722 - val_loss: 0.3249 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3362 - f_2: 1.3539e-08 - acc: 0.8722 - val_loss: 0.3243 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3336 - f_2: 1.3871e-08 - acc: 0.8722 - val_loss: 0.3236 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3350 - f_2: 1.3556e-08 - acc: 0.8722 - val_loss: 0.3227 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3324 - f_2: 1.4075e-08 - acc: 0.8722 - val_loss: 0.3222 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3307 - f_2: 1.3643e-08 - acc: 0.8722 - val_loss: 0.3237 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3324 - f_2: 1.3756e-08 - acc: 0.8722 - val_loss: 0.3212 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3292 - f_2: 1.3647e-08 - acc: 0.8722 - val_loss: 0.3222 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3314 - f_2: 1.3719e-08 - acc: 0.8722 - val_loss: 0.3234 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3324 - f_2: 1.3358e-08 - acc: 0.8722 - val_loss: 0.3204 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3316 - f_2: 0.0302 - acc: 0.8746 - val_loss: 0.3206 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2037 - acc: 0.8895 - val_loss: 0.3196 - val_f_2: 0.3170 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.2176 - acc: 0.8907 - val_loss: 0.3190 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3305 - f_2: 0.2316 - acc: 0.8924 - val_loss: 0.3191 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3262 - f_2: 0.2567 - acc: 0.8939 - val_loss: 0.3186 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3281 - f_2: 0.2617 - acc: 0.8948 - val_loss: 0.3178 - val_f_2: 0.2853 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.2635 - acc: 0.8946 - val_loss: 0.3176 - val_f_2: 0.2853 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3278 - f_2: 0.2533 - acc: 0.8934 - val_loss: 0.3178 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3287 - f_2: 0.2499 - acc: 0.8939 - val_loss: 0.3165 - val_f_2: 0.2932 - val_acc: 0.8994\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3283 - f_2: 0.2668 - acc: 0.8948 - val_loss: 0.3165 - val_f_2: 0.2896 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.2729 - acc: 0.8966 - val_loss: 0.3158 - val_f_2: 0.3054 - val_acc: 0.8997\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2772 - acc: 0.8961 - val_loss: 0.3178 - val_f_2: 0.2896 - val_acc: 0.9006\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.2717 - acc: 0.8948 - val_loss: 0.3164 - val_f_2: 0.2913 - val_acc: 0.8997\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.2812 - acc: 0.8986 - val_loss: 0.3156 - val_f_2: 0.3028 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3237 - f_2: 0.2868 - acc: 0.8987 - val_loss: 0.3151 - val_f_2: 0.2994 - val_acc: 0.9003\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.2828 - acc: 0.8969 - val_loss: 0.3196 - val_f_2: 0.3537 - val_acc: 0.9041\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3258 - f_2: 0.2885 - acc: 0.8977 - val_loss: 0.3138 - val_f_2: 0.3206 - val_acc: 0.9003\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.2890 - acc: 0.8982 - val_loss: 0.3146 - val_f_2: 0.3796 - val_acc: 0.9009\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.2857 - acc: 0.8974 - val_loss: 0.3150 - val_f_2: 0.2896 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3285 - f_2: 0.2966 - acc: 0.8984 - val_loss: 0.3132 - val_f_2: 0.2986 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3212 - f_2: 0.2970 - acc: 0.8972 - val_loss: 0.3197 - val_f_2: 0.2896 - val_acc: 0.9006\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.01, total=  33.5s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7400 - f_2: 0.0374 - acc: 0.8346 - val_loss: 0.4939 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4671 - f_2: 1.3429e-08 - acc: 0.8691 - val_loss: 0.3831 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3988 - f_2: 1.3085e-08 - acc: 0.8691 - val_loss: 0.3540 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3804 - f_2: 1.3480e-08 - acc: 0.8691 - val_loss: 0.3423 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3714 - f_2: 1.3422e-08 - acc: 0.8691 - val_loss: 0.3385 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3637 - f_2: 1.3167e-08 - acc: 0.8691 - val_loss: 0.3296 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3576 - f_2: 1.3328e-08 - acc: 0.8691 - val_loss: 0.3254 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3581 - f_2: 1.2959e-08 - acc: 0.8691 - val_loss: 0.3234 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3552 - f_2: 1.2929e-08 - acc: 0.8691 - val_loss: 0.3225 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3525 - f_2: 1.3151e-08 - acc: 0.8691 - val_loss: 0.3257 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3523 - f_2: 1.3034e-08 - acc: 0.8691 - val_loss: 0.3183 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3519 - f_2: 1.3158e-08 - acc: 0.8691 - val_loss: 0.3180 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3501 - f_2: 1.3484e-08 - acc: 0.8691 - val_loss: 0.3166 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3493 - f_2: 1.3295e-08 - acc: 0.8691 - val_loss: 0.3159 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3488 - f_2: 1.2887e-08 - acc: 0.8691 - val_loss: 0.3148 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3466 - f_2: 1.3029e-08 - acc: 0.8691 - val_loss: 0.3134 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3445 - f_2: 1.3083e-08 - acc: 0.8691 - val_loss: 0.3136 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3429 - f_2: 1.3552e-08 - acc: 0.8691 - val_loss: 0.3120 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3453 - f_2: 1.3043e-08 - acc: 0.8691 - val_loss: 0.3113 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3446 - f_2: 1.3275e-08 - acc: 0.8691 - val_loss: 0.3111 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3436 - f_2: 1.3549e-08 - acc: 0.8691 - val_loss: 0.3105 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3418 - f_2: 1.3416e-08 - acc: 0.8691 - val_loss: 0.3101 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3424 - f_2: 1.3054e-08 - acc: 0.8691 - val_loss: 0.3116 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3406 - f_2: 1.3025e-08 - acc: 0.8691 - val_loss: 0.3094 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3452 - f_2: 1.3205e-08 - acc: 0.8691 - val_loss: 0.3103 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3414 - f_2: 1.3223e-08 - acc: 0.8691 - val_loss: 0.3095 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3402 - f_2: 1.3163e-08 - acc: 0.8691 - val_loss: 0.3088 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3400 - f_2: 1.3644e-08 - acc: 0.8691 - val_loss: 0.3080 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3414 - f_2: 1.3018e-08 - acc: 0.8691 - val_loss: 0.3075 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3379 - f_2: 1.3243e-08 - acc: 0.8691 - val_loss: 0.3070 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3418 - f_2: 1.3004e-08 - acc: 0.8691 - val_loss: 0.3094 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3381 - f_2: 1.3200e-08 - acc: 0.8691 - val_loss: 0.3077 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3358 - f_2: 1.3754e-08 - acc: 0.8691 - val_loss: 0.3063 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3381 - f_2: 1.3093e-08 - acc: 0.8691 - val_loss: 0.3059 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3370 - f_2: 1.3317e-08 - acc: 0.8691 - val_loss: 0.3064 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3352 - f_2: 1.3395e-08 - acc: 0.8691 - val_loss: 0.3047 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3370 - f_2: 1.3025e-08 - acc: 0.8691 - val_loss: 0.3048 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3392 - f_2: 1.3744e-08 - acc: 0.8691 - val_loss: 0.3049 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3381 - f_2: 1.3949e-08 - acc: 0.8691 - val_loss: 0.3046 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3364 - f_2: 1.3993e-08 - acc: 0.8691 - val_loss: 0.3042 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3347 - f_2: 1.3192e-08 - acc: 0.8691 - val_loss: 0.3037 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3358 - f_2: 1.2908e-08 - acc: 0.8691 - val_loss: 0.3036 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3372 - f_2: 1.3481e-08 - acc: 0.8691 - val_loss: 0.3034 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3347 - f_2: 1.3416e-08 - acc: 0.8691 - val_loss: 0.3045 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3373 - f_2: 1.3373e-08 - acc: 0.8691 - val_loss: 0.3026 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3330 - f_2: 1.3286e-08 - acc: 0.8691 - val_loss: 0.3028 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3330 - f_2: 1.3681e-08 - acc: 0.8691 - val_loss: 0.3049 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3357 - f_2: 1.3264e-08 - acc: 0.8691 - val_loss: 0.3039 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3328 - f_2: 1.3249e-08 - acc: 0.8691 - val_loss: 0.3024 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3294 - f_2: 1.3378e-08 - acc: 0.8691 - val_loss: 0.3019 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.01, total=  33.3s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4862 - f_2: 0.0315 - acc: 0.8438 - val_loss: 0.3740 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3817 - f_2: 1.3780e-08 - acc: 0.8753 - val_loss: 0.3496 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3613 - f_2: 1.4035e-08 - acc: 0.8753 - val_loss: 0.3370 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3518 - f_2: 1.4404e-08 - acc: 0.8753 - val_loss: 0.3317 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3469 - f_2: 1.4319e-08 - acc: 0.8753 - val_loss: 0.3290 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3416 - f_2: 1.4010e-08 - acc: 0.8753 - val_loss: 0.3274 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3372 - f_2: 1.4116e-08 - acc: 0.8753 - val_loss: 0.3258 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3352 - f_2: 1.3975e-08 - acc: 0.8753 - val_loss: 0.3240 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3353 - f_2: 1.4268e-08 - acc: 0.8753 - val_loss: 0.3228 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3307 - f_2: 1.3852e-08 - acc: 0.8753 - val_loss: 0.3219 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3287 - f_2: 1.4309e-08 - acc: 0.8753 - val_loss: 0.3205 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3273 - f_2: 1.3855e-08 - acc: 0.8753 - val_loss: 0.3202 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3240 - f_2: 1.4208e-08 - acc: 0.8753 - val_loss: 0.3190 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3222 - f_2: 1.3593e-08 - acc: 0.8753 - val_loss: 0.3183 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3235 - f_2: 1.3831e-08 - acc: 0.8753 - val_loss: 0.3192 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3198 - f_2: 1.4603e-08 - acc: 0.8753 - val_loss: 0.3170 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3195 - f_2: 1.3918e-08 - acc: 0.8753 - val_loss: 0.3185 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3199 - f_2: 1.4008e-08 - acc: 0.8753 - val_loss: 0.3167 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3181 - f_2: 1.3956e-08 - acc: 0.8753 - val_loss: 0.3166 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3162 - f_2: 1.4420e-08 - acc: 0.8753 - val_loss: 0.3182 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3151 - f_2: 1.4093e-08 - acc: 0.8753 - val_loss: 0.3192 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.1580 - acc: 0.8879 - val_loss: 0.3162 - val_f_2: 0.2971 - val_acc: 0.8997\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3137 - f_2: 0.2587 - acc: 0.8965 - val_loss: 0.3157 - val_f_2: 0.3319 - val_acc: 0.8985\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.2744 - acc: 0.8978 - val_loss: 0.3152 - val_f_2: 0.3170 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3149 - f_2: 0.2737 - acc: 0.8989 - val_loss: 0.3145 - val_f_2: 0.3267 - val_acc: 0.8985\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3112 - f_2: 0.2858 - acc: 0.8998 - val_loss: 0.3143 - val_f_2: 0.3763 - val_acc: 0.8988\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.2935 - acc: 0.8987 - val_loss: 0.3172 - val_f_2: 0.3214 - val_acc: 0.8997\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.3013 - acc: 0.8999 - val_loss: 0.3143 - val_f_2: 0.3175 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3040 - acc: 0.9007 - val_loss: 0.3148 - val_f_2: 0.3826 - val_acc: 0.8994\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3271 - acc: 0.9017 - val_loss: 0.3156 - val_f_2: 0.3195 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.2852 - acc: 0.8979 - val_loss: 0.3140 - val_f_2: 0.3962 - val_acc: 0.8973\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3070 - acc: 0.8978 - val_loss: 0.3134 - val_f_2: 0.3602 - val_acc: 0.9009\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3111 - f_2: 0.3080 - acc: 0.9007 - val_loss: 0.3158 - val_f_2: 0.3702 - val_acc: 0.8985\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3217 - acc: 0.9006 - val_loss: 0.3132 - val_f_2: 0.4239 - val_acc: 0.8979\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3272 - acc: 0.9011 - val_loss: 0.3160 - val_f_2: 0.3838 - val_acc: 0.8997\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3058 - f_2: 0.3069 - acc: 0.8992 - val_loss: 0.3153 - val_f_2: 0.3911 - val_acc: 0.9003\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3094 - f_2: 0.3256 - acc: 0.9023 - val_loss: 0.3136 - val_f_2: 0.3973 - val_acc: 0.8979\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3022 - acc: 0.8986 - val_loss: 0.3152 - val_f_2: 0.4208 - val_acc: 0.8973\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3156 - acc: 0.9011 - val_loss: 0.3125 - val_f_2: 0.3894 - val_acc: 0.9003\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.3158 - acc: 0.9001 - val_loss: 0.3153 - val_f_2: 0.3963 - val_acc: 0.8991\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3072 - f_2: 0.3251 - acc: 0.9001 - val_loss: 0.3152 - val_f_2: 0.4179 - val_acc: 0.8973\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3198 - acc: 0.8993 - val_loss: 0.3158 - val_f_2: 0.4174 - val_acc: 0.8956\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3102 - acc: 0.8989 - val_loss: 0.3146 - val_f_2: 0.4306 - val_acc: 0.8953\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.3254 - acc: 0.8989 - val_loss: 0.3163 - val_f_2: 0.4033 - val_acc: 0.8973\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3339 - acc: 0.9013 - val_loss: 0.3211 - val_f_2: 0.3981 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3056 - f_2: 0.3251 - acc: 0.8995 - val_loss: 0.3222 - val_f_2: 0.3850 - val_acc: 0.8982\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3462 - acc: 0.9011 - val_loss: 0.3153 - val_f_2: 0.3923 - val_acc: 0.8985\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3316 - acc: 0.9009 - val_loss: 0.3156 - val_f_2: 0.4218 - val_acc: 0.8979\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2996 - f_2: 0.3444 - acc: 0.9012 - val_loss: 0.3137 - val_f_2: 0.4460 - val_acc: 0.8971\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3370 - acc: 0.9006 - val_loss: 0.3169 - val_f_2: 0.4258 - val_acc: 0.8979\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.001, total=  32.4s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4902 - f_2: 0.0315 - acc: 0.8390 - val_loss: 0.3729 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3793 - f_2: 0.0318 - acc: 0.8756 - val_loss: 0.3427 - val_f_2: 0.0784 - val_acc: 0.8755\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3541 - f_2: 0.1879 - acc: 0.8897 - val_loss: 0.3252 - val_f_2: 0.2877 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3418 - f_2: 0.2077 - acc: 0.8893 - val_loss: 0.3159 - val_f_2: 0.3027 - val_acc: 0.9015\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3320 - f_2: 0.2472 - acc: 0.8936 - val_loss: 0.3094 - val_f_2: 0.3336 - val_acc: 0.9009\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.2914 - acc: 0.8982 - val_loss: 0.3044 - val_f_2: 0.3448 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3195 - f_2: 0.2895 - acc: 0.8958 - val_loss: 0.3028 - val_f_2: 0.3347 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3167 - f_2: 0.3082 - acc: 0.8998 - val_loss: 0.3019 - val_f_2: 0.3320 - val_acc: 0.9003\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.3017 - acc: 0.8971 - val_loss: 0.3000 - val_f_2: 0.3458 - val_acc: 0.9003\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3091 - f_2: 0.3230 - acc: 0.8993 - val_loss: 0.2983 - val_f_2: 0.3960 - val_acc: 0.8982\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3415 - acc: 0.9020 - val_loss: 0.2978 - val_f_2: 0.3483 - val_acc: 0.8982\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3078 - f_2: 0.3276 - acc: 0.8993 - val_loss: 0.2976 - val_f_2: 0.3439 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3058 - f_2: 0.3338 - acc: 0.9012 - val_loss: 0.2972 - val_f_2: 0.3705 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3033 - f_2: 0.3304 - acc: 0.8992 - val_loss: 0.2953 - val_f_2: 0.3908 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3489 - acc: 0.9011 - val_loss: 0.2962 - val_f_2: 0.3641 - val_acc: 0.8994\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3296 - acc: 0.9004 - val_loss: 0.2971 - val_f_2: 0.3712 - val_acc: 0.8991\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.3516 - acc: 0.9017 - val_loss: 0.2960 - val_f_2: 0.3771 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2998 - f_2: 0.3641 - acc: 0.9031 - val_loss: 0.2970 - val_f_2: 0.3520 - val_acc: 0.8976\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.2956 - f_2: 0.3554 - acc: 0.9030 - val_loss: 0.2949 - val_f_2: 0.4198 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.3643 - acc: 0.9035 - val_loss: 0.2951 - val_f_2: 0.3748 - val_acc: 0.8982\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.3621 - acc: 0.9034 - val_loss: 0.2951 - val_f_2: 0.3978 - val_acc: 0.8991\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.2958 - f_2: 0.3672 - acc: 0.9036 - val_loss: 0.2949 - val_f_2: 0.4020 - val_acc: 0.8997\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3768 - acc: 0.9020 - val_loss: 0.2955 - val_f_2: 0.3632 - val_acc: 0.8991\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.3758 - acc: 0.9044 - val_loss: 0.2966 - val_f_2: 0.3676 - val_acc: 0.8982\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.2935 - f_2: 0.3721 - acc: 0.9046 - val_loss: 0.2953 - val_f_2: 0.4356 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.3658 - acc: 0.9032 - val_loss: 0.2963 - val_f_2: 0.3631 - val_acc: 0.8973\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.2916 - f_2: 0.3795 - acc: 0.9044 - val_loss: 0.2949 - val_f_2: 0.4441 - val_acc: 0.9009\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.2901 - f_2: 0.3845 - acc: 0.9043 - val_loss: 0.2954 - val_f_2: 0.4360 - val_acc: 0.9027\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.2902 - f_2: 0.3824 - acc: 0.9044 - val_loss: 0.2952 - val_f_2: 0.4314 - val_acc: 0.9018\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2893 - f_2: 0.3917 - acc: 0.9046 - val_loss: 0.2955 - val_f_2: 0.4174 - val_acc: 0.9009\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2908 - f_2: 0.3795 - acc: 0.9038 - val_loss: 0.2943 - val_f_2: 0.4252 - val_acc: 0.9018\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2899 - f_2: 0.3793 - acc: 0.9033 - val_loss: 0.2969 - val_f_2: 0.3798 - val_acc: 0.8982\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2903 - f_2: 0.3751 - acc: 0.9040 - val_loss: 0.2978 - val_f_2: 0.3789 - val_acc: 0.8979\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2876 - f_2: 0.3897 - acc: 0.9050 - val_loss: 0.2998 - val_f_2: 0.3938 - val_acc: 0.8994\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2892 - f_2: 0.3870 - acc: 0.9049 - val_loss: 0.2990 - val_f_2: 0.4530 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2891 - f_2: 0.3978 - acc: 0.9058 - val_loss: 0.2963 - val_f_2: 0.4363 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2889 - f_2: 0.3815 - acc: 0.9043 - val_loss: 0.2980 - val_f_2: 0.4392 - val_acc: 0.9003\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2866 - f_2: 0.3923 - acc: 0.9046 - val_loss: 0.2966 - val_f_2: 0.4407 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.3939 - acc: 0.9048 - val_loss: 0.2966 - val_f_2: 0.4271 - val_acc: 0.9012\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2854 - f_2: 0.3990 - acc: 0.9073 - val_loss: 0.2979 - val_f_2: 0.3760 - val_acc: 0.8994\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2849 - f_2: 0.4052 - acc: 0.9068 - val_loss: 0.2965 - val_f_2: 0.3873 - val_acc: 0.8982\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2858 - f_2: 0.3971 - acc: 0.9054 - val_loss: 0.2958 - val_f_2: 0.4255 - val_acc: 0.9003\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2834 - f_2: 0.4077 - acc: 0.9062 - val_loss: 0.2981 - val_f_2: 0.4051 - val_acc: 0.9012\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2867 - f_2: 0.4014 - acc: 0.9060 - val_loss: 0.2953 - val_f_2: 0.4359 - val_acc: 0.9021\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2838 - f_2: 0.4064 - acc: 0.9072 - val_loss: 0.2966 - val_f_2: 0.4131 - val_acc: 0.9003\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2853 - f_2: 0.4015 - acc: 0.9064 - val_loss: 0.2968 - val_f_2: 0.4443 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.4189 - acc: 0.9083 - val_loss: 0.2974 - val_f_2: 0.4499 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2851 - f_2: 0.4108 - acc: 0.9083 - val_loss: 0.2970 - val_f_2: 0.4135 - val_acc: 0.8997\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2851 - f_2: 0.4095 - acc: 0.9071 - val_loss: 0.2966 - val_f_2: 0.4363 - val_acc: 0.8994\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2843 - f_2: 0.4106 - acc: 0.9074 - val_loss: 0.2966 - val_f_2: 0.4175 - val_acc: 0.9009\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.001, total=  33.3s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=10, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4970 - f_2: 0.0366 - acc: 0.8351 - val_loss: 0.3586 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3890 - f_2: 0.0107 - acc: 0.8703 - val_loss: 0.3279 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3674 - f_2: 0.1071 - acc: 0.8804 - val_loss: 0.3151 - val_f_2: 0.2701 - val_acc: 0.9074\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3529 - f_2: 0.1691 - acc: 0.8846 - val_loss: 0.3033 - val_f_2: 0.3099 - val_acc: 0.9118\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3405 - f_2: 0.2096 - acc: 0.8883 - val_loss: 0.2967 - val_f_2: 0.3430 - val_acc: 0.9097\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3345 - f_2: 0.2217 - acc: 0.8872 - val_loss: 0.2935 - val_f_2: 0.3443 - val_acc: 0.9112\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.2477 - acc: 0.8917 - val_loss: 0.2893 - val_f_2: 0.3734 - val_acc: 0.9097\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2581 - acc: 0.8911 - val_loss: 0.2875 - val_f_2: 0.3640 - val_acc: 0.9094\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2724 - acc: 0.8926 - val_loss: 0.2846 - val_f_2: 0.3477 - val_acc: 0.9097\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2761 - acc: 0.8934 - val_loss: 0.2837 - val_f_2: 0.3632 - val_acc: 0.9100\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.2883 - acc: 0.8932 - val_loss: 0.2835 - val_f_2: 0.3621 - val_acc: 0.9100\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.2862 - acc: 0.8932 - val_loss: 0.2816 - val_f_2: 0.3476 - val_acc: 0.9086\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.2792 - acc: 0.8919 - val_loss: 0.2815 - val_f_2: 0.3732 - val_acc: 0.9097\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3151 - f_2: 0.2903 - acc: 0.8930 - val_loss: 0.2806 - val_f_2: 0.3769 - val_acc: 0.9103\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3115 - f_2: 0.3004 - acc: 0.8936 - val_loss: 0.2817 - val_f_2: 0.3769 - val_acc: 0.9100\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.2766 - acc: 0.8932 - val_loss: 0.2791 - val_f_2: 0.3857 - val_acc: 0.9112\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3076 - f_2: 0.3269 - acc: 0.8955 - val_loss: 0.2780 - val_f_2: 0.3808 - val_acc: 0.9106\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3092 - f_2: 0.3230 - acc: 0.8961 - val_loss: 0.2778 - val_f_2: 0.3752 - val_acc: 0.9097\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3190 - acc: 0.8953 - val_loss: 0.2769 - val_f_2: 0.3718 - val_acc: 0.9097\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3344 - acc: 0.8973 - val_loss: 0.2767 - val_f_2: 0.3653 - val_acc: 0.9094\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3053 - f_2: 0.3296 - acc: 0.8970 - val_loss: 0.2756 - val_f_2: 0.4037 - val_acc: 0.9121\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3062 - f_2: 0.3291 - acc: 0.8964 - val_loss: 0.2761 - val_f_2: 0.3804 - val_acc: 0.9112\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3360 - acc: 0.8964 - val_loss: 0.2766 - val_f_2: 0.3971 - val_acc: 0.9106\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3322 - acc: 0.8956 - val_loss: 0.2751 - val_f_2: 0.3915 - val_acc: 0.9109\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3029 - f_2: 0.3299 - acc: 0.8965 - val_loss: 0.2751 - val_f_2: 0.4060 - val_acc: 0.9115\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.3401 - acc: 0.8968 - val_loss: 0.2743 - val_f_2: 0.4116 - val_acc: 0.9097\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3199 - acc: 0.8941 - val_loss: 0.2744 - val_f_2: 0.4174 - val_acc: 0.9097\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3339 - acc: 0.8945 - val_loss: 0.2739 - val_f_2: 0.3554 - val_acc: 0.9091\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3009 - f_2: 0.3396 - acc: 0.8965 - val_loss: 0.2751 - val_f_2: 0.4105 - val_acc: 0.9077\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.3545 - acc: 0.8964 - val_loss: 0.2746 - val_f_2: 0.3753 - val_acc: 0.9094\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3406 - acc: 0.8979 - val_loss: 0.2746 - val_f_2: 0.4294 - val_acc: 0.9088\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.3486 - acc: 0.8970 - val_loss: 0.2757 - val_f_2: 0.4280 - val_acc: 0.9083\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3486 - acc: 0.8971 - val_loss: 0.2751 - val_f_2: 0.4105 - val_acc: 0.9100\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2988 - f_2: 0.3472 - acc: 0.8979 - val_loss: 0.2742 - val_f_2: 0.4074 - val_acc: 0.9115\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.3552 - acc: 0.8976 - val_loss: 0.2736 - val_f_2: 0.4017 - val_acc: 0.9109\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2940 - f_2: 0.3619 - acc: 0.8998 - val_loss: 0.2747 - val_f_2: 0.4230 - val_acc: 0.9083\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.3576 - acc: 0.8985 - val_loss: 0.2745 - val_f_2: 0.4111 - val_acc: 0.9091\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2956 - f_2: 0.3673 - acc: 0.8986 - val_loss: 0.2727 - val_f_2: 0.4198 - val_acc: 0.9077\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3583 - acc: 0.8975 - val_loss: 0.2733 - val_f_2: 0.4139 - val_acc: 0.9083\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2961 - f_2: 0.3662 - acc: 0.8970 - val_loss: 0.2726 - val_f_2: 0.3933 - val_acc: 0.9100\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2934 - f_2: 0.3589 - acc: 0.8980 - val_loss: 0.2736 - val_f_2: 0.4287 - val_acc: 0.9077\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3760 - acc: 0.8984 - val_loss: 0.2750 - val_f_2: 0.4256 - val_acc: 0.9029\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3668 - acc: 0.8990 - val_loss: 0.2744 - val_f_2: 0.4106 - val_acc: 0.9094\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2943 - f_2: 0.3488 - acc: 0.8959 - val_loss: 0.2746 - val_f_2: 0.4092 - val_acc: 0.9083\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2928 - f_2: 0.3708 - acc: 0.8994 - val_loss: 0.2736 - val_f_2: 0.4053 - val_acc: 0.9112\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2934 - f_2: 0.3708 - acc: 0.8991 - val_loss: 0.2732 - val_f_2: 0.4224 - val_acc: 0.9074\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2934 - f_2: 0.3761 - acc: 0.9000 - val_loss: 0.2756 - val_f_2: 0.3917 - val_acc: 0.9086\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2931 - f_2: 0.3669 - acc: 0.8984 - val_loss: 0.2733 - val_f_2: 0.4272 - val_acc: 0.9077\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2888 - f_2: 0.3934 - acc: 0.9021 - val_loss: 0.2721 - val_f_2: 0.4173 - val_acc: 0.9059\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2925 - f_2: 0.3736 - acc: 0.8983 - val_loss: 0.2732 - val_f_2: 0.4103 - val_acc: 0.9077\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=10, penalty=0.001, total=  32.7s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7810 - f_2: 0.0248 - acc: 0.8435 - val_loss: 0.5117 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4452 - f_2: 1.4240e-08 - acc: 0.8753 - val_loss: 0.3901 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3860 - f_2: 1.4088e-08 - acc: 0.8753 - val_loss: 0.3685 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3658 - f_2: 1.4282e-08 - acc: 0.8753 - val_loss: 0.3577 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3612 - f_2: 1.3822e-08 - acc: 0.8753 - val_loss: 0.3518 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3548 - f_2: 1.3999e-08 - acc: 0.8753 - val_loss: 0.3460 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3507 - f_2: 1.4008e-08 - acc: 0.8753 - val_loss: 0.3417 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3448 - f_2: 1.4293e-08 - acc: 0.8753 - val_loss: 0.3380 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3455 - f_2: 1.4786e-08 - acc: 0.8753 - val_loss: 0.3354 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3402 - f_2: 1.4149e-08 - acc: 0.8753 - val_loss: 0.3351 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3397 - f_2: 1.3835e-08 - acc: 0.8753 - val_loss: 0.3332 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3377 - f_2: 1.3671e-08 - acc: 0.8753 - val_loss: 0.3311 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3377 - f_2: 1.4000e-08 - acc: 0.8753 - val_loss: 0.3293 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3350 - f_2: 1.4196e-08 - acc: 0.8753 - val_loss: 0.3280 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3318 - f_2: 1.3921e-08 - acc: 0.8753 - val_loss: 0.3274 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3330 - f_2: 1.3783e-08 - acc: 0.8753 - val_loss: 0.3264 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3333 - f_2: 0.0027 - acc: 0.8753 - val_loss: 0.3296 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3326 - f_2: 1.4171e-08 - acc: 0.8753 - val_loss: 0.3266 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3312 - f_2: 1.4518e-08 - acc: 0.8753 - val_loss: 0.3259 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3307 - f_2: 1.3887e-08 - acc: 0.8753 - val_loss: 0.3244 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3313 - f_2: 1.3780e-08 - acc: 0.8753 - val_loss: 0.3243 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3292 - f_2: 1.3942e-08 - acc: 0.8753 - val_loss: 0.3230 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3288 - f_2: 1.4081e-08 - acc: 0.8753 - val_loss: 0.3235 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3298 - f_2: 1.3765e-08 - acc: 0.8753 - val_loss: 0.3231 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3274 - f_2: 1.3927e-08 - acc: 0.8753 - val_loss: 0.3230 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3283 - f_2: 1.3680e-08 - acc: 0.8753 - val_loss: 0.3218 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3282 - f_2: 1.3659e-08 - acc: 0.8753 - val_loss: 0.3221 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3257 - f_2: 1.4198e-08 - acc: 0.8753 - val_loss: 0.3226 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3267 - f_2: 1.3908e-08 - acc: 0.8753 - val_loss: 0.3209 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.0715 - acc: 0.8826 - val_loss: 0.3220 - val_f_2: 0.2981 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3266 - f_2: 0.1733 - acc: 0.8913 - val_loss: 0.3203 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2295 - acc: 0.8976 - val_loss: 0.3215 - val_f_2: 0.2999 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3242 - f_2: 0.2768 - acc: 0.9006 - val_loss: 0.3226 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3239 - f_2: 0.2761 - acc: 0.9004 - val_loss: 0.3198 - val_f_2: 0.3020 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3216 - f_2: 0.2789 - acc: 0.9015 - val_loss: 0.3197 - val_f_2: 0.3085 - val_acc: 0.8997\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2980 - acc: 0.9026 - val_loss: 0.3188 - val_f_2: 0.3094 - val_acc: 0.9018\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.2899 - acc: 0.9020 - val_loss: 0.3197 - val_f_2: 0.3038 - val_acc: 0.9009\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3244 - f_2: 0.2785 - acc: 0.9022 - val_loss: 0.3189 - val_f_2: 0.3534 - val_acc: 0.8988\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.3063 - acc: 0.9026 - val_loss: 0.3181 - val_f_2: 0.3124 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.3025 - acc: 0.9031 - val_loss: 0.3186 - val_f_2: 0.3156 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.3124 - acc: 0.9037 - val_loss: 0.3209 - val_f_2: 0.2984 - val_acc: 0.9003\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3215 - f_2: 0.2938 - acc: 0.9029 - val_loss: 0.3178 - val_f_2: 0.3288 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.3114 - acc: 0.9033 - val_loss: 0.3184 - val_f_2: 0.3343 - val_acc: 0.8985\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3213 - f_2: 0.3019 - acc: 0.9028 - val_loss: 0.3188 - val_f_2: 0.3160 - val_acc: 0.9015\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.3015 - acc: 0.9022 - val_loss: 0.3177 - val_f_2: 0.3176 - val_acc: 0.9012\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.3028 - acc: 0.9014 - val_loss: 0.3204 - val_f_2: 0.3382 - val_acc: 0.8997\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3219 - f_2: 0.3210 - acc: 0.9030 - val_loss: 0.3180 - val_f_2: 0.3618 - val_acc: 0.8985\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3172 - f_2: 0.3229 - acc: 0.9039 - val_loss: 0.3176 - val_f_2: 0.3233 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.3124 - acc: 0.9038 - val_loss: 0.3173 - val_f_2: 0.3352 - val_acc: 0.9003\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3192 - f_2: 0.3229 - acc: 0.9034 - val_loss: 0.3182 - val_f_2: 0.3526 - val_acc: 0.8988\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.01, total=  34.1s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7768 - f_2: 0.0275 - acc: 0.8469 - val_loss: 0.5108 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4420 - f_2: 1.3449e-08 - acc: 0.8722 - val_loss: 0.3878 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3843 - f_2: 1.3614e-08 - acc: 0.8722 - val_loss: 0.3668 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3679 - f_2: 1.3263e-08 - acc: 0.8722 - val_loss: 0.3566 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3603 - f_2: 1.3612e-08 - acc: 0.8722 - val_loss: 0.3477 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3527 - f_2: 1.3453e-08 - acc: 0.8722 - val_loss: 0.3427 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3482 - f_2: 1.3476e-08 - acc: 0.8722 - val_loss: 0.3404 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3451 - f_2: 1.3608e-08 - acc: 0.8722 - val_loss: 0.3380 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3437 - f_2: 1.3697e-08 - acc: 0.8722 - val_loss: 0.3343 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3406 - f_2: 1.3500e-08 - acc: 0.8722 - val_loss: 0.3318 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3402 - f_2: 1.3865e-08 - acc: 0.8722 - val_loss: 0.3320 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3378 - f_2: 1.3371e-08 - acc: 0.8722 - val_loss: 0.3301 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3389 - f_2: 1.3532e-08 - acc: 0.8722 - val_loss: 0.3294 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3372 - f_2: 1.3446e-08 - acc: 0.8722 - val_loss: 0.3283 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3372 - f_2: 1.3560e-08 - acc: 0.8722 - val_loss: 0.3278 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3349 - f_2: 1.3746e-08 - acc: 0.8722 - val_loss: 0.3263 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3330 - f_2: 1.3558e-08 - acc: 0.8722 - val_loss: 0.3275 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3311 - f_2: 0.0029 - acc: 0.8725 - val_loss: 0.3236 - val_f_2: 0.0146 - val_acc: 0.8684\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3334 - f_2: 0.1388 - acc: 0.8866 - val_loss: 0.3236 - val_f_2: 0.2115 - val_acc: 0.8909\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3318 - f_2: 0.1642 - acc: 0.8882 - val_loss: 0.3248 - val_f_2: 0.0457 - val_acc: 0.8717\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3319 - f_2: 0.1798 - acc: 0.8896 - val_loss: 0.3219 - val_f_2: 0.2701 - val_acc: 0.8979\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.1967 - acc: 0.8920 - val_loss: 0.3215 - val_f_2: 0.2758 - val_acc: 0.8988\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3269 - f_2: 0.2178 - acc: 0.8936 - val_loss: 0.3212 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.2066 - acc: 0.8913 - val_loss: 0.3201 - val_f_2: 0.2854 - val_acc: 0.8997\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3275 - f_2: 0.2134 - acc: 0.8925 - val_loss: 0.3196 - val_f_2: 0.2901 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3274 - f_2: 0.2078 - acc: 0.8913 - val_loss: 0.3223 - val_f_2: 0.2834 - val_acc: 0.8997\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3276 - f_2: 0.2162 - acc: 0.8930 - val_loss: 0.3197 - val_f_2: 0.3390 - val_acc: 0.8994\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.2325 - acc: 0.8939 - val_loss: 0.3182 - val_f_2: 0.2888 - val_acc: 0.8997\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3276 - f_2: 0.2095 - acc: 0.8929 - val_loss: 0.3182 - val_f_2: 0.2913 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3243 - f_2: 0.2201 - acc: 0.8925 - val_loss: 0.3171 - val_f_2: 0.3074 - val_acc: 0.9003\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3258 - f_2: 0.2322 - acc: 0.8956 - val_loss: 0.3164 - val_f_2: 0.2932 - val_acc: 0.9009\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3285 - f_2: 0.2053 - acc: 0.8920 - val_loss: 0.3172 - val_f_2: 0.3059 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3251 - f_2: 0.2301 - acc: 0.8940 - val_loss: 0.3160 - val_f_2: 0.2965 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.2367 - acc: 0.8941 - val_loss: 0.3164 - val_f_2: 0.2944 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3259 - f_2: 0.2551 - acc: 0.8961 - val_loss: 0.3158 - val_f_2: 0.2931 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2810 - acc: 0.8993 - val_loss: 0.3146 - val_f_2: 0.3031 - val_acc: 0.9006\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.2770 - acc: 0.8968 - val_loss: 0.3147 - val_f_2: 0.3076 - val_acc: 0.9003\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3250 - f_2: 0.2845 - acc: 0.8988 - val_loss: 0.3144 - val_f_2: 0.3045 - val_acc: 0.9003\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2647 - acc: 0.8964 - val_loss: 0.3150 - val_f_2: 0.3158 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2905 - acc: 0.8990 - val_loss: 0.3141 - val_f_2: 0.3141 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3207 - f_2: 0.2916 - acc: 0.9001 - val_loss: 0.3163 - val_f_2: 0.3380 - val_acc: 0.9003\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3242 - f_2: 0.2915 - acc: 0.8986 - val_loss: 0.3133 - val_f_2: 0.3421 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.3180 - acc: 0.9026 - val_loss: 0.3143 - val_f_2: 0.3565 - val_acc: 0.9015\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.3228 - acc: 0.9012 - val_loss: 0.3148 - val_f_2: 0.3012 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.3193 - acc: 0.9012 - val_loss: 0.3164 - val_f_2: 0.3198 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.3275 - acc: 0.9021 - val_loss: 0.3156 - val_f_2: 0.2988 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3224 - f_2: 0.3133 - acc: 0.9022 - val_loss: 0.3117 - val_f_2: 0.3385 - val_acc: 0.9006\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3221 - f_2: 0.3254 - acc: 0.9032 - val_loss: 0.3115 - val_f_2: 0.3062 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.3211 - acc: 0.9020 - val_loss: 0.3141 - val_f_2: 0.3285 - val_acc: 0.9006\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.3318 - acc: 0.9021 - val_loss: 0.3130 - val_f_2: 0.3700 - val_acc: 0.9018\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.01, total=  34.1s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.7856 - f_2: 0.0247 - acc: 0.8388 - val_loss: 0.4921 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4526 - f_2: 1.3307e-08 - acc: 0.8691 - val_loss: 0.3691 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3867 - f_2: 1.3184e-08 - acc: 0.8691 - val_loss: 0.3475 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3734 - f_2: 1.3046e-08 - acc: 0.8691 - val_loss: 0.3382 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3669 - f_2: 1.3344e-08 - acc: 0.8691 - val_loss: 0.3323 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3612 - f_2: 1.3738e-08 - acc: 0.8691 - val_loss: 0.3292 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3575 - f_2: 1.3324e-08 - acc: 0.8691 - val_loss: 0.3269 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3564 - f_2: 1.2935e-08 - acc: 0.8691 - val_loss: 0.3233 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3502 - f_2: 1.3490e-08 - acc: 0.8691 - val_loss: 0.3189 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3501 - f_2: 1.3023e-08 - acc: 0.8691 - val_loss: 0.3161 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3434 - f_2: 1.3241e-08 - acc: 0.8691 - val_loss: 0.3151 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3497 - f_2: 1.3100e-08 - acc: 0.8691 - val_loss: 0.3140 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3426 - f_2: 1.3226e-08 - acc: 0.8691 - val_loss: 0.3122 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3427 - f_2: 1.3148e-08 - acc: 0.8691 - val_loss: 0.3111 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3426 - f_2: 1.3142e-08 - acc: 0.8691 - val_loss: 0.3124 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3434 - f_2: 1.3464e-08 - acc: 0.8691 - val_loss: 0.3106 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3444 - f_2: 1.2988e-08 - acc: 0.8691 - val_loss: 0.3095 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3388 - f_2: 1.3770e-08 - acc: 0.8691 - val_loss: 0.3086 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3388 - f_2: 1.3149e-08 - acc: 0.8691 - val_loss: 0.3103 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3393 - f_2: 1.3225e-08 - acc: 0.8691 - val_loss: 0.3078 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3362 - f_2: 1.3285e-08 - acc: 0.8691 - val_loss: 0.3061 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3356 - f_2: 1.3534e-08 - acc: 0.8691 - val_loss: 0.3061 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3384 - f_2: 1.3254e-08 - acc: 0.8691 - val_loss: 0.3056 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3341 - f_2: 1.3142e-08 - acc: 0.8691 - val_loss: 0.3038 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3341 - f_2: 1.3018e-08 - acc: 0.8691 - val_loss: 0.3041 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3347 - f_2: 1.3379e-08 - acc: 0.8691 - val_loss: 0.3051 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3343 - f_2: 0.0747 - acc: 0.8765 - val_loss: 0.3031 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3342 - f_2: 0.2142 - acc: 0.8894 - val_loss: 0.3048 - val_f_2: 0.3121 - val_acc: 0.9109\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3340 - f_2: 0.2432 - acc: 0.8925 - val_loss: 0.3084 - val_f_2: 0.3180 - val_acc: 0.9094\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3315 - f_2: 0.2713 - acc: 0.8950 - val_loss: 0.3030 - val_f_2: 0.3078 - val_acc: 0.9109\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.2901 - acc: 0.8964 - val_loss: 0.3083 - val_f_2: 0.3144 - val_acc: 0.9106\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3308 - f_2: 0.2873 - acc: 0.8970 - val_loss: 0.3023 - val_f_2: 0.3149 - val_acc: 0.9112\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3312 - f_2: 0.2884 - acc: 0.8967 - val_loss: 0.3017 - val_f_2: 0.3099 - val_acc: 0.9109\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3342 - f_2: 0.2815 - acc: 0.8963 - val_loss: 0.3017 - val_f_2: 0.3160 - val_acc: 0.9097\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3317 - f_2: 0.2967 - acc: 0.8971 - val_loss: 0.3031 - val_f_2: 0.3187 - val_acc: 0.9094\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3316 - f_2: 0.2944 - acc: 0.8976 - val_loss: 0.3042 - val_f_2: 0.3183 - val_acc: 0.9100\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.3022 - acc: 0.8981 - val_loss: 0.3098 - val_f_2: 0.3186 - val_acc: 0.9106\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3320 - f_2: 0.3022 - acc: 0.8961 - val_loss: 0.3007 - val_f_2: 0.3121 - val_acc: 0.9109\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3304 - f_2: 0.3087 - acc: 0.8960 - val_loss: 0.2997 - val_f_2: 0.3121 - val_acc: 0.9109\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.2948 - acc: 0.8978 - val_loss: 0.2997 - val_f_2: 0.3188 - val_acc: 0.9109\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.3025 - acc: 0.8987 - val_loss: 0.2992 - val_f_2: 0.3188 - val_acc: 0.9109\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3273 - f_2: 0.3141 - acc: 0.8978 - val_loss: 0.3010 - val_f_2: 0.3095 - val_acc: 0.9103\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3307 - f_2: 0.3063 - acc: 0.8973 - val_loss: 0.2997 - val_f_2: 0.3095 - val_acc: 0.9103\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3301 - f_2: 0.3080 - acc: 0.8970 - val_loss: 0.2977 - val_f_2: 0.3170 - val_acc: 0.9112\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3260 - f_2: 0.3131 - acc: 0.8981 - val_loss: 0.3004 - val_f_2: 0.3225 - val_acc: 0.9083\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3289 - f_2: 0.3093 - acc: 0.8975 - val_loss: 0.2987 - val_f_2: 0.3316 - val_acc: 0.9086\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3294 - f_2: 0.3225 - acc: 0.8993 - val_loss: 0.2975 - val_f_2: 0.3215 - val_acc: 0.9112\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3288 - f_2: 0.3081 - acc: 0.8959 - val_loss: 0.2995 - val_f_2: 0.3631 - val_acc: 0.9080\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3299 - f_2: 0.3222 - acc: 0.8990 - val_loss: 0.2969 - val_f_2: 0.3144 - val_acc: 0.9106\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3265 - f_2: 0.3255 - acc: 0.8987 - val_loss: 0.2977 - val_f_2: 0.3201 - val_acc: 0.9091\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.01, total=  33.1s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4893 - f_2: 0.0203 - acc: 0.8517 - val_loss: 0.3851 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3842 - f_2: 1.3986e-08 - acc: 0.8753 - val_loss: 0.3565 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3574 - f_2: 1.4018e-08 - acc: 0.8753 - val_loss: 0.3381 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3446 - f_2: 1.4565e-08 - acc: 0.8753 - val_loss: 0.3319 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3395 - f_2: 1.4137e-08 - acc: 0.8753 - val_loss: 0.3293 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3347 - f_2: 1.3728e-08 - acc: 0.8753 - val_loss: 0.3254 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3299 - f_2: 1.4027e-08 - acc: 0.8753 - val_loss: 0.3225 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3248 - f_2: 1.3702e-08 - acc: 0.8753 - val_loss: 0.3208 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3266 - f_2: 1.3784e-08 - acc: 0.8753 - val_loss: 0.3192 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3230 - f_2: 1.4201e-08 - acc: 0.8753 - val_loss: 0.3186 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3237 - f_2: 1.3985e-08 - acc: 0.8753 - val_loss: 0.3174 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3161 - f_2: 0.0930 - acc: 0.8838 - val_loss: 0.3163 - val_f_2: 0.3257 - val_acc: 0.8994\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3190 - f_2: 0.2779 - acc: 0.8992 - val_loss: 0.3157 - val_f_2: 0.3278 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2728 - acc: 0.9002 - val_loss: 0.3160 - val_f_2: 0.3382 - val_acc: 0.8979\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.2823 - acc: 0.9007 - val_loss: 0.3161 - val_f_2: 0.3548 - val_acc: 0.8982\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.2984 - acc: 0.9007 - val_loss: 0.3139 - val_f_2: 0.3404 - val_acc: 0.8985\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3103 - f_2: 0.3047 - acc: 0.9006 - val_loss: 0.3126 - val_f_2: 0.3611 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3090 - f_2: 0.3188 - acc: 0.9034 - val_loss: 0.3122 - val_f_2: 0.3428 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3089 - acc: 0.9003 - val_loss: 0.3126 - val_f_2: 0.3835 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3386 - acc: 0.9040 - val_loss: 0.3141 - val_f_2: 0.3331 - val_acc: 0.9009\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.3254 - acc: 0.9032 - val_loss: 0.3124 - val_f_2: 0.3992 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3126 - acc: 0.9014 - val_loss: 0.3153 - val_f_2: 0.3707 - val_acc: 0.8988\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3087 - f_2: 0.3261 - acc: 0.9032 - val_loss: 0.3139 - val_f_2: 0.3924 - val_acc: 0.8997\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3059 - f_2: 0.3409 - acc: 0.9042 - val_loss: 0.3126 - val_f_2: 0.3748 - val_acc: 0.8994\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3401 - acc: 0.9040 - val_loss: 0.3120 - val_f_2: 0.4126 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3036 - f_2: 0.3429 - acc: 0.9034 - val_loss: 0.3123 - val_f_2: 0.3977 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3460 - acc: 0.9046 - val_loss: 0.3137 - val_f_2: 0.3853 - val_acc: 0.8997\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.3552 - acc: 0.9049 - val_loss: 0.3137 - val_f_2: 0.4057 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3030 - f_2: 0.3617 - acc: 0.9038 - val_loss: 0.3122 - val_f_2: 0.3856 - val_acc: 0.9006\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3015 - f_2: 0.3612 - acc: 0.9054 - val_loss: 0.3121 - val_f_2: 0.4222 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3480 - acc: 0.9034 - val_loss: 0.3104 - val_f_2: 0.4021 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3528 - acc: 0.9051 - val_loss: 0.3111 - val_f_2: 0.4072 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3660 - acc: 0.9039 - val_loss: 0.3130 - val_f_2: 0.3923 - val_acc: 0.9012\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2994 - f_2: 0.3632 - acc: 0.9049 - val_loss: 0.3136 - val_f_2: 0.4136 - val_acc: 0.8985\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3668 - acc: 0.9045 - val_loss: 0.3147 - val_f_2: 0.3996 - val_acc: 0.8994\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2987 - f_2: 0.3779 - acc: 0.9065 - val_loss: 0.3138 - val_f_2: 0.3997 - val_acc: 0.8997\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.3722 - acc: 0.9062 - val_loss: 0.3124 - val_f_2: 0.4117 - val_acc: 0.8997\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.3765 - acc: 0.9069 - val_loss: 0.3139 - val_f_2: 0.3988 - val_acc: 0.8985\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.3687 - acc: 0.9046 - val_loss: 0.3152 - val_f_2: 0.4034 - val_acc: 0.8997\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.3652 - acc: 0.9046 - val_loss: 0.3132 - val_f_2: 0.4314 - val_acc: 0.9009\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.3863 - acc: 0.9060 - val_loss: 0.3162 - val_f_2: 0.4270 - val_acc: 0.9003\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2951 - f_2: 0.3745 - acc: 0.9057 - val_loss: 0.3160 - val_f_2: 0.4011 - val_acc: 0.8997\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2962 - f_2: 0.3905 - acc: 0.9068 - val_loss: 0.3135 - val_f_2: 0.4054 - val_acc: 0.8988\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2969 - f_2: 0.3881 - acc: 0.9060 - val_loss: 0.3136 - val_f_2: 0.3990 - val_acc: 0.8991\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.3606 - acc: 0.9053 - val_loss: 0.3144 - val_f_2: 0.4033 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.3798 - acc: 0.9058 - val_loss: 0.3139 - val_f_2: 0.4018 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.3783 - acc: 0.9046 - val_loss: 0.3123 - val_f_2: 0.4002 - val_acc: 0.8991\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2982 - f_2: 0.3789 - acc: 0.9062 - val_loss: 0.3148 - val_f_2: 0.3950 - val_acc: 0.8988\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.3725 - acc: 0.9044 - val_loss: 0.3140 - val_f_2: 0.4071 - val_acc: 0.8991\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2959 - f_2: 0.3859 - acc: 0.9046 - val_loss: 0.3143 - val_f_2: 0.4379 - val_acc: 0.8976\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.001, total=  33.8s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4910 - f_2: 0.0258 - acc: 0.8492 - val_loss: 0.3853 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3804 - f_2: 1.3777e-08 - acc: 0.8722 - val_loss: 0.3524 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3557 - f_2: 1.3190e-08 - acc: 0.8722 - val_loss: 0.3378 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3439 - f_2: 1.3502e-08 - acc: 0.8722 - val_loss: 0.3312 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3399 - f_2: 1.3541e-08 - acc: 0.8722 - val_loss: 0.3283 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3357 - f_2: 1.3284e-08 - acc: 0.8722 - val_loss: 0.3261 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3327 - f_2: 1.3464e-08 - acc: 0.8722 - val_loss: 0.3229 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3265 - f_2: 1.3295e-08 - acc: 0.8722 - val_loss: 0.3206 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3247 - f_2: 1.3677e-08 - acc: 0.8722 - val_loss: 0.3204 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3220 - f_2: 1.3583e-08 - acc: 0.8722 - val_loss: 0.3201 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.2363 - acc: 0.8941 - val_loss: 0.3172 - val_f_2: 0.2982 - val_acc: 0.8988\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3185 - f_2: 0.2887 - acc: 0.8987 - val_loss: 0.3161 - val_f_2: 0.3325 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.2999 - acc: 0.8989 - val_loss: 0.3136 - val_f_2: 0.3665 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.3047 - acc: 0.8985 - val_loss: 0.3127 - val_f_2: 0.3759 - val_acc: 0.9009\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.3037 - acc: 0.8988 - val_loss: 0.3121 - val_f_2: 0.3889 - val_acc: 0.9021\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.3185 - acc: 0.8995 - val_loss: 0.3128 - val_f_2: 0.3846 - val_acc: 0.9021\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3238 - acc: 0.9003 - val_loss: 0.3106 - val_f_2: 0.3940 - val_acc: 0.9015\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3142 - f_2: 0.3293 - acc: 0.8998 - val_loss: 0.3102 - val_f_2: 0.4034 - val_acc: 0.9029\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3117 - f_2: 0.3357 - acc: 0.8997 - val_loss: 0.3091 - val_f_2: 0.3787 - val_acc: 0.9015\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3106 - f_2: 0.3455 - acc: 0.9017 - val_loss: 0.3103 - val_f_2: 0.3900 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3526 - acc: 0.9014 - val_loss: 0.3118 - val_f_2: 0.4037 - val_acc: 0.9018\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.3500 - acc: 0.9006 - val_loss: 0.3082 - val_f_2: 0.4127 - val_acc: 0.9032\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.3651 - acc: 0.9020 - val_loss: 0.3084 - val_f_2: 0.4111 - val_acc: 0.9029\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3066 - f_2: 0.3574 - acc: 0.9006 - val_loss: 0.3092 - val_f_2: 0.3908 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3070 - f_2: 0.3439 - acc: 0.9003 - val_loss: 0.3079 - val_f_2: 0.4296 - val_acc: 0.9044\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3044 - f_2: 0.3474 - acc: 0.9008 - val_loss: 0.3117 - val_f_2: 0.4209 - val_acc: 0.9038\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3700 - acc: 0.9014 - val_loss: 0.3090 - val_f_2: 0.4255 - val_acc: 0.9041\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3924 - acc: 0.9042 - val_loss: 0.3150 - val_f_2: 0.3768 - val_acc: 0.9006\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3017 - f_2: 0.3716 - acc: 0.9030 - val_loss: 0.3111 - val_f_2: 0.4226 - val_acc: 0.9027\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3022 - f_2: 0.3811 - acc: 0.9038 - val_loss: 0.3092 - val_f_2: 0.4296 - val_acc: 0.9018\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.3888 - acc: 0.9055 - val_loss: 0.3143 - val_f_2: 0.4316 - val_acc: 0.9027\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.3969 - acc: 0.9047 - val_loss: 0.3078 - val_f_2: 0.4052 - val_acc: 0.9032\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.3772 - acc: 0.9016 - val_loss: 0.3087 - val_f_2: 0.4283 - val_acc: 0.9006\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3891 - acc: 0.9036 - val_loss: 0.3107 - val_f_2: 0.4249 - val_acc: 0.9035\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3023 - f_2: 0.3858 - acc: 0.9034 - val_loss: 0.3087 - val_f_2: 0.4235 - val_acc: 0.9012\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3805 - acc: 0.9033 - val_loss: 0.3095 - val_f_2: 0.4275 - val_acc: 0.9009\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.4018 - acc: 0.9049 - val_loss: 0.3119 - val_f_2: 0.4280 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2972 - f_2: 0.4154 - acc: 0.9047 - val_loss: 0.3097 - val_f_2: 0.4498 - val_acc: 0.8988\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2974 - f_2: 0.4080 - acc: 0.9049 - val_loss: 0.3113 - val_f_2: 0.4228 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2993 - f_2: 0.4053 - acc: 0.9046 - val_loss: 0.3074 - val_f_2: 0.4262 - val_acc: 0.9029\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2967 - f_2: 0.4014 - acc: 0.9048 - val_loss: 0.3093 - val_f_2: 0.4252 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.4032 - acc: 0.9053 - val_loss: 0.3093 - val_f_2: 0.4312 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2979 - f_2: 0.4036 - acc: 0.9040 - val_loss: 0.3142 - val_f_2: 0.4325 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2945 - f_2: 0.4170 - acc: 0.9067 - val_loss: 0.3090 - val_f_2: 0.4247 - val_acc: 0.9009\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.4109 - acc: 0.9040 - val_loss: 0.3114 - val_f_2: 0.4249 - val_acc: 0.9009\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2964 - f_2: 0.4052 - acc: 0.9035 - val_loss: 0.3099 - val_f_2: 0.4224 - val_acc: 0.9029\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2947 - f_2: 0.4119 - acc: 0.9053 - val_loss: 0.3123 - val_f_2: 0.4187 - val_acc: 0.9029\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.4091 - acc: 0.9046 - val_loss: 0.3120 - val_f_2: 0.4324 - val_acc: 0.9003\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2968 - f_2: 0.4080 - acc: 0.9043 - val_loss: 0.3115 - val_f_2: 0.4439 - val_acc: 0.8994\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.4155 - acc: 0.9053 - val_loss: 0.3143 - val_f_2: 0.4256 - val_acc: 0.9024\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.001, total=  33.7s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=15, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4948 - f_2: 0.0263 - acc: 0.8458 - val_loss: 0.3624 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3895 - f_2: 1.3339e-08 - acc: 0.8691 - val_loss: 0.3335 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3618 - f_2: 1.3687e-08 - acc: 0.8691 - val_loss: 0.3216 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3529 - f_2: 1.3238e-08 - acc: 0.8691 - val_loss: 0.3152 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3471 - f_2: 1.3023e-08 - acc: 0.8691 - val_loss: 0.3116 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3423 - f_2: 1.3387e-08 - acc: 0.8691 - val_loss: 0.3095 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3362 - f_2: 1.3241e-08 - acc: 0.8691 - val_loss: 0.3089 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3347 - f_2: 1.3209e-08 - acc: 0.8691 - val_loss: 0.3052 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3330 - f_2: 1.3663e-08 - acc: 0.8691 - val_loss: 0.3049 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3279 - f_2: 1.3268e-08 - acc: 0.8691 - val_loss: 0.3038 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.1267 - acc: 0.8814 - val_loss: 0.3006 - val_f_2: 0.2096 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3224 - f_2: 0.2375 - acc: 0.8909 - val_loss: 0.2991 - val_f_2: 0.3240 - val_acc: 0.9097\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3247 - f_2: 0.2200 - acc: 0.8894 - val_loss: 0.2982 - val_f_2: 0.3391 - val_acc: 0.9106\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.2475 - acc: 0.8905 - val_loss: 0.2990 - val_f_2: 0.3570 - val_acc: 0.9077\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3229 - f_2: 0.2497 - acc: 0.8900 - val_loss: 0.2958 - val_f_2: 0.3160 - val_acc: 0.9103\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.2528 - acc: 0.8921 - val_loss: 0.2974 - val_f_2: 0.3456 - val_acc: 0.9065\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2504 - acc: 0.8903 - val_loss: 0.2950 - val_f_2: 0.3388 - val_acc: 0.9077\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.2901 - acc: 0.8951 - val_loss: 0.2975 - val_f_2: 0.3989 - val_acc: 0.9091\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3162 - f_2: 0.2895 - acc: 0.8947 - val_loss: 0.2955 - val_f_2: 0.3769 - val_acc: 0.9074\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.3526 - acc: 0.8978 - val_loss: 0.2982 - val_f_2: 0.4243 - val_acc: 0.9086\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.3545 - acc: 0.8984 - val_loss: 0.2945 - val_f_2: 0.4015 - val_acc: 0.9086\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3130 - f_2: 0.3591 - acc: 0.8976 - val_loss: 0.2945 - val_f_2: 0.3718 - val_acc: 0.9074\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.3616 - acc: 0.8981 - val_loss: 0.2953 - val_f_2: 0.3979 - val_acc: 0.9080\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3813 - acc: 0.9006 - val_loss: 0.2954 - val_f_2: 0.4120 - val_acc: 0.9083\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3991 - acc: 0.9014 - val_loss: 0.2955 - val_f_2: 0.3941 - val_acc: 0.9077\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3095 - f_2: 0.3880 - acc: 0.9012 - val_loss: 0.2956 - val_f_2: 0.4163 - val_acc: 0.9086\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3108 - f_2: 0.3844 - acc: 0.9021 - val_loss: 0.2975 - val_f_2: 0.4117 - val_acc: 0.9083\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.3900 - acc: 0.9029 - val_loss: 0.2982 - val_f_2: 0.4360 - val_acc: 0.9062\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3952 - acc: 0.9006 - val_loss: 0.2935 - val_f_2: 0.3949 - val_acc: 0.9086\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3082 - f_2: 0.3992 - acc: 0.9023 - val_loss: 0.2962 - val_f_2: 0.4073 - val_acc: 0.9080\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.4098 - acc: 0.9021 - val_loss: 0.2952 - val_f_2: 0.4057 - val_acc: 0.9086\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3057 - f_2: 0.3987 - acc: 0.9032 - val_loss: 0.2940 - val_f_2: 0.4115 - val_acc: 0.9086\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.4167 - acc: 0.9030 - val_loss: 0.2959 - val_f_2: 0.4262 - val_acc: 0.9056\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.4149 - acc: 0.9028 - val_loss: 0.2956 - val_f_2: 0.4241 - val_acc: 0.9083\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.4127 - acc: 0.9031 - val_loss: 0.2933 - val_f_2: 0.3939 - val_acc: 0.9094\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3045 - f_2: 0.3963 - acc: 0.9018 - val_loss: 0.2966 - val_f_2: 0.4293 - val_acc: 0.9059\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3035 - f_2: 0.4259 - acc: 0.9017 - val_loss: 0.2988 - val_f_2: 0.4180 - val_acc: 0.9077\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3031 - f_2: 0.4242 - acc: 0.9019 - val_loss: 0.2958 - val_f_2: 0.4237 - val_acc: 0.9050\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.4109 - acc: 0.9035 - val_loss: 0.2960 - val_f_2: 0.4183 - val_acc: 0.9053\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3037 - f_2: 0.4114 - acc: 0.9018 - val_loss: 0.2937 - val_f_2: 0.4084 - val_acc: 0.9056\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3050 - f_2: 0.4216 - acc: 0.9020 - val_loss: 0.2946 - val_f_2: 0.4044 - val_acc: 0.9077\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3016 - f_2: 0.4260 - acc: 0.9033 - val_loss: 0.2933 - val_f_2: 0.4115 - val_acc: 0.9062\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3046 - f_2: 0.4044 - acc: 0.9009 - val_loss: 0.2960 - val_f_2: 0.4084 - val_acc: 0.9074\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3048 - f_2: 0.4067 - acc: 0.9023 - val_loss: 0.2957 - val_f_2: 0.4137 - val_acc: 0.9053\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3060 - f_2: 0.4216 - acc: 0.9028 - val_loss: 0.2944 - val_f_2: 0.3980 - val_acc: 0.9083\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.4293 - acc: 0.9043 - val_loss: 0.2956 - val_f_2: 0.4240 - val_acc: 0.9050\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3001 - f_2: 0.4351 - acc: 0.9043 - val_loss: 0.2952 - val_f_2: 0.4203 - val_acc: 0.9047\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.4404 - acc: 0.9035 - val_loss: 0.2952 - val_f_2: 0.4199 - val_acc: 0.9032\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3024 - f_2: 0.4215 - acc: 0.9032 - val_loss: 0.2979 - val_f_2: 0.4322 - val_acc: 0.9024\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.4279 - acc: 0.9043 - val_loss: 0.2947 - val_f_2: 0.4315 - val_acc: 0.9044\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=15, penalty=0.001, total=  33.6s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.8011 - f_2: 0.0169 - acc: 0.8551 - val_loss: 0.5120 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4376 - f_2: 1.3703e-08 - acc: 0.8753 - val_loss: 0.3862 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3796 - f_2: 1.3857e-08 - acc: 0.8753 - val_loss: 0.3639 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3623 - f_2: 1.4033e-08 - acc: 0.8753 - val_loss: 0.3513 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3506 - f_2: 1.3788e-08 - acc: 0.8753 - val_loss: 0.3433 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3442 - f_2: 1.3756e-08 - acc: 0.8753 - val_loss: 0.3403 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3424 - f_2: 1.3864e-08 - acc: 0.8753 - val_loss: 0.3362 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3377 - f_2: 1.3840e-08 - acc: 0.8753 - val_loss: 0.3364 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3338 - f_2: 0.0027 - acc: 0.8753 - val_loss: 0.3332 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3334 - f_2: 1.4600e-08 - acc: 0.8753 - val_loss: 0.3298 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3323 - f_2: 1.3913e-08 - acc: 0.8753 - val_loss: 0.3300 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3312 - f_2: 1.4541e-08 - acc: 0.8753 - val_loss: 0.3271 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3300 - f_2: 0.0504 - acc: 0.8806 - val_loss: 0.3261 - val_f_2: 0.2778 - val_acc: 0.8991\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.1959 - acc: 0.8941 - val_loss: 0.3245 - val_f_2: 0.2918 - val_acc: 0.8994\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3276 - f_2: 0.2211 - acc: 0.8965 - val_loss: 0.3239 - val_f_2: 0.3017 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2298 - acc: 0.8978 - val_loss: 0.3224 - val_f_2: 0.2885 - val_acc: 0.8991\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3249 - f_2: 0.2397 - acc: 0.8971 - val_loss: 0.3234 - val_f_2: 0.2918 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3242 - f_2: 0.2504 - acc: 0.8992 - val_loss: 0.3221 - val_f_2: 0.2920 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3262 - f_2: 0.2479 - acc: 0.8988 - val_loss: 0.3216 - val_f_2: 0.2855 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3228 - f_2: 0.2493 - acc: 0.8979 - val_loss: 0.3206 - val_f_2: 0.3356 - val_acc: 0.9003\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.2627 - acc: 0.8991 - val_loss: 0.3207 - val_f_2: 0.3000 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3235 - f_2: 0.2664 - acc: 0.8981 - val_loss: 0.3200 - val_f_2: 0.3000 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3196 - f_2: 0.2730 - acc: 0.9001 - val_loss: 0.3191 - val_f_2: 0.3048 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2778 - acc: 0.9007 - val_loss: 0.3178 - val_f_2: 0.3086 - val_acc: 0.9009\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.2842 - acc: 0.9006 - val_loss: 0.3178 - val_f_2: 0.3000 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.2785 - acc: 0.9002 - val_loss: 0.3168 - val_f_2: 0.3391 - val_acc: 0.9003\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.2917 - acc: 0.9016 - val_loss: 0.3162 - val_f_2: 0.3122 - val_acc: 0.9009\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3158 - f_2: 0.2884 - acc: 0.9020 - val_loss: 0.3185 - val_f_2: 0.3066 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.3044 - acc: 0.9014 - val_loss: 0.3168 - val_f_2: 0.3545 - val_acc: 0.8997\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3156 - f_2: 0.3007 - acc: 0.9012 - val_loss: 0.3151 - val_f_2: 0.3160 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3186 - f_2: 0.2875 - acc: 0.8997 - val_loss: 0.3164 - val_f_2: 0.2914 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3145 - f_2: 0.2993 - acc: 0.9031 - val_loss: 0.3145 - val_f_2: 0.3599 - val_acc: 0.9012\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3022 - acc: 0.9014 - val_loss: 0.3154 - val_f_2: 0.3066 - val_acc: 0.9009\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3082 - acc: 0.9016 - val_loss: 0.3139 - val_f_2: 0.3125 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.3064 - acc: 0.9023 - val_loss: 0.3143 - val_f_2: 0.3235 - val_acc: 0.8997\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3241 - acc: 0.9026 - val_loss: 0.3177 - val_f_2: 0.3196 - val_acc: 0.9009\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3163 - f_2: 0.3124 - acc: 0.9013 - val_loss: 0.3123 - val_f_2: 0.3165 - val_acc: 0.9018\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3128 - f_2: 0.3187 - acc: 0.9023 - val_loss: 0.3138 - val_f_2: 0.3358 - val_acc: 0.9003\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3150 - f_2: 0.3087 - acc: 0.9023 - val_loss: 0.3135 - val_f_2: 0.3373 - val_acc: 0.9003\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.3263 - acc: 0.9028 - val_loss: 0.3127 - val_f_2: 0.3247 - val_acc: 0.8997\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3213 - acc: 0.9023 - val_loss: 0.3130 - val_f_2: 0.3347 - val_acc: 0.9003\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.3091 - acc: 0.9020 - val_loss: 0.3146 - val_f_2: 0.3209 - val_acc: 0.9009\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3134 - acc: 0.9017 - val_loss: 0.3176 - val_f_2: 0.3173 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3185 - acc: 0.9029 - val_loss: 0.3124 - val_f_2: 0.3951 - val_acc: 0.9006\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3109 - f_2: 0.3238 - acc: 0.9032 - val_loss: 0.3136 - val_f_2: 0.4140 - val_acc: 0.9006\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.3371 - acc: 0.9037 - val_loss: 0.3135 - val_f_2: 0.3445 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3114 - f_2: 0.3149 - acc: 0.9017 - val_loss: 0.3118 - val_f_2: 0.3550 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3057 - f_2: 0.3338 - acc: 0.9040 - val_loss: 0.3117 - val_f_2: 0.3786 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3107 - f_2: 0.3329 - acc: 0.9048 - val_loss: 0.3115 - val_f_2: 0.3834 - val_acc: 0.8997\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3079 - f_2: 0.3320 - acc: 0.9043 - val_loss: 0.3174 - val_f_2: 0.4320 - val_acc: 0.9003\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.01, total=  34.2s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.8020 - f_2: 0.0179 - acc: 0.8537 - val_loss: 0.5133 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4414 - f_2: 1.3544e-08 - acc: 0.8722 - val_loss: 0.3864 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3778 - f_2: 1.3705e-08 - acc: 0.8722 - val_loss: 0.3631 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3623 - f_2: 1.3544e-08 - acc: 0.8722 - val_loss: 0.3512 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3536 - f_2: 1.4147e-08 - acc: 0.8722 - val_loss: 0.3447 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3482 - f_2: 1.3409e-08 - acc: 0.8722 - val_loss: 0.3413 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3428 - f_2: 1.3406e-08 - acc: 0.8722 - val_loss: 0.3362 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3391 - f_2: 1.3645e-08 - acc: 0.8722 - val_loss: 0.3342 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3373 - f_2: 1.3458e-08 - acc: 0.8722 - val_loss: 0.3330 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3368 - f_2: 1.3497e-08 - acc: 0.8722 - val_loss: 0.3313 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3323 - f_2: 1.3344e-08 - acc: 0.8722 - val_loss: 0.3280 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3310 - f_2: 0.1142 - acc: 0.8837 - val_loss: 0.3271 - val_f_2: 0.2758 - val_acc: 0.8988\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3295 - f_2: 0.1988 - acc: 0.8925 - val_loss: 0.3297 - val_f_2: 0.2899 - val_acc: 0.8988\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3313 - f_2: 0.2296 - acc: 0.8953 - val_loss: 0.3242 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3290 - f_2: 0.2370 - acc: 0.8958 - val_loss: 0.3228 - val_f_2: 0.2897 - val_acc: 0.8985\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3272 - f_2: 0.2441 - acc: 0.8956 - val_loss: 0.3223 - val_f_2: 0.2835 - val_acc: 0.8997\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3285 - f_2: 0.2496 - acc: 0.8969 - val_loss: 0.3248 - val_f_2: 0.2814 - val_acc: 0.8994\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3253 - f_2: 0.2712 - acc: 0.8975 - val_loss: 0.3206 - val_f_2: 0.2871 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.2682 - acc: 0.8984 - val_loss: 0.3198 - val_f_2: 0.2882 - val_acc: 0.8988\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2773 - acc: 0.8991 - val_loss: 0.3196 - val_f_2: 0.3419 - val_acc: 0.9009\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3236 - f_2: 0.2721 - acc: 0.8984 - val_loss: 0.3185 - val_f_2: 0.3067 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3234 - f_2: 0.2907 - acc: 0.8995 - val_loss: 0.3175 - val_f_2: 0.2967 - val_acc: 0.9003\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.2832 - acc: 0.8981 - val_loss: 0.3175 - val_f_2: 0.3579 - val_acc: 0.9012\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3224 - f_2: 0.2958 - acc: 0.8993 - val_loss: 0.3167 - val_f_2: 0.3479 - val_acc: 0.9015\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.3125 - acc: 0.8996 - val_loss: 0.3159 - val_f_2: 0.3437 - val_acc: 0.9021\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3232 - f_2: 0.2981 - acc: 0.8995 - val_loss: 0.3176 - val_f_2: 0.3130 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3210 - f_2: 0.3112 - acc: 0.9008 - val_loss: 0.3183 - val_f_2: 0.2979 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3201 - f_2: 0.3128 - acc: 0.9012 - val_loss: 0.3158 - val_f_2: 0.3903 - val_acc: 0.9018\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.3167 - acc: 0.9003 - val_loss: 0.3154 - val_f_2: 0.3565 - val_acc: 0.9015\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3182 - f_2: 0.3128 - acc: 0.9005 - val_loss: 0.3133 - val_f_2: 0.3108 - val_acc: 0.9003\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.2993 - acc: 0.8998 - val_loss: 0.3134 - val_f_2: 0.3113 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.3206 - acc: 0.9018 - val_loss: 0.3141 - val_f_2: 0.3555 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3165 - f_2: 0.3114 - acc: 0.9001 - val_loss: 0.3140 - val_f_2: 0.3731 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3190 - f_2: 0.3114 - acc: 0.8991 - val_loss: 0.3127 - val_f_2: 0.2947 - val_acc: 0.9006\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.3248 - acc: 0.9019 - val_loss: 0.3118 - val_f_2: 0.3610 - val_acc: 0.9003\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3176 - f_2: 0.3192 - acc: 0.9009 - val_loss: 0.3116 - val_f_2: 0.3460 - val_acc: 0.9012\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3139 - f_2: 0.3259 - acc: 0.9018 - val_loss: 0.3139 - val_f_2: 0.2979 - val_acc: 0.9006\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3125 - f_2: 0.3270 - acc: 0.9018 - val_loss: 0.3127 - val_f_2: 0.3591 - val_acc: 0.9021\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3153 - f_2: 0.3279 - acc: 0.9006 - val_loss: 0.3154 - val_f_2: 0.3668 - val_acc: 0.9006\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3154 - f_2: 0.3406 - acc: 0.9023 - val_loss: 0.3101 - val_f_2: 0.3488 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.3209 - acc: 0.9001 - val_loss: 0.3101 - val_f_2: 0.3616 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3126 - f_2: 0.3353 - acc: 0.9026 - val_loss: 0.3135 - val_f_2: 0.3384 - val_acc: 0.9015\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3140 - f_2: 0.3203 - acc: 0.9006 - val_loss: 0.3135 - val_f_2: 0.3039 - val_acc: 0.9006\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3131 - f_2: 0.3353 - acc: 0.9018 - val_loss: 0.3110 - val_f_2: 0.4157 - val_acc: 0.9018\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3132 - f_2: 0.3410 - acc: 0.9026 - val_loss: 0.3094 - val_f_2: 0.3534 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3119 - f_2: 0.3269 - acc: 0.9021 - val_loss: 0.3120 - val_f_2: 0.4062 - val_acc: 0.9009\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3144 - f_2: 0.3592 - acc: 0.9026 - val_loss: 0.3138 - val_f_2: 0.3061 - val_acc: 0.9012\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3127 - f_2: 0.3475 - acc: 0.9045 - val_loss: 0.3118 - val_f_2: 0.3380 - val_acc: 0.9006\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3479 - acc: 0.9020 - val_loss: 0.3088 - val_f_2: 0.3810 - val_acc: 0.9024\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3077 - f_2: 0.3525 - acc: 0.9041 - val_loss: 0.3103 - val_f_2: 0.3851 - val_acc: 0.9024\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.01, total=  33.6s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.01 ..............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.8142 - f_2: 0.0195 - acc: 0.8496 - val_loss: 0.4914 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4479 - f_2: 1.3550e-08 - acc: 0.8691 - val_loss: 0.3653 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3856 - f_2: 1.3224e-08 - acc: 0.8691 - val_loss: 0.3438 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3702 - f_2: 1.3145e-08 - acc: 0.8691 - val_loss: 0.3333 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3583 - f_2: 1.3269e-08 - acc: 0.8691 - val_loss: 0.3289 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3536 - f_2: 1.2887e-08 - acc: 0.8691 - val_loss: 0.3261 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3491 - f_2: 1.3148e-08 - acc: 0.8691 - val_loss: 0.3187 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3452 - f_2: 1.3479e-08 - acc: 0.8691 - val_loss: 0.3219 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3436 - f_2: 1.3360e-08 - acc: 0.8691 - val_loss: 0.3142 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3396 - f_2: 1.3116e-08 - acc: 0.8691 - val_loss: 0.3126 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3391 - f_2: 1.3299e-08 - acc: 0.8691 - val_loss: 0.3104 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3396 - f_2: 0.0820 - acc: 0.8770 - val_loss: 0.3094 - val_f_2: 0.2694 - val_acc: 0.9071\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3332 - f_2: 0.2081 - acc: 0.8902 - val_loss: 0.3102 - val_f_2: 0.3104 - val_acc: 0.9109\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3344 - f_2: 0.2171 - acc: 0.8908 - val_loss: 0.3081 - val_f_2: 0.3151 - val_acc: 0.9115\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3343 - f_2: 0.2451 - acc: 0.8937 - val_loss: 0.3056 - val_f_2: 0.3054 - val_acc: 0.9112\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3342 - f_2: 0.2418 - acc: 0.8936 - val_loss: 0.3052 - val_f_2: 0.3054 - val_acc: 0.9109\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3297 - f_2: 0.2522 - acc: 0.8950 - val_loss: 0.3029 - val_f_2: 0.3054 - val_acc: 0.9109\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3324 - f_2: 0.2552 - acc: 0.8932 - val_loss: 0.3031 - val_f_2: 0.3125 - val_acc: 0.9109\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3311 - f_2: 0.2776 - acc: 0.8956 - val_loss: 0.3038 - val_f_2: 0.3030 - val_acc: 0.9109\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3291 - f_2: 0.2697 - acc: 0.8949 - val_loss: 0.3038 - val_f_2: 0.3600 - val_acc: 0.9080\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3298 - f_2: 0.2864 - acc: 0.8962 - val_loss: 0.3040 - val_f_2: 0.3448 - val_acc: 0.9071\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3304 - f_2: 0.2817 - acc: 0.8945 - val_loss: 0.3004 - val_f_2: 0.3190 - val_acc: 0.9112\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3282 - f_2: 0.2933 - acc: 0.8959 - val_loss: 0.2999 - val_f_2: 0.3185 - val_acc: 0.9103\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3262 - f_2: 0.2934 - acc: 0.8978 - val_loss: 0.2994 - val_f_2: 0.3234 - val_acc: 0.9106\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3254 - f_2: 0.2939 - acc: 0.8963 - val_loss: 0.2984 - val_f_2: 0.3099 - val_acc: 0.9109\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3271 - f_2: 0.2930 - acc: 0.8964 - val_loss: 0.3005 - val_f_2: 0.3640 - val_acc: 0.9062\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3262 - f_2: 0.3025 - acc: 0.8960 - val_loss: 0.3002 - val_f_2: 0.3301 - val_acc: 0.9088\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.3008 - acc: 0.8970 - val_loss: 0.2989 - val_f_2: 0.3674 - val_acc: 0.9074\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3226 - f_2: 0.3034 - acc: 0.8970 - val_loss: 0.2985 - val_f_2: 0.3720 - val_acc: 0.9074\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3256 - f_2: 0.3191 - acc: 0.8970 - val_loss: 0.2959 - val_f_2: 0.3121 - val_acc: 0.9109\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3220 - f_2: 0.3125 - acc: 0.8978 - val_loss: 0.2995 - val_f_2: 0.3813 - val_acc: 0.9080\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3233 - f_2: 0.3064 - acc: 0.8968 - val_loss: 0.2964 - val_f_2: 0.3543 - val_acc: 0.9074\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3203 - f_2: 0.3187 - acc: 0.8967 - val_loss: 0.2947 - val_f_2: 0.3188 - val_acc: 0.9109\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3209 - f_2: 0.3138 - acc: 0.8971 - val_loss: 0.2965 - val_f_2: 0.3540 - val_acc: 0.9077\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3211 - f_2: 0.3168 - acc: 0.8980 - val_loss: 0.2941 - val_f_2: 0.3142 - val_acc: 0.9103\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3202 - f_2: 0.3399 - acc: 0.8996 - val_loss: 0.2979 - val_f_2: 0.3555 - val_acc: 0.9071\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3200 - f_2: 0.3209 - acc: 0.8986 - val_loss: 0.2934 - val_f_2: 0.3263 - val_acc: 0.9118\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3204 - f_2: 0.3229 - acc: 0.8962 - val_loss: 0.2933 - val_f_2: 0.3303 - val_acc: 0.9109\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.3237 - acc: 0.8980 - val_loss: 0.3036 - val_f_2: 0.4029 - val_acc: 0.9071\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3380 - acc: 0.8992 - val_loss: 0.2964 - val_f_2: 0.3484 - val_acc: 0.9094\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3175 - f_2: 0.3308 - acc: 0.8967 - val_loss: 0.2923 - val_f_2: 0.3119 - val_acc: 0.9106\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3174 - f_2: 0.3285 - acc: 0.8984 - val_loss: 0.2935 - val_f_2: 0.3712 - val_acc: 0.9068\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.3328 - acc: 0.8990 - val_loss: 0.2942 - val_f_2: 0.3510 - val_acc: 0.9083\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3178 - f_2: 0.3372 - acc: 0.8993 - val_loss: 0.2955 - val_f_2: 0.3930 - val_acc: 0.9068\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3184 - f_2: 0.3472 - acc: 0.8980 - val_loss: 0.2917 - val_f_2: 0.3321 - val_acc: 0.9118\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3181 - f_2: 0.3187 - acc: 0.8971 - val_loss: 0.2948 - val_f_2: 0.4020 - val_acc: 0.9080\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3449 - acc: 0.8993 - val_loss: 0.2913 - val_f_2: 0.3506 - val_acc: 0.9094\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3188 - f_2: 0.3475 - acc: 0.8993 - val_loss: 0.2910 - val_f_2: 0.3393 - val_acc: 0.9100\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3449 - acc: 0.8984 - val_loss: 0.2911 - val_f_2: 0.3397 - val_acc: 0.9106\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3143 - f_2: 0.3271 - acc: 0.8984 - val_loss: 0.2922 - val_f_2: 0.3783 - val_acc: 0.9091\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.01, total=  33.3s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4787 - f_2: 0.0224 - acc: 0.8610 - val_loss: 0.3893 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3822 - f_2: 1.3811e-08 - acc: 0.8753 - val_loss: 0.3603 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3617 - f_2: 1.4022e-08 - acc: 0.8753 - val_loss: 0.3464 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3487 - f_2: 0.0027 - acc: 0.8753 - val_loss: 0.3392 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3398 - f_2: 1.3777e-08 - acc: 0.8753 - val_loss: 0.3329 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3370 - f_2: 1.3808e-08 - acc: 0.8753 - val_loss: 0.3309 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3287 - f_2: 1.3490e-08 - acc: 0.8753 - val_loss: 0.3272 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3259 - f_2: 1.3893e-08 - acc: 0.8753 - val_loss: 0.3243 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3240 - f_2: 0.1139 - acc: 0.8866 - val_loss: 0.3225 - val_f_2: 0.2747 - val_acc: 0.8985\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3194 - f_2: 0.2564 - acc: 0.9009 - val_loss: 0.3195 - val_f_2: 0.3231 - val_acc: 0.9006\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3214 - f_2: 0.2795 - acc: 0.9018 - val_loss: 0.3201 - val_f_2: 0.3250 - val_acc: 0.9003\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3147 - f_2: 0.2818 - acc: 0.9023 - val_loss: 0.3176 - val_f_2: 0.3273 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3148 - f_2: 0.2913 - acc: 0.9017 - val_loss: 0.3180 - val_f_2: 0.3223 - val_acc: 0.9012\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3141 - f_2: 0.2891 - acc: 0.9020 - val_loss: 0.3197 - val_f_2: 0.3224 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3121 - f_2: 0.3199 - acc: 0.9034 - val_loss: 0.3159 - val_f_2: 0.3493 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3099 - f_2: 0.3092 - acc: 0.9026 - val_loss: 0.3150 - val_f_2: 0.3648 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3089 - f_2: 0.3346 - acc: 0.9048 - val_loss: 0.3145 - val_f_2: 0.3625 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3063 - f_2: 0.3260 - acc: 0.9037 - val_loss: 0.3148 - val_f_2: 0.3619 - val_acc: 0.9009\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3083 - f_2: 0.3314 - acc: 0.9040 - val_loss: 0.3163 - val_f_2: 0.3594 - val_acc: 0.9006\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3067 - f_2: 0.3348 - acc: 0.9038 - val_loss: 0.3144 - val_f_2: 0.3725 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3496 - acc: 0.9045 - val_loss: 0.3141 - val_f_2: 0.3858 - val_acc: 0.9021\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3084 - f_2: 0.3460 - acc: 0.9054 - val_loss: 0.3156 - val_f_2: 0.3692 - val_acc: 0.9009\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3415 - acc: 0.9034 - val_loss: 0.3147 - val_f_2: 0.3848 - val_acc: 0.9015\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3647 - acc: 0.9060 - val_loss: 0.3160 - val_f_2: 0.3703 - val_acc: 0.9015\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3026 - f_2: 0.3539 - acc: 0.9055 - val_loss: 0.3172 - val_f_2: 0.3846 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3684 - acc: 0.9060 - val_loss: 0.3205 - val_f_2: 0.3822 - val_acc: 0.9009\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3002 - f_2: 0.3562 - acc: 0.9037 - val_loss: 0.3144 - val_f_2: 0.4044 - val_acc: 0.8991\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.3628 - acc: 0.9051 - val_loss: 0.3187 - val_f_2: 0.3927 - val_acc: 0.8994\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.3712 - acc: 0.9057 - val_loss: 0.3163 - val_f_2: 0.3945 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3021 - f_2: 0.3822 - acc: 0.9056 - val_loss: 0.3151 - val_f_2: 0.4004 - val_acc: 0.8994\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.3778 - acc: 0.9060 - val_loss: 0.3161 - val_f_2: 0.3955 - val_acc: 0.9006\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.3697 - acc: 0.9062 - val_loss: 0.3163 - val_f_2: 0.3909 - val_acc: 0.9003\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3666 - acc: 0.9042 - val_loss: 0.3153 - val_f_2: 0.3946 - val_acc: 0.9015\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.3739 - acc: 0.9057 - val_loss: 0.3199 - val_f_2: 0.3772 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2983 - f_2: 0.3627 - acc: 0.9054 - val_loss: 0.3172 - val_f_2: 0.3890 - val_acc: 0.9009\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2968 - f_2: 0.3789 - acc: 0.9068 - val_loss: 0.3260 - val_f_2: 0.3752 - val_acc: 0.8994\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3007 - f_2: 0.3679 - acc: 0.9050 - val_loss: 0.3169 - val_f_2: 0.4088 - val_acc: 0.8979\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.3785 - acc: 0.9057 - val_loss: 0.3275 - val_f_2: 0.3840 - val_acc: 0.8988\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2939 - f_2: 0.3919 - acc: 0.9066 - val_loss: 0.3195 - val_f_2: 0.3944 - val_acc: 0.8997\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2968 - f_2: 0.3767 - acc: 0.9049 - val_loss: 0.3157 - val_f_2: 0.4231 - val_acc: 0.8994\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.3871 - acc: 0.9068 - val_loss: 0.3190 - val_f_2: 0.3974 - val_acc: 0.8994\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2955 - f_2: 0.3842 - acc: 0.9063 - val_loss: 0.3189 - val_f_2: 0.4071 - val_acc: 0.8994\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2976 - f_2: 0.3848 - acc: 0.9058 - val_loss: 0.3159 - val_f_2: 0.4137 - val_acc: 0.8994\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2963 - f_2: 0.3860 - acc: 0.9064 - val_loss: 0.3288 - val_f_2: 0.3783 - val_acc: 0.9012\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2946 - f_2: 0.3888 - acc: 0.9069 - val_loss: 0.3289 - val_f_2: 0.3736 - val_acc: 0.8997\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2983 - f_2: 0.3871 - acc: 0.9068 - val_loss: 0.3167 - val_f_2: 0.4038 - val_acc: 0.8988\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2953 - f_2: 0.4008 - acc: 0.9073 - val_loss: 0.3167 - val_f_2: 0.4098 - val_acc: 0.8982\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2964 - f_2: 0.3893 - acc: 0.9061 - val_loss: 0.3172 - val_f_2: 0.4015 - val_acc: 0.8994\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2914 - f_2: 0.4039 - acc: 0.9076 - val_loss: 0.3221 - val_f_2: 0.3972 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2929 - f_2: 0.4045 - acc: 0.9077 - val_loss: 0.3171 - val_f_2: 0.4166 - val_acc: 0.8985\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.001, total=  33.6s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13557 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4786 - f_2: 0.0125 - acc: 0.8554 - val_loss: 0.3865 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3800 - f_2: 1.3988e-08 - acc: 0.8722 - val_loss: 0.3573 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3604 - f_2: 1.3249e-08 - acc: 0.8722 - val_loss: 0.3452 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3467 - f_2: 1.3876e-08 - acc: 0.8722 - val_loss: 0.3395 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3411 - f_2: 1.3430e-08 - acc: 0.8722 - val_loss: 0.3360 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3378 - f_2: 1.3888e-08 - acc: 0.8722 - val_loss: 0.3303 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3305 - f_2: 1.3713e-08 - acc: 0.8722 - val_loss: 0.3265 - val_f_2: 1.3090e-08 - val_acc: 0.8667\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3296 - f_2: 0.1706 - acc: 0.8897 - val_loss: 0.3242 - val_f_2: 0.2726 - val_acc: 0.8982\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3260 - f_2: 0.2517 - acc: 0.8995 - val_loss: 0.3227 - val_f_2: 0.2889 - val_acc: 0.8997\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3231 - f_2: 0.2816 - acc: 0.9020 - val_loss: 0.3216 - val_f_2: 0.2988 - val_acc: 0.9009\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3223 - f_2: 0.2712 - acc: 0.9005 - val_loss: 0.3178 - val_f_2: 0.3111 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3146 - f_2: 0.3026 - acc: 0.9026 - val_loss: 0.3187 - val_f_2: 0.3197 - val_acc: 0.9003\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3155 - f_2: 0.3087 - acc: 0.9026 - val_loss: 0.3162 - val_f_2: 0.3408 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3142 - f_2: 0.3216 - acc: 0.9031 - val_loss: 0.3165 - val_f_2: 0.3383 - val_acc: 0.9003\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3101 - f_2: 0.3281 - acc: 0.9029 - val_loss: 0.3144 - val_f_2: 0.3579 - val_acc: 0.8985\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3097 - f_2: 0.3526 - acc: 0.9047 - val_loss: 0.3189 - val_f_2: 0.3280 - val_acc: 0.9003\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3105 - f_2: 0.3353 - acc: 0.9023 - val_loss: 0.3142 - val_f_2: 0.3413 - val_acc: 0.8985\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3085 - f_2: 0.3505 - acc: 0.9035 - val_loss: 0.3126 - val_f_2: 0.3562 - val_acc: 0.8994\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3052 - f_2: 0.3659 - acc: 0.9051 - val_loss: 0.3262 - val_f_2: 0.3522 - val_acc: 0.8994\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3049 - f_2: 0.3415 - acc: 0.9033 - val_loss: 0.3136 - val_f_2: 0.3862 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3061 - f_2: 0.3635 - acc: 0.9031 - val_loss: 0.3166 - val_f_2: 0.3454 - val_acc: 0.8994\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3073 - f_2: 0.3494 - acc: 0.9044 - val_loss: 0.3194 - val_f_2: 0.3535 - val_acc: 0.8982\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3041 - f_2: 0.3726 - acc: 0.9051 - val_loss: 0.3215 - val_f_2: 0.3558 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3032 - f_2: 0.3545 - acc: 0.9031 - val_loss: 0.3124 - val_f_2: 0.3880 - val_acc: 0.9021\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3707 - acc: 0.9047 - val_loss: 0.3215 - val_f_2: 0.3996 - val_acc: 0.9029\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3042 - f_2: 0.3805 - acc: 0.9051 - val_loss: 0.3148 - val_f_2: 0.4003 - val_acc: 0.9032\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3028 - f_2: 0.3833 - acc: 0.9047 - val_loss: 0.3133 - val_f_2: 0.3984 - val_acc: 0.9024\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3816 - acc: 0.9046 - val_loss: 0.3122 - val_f_2: 0.4009 - val_acc: 0.9027\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3011 - f_2: 0.3772 - acc: 0.9060 - val_loss: 0.3146 - val_f_2: 0.4099 - val_acc: 0.9038\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.4066 - acc: 0.9071 - val_loss: 0.3129 - val_f_2: 0.4137 - val_acc: 0.9021\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.3913 - acc: 0.9054 - val_loss: 0.3142 - val_f_2: 0.4300 - val_acc: 0.9027\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3027 - f_2: 0.3923 - acc: 0.9061 - val_loss: 0.3129 - val_f_2: 0.4044 - val_acc: 0.9021\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.2999 - f_2: 0.3828 - acc: 0.9054 - val_loss: 0.3171 - val_f_2: 0.4048 - val_acc: 0.9032\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.2990 - f_2: 0.3833 - acc: 0.9054 - val_loss: 0.3198 - val_f_2: 0.4184 - val_acc: 0.9035\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.3951 - acc: 0.9043 - val_loss: 0.3130 - val_f_2: 0.4151 - val_acc: 0.9024\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.4125 - acc: 0.9065 - val_loss: 0.3168 - val_f_2: 0.4128 - val_acc: 0.9024\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.3858 - acc: 0.9048 - val_loss: 0.3218 - val_f_2: 0.3999 - val_acc: 0.9018\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.3931 - acc: 0.9046 - val_loss: 0.3147 - val_f_2: 0.4160 - val_acc: 0.9021\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2986 - f_2: 0.3944 - acc: 0.9055 - val_loss: 0.3131 - val_f_2: 0.4096 - val_acc: 0.9029\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.4009 - acc: 0.9053 - val_loss: 0.3170 - val_f_2: 0.4135 - val_acc: 0.9032\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2948 - f_2: 0.4029 - acc: 0.9053 - val_loss: 0.3160 - val_f_2: 0.4352 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.2957 - f_2: 0.4180 - acc: 0.9066 - val_loss: 0.3119 - val_f_2: 0.4207 - val_acc: 0.9024\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.2966 - f_2: 0.4069 - acc: 0.9054 - val_loss: 0.3157 - val_f_2: 0.4130 - val_acc: 0.9018\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.2950 - f_2: 0.4081 - acc: 0.9067 - val_loss: 0.3155 - val_f_2: 0.4168 - val_acc: 0.9027\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.2963 - f_2: 0.4077 - acc: 0.9060 - val_loss: 0.3146 - val_f_2: 0.4153 - val_acc: 0.9032\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2965 - f_2: 0.4174 - acc: 0.9082 - val_loss: 0.3123 - val_f_2: 0.4119 - val_acc: 0.9029\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.4057 - acc: 0.9048 - val_loss: 0.3160 - val_f_2: 0.4096 - val_acc: 0.9018\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2926 - f_2: 0.4197 - acc: 0.9081 - val_loss: 0.3205 - val_f_2: 0.3985 - val_acc: 0.9024\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2938 - f_2: 0.4096 - acc: 0.9050 - val_loss: 0.3129 - val_f_2: 0.4234 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2970 - f_2: 0.4000 - acc: 0.9043 - val_loss: 0.3132 - val_f_2: 0.4242 - val_acc: 0.9009\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.001, total=  33.5s\n",
      "[CV] batch_size=80, dropout=0.3, nodes=20, penalty=0.001 .............\n",
      "Train on 13558 samples, validate on 3390 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.4800 - f_2: 0.0175 - acc: 0.8535 - val_loss: 0.3650 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.3870 - f_2: 1.3204e-08 - acc: 0.8691 - val_loss: 0.3350 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.3678 - f_2: 1.3230e-08 - acc: 0.8691 - val_loss: 0.3255 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.3574 - f_2: 1.3459e-08 - acc: 0.8691 - val_loss: 0.3178 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3489 - f_2: 1.3199e-08 - acc: 0.8691 - val_loss: 0.3183 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3425 - f_2: 1.3162e-08 - acc: 0.8691 - val_loss: 0.3137 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3359 - f_2: 1.3209e-08 - acc: 0.8691 - val_loss: 0.3085 - val_f_2: 1.5965e-08 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3299 - f_2: 0.1425 - acc: 0.8841 - val_loss: 0.3062 - val_f_2: 0.3239 - val_acc: 0.9100\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3280 - f_2: 0.2619 - acc: 0.8976 - val_loss: 0.3024 - val_f_2: 0.3217 - val_acc: 0.9097\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3277 - f_2: 0.2992 - acc: 0.8991 - val_loss: 0.3011 - val_f_2: 0.3287 - val_acc: 0.9100\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3252 - f_2: 0.3040 - acc: 0.8992 - val_loss: 0.3006 - val_f_2: 0.3343 - val_acc: 0.9100\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3222 - f_2: 0.3219 - acc: 0.9004 - val_loss: 0.2992 - val_f_2: 0.3423 - val_acc: 0.9097\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3197 - f_2: 0.3178 - acc: 0.9007 - val_loss: 0.2979 - val_f_2: 0.3452 - val_acc: 0.9086\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3187 - f_2: 0.3190 - acc: 0.9004 - val_loss: 0.3016 - val_f_2: 0.3365 - val_acc: 0.9086\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3168 - f_2: 0.3264 - acc: 0.9004 - val_loss: 0.2968 - val_f_2: 0.3514 - val_acc: 0.9088\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3179 - f_2: 0.3298 - acc: 0.8999 - val_loss: 0.2967 - val_f_2: 0.3862 - val_acc: 0.9100\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3152 - f_2: 0.3452 - acc: 0.9005 - val_loss: 0.2946 - val_f_2: 0.3351 - val_acc: 0.9086\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3120 - f_2: 0.3282 - acc: 0.8992 - val_loss: 0.2957 - val_f_2: 0.3998 - val_acc: 0.9103\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3113 - f_2: 0.3481 - acc: 0.9011 - val_loss: 0.2950 - val_f_2: 0.4017 - val_acc: 0.9100\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3124 - f_2: 0.3591 - acc: 0.9002 - val_loss: 0.2950 - val_f_2: 0.3711 - val_acc: 0.9088\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3129 - f_2: 0.3681 - acc: 0.9008 - val_loss: 0.2936 - val_f_2: 0.4127 - val_acc: 0.9103\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3102 - f_2: 0.3657 - acc: 0.9005 - val_loss: 0.2948 - val_f_2: 0.4077 - val_acc: 0.9109\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3075 - f_2: 0.3634 - acc: 0.8996 - val_loss: 0.2925 - val_f_2: 0.3906 - val_acc: 0.9094\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3072 - f_2: 0.3763 - acc: 0.9012 - val_loss: 0.2940 - val_f_2: 0.4157 - val_acc: 0.9080\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3054 - f_2: 0.3668 - acc: 0.9010 - val_loss: 0.2925 - val_f_2: 0.3588 - val_acc: 0.9086\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3047 - f_2: 0.3765 - acc: 0.9015 - val_loss: 0.2905 - val_f_2: 0.3701 - val_acc: 0.9094\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3093 - f_2: 0.3673 - acc: 0.9018 - val_loss: 0.2913 - val_f_2: 0.4134 - val_acc: 0.9077\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3043 - f_2: 0.3715 - acc: 0.9005 - val_loss: 0.2920 - val_f_2: 0.4053 - val_acc: 0.9065\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3069 - f_2: 0.3851 - acc: 0.9030 - val_loss: 0.2927 - val_f_2: 0.3698 - val_acc: 0.9103\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3034 - f_2: 0.3812 - acc: 0.9016 - val_loss: 0.2923 - val_f_2: 0.4326 - val_acc: 0.9071\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3012 - f_2: 0.3955 - acc: 0.9031 - val_loss: 0.2914 - val_f_2: 0.3795 - val_acc: 0.9097\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3003 - f_2: 0.3920 - acc: 0.9027 - val_loss: 0.2921 - val_f_2: 0.4253 - val_acc: 0.9071\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3064 - f_2: 0.3858 - acc: 0.9015 - val_loss: 0.2930 - val_f_2: 0.3916 - val_acc: 0.9086\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3008 - f_2: 0.3888 - acc: 0.9025 - val_loss: 0.2909 - val_f_2: 0.3939 - val_acc: 0.9074\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.2985 - f_2: 0.4009 - acc: 0.9019 - val_loss: 0.2894 - val_f_2: 0.3869 - val_acc: 0.9071\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3040 - f_2: 0.3814 - acc: 0.9024 - val_loss: 0.2926 - val_f_2: 0.4210 - val_acc: 0.9038\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.2988 - f_2: 0.3996 - acc: 0.9023 - val_loss: 0.2925 - val_f_2: 0.4355 - val_acc: 0.9050\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3013 - f_2: 0.4210 - acc: 0.9028 - val_loss: 0.2906 - val_f_2: 0.3860 - val_acc: 0.9083\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.2989 - f_2: 0.4118 - acc: 0.9050 - val_loss: 0.2910 - val_f_2: 0.3943 - val_acc: 0.9071\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.2975 - f_2: 0.4137 - acc: 0.9039 - val_loss: 0.2931 - val_f_2: 0.3998 - val_acc: 0.9068\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.2995 - f_2: 0.3935 - acc: 0.9024 - val_loss: 0.2943 - val_f_2: 0.4185 - val_acc: 0.9047\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3004 - f_2: 0.4019 - acc: 0.9017 - val_loss: 0.2914 - val_f_2: 0.4203 - val_acc: 0.9050\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3000 - f_2: 0.4135 - acc: 0.9044 - val_loss: 0.2908 - val_f_2: 0.4055 - val_acc: 0.9053\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3010 - f_2: 0.4154 - acc: 0.9040 - val_loss: 0.2917 - val_f_2: 0.4255 - val_acc: 0.9047\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3006 - f_2: 0.4081 - acc: 0.9021 - val_loss: 0.2920 - val_f_2: 0.4167 - val_acc: 0.9053\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.2981 - f_2: 0.4180 - acc: 0.9025 - val_loss: 0.2932 - val_f_2: 0.3904 - val_acc: 0.9083\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.2984 - f_2: 0.4246 - acc: 0.9049 - val_loss: 0.2955 - val_f_2: 0.3868 - val_acc: 0.9086\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.2942 - f_2: 0.4142 - acc: 0.9042 - val_loss: 0.2924 - val_f_2: 0.4158 - val_acc: 0.9047\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.2980 - f_2: 0.4237 - acc: 0.9039 - val_loss: 0.2972 - val_f_2: 0.4043 - val_acc: 0.9041\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.2973 - f_2: 0.4279 - acc: 0.9042 - val_loss: 0.2920 - val_f_2: 0.4146 - val_acc: 0.9062\n",
      "[CV]  batch_size=80, dropout=0.3, nodes=20, penalty=0.001, total=  34.3s\n",
      "Train on 20336 samples, validate on 5085 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 58.5min finished\n",
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4085 - f_2: 0.0365 - acc: 0.8637 - val_loss: 0.3367 - val_f_2: 0.2408 - val_acc: 0.8962\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3367 - f_2: 0.2154 - acc: 0.8933 - val_loss: 0.3125 - val_f_2: 0.3203 - val_acc: 0.9021\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3219 - f_2: 0.2810 - acc: 0.8967 - val_loss: 0.3037 - val_f_2: 0.3263 - val_acc: 0.9027\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3146 - f_2: 0.2989 - acc: 0.8993 - val_loss: 0.2982 - val_f_2: 0.3394 - val_acc: 0.9021\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3086 - f_2: 0.2972 - acc: 0.8992 - val_loss: 0.2971 - val_f_2: 0.3906 - val_acc: 0.9011\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3071 - f_2: 0.3149 - acc: 0.8998 - val_loss: 0.2953 - val_f_2: 0.3449 - val_acc: 0.9029\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3023 - f_2: 0.3176 - acc: 0.9007 - val_loss: 0.2935 - val_f_2: 0.3522 - val_acc: 0.9032\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.2986 - f_2: 0.3272 - acc: 0.8997 - val_loss: 0.2924 - val_f_2: 0.3835 - val_acc: 0.8999\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.2988 - f_2: 0.3286 - acc: 0.9015 - val_loss: 0.2926 - val_f_2: 0.3808 - val_acc: 0.9019\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.2969 - f_2: 0.3387 - acc: 0.9015 - val_loss: 0.2911 - val_f_2: 0.3675 - val_acc: 0.9013\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.2963 - f_2: 0.3402 - acc: 0.8999 - val_loss: 0.2895 - val_f_2: 0.3688 - val_acc: 0.9005\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.2958 - f_2: 0.3318 - acc: 0.9006 - val_loss: 0.2901 - val_f_2: 0.3789 - val_acc: 0.9009\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.2922 - f_2: 0.3433 - acc: 0.9009 - val_loss: 0.2899 - val_f_2: 0.3718 - val_acc: 0.9005\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.2944 - f_2: 0.3445 - acc: 0.9016 - val_loss: 0.2894 - val_f_2: 0.4086 - val_acc: 0.9009\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.2913 - f_2: 0.3459 - acc: 0.9013 - val_loss: 0.2896 - val_f_2: 0.4325 - val_acc: 0.9015\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.2915 - f_2: 0.3436 - acc: 0.9007 - val_loss: 0.2907 - val_f_2: 0.3927 - val_acc: 0.9036\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.2903 - f_2: 0.3496 - acc: 0.9015 - val_loss: 0.2894 - val_f_2: 0.3913 - val_acc: 0.9029\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.2911 - f_2: 0.3559 - acc: 0.9024 - val_loss: 0.2920 - val_f_2: 0.3407 - val_acc: 0.9019\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.2886 - f_2: 0.3619 - acc: 0.9031 - val_loss: 0.2887 - val_f_2: 0.3972 - val_acc: 0.9025\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.2871 - f_2: 0.3661 - acc: 0.9026 - val_loss: 0.2891 - val_f_2: 0.3974 - val_acc: 0.9025\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.2879 - f_2: 0.3706 - acc: 0.9026 - val_loss: 0.2904 - val_f_2: 0.4171 - val_acc: 0.9019\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.2857 - f_2: 0.3726 - acc: 0.9027 - val_loss: 0.2904 - val_f_2: 0.3912 - val_acc: 0.9023\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.2879 - f_2: 0.3693 - acc: 0.9029 - val_loss: 0.2899 - val_f_2: 0.4004 - val_acc: 0.9027\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.2859 - f_2: 0.3538 - acc: 0.9029 - val_loss: 0.2874 - val_f_2: 0.3582 - val_acc: 0.9019\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.2859 - f_2: 0.3792 - acc: 0.9037 - val_loss: 0.2878 - val_f_2: 0.4340 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.2865 - f_2: 0.3638 - acc: 0.9012 - val_loss: 0.2871 - val_f_2: 0.4019 - val_acc: 0.8997\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2859 - f_2: 0.3863 - acc: 0.9043 - val_loss: 0.2874 - val_f_2: 0.3973 - val_acc: 0.9021\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2871 - f_2: 0.3687 - acc: 0.9018 - val_loss: 0.2875 - val_f_2: 0.4454 - val_acc: 0.9023\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2860 - f_2: 0.3733 - acc: 0.9014 - val_loss: 0.2862 - val_f_2: 0.3938 - val_acc: 0.9021\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2848 - f_2: 0.3726 - acc: 0.9037 - val_loss: 0.2874 - val_f_2: 0.3963 - val_acc: 0.9005\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2838 - f_2: 0.3767 - acc: 0.9030 - val_loss: 0.2887 - val_f_2: 0.3877 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2855 - f_2: 0.3675 - acc: 0.9031 - val_loss: 0.2899 - val_f_2: 0.3809 - val_acc: 0.9015\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2826 - f_2: 0.3842 - acc: 0.9036 - val_loss: 0.2870 - val_f_2: 0.3847 - val_acc: 0.9011\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2848 - f_2: 0.3884 - acc: 0.9039 - val_loss: 0.2879 - val_f_2: 0.4541 - val_acc: 0.9023\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2843 - f_2: 0.3686 - acc: 0.9019 - val_loss: 0.2886 - val_f_2: 0.3595 - val_acc: 0.9015\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2849 - f_2: 0.3666 - acc: 0.9016 - val_loss: 0.2880 - val_f_2: 0.4437 - val_acc: 0.8989\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2846 - f_2: 0.3863 - acc: 0.9029 - val_loss: 0.2871 - val_f_2: 0.4133 - val_acc: 0.9013\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2826 - f_2: 0.3974 - acc: 0.9039 - val_loss: 0.2911 - val_f_2: 0.3987 - val_acc: 0.9023\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2833 - f_2: 0.3801 - acc: 0.9040 - val_loss: 0.2883 - val_f_2: 0.4455 - val_acc: 0.9025\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2823 - f_2: 0.3812 - acc: 0.9041 - val_loss: 0.2874 - val_f_2: 0.4404 - val_acc: 0.9021\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2805 - f_2: 0.4000 - acc: 0.9045 - val_loss: 0.2918 - val_f_2: 0.3986 - val_acc: 0.9009\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2809 - f_2: 0.3964 - acc: 0.9053 - val_loss: 0.2873 - val_f_2: 0.3893 - val_acc: 0.9027\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2837 - f_2: 0.3867 - acc: 0.9032 - val_loss: 0.2878 - val_f_2: 0.4229 - val_acc: 0.9029\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2818 - f_2: 0.3878 - acc: 0.9029 - val_loss: 0.2871 - val_f_2: 0.4480 - val_acc: 0.9023\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2809 - f_2: 0.3939 - acc: 0.9040 - val_loss: 0.2888 - val_f_2: 0.4545 - val_acc: 0.9019\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2819 - f_2: 0.3835 - acc: 0.9022 - val_loss: 0.2875 - val_f_2: 0.4571 - val_acc: 0.9003\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2810 - f_2: 0.3984 - acc: 0.9054 - val_loss: 0.2921 - val_f_2: 0.4519 - val_acc: 0.8995\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2824 - f_2: 0.3765 - acc: 0.9042 - val_loss: 0.2879 - val_f_2: 0.4481 - val_acc: 0.9011\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2806 - f_2: 0.3931 - acc: 0.9050 - val_loss: 0.2890 - val_f_2: 0.4111 - val_acc: 0.9009\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2814 - f_2: 0.3913 - acc: 0.9034 - val_loss: 0.2887 - val_f_2: 0.3948 - val_acc: 0.9001\n"
     ]
    }
   ],
   "source": [
    "# Wrap model in wrapper\n",
    "classifier = KerasClassifier(build_fn=build_model_2_rms)\n",
    "    \n",
    "# Define grid (scores used are the same as the scores used in GSCV for the SVC benchmark model)\n",
    "mlp2_rms_grid = GridSearchCV(estimator=classifier, \n",
    "                    param_grid=parameters, \n",
    "                    scoring=scores, \n",
    "                    refit='F2-Score', verbose=2)\n",
    "\n",
    "# Train grid, passing fit_params of keras.models.Sequential.fit here\n",
    "mlp2_rms_grid_result = mlp2_rms_grid.fit(X_train_red, y_train, epochs=50, \n",
    "                                 verbose=2, shuffle=True, \n",
    "                                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance: F-2 Score of 0.45 with parameters: {'batch_size': 40, 'dropout': 0.2, 'nodes': 10, 'penalty': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Observe best performance\n",
    "print('Best performance: F-2 Score of {:.2f} with parameters: {}'.format(mlp2_rms_grid_result.best_score_, mlp2_rms_grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the performance of the second model with RMSprop as optimizer was much better. Therefore, I will be using the parameters obtained during that GSCV attempt. \n",
    "\n",
    "In summary, then, the result of GSCV for the first model is:\n",
    "\n",
    "    Best performance: F-2 Score of 0.43 with parameters: {'batch_size': 40, 'dropout': 0.3, 'nodes': 15, 'penalty': 0.001}\n",
    "    \n",
    "For the second model, it is:\n",
    "\n",
    "    Best performance: F-2 Score of 0.45 with parameters: {'batch_size': 40, 'dropout': 0.2, 'nodes': 10, 'penalty': 0.001}\n",
    "    \n",
    "We can now design and train both models with those parameters, and evaluate their performance against the testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20336 samples, validate on 5085 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4063 - f_2: 0.0098 - acc: 0.8659 - val_loss: 0.3449 - val_f_2: 9.8331e-04 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34491, saving model to weights/model_1.best.hdf5\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3425 - f_2: 0.0039 - acc: 0.8726 - val_loss: 0.3290 - val_f_2: 9.8331e-04 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34491 to 0.32904, saving model to weights/model_1.best.hdf5\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3309 - f_2: 0.0881 - acc: 0.8813 - val_loss: 0.3214 - val_f_2: 0.2847 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32904 to 0.32139, saving model to weights/model_1.best.hdf5\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3222 - f_2: 0.2539 - acc: 0.8960 - val_loss: 0.3164 - val_f_2: 0.3219 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32139 to 0.31643, saving model to weights/model_1.best.hdf5\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3180 - f_2: 0.2763 - acc: 0.8981 - val_loss: 0.3146 - val_f_2: 0.3541 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31643 to 0.31465, saving model to weights/model_1.best.hdf5\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3136 - f_2: 0.2961 - acc: 0.8987 - val_loss: 0.3125 - val_f_2: 0.3467 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.31465 to 0.31254, saving model to weights/model_1.best.hdf5\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3109 - f_2: 0.3152 - acc: 0.8989 - val_loss: 0.3105 - val_f_2: 0.3475 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.31254 to 0.31045, saving model to weights/model_1.best.hdf5\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.3086 - f_2: 0.3246 - acc: 0.9018 - val_loss: 0.3090 - val_f_2: 0.3920 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.31045 to 0.30905, saving model to weights/model_1.best.hdf5\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.3066 - f_2: 0.3230 - acc: 0.8996 - val_loss: 0.3074 - val_f_2: 0.3795 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.30905 to 0.30738, saving model to weights/model_1.best.hdf5\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.3045 - f_2: 0.3311 - acc: 0.9004 - val_loss: 0.3058 - val_f_2: 0.3931 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.30738 to 0.30583, saving model to weights/model_1.best.hdf5\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.3016 - f_2: 0.3433 - acc: 0.9019 - val_loss: 0.3069 - val_f_2: 0.3543 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.30583\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.3023 - f_2: 0.3331 - acc: 0.9002 - val_loss: 0.3033 - val_f_2: 0.3779 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.30583 to 0.30328, saving model to weights/model_1.best.hdf5\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3011 - f_2: 0.3333 - acc: 0.9000 - val_loss: 0.3058 - val_f_2: 0.3618 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.30328\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.2974 - f_2: 0.3359 - acc: 0.9010 - val_loss: 0.3015 - val_f_2: 0.3610 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.30328 to 0.30155, saving model to weights/model_1.best.hdf5\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.2970 - f_2: 0.3503 - acc: 0.9013 - val_loss: 0.3009 - val_f_2: 0.3708 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.30155 to 0.30091, saving model to weights/model_1.best.hdf5\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.2952 - f_2: 0.3462 - acc: 0.9020 - val_loss: 0.2997 - val_f_2: 0.3732 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.30091 to 0.29971, saving model to weights/model_1.best.hdf5\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.2954 - f_2: 0.3436 - acc: 0.9019 - val_loss: 0.3017 - val_f_2: 0.3578 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.29971\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.2938 - f_2: 0.3568 - acc: 0.9017 - val_loss: 0.2988 - val_f_2: 0.3693 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.29971 to 0.29880, saving model to weights/model_1.best.hdf5\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.2938 - f_2: 0.3573 - acc: 0.9015 - val_loss: 0.3015 - val_f_2: 0.3940 - val_acc: 0.9007\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.29880\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.2927 - f_2: 0.3425 - acc: 0.9016 - val_loss: 0.3017 - val_f_2: 0.3942 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.29880\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.2909 - f_2: 0.3508 - acc: 0.9026 - val_loss: 0.2996 - val_f_2: 0.4029 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.29880\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.2908 - f_2: 0.3567 - acc: 0.9024 - val_loss: 0.3008 - val_f_2: 0.3944 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.29880\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.2893 - f_2: 0.3790 - acc: 0.9060 - val_loss: 0.2972 - val_f_2: 0.4024 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.29880 to 0.29720, saving model to weights/model_1.best.hdf5\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.2901 - f_2: 0.3744 - acc: 0.9044 - val_loss: 0.3040 - val_f_2: 0.3877 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.29720\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.2872 - f_2: 0.3979 - acc: 0.9068 - val_loss: 0.3022 - val_f_2: 0.3800 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.29720\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.2876 - f_2: 0.3972 - acc: 0.9061 - val_loss: 0.3017 - val_f_2: 0.4038 - val_acc: 0.9042\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.29720\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2905 - f_2: 0.3894 - acc: 0.9034 - val_loss: 0.2973 - val_f_2: 0.4138 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.29720\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2875 - f_2: 0.3886 - acc: 0.9035 - val_loss: 0.2993 - val_f_2: 0.4197 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.29720\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2865 - f_2: 0.3824 - acc: 0.9055 - val_loss: 0.2990 - val_f_2: 0.4083 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.29720\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2866 - f_2: 0.3914 - acc: 0.9061 - val_loss: 0.3000 - val_f_2: 0.3605 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.29720\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2866 - f_2: 0.3906 - acc: 0.9053 - val_loss: 0.3017 - val_f_2: 0.3996 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.29720\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2848 - f_2: 0.4013 - acc: 0.9061 - val_loss: 0.2982 - val_f_2: 0.4378 - val_acc: 0.9048\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.29720\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2852 - f_2: 0.4020 - acc: 0.9067 - val_loss: 0.2985 - val_f_2: 0.3955 - val_acc: 0.9009\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.29720\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2844 - f_2: 0.4015 - acc: 0.9066 - val_loss: 0.2969 - val_f_2: 0.3943 - val_acc: 0.9034\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.29720 to 0.29686, saving model to weights/model_1.best.hdf5\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2844 - f_2: 0.4061 - acc: 0.9076 - val_loss: 0.2980 - val_f_2: 0.3883 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.29686\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2846 - f_2: 0.3943 - acc: 0.9064 - val_loss: 0.2983 - val_f_2: 0.4065 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.29686\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2831 - f_2: 0.4100 - acc: 0.9079 - val_loss: 0.2986 - val_f_2: 0.4397 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.29686\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2847 - f_2: 0.4123 - acc: 0.9070 - val_loss: 0.2976 - val_f_2: 0.3775 - val_acc: 0.9034\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.29686\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2833 - f_2: 0.4092 - acc: 0.9070 - val_loss: 0.3027 - val_f_2: 0.3696 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.29686\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2823 - f_2: 0.4049 - acc: 0.9060 - val_loss: 0.3122 - val_f_2: 0.3747 - val_acc: 0.9042\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.29686\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2837 - f_2: 0.4088 - acc: 0.9072 - val_loss: 0.3013 - val_f_2: 0.4593 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.29686\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2831 - f_2: 0.3999 - acc: 0.9067 - val_loss: 0.3011 - val_f_2: 0.4357 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.29686\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2828 - f_2: 0.4114 - acc: 0.9072 - val_loss: 0.2984 - val_f_2: 0.4334 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.29686\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2813 - f_2: 0.4182 - acc: 0.9087 - val_loss: 0.2999 - val_f_2: 0.3823 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.29686\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2826 - f_2: 0.4038 - acc: 0.9071 - val_loss: 0.2997 - val_f_2: 0.4120 - val_acc: 0.9030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00045: val_loss did not improve from 0.29686\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2825 - f_2: 0.4100 - acc: 0.9069 - val_loss: 0.2957 - val_f_2: 0.3916 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.29686 to 0.29567, saving model to weights/model_1.best.hdf5\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2817 - f_2: 0.4090 - acc: 0.9066 - val_loss: 0.2973 - val_f_2: 0.4453 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.29567\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2812 - f_2: 0.4156 - acc: 0.9075 - val_loss: 0.2967 - val_f_2: 0.4087 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.29567\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2822 - f_2: 0.4155 - acc: 0.9074 - val_loss: 0.2981 - val_f_2: 0.4172 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.29567\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2823 - f_2: 0.4020 - acc: 0.9067 - val_loss: 0.2998 - val_f_2: 0.4586 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.29567\n",
      "6356/6356 [==============================] - 0s 71us/step\n",
      "{'loss': 0.294882823454651, 'f_2': 0.36710614580236328, 'acc': 0.90387035864115206}\n"
     ]
    }
   ],
   "source": [
    "model_1 = build_model_1(nodes=15, penalty=0.001, dropout=0.3)\n",
    "model_1.name = 'model_1'\n",
    "\n",
    "# Create checkpointer and filepath\n",
    "filepath = 'weights/' + str(model_1.name) + '.best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_weights_only=True, save_best_only=True, verbose=1)\n",
    "\n",
    "# Train model\n",
    "hist = model_1.fit(X_train_red, y_train, batch_size=40, epochs=50,\n",
    "                   validation_split=0.2, shuffle=True,\n",
    "                   callbacks=[checkpoint], verbose=2)\n",
    "\n",
    "# Load best weights\n",
    "model_1.load_weights(filepath)\n",
    "\n",
    "# Observe performance\n",
    "names = model_1.metrics_names\n",
    "scores = model_1.evaluate(X_test_red, y_test)\n",
    "score_dict = dict((name, score) for name, score in zip(names, scores))\n",
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20336 samples, validate on 5085 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.025). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4044 - f_2: 0.0573 - acc: 0.8676 - val_loss: 0.3360 - val_f_2: 0.2548 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33604, saving model to weights/model_2.best.hdf5\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.3365 - f_2: 0.2378 - acc: 0.8953 - val_loss: 0.3102 - val_f_2: 0.3252 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33604 to 0.31020, saving model to weights/model_2.best.hdf5\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3201 - f_2: 0.2850 - acc: 0.8976 - val_loss: 0.3042 - val_f_2: 0.3197 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31020 to 0.30424, saving model to weights/model_2.best.hdf5\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.3128 - f_2: 0.2963 - acc: 0.8985 - val_loss: 0.2990 - val_f_2: 0.3572 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30424 to 0.29897, saving model to weights/model_2.best.hdf5\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.3095 - f_2: 0.3158 - acc: 0.9010 - val_loss: 0.2980 - val_f_2: 0.3243 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.29897 to 0.29797, saving model to weights/model_2.best.hdf5\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.3053 - f_2: 0.3259 - acc: 0.9006 - val_loss: 0.2958 - val_f_2: 0.3183 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29797 to 0.29576, saving model to weights/model_2.best.hdf5\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.3041 - f_2: 0.3217 - acc: 0.9007 - val_loss: 0.2932 - val_f_2: 0.3371 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.29576 to 0.29322, saving model to weights/model_2.best.hdf5\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.2991 - f_2: 0.3258 - acc: 0.9011 - val_loss: 0.2928 - val_f_2: 0.3844 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.29322 to 0.29275, saving model to weights/model_2.best.hdf5\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.2979 - f_2: 0.3363 - acc: 0.8994 - val_loss: 0.2951 - val_f_2: 0.3510 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.29275\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.2974 - f_2: 0.3166 - acc: 0.9006 - val_loss: 0.2912 - val_f_2: 0.3488 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.29275 to 0.29116, saving model to weights/model_2.best.hdf5\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.2977 - f_2: 0.3314 - acc: 0.9012 - val_loss: 0.2904 - val_f_2: 0.3620 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.29116 to 0.29041, saving model to weights/model_2.best.hdf5\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.2943 - f_2: 0.3486 - acc: 0.9018 - val_loss: 0.2891 - val_f_2: 0.3814 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.29041 to 0.28906, saving model to weights/model_2.best.hdf5\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.2927 - f_2: 0.3542 - acc: 0.9022 - val_loss: 0.2898 - val_f_2: 0.3620 - val_acc: 0.9005\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.28906\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.2924 - f_2: 0.3519 - acc: 0.9024 - val_loss: 0.2908 - val_f_2: 0.3556 - val_acc: 0.9011\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.28906\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.2925 - f_2: 0.3428 - acc: 0.9023 - val_loss: 0.2885 - val_f_2: 0.3867 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.28906 to 0.28854, saving model to weights/model_2.best.hdf5\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.2908 - f_2: 0.3616 - acc: 0.9031 - val_loss: 0.2878 - val_f_2: 0.3867 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.28854 to 0.28775, saving model to weights/model_2.best.hdf5\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.2897 - f_2: 0.3541 - acc: 0.9020 - val_loss: 0.2907 - val_f_2: 0.3955 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.28775\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.2875 - f_2: 0.3617 - acc: 0.9022 - val_loss: 0.2881 - val_f_2: 0.4022 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.28775\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.2881 - f_2: 0.3568 - acc: 0.9019 - val_loss: 0.2898 - val_f_2: 0.4091 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.28775\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.2867 - f_2: 0.3574 - acc: 0.9021 - val_loss: 0.2881 - val_f_2: 0.3904 - val_acc: 0.9007\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.28775\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.2868 - f_2: 0.3753 - acc: 0.9026 - val_loss: 0.2913 - val_f_2: 0.3175 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.28775\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.2865 - f_2: 0.3624 - acc: 0.9029 - val_loss: 0.2889 - val_f_2: 0.4009 - val_acc: 0.9009\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.28775\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.2866 - f_2: 0.3631 - acc: 0.9027 - val_loss: 0.2886 - val_f_2: 0.4012 - val_acc: 0.9011\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.28775\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.2866 - f_2: 0.3764 - acc: 0.9031 - val_loss: 0.2874 - val_f_2: 0.3605 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.28775 to 0.28740, saving model to weights/model_2.best.hdf5\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.2863 - f_2: 0.3759 - acc: 0.9029 - val_loss: 0.2882 - val_f_2: 0.4086 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.28740\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.2856 - f_2: 0.3792 - acc: 0.9028 - val_loss: 0.2890 - val_f_2: 0.4299 - val_acc: 0.9011\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.28740\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2844 - f_2: 0.3687 - acc: 0.9022 - val_loss: 0.2894 - val_f_2: 0.3926 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.28740\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2840 - f_2: 0.3793 - acc: 0.9029 - val_loss: 0.2869 - val_f_2: 0.4224 - val_acc: 0.9009\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.28740 to 0.28692, saving model to weights/model_2.best.hdf5\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2842 - f_2: 0.3855 - acc: 0.9032 - val_loss: 0.2881 - val_f_2: 0.4297 - val_acc: 0.9001\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.28692\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2855 - f_2: 0.3677 - acc: 0.9016 - val_loss: 0.2888 - val_f_2: 0.3414 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.28692\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2828 - f_2: 0.3775 - acc: 0.9044 - val_loss: 0.2878 - val_f_2: 0.3926 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.28692\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2830 - f_2: 0.3774 - acc: 0.9020 - val_loss: 0.2887 - val_f_2: 0.4184 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.28692\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2815 - f_2: 0.3992 - acc: 0.9048 - val_loss: 0.2881 - val_f_2: 0.4227 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.28692\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2818 - f_2: 0.3911 - acc: 0.9048 - val_loss: 0.2880 - val_f_2: 0.3916 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.28692\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2831 - f_2: 0.3795 - acc: 0.9021 - val_loss: 0.2894 - val_f_2: 0.4163 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.28692\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2812 - f_2: 0.3887 - acc: 0.9043 - val_loss: 0.2892 - val_f_2: 0.4298 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.28692\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2818 - f_2: 0.3788 - acc: 0.9036 - val_loss: 0.2885 - val_f_2: 0.4484 - val_acc: 0.9005\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.28692\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2829 - f_2: 0.3850 - acc: 0.9044 - val_loss: 0.2888 - val_f_2: 0.4271 - val_acc: 0.9034\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.28692\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2810 - f_2: 0.3984 - acc: 0.9045 - val_loss: 0.2864 - val_f_2: 0.3817 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.28692 to 0.28635, saving model to weights/model_2.best.hdf5\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2824 - f_2: 0.3799 - acc: 0.9035 - val_loss: 0.2893 - val_f_2: 0.3467 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.28635\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2795 - f_2: 0.3956 - acc: 0.9041 - val_loss: 0.2870 - val_f_2: 0.3430 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.28635\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2813 - f_2: 0.3737 - acc: 0.9025 - val_loss: 0.2884 - val_f_2: 0.4022 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.28635\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2803 - f_2: 0.3895 - acc: 0.9046 - val_loss: 0.2896 - val_f_2: 0.3806 - val_acc: 0.8997\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.28635\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2813 - f_2: 0.3775 - acc: 0.9022 - val_loss: 0.2898 - val_f_2: 0.4204 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.28635\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2783 - f_2: 0.3937 - acc: 0.9044 - val_loss: 0.2876 - val_f_2: 0.4356 - val_acc: 0.9007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00045: val_loss did not improve from 0.28635\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2793 - f_2: 0.3926 - acc: 0.9049 - val_loss: 0.2919 - val_f_2: 0.4413 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.28635\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2805 - f_2: 0.3861 - acc: 0.9020 - val_loss: 0.2883 - val_f_2: 0.4457 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.28635\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2801 - f_2: 0.3871 - acc: 0.9038 - val_loss: 0.2904 - val_f_2: 0.4364 - val_acc: 0.9011\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.28635\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2783 - f_2: 0.3969 - acc: 0.9052 - val_loss: 0.2895 - val_f_2: 0.4420 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.28635\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2789 - f_2: 0.4013 - acc: 0.9042 - val_loss: 0.2885 - val_f_2: 0.3647 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.28635\n",
      "6356/6356 [==============================] - 0s 71us/step\n",
      "{'loss': 0.28760447846860387, 'f_2': 0.38095998929706354, 'acc': 0.90497168026456598}\n"
     ]
    }
   ],
   "source": [
    "model_2 = build_model_2_rms(nodes=10, penalty=0.001, dropout=0.2)\n",
    "model_2.name = 'model_2'\n",
    "\n",
    "# Create checkpointer and filepath\n",
    "filepath = 'weights/' + str(model_2.name) + '.best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_weights_only=True, save_best_only=True, verbose=1)\n",
    "\n",
    "# Train model\n",
    "hist = model_2.fit(X_train_red, y_train, batch_size=40, epochs=50,\n",
    "                   validation_split=0.2, shuffle=True,\n",
    "                   callbacks=[checkpoint], verbose=2)\n",
    "\n",
    "# Load best weights\n",
    "model_2.load_weights(filepath)\n",
    "\n",
    "# Observe performance\n",
    "names = model_2.metrics_names\n",
    "scores = model_2.evaluate(X_test_red, y_test)\n",
    "score_dict = dict((name, score) for name, score in zip(names, scores))\n",
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can conclude that the final MLP for this project is Model 2. Its loss on the testing set is lower, and its accuracy and F-2 scores are higher. Thus, we have accomplished this project's goal of devising an MLP to predict the success rate of cold calling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the pre- and post-GSCV performances of Model 1 and Model 2, as well as the performances of Model 1 and Model 2 post-GCSV, we will define a simple method to compare the variation for each metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scores(final, test):\n",
    "    assert len(final) == len(test)\n",
    "    delta = []\n",
    "    for final_score, test_score in zip(final, test):\n",
    "        delta.append(((final_score * 100) / test_score) - 100)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before and After GSCV: Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage changes in performance are:\n",
      " {'loss': 1.3884842200512537, 'f_2': -7.208297487894981, 'acc': 0.05224660397507819}\n"
     ]
    }
   ],
   "source": [
    "# Scores are as follows: ['loss', 'f_2', 'accuracy']\n",
    "\n",
    "# Score obtained from Architecture Exploration notebook\n",
    "before = [0.29084449355672787, 0.3956238929385663, 0.9033983636757651]\n",
    "\n",
    "# Score obtained from above\n",
    "after = [0.294882823454651, 0.36710614580236328, 0.90387035864115206]\n",
    "\n",
    "# Comparison\n",
    "delta = compare_scores(after, before)\n",
    "print('The percentage changes in performance are:\\n', dict((name, score) for name, score in zip(names, delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before and After GSCV: Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage changes in performance are:\n",
      " {'loss': 1.6693065655415324, 'f_2': -0.6889305020831813, 'acc': 0.03478260869708549}\n"
     ]
    }
   ],
   "source": [
    "# Scores are as follows: ['loss', 'f_2', 'accuracy']\n",
    "\n",
    "# Score obtained from Architecture Exploration notebook\n",
    "before = [0.2828823055689855, 0.38360274561845764, 0.904657016954308]\n",
    "\n",
    "# Score obtained from above\n",
    "after = [0.28760447846860387, 0.38095998929706354, 0.90497168026456598]\n",
    "\n",
    "# Comparison\n",
    "delta = compare_scores(after, before)\n",
    "print('The percentage changes in performance are:\\n', dict((name, score) for name, score in zip(names, delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 vs Model 2: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage changes in performance are:\n",
      " {'loss': -2.4682159851763714, 'f_2': 3.773797756619061, 'acc': 0.1218450868407217}\n"
     ]
    }
   ],
   "source": [
    "# Scores are as follows: ['loss', 'f_2', 'accuracy']\n",
    "\n",
    "# Score obtained from Architecture Exploration notebook\n",
    "model_1 = [0.294882823454651, 0.36710614580236328, 0.90387035864115206]\n",
    "\n",
    "# Score obtained from above\n",
    "model_2 = [0.28760447846860387, 0.38095998929706354, 0.90497168026456598]\n",
    "\n",
    "# Comparison\n",
    "delta = compare_scores(model_2, model_1)\n",
    "print('The percentage changes in performance are:\\n', dict((name, score) for name, score in zip(names, delta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
