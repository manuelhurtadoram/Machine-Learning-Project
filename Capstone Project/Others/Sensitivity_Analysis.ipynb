{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis\n",
    "\n",
    "This is a notebook dedicated specifically to testing four different alterations of the bank dataset in order to examine the robustness of the best model obtained in the Capstone Notebook. The alterations consist of increasing and decreasing the values of the numeric features by one and two standard deviations. All the other preprocessing steps are held constant. The best model is asked to predict on these datasets, and its performance is evaluated to analyze its sensitivity. \n",
    "\n",
    "To do so, we will design a pipeline that will alter the data, preprocess it to make it compatible with the best model, and evaluate its performance. \n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing steps below are copied verbatim from the Capstone notebook. Only the name of the Data Frame variable has been changed from 'df' to 'data', to avoid confusions when feeding it through the pipeline designed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('bank-full.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'duration' column from dataset\n",
    "data = data.drop('duration', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numerical and categorical labels\n",
    "numerical = ['age', 'balance', 'campaign', 'pdays', 'previous']\n",
    "categorical = ['job', 'day', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mapping function\n",
    "def day2week_mapper(day):\n",
    "    if 1 <= day and day <= 7:\n",
    "        return 1\n",
    "    elif 8 <= day and day <= 14:\n",
    "        return 2\n",
    "    elif 15 <= day and day <= 21:\n",
    "        return 3\n",
    "    elif 22 <= day and day <= 31:\n",
    "        return 4\n",
    "    else: return 0 #unnecessary but it closes the if-statement\n",
    "\n",
    "# apply mapping   \n",
    "data['day'] = data['day'].apply(day2week_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply func\n",
    "data[categorical] = data[categorical].apply(lambda x: x.astype('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract 1 to all the values in the 'campaign' column\n",
    "data['campaign'] = data['campaign'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to count outliers in a series \n",
    "def get_outliers(feature, k):\n",
    "    feature_outliers = pd.DataFrame()\n",
    "    Q3 = data[feature].quantile(0.75)\n",
    "    Q1 = data[feature].quantile(0.25)\n",
    "    iqr = Q3 - Q1\n",
    "    for i in range(data.index.size):\n",
    "        client = data.loc[i]\n",
    "        if client[feature] < Q1 - k * iqr or client[feature] > Q3 + k * iqr:\n",
    "            feature_outliers = feature_outliers.append(client)\n",
    "    return feature_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14804 outliers in the data\n"
     ]
    }
   ],
   "source": [
    "# Select all outliers in the dataframe and drop duplicate rows. \n",
    "outliers = pd.DataFrame()\n",
    "\n",
    "for feature in numerical:\n",
    "    outliers = outliers.append(get_outliers(feature, 1.5))\n",
    "    \n",
    "outliers = outliers.drop_duplicates()\n",
    "\n",
    "# Number of unique outliers in the data\n",
    "print(outliers.shape[0], 'outliers in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 2684, 'no': 27723}\n",
      "New ratio of \"yes\" to \"no\" is: 1:10.3\n"
     ]
    }
   ],
   "source": [
    "# Calculate and report new value counts of target variable after having dropped outliers\n",
    "val_counts_dropped = {'yes': data.y.value_counts()[1] - outliers.y.value_counts()[1], \n",
    "                      'no': data.y.value_counts()[0] - outliers.y.value_counts()[0]}\n",
    "\n",
    "print(val_counts_dropped)\n",
    "print('New ratio of \"yes\" to \"no\" is:', '1:{:.1f}'.format(val_counts_dropped['no'] / val_counts_dropped['yes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate outliers with 'yes' and 'no' target variables\n",
    "outliers_y = outliers.loc[outliers.y == 'yes']\n",
    "outliers_n = outliers.drop(outliers_y.index.values)\n",
    "assert outliers_y.index.size == outliers.y.value_counts()[1] and outliers_n.index.size == outliers.y.value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Data Frame with only the rows that contain the target variable 'yes' and check for equality\n",
    "data_y = data.loc[data.y == 'yes']\n",
    "assert data_y.index.size == data.y.value_counts()[1]\n",
    "\n",
    "# Adaptation of method to count outliers in a series (this time using df_y instead of df)\n",
    "def get_outliers_y(feature, k):\n",
    "    feature_outliers = pd.DataFrame()\n",
    "    Q3 = data_y[feature].quantile(0.75)\n",
    "    Q1 = data_y[feature].quantile(0.25)\n",
    "    iqr = Q3 - Q1\n",
    "    for i in data_y.index.values:\n",
    "        client = data_y.loc[i]\n",
    "        if client[feature] < Q1 - k * iqr or client[feature] > Q3 + k * iqr:\n",
    "            feature_outliers = feature_outliers.append(client)\n",
    "    return feature_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235 \"yes\" outliers in the data\n"
     ]
    }
   ],
   "source": [
    "# Select all 'outliers' in df_y and drop duplicate rows. \n",
    "outliers_y = pd.DataFrame()\n",
    "\n",
    "for feature in numerical:\n",
    "    outliers_y = outliers_y.append(get_outliers_y(feature, 2))\n",
    "    \n",
    "outliers_y = outliers_y.drop_duplicates()\n",
    "\n",
    "# Number of unique outliers in the data\n",
    "print(outliers_y.shape[0], '\"yes\" outliers in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'no' outliers\n",
    "data = data.drop(outliers_n.index.values)\n",
    "\n",
    "# drop 'yes' outliers\n",
    "data = data.drop(outliers_y.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Data Frame index for better presentation from now onwards\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -1 with new value in df.pdays\n",
    "data.pdays = data.pdays.replace(to_replace=-1, value=1.5*data.pdays.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we will now alter the data and feed it to the trained model (obtained from the Capstone notebook). After altering the data, all the rest of the pre-processing steps will need to be carried out. Therefore, it would be useful to create a single pipeline that could automate all this. \n",
    "\n",
    "## Streamlining Preprocessing Sub-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Sub-steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def transform_skewed(df):\n",
    "    # load features into a dummy data frame\n",
    "    df_test = df[numerical]\n",
    "    \n",
    "    # Initiate a dictionary and possible values of lambda\n",
    "    std = {}\n",
    "    lmbda = [-2.0, -0.5, 0.0, 0.5, 1.0, 2.0]\n",
    "\n",
    "    # Iterate over numerical features and each value of lambda for each feature\n",
    "    for feature in numerical:\n",
    "        feature_std = {}\n",
    "        for i in lmbda:\n",
    "            dummy = df_test[feature]\n",
    "            if i == 0.0:\n",
    "                feature_std[str(i)] = dummy.apply(lambda x: np.log(x + 0.01)).std()\n",
    "            else:\n",
    "                feature_std[str(i)] = dummy.apply(lambda x: (x + 0.01) ** i).std()\n",
    "        std[str(feature)] = feature_std\n",
    "        \n",
    "    # Select best value of lambda for each feature\n",
    "    best_lmbda = dict((feature, min(std[feature], key=(lambda k: std[feature][k]))) for feature in numerical)\n",
    "\n",
    "    # Transform skewed features with best value of lambda:\n",
    "    for feature in numerical:\n",
    "        df[feature] = df[feature].apply(lambda x: (x + 0.01) ** float(best_lmbda[feature]))\n",
    "        \n",
    "    return df\n",
    "\n",
    "def normalize(df):\n",
    "    # Initialize a scaler, then apply it to the features\n",
    "    scaler = MinMaxScaler() # default=(0, 1)\n",
    "    df[numerical] = scaler.fit_transform(df[numerical])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def one_hot(df):\n",
    "    # Replace values in target variable\n",
    "    target = df.y.replace(to_replace=['yes', 'no'], value=[1, 0])\n",
    "\n",
    "    # Drop target variable from data set before one-hot encoding\n",
    "    features = df.drop('y', axis=1)\n",
    "\n",
    "    # One-hot encode features\n",
    "    features = pd.get_dummies(features)\n",
    "\n",
    "    return target, features\n",
    "\n",
    "def pca_red(features):\n",
    "    # Create PCA object that seeks to create 27 components\n",
    "    pca = PCA(n_components=27, random_state=42)\n",
    "    pca.fit(features)\n",
    "\n",
    "    # Transform features using PCA\n",
    "    features_red = pca.transform(features)\n",
    "    \n",
    "    return features_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automating Alteration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter numeric features\n",
    "def alter(df, std):\n",
    "    for feature in numerical:\n",
    "        df[feature] = df[feature] + std * df[feature].std()\n",
    "    return df\n",
    "\n",
    "# Complete preprocessing\n",
    "def preprocess(df):\n",
    "    df = transform_skewed(df)\n",
    "    df = normalize(df)\n",
    "    target, features = one_hot(df)\n",
    "    features_red = pca_red(features)\n",
    "    \n",
    "    return target, features_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Pipeline to Alter and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_and_predict(df, std):\n",
    "    df = alter(df, std)\n",
    "    target, features_red = preprocess(df)\n",
    "    scores = best_model.evaluate(features_red, target)\n",
    "    names = best_model.metrics_names\n",
    "    print('For a change of +' + str(std) + ' std, the performance of the best model is:\\n', \\\n",
    "           dict((name, score) for name, score in zip(names, scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "The model created below is the model that was selected as 'best' from the Capstone notebook. Its training weights are loaded below, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MHurtado/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary keras elements\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FULL CREDIT TO ARSENY KRAVCHENKO FOR THE CODE BELOW\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def f_2(y_true, y_pred, threshold_shift=0):\n",
    "    beta = 2\n",
    "\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "best_model = Sequential()\n",
    "best_model.add(Dense(units=27, input_dim=27, activation='relu'))\n",
    "best_model.add(Dense(10, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "best_model.add(Dropout(0.2)) \n",
    "best_model.add(Dense(15, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "best_model.add(Dropout(0.2)) \n",
    "best_model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "best_model.add(Dropout(0.2))\n",
    "best_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "best_model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=[f_2, 'accuracy'])\n",
    "\n",
    "# Load weights\n",
    "best_model.load_weights('weights/model_2.best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Test\n",
    "\n",
    "We will now use the pipeline constructed above to test the sensitivity of the model, shifting the values in the numerical features by 1, 2, -1, and -2 standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### +1 Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31777/31777 [==============================] - 1s 25us/step\n",
      "For a change of +1 std, the performance of the best model is:\n",
      " {'loss': 0.5914005916106693, 'f_2': 0.18203985381664878, 'acc': 0.7207099474462662}\n"
     ]
    }
   ],
   "source": [
    "alter_and_predict(data, std=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### +2 Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31777/31777 [==============================] - 1s 25us/step\n",
      "For a change of +2 std, the performance of the best model is:\n",
      " {'loss': 0.69749638551987, 'f_2': 0.1619456995886197, 'acc': 0.6749535827799981}\n"
     ]
    }
   ],
   "source": [
    "alter_and_predict(data, std=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -1 Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MHurtado/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:433: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  array = np.array(array, dtype=dtype, order=order, copy=copy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31777/31777 [==============================] - 1s 25us/step\n",
      "For a change of +-1 std, the performance of the best model is:\n",
      " {'loss': 0.5343055053000281, 'f_2': 0.18042761148223552, 'acc': 0.7548856090883344}\n"
     ]
    }
   ],
   "source": [
    "alter_and_predict(data, std=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -2 Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MHurtado/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:433: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  array = np.array(array, dtype=dtype, order=order, copy=copy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31777/31777 [==============================] - 1s 24us/step\n",
      "For a change of +-2 std, the performance of the best model is:\n",
      " {'loss': 0.516189042806464, 'f_2': 0.18851469662381398, 'acc': 0.7759071026213928}\n"
     ]
    }
   ],
   "source": [
    "alter_and_predict(data, std=-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
